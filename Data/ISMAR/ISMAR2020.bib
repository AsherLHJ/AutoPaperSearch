@INPROCEEDINGS{9284811,
  author={Wang, Lili and Li, Runze and Shi, Xuehuai and Yan, Ling-Qi and Li, Zhichao},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Foveated Instant Radiosity}, 
  year={2020},
  volume={},
  number={},
  pages={1-11},
  abstract={Foveated rendering distributes computational resources based on visual acuity, more in the foveal regions of our eyes and less in the periphery. The traditional rasterization method can be adapted into the foveated rendering framework in a quite straightforward way, but it's difficult for estimating global illumination. Instant Radiosity is an efficient global illumination method. It generates Virtual Point Lights (VPLs) on the surface of the virtual scenes from light sources and uses these VPLs to simulate light bounces. However, instant radiosity can not be adapted into the foveated rendering pipeline directly, and is too slow for virtual reality experience. What's more, instant radiosity does not consider temporal coherence, therefore it lacks temporal stability for dynamic scenes. In this paper, we propose a foveated rendering method for instant radiosity with more accurate global illumination effects in the foveal region and less accurate global illumination in the peripheral region. We define a foveated importance for each VPL, and use it to smartly distribute the VPLs to guarantee the rendering precision of the foveal region. Meanwhile, we propose a novel VPL reuse scheme, which updates only a small fraction of VPLs over frames, which ensures temporal coherence and improves time efficiency. Our method supports dynamic scenes and achieves high quality in the foveal regions at interactive frame rates.},
  keywords={Visualization;Three-dimensional displays;Pipelines;Lighting;Coherence;Rendering (computer graphics);Stability analysis;3D scene rendering;Real-time rendering;Foveated rendering;Indirect illumination},
  doi={10.1109/ISMAR50242.2020.00017},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284720,
  author={Wang, Siyuan and Pan, Junjun and Bai, Junxuan and Wang, Jinglei},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Flower Factory: A Component-based Approach for Rapid Flower Modeling}, 
  year={2020},
  volume={},
  number={},
  pages={12-23},
  abstract={The rapid 3D objects modeling provides an effective way to enrich digital content, which is one of the essential tasks in VR/AR research. Flowers are frequently utilized in real-time applications, such as video games and VR/AR scenes. Technically, a realistic flower generation using the existing 3D modeling software is complicated and time-consuming for designers. Moreover, it is difficult to create imaginary and surreal flowers, which might be more interesting and attractive for the artists and game players. In this paper, we propose a component-based framework for rapid flower modeling, called Flower Factory. The flowers are assembled by different components, e.g., petals, stamens, receptacles and leaves. The shape of these components are created using simple primitives such as points and splines. After the shape of models are determined, the textures are synthesized automatically based on a predefine mask, according to a number of rules from real flowers. The whole modeling process can be controlled by several parameters, which describe the physical attributes of the flowers. Our technique is capable of producing a variety of flowers rapidly. Even novices without any modeling skills are able to control and model the 3D flowers. Furthermore, the developed system will be integrated in a lightweight application of smartphone due to its low computational cost.},
  keywords={Geometry;Solid modeling;Three-dimensional displays;Shape;Computational modeling;Games;Production facilities;Procedural modeling;geometric modeling;3D flower;component;texture synthesis;Computing methodologies;Computer graphics;Shape modeling;Parametric curve and surface models;Computing methodologies;Computer graphics;Image manipulation;Texturing},
  doi={10.1109/ISMAR50242.2020.00019},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284667,
  author={Bhattacharya, Uttaran and Rewkowski, Nicholas and Guhan, Pooja and Williams, Niall L. and Mittal, Trisha and Bera, Aniket and Manocha, Dinesh},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Generating Emotive Gaits for Virtual Agents Using Affect-Based Autoregression}, 
  year={2020},
  volume={},
  number={},
  pages={24-35},
  abstract={We present a novel autoregression network to generate virtual agents that convey various emotions through their walking styles or gaits. Given the 3D pose sequences of a gait, our network extracts pertinent movement features and affective features from the gait. We use these features to synthesize subsequent gaits such that the virtual agents can express and transition between emotions represented as combinations of happy, sad, angry, and neutral. We incorporate multiple regularizations in the training of our network to simultaneously enforce plausible movements and noticeable emotions on the virtual agents. We also integrate our approach with an AR environment using a Microsoft HoloLens and can generate emotive gaits at interactive rates to increase the social presence. We evaluate how human observers perceive both the naturalness and the emotions from the generated gaits of the virtual agents in a web-based study. Our results indicate around 89% of the users found the naturalness of the gaits satisfactory on a five-point Likert scale, and the emotions they perceived from the virtual agents are statistically similar to the intended emotions of the virtual agents. We also use our network to augment existing gait datasets with emotive gaits and will release this augmented dataset for future research in emotion prediction and emotive gait synthesis. Our project website is available at https://gamma.umd.edu/gen-emotive-gaits/.},
  keywords={Training;Legged locomotion;Three-dimensional displays;Observers;Feature extraction;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Mixed / augmented reality;Computing methodologies;Machine learning;Machine learning approaches;Neural networks},
  doi={10.1109/ISMAR50242.2020.00020},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284812,
  author={Zhang, Songhai and Li, Xiangli and Liu, Yingtian and Fu, Hongbo},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Scale-aware Insertion of Virtual Objects in Monocular Videos}, 
  year={2020},
  volume={},
  number={},
  pages={36-44},
  abstract={In this paper, we propose a scale-aware method for inserting virtual objects with proper sizes into monocular videos. To tackle the scale ambiguity problem of geometry recovery from monocular videos, we estimate the global scale objects in a video with a Bayesian approach incorporating the size priors of objects, where the scene objects sizes should strictly conform to the same global scale and the possibilities of global scales are maximized according to the size distribution of object categories. To do so, we propose a dataset of sizes of object categories: Metric-Tree, a hierarchical representation of sizes of more than 900 object categories with the corresponding images. To handle the incompleteness of objects recovered from videos, we propose a novel scale estimation method that extracts plausible dimensions of objects for scale optimization. Experiments have shown that our method for scale estimation performs better than the state-of-the-art methods, and has considerable validity and robustness for different video scenes. Metric-Tree has been made available at: https://metric-tree.github.io},
  keywords={Geometry;Estimation;Robustness;Bayes methods;Optimization;Augmented reality;Videos;Computing methodologies;Computer graphics;Graphics systems and interfaces},
  doi={10.1109/ISMAR50242.2020.00022},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284725,
  author={Giraldo, Gabriel and Servières, Myriam and Moreau, Guillaume},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perception of Multisensory Wind Representation in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={45-53},
  abstract={The set of physical and sensitive phenomena that interacts with the urban morphology acts on the resulting perception from the users of a place. Its study and representation provides elements beyond the aesthetics aspects that can allow a better understanding of the space and future urban projects. We aim to analyze the effects of three different wind representations in terms of perception and sense of presence in virtual reality (VR). We focus on the following conditions: (R) reference scene with the audiovisual representation of the mechanical effects of the wind on the elements of the context, (V) reference scene plus the visualization of the wind flow, present (among others) in the architecture field, (T) reference scene plus tactile restitution of wind and eventually (V+T) assembling all previous conditions. For the experiment, we present to the participants (R), then (V) followed by (T), and finish with (V+T). 37 participants evaluated 12 different stop points (divided into four routes in the same simulated street), where they had to determine the perceived wind force and direction concerning the four different conditions (each one corresponding to one route). At the end of each route, participants evaluated their sense of presence in the VR scene. Our analysis showed significant effects of tactile restitution over the visual effects used in the study, both for understanding wind properties and for increasing the sense of presence in the VR scene. In terms of wind direction, (T) reduced the estimation error by 27% compared to (V). Concerning wind force, the reduction was 9.8%. As far as presence was concerned, (T) increased the sense of presence by 12.2% compared to (V).},
  keywords={Solid modeling;Fans;Three-dimensional displays;Force;Morphology;Computer architecture;Visual effects;Wind Perception;Wind Representation;Virtual Reality;User Experiment;Sense of Presence;CFD Visualization;Urban 3D model;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR50242.2020.00024},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284776,
  author={Eubanks, James Coleman and Moore, Alec G. and Fishwick, Paul A. and McMahan, Ryan P.},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Body Tracking Fidelity on Embodiment of an Inverse-Kinematic Avatar for Male Participants}, 
  year={2020},
  volume={},
  number={},
  pages={54-63},
  abstract={Many research studies have investigated avatar embodiment and its effects on self-location, agency, and body ownership. Researchers have also investigated the effects of various external stimuli and avatar appearances during embodiment. However, the effects of body tracking fidelity while embodying an inverse-kinematic avatar are relatively unexplored. In this paper, we present two studies using a set of six trackers that investigate four levels of body tracking fidelity during avatar embodiment for male participants only: Complete (head, hands, feet, and pelvis trackers), Head-and-Extremities (head, hands, and feet trackers), Head-and-Hands (head and hands trackers), and No-Avatar (head and hands trackers; only controllers visible). Our results indicate that tracking the head, hands, and feet significantly increases the sense of embodiment and the sense of spatial presence when embodying an inverse-kinematic avatar for male participants.},
  keywords={Avatars;Design methodology;External stimuli;Pelvis;Augmented reality;Embodiment;virtual reality;body tracking fidelity;avatars. Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality; Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Embodiment, virtual reality, body tracking fidelity, avatars. Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR50242.2020.00025},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284799,
  author={Do, Tiffany D. and LaViola, Joseph J. and McMahan, Ryan P.},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Object Shape, Fidelity, Color, and Luminance on Depth Perception in Handheld Mobile Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={64-72},
  abstract={Depth perception of objects can greatly affect a user's experience of an augmented reality (AR) application. Many AR applications require depth matching of real and virtual objects and have the possibility to be influenced by depth cues. Color and luminance are depth cues that have been traditionally studied in two-dimensional (2D) objects. However, there is little research investigating how the properties of three-dimensional (3D) virtual objects interact with color and luminance to affect depth perception, despite the substantial use of 3D objects in visual applications. In this paper, we present the results of a paired comparison experiment that investigates the effects of object shape, fidelity, color, and luminance on depth perception of 3D objects in handheld mobile AR. The results of our study indicate that bright colors are perceived as nearer than dark colors for a high-fidelity, simple 3D object, regardless of hue. Additionally, bright red is perceived as nearer than any other color. These effects were not observed for a low-fidelity version of the simple object or for a more-complex 3D object. High-fidelity objects had more perceptual differences than low-fidelity objects, indicating that fidelity interacts with color and luminance to affect depth perception. These findings reveal how the properties of 3D models influence the effects of color and luminance on depth perception in handheld mobile AR and can help developers select colors for their applications.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Shape;Two dimensional displays;Color;Augmented reality;Depth perception;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR50242.2020.00026},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284675,
  author={Tong, Jonathan and Allison, Robert S. and Wilcox, Laurie M.},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Optical distortions in VR bias the perceived slant of moving surfaces}, 
  year={2020},
  volume={},
  number={},
  pages={73-79},
  abstract={The magnifying optics of virtual reality (VR) head-mounted displays (HMD) often cause undesirable pincushion distortion in the displayed imagery. Eccentrically increasing magnification radially displaces image-points away from the optical axis, causing straight lines to curve outwards. This, in turn, should affect the 3D perception of surface shape by warping binocular and monocular depth cues. Previous research has shown that distortion-induced biases in perceived slant do occur in static images. However, most use cases in VR involve moving images. Here we evaluate the impact of motion on biases in perceived slant. An HMD was used to present flat, textured surfaces that varied in slant and were either stationary, or translated laterally by the observer. In separate studies we varied the degree of distortion and evaluated the impact on perceived slant at several locations along the surface. We found that, irrespective of whether the surface was moving or stationary, distortion introduced significant bias into local slant estimates. The pattern of results is consistent with the surface appearing to be concave (as if viewing the inside surface of a bowl), as predicted from the warping of binocular and monocular cues. Importantly, the intermediate distortion level produced the same, but weaker, pattern of biases seen in the fully-distorted condition. When an appropriate level of pre-warping was applied, slant perception was veridical. Overall, our results highlight the importance of sufficiently correcting for optical distortions in VR HMDs to enable veridical perception of surface attitude.},
  keywords={Visualization;Three-dimensional displays;Optical distortion;Resists;Distortion;Predistortion;Lenses;Human-centered computing;Virtual reality;Human-centered computing;Empirical studies in HCI;Computing methodologies;Perception;Computing methodologies;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00027},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284699,
  author={Peillard, Etienne and Itoh, Yuta and Moreau, Guillaume and Normand, Jean-Marie and Lécuyer, Anatole and Argelaguet, Ferran},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Can Retinal Projection Displays Improve Spatial Perception in Augmented Reality?}, 
  year={2020},
  volume={},
  number={},
  pages={80-89},
  abstract={Commonly used Head Mounted Displays (HMDs) in Augmented Reality (AR), namely Optical See-Through (OST) displays, suffer from a main drawback: their focal lenses can only provide a fixed focal distance. Such a limitation is suspected to be one of the main factors for distance misperception in AR. In this paper, we studied the use of an emerging new kind of AR display to tackle such perception issues: Retinal Projection Displays (RPDs). With RPDs, virtual images have no focal distance and the AR content is always in focus. We conducted the first reported experiment evaluating egocentric distance perception of observers using Retinal Projection Displays. We compared the precision and accuracy of the depth estimation between real and virtual targets, displayed by either OST HMDs or RPDs. Interestingly, our results show that RPDs provide depth estimates in AR closer to real ones compared to OST HMDs. Indeed, the use of an OST device was found to lead to an overestimation of the perceived distance by 16%, whereas the distance overestimation bias dropped to 4% with RPDs. Besides, the task was reported with the same level of difficulty and no difference in precision. As such, our results shed the first light on retinal projection displays' benefits in terms of user's perception in Augmented Reality, suggesting that RPD is a promising technology for AR applications in which an accurate distance perception is required.},
  keywords={Performance evaluation;Estimation;Resists;Retina;Optical imaging;Task analysis;Augmented reality;Perception;Distance;Augmented Reality;User Experiment;Psychophysical Study},
  doi={10.1109/ISMAR50242.2020.00028},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284796,
  author={Dunn, David and Tursun, Okan and Yu, Hyeonseung and Didyk, Piotr and Myszkowski, Karol and Fuchs, Henry},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Stimulating the Human Visual System Beyond Real World Performance in Future Augmented Reality Displays}, 
  year={2020},
  volume={},
  number={},
  pages={90-100},
  abstract={New augmented-reality near-eye displays provide capabilities for enriching real-world visual experiences with digital content. Most current research focuses on improving both hardware and software to provide digital content that seamlessly blends with the real world. This is believed to not only contribute to the visual experience but also increase human task performance. In this work, we take a step further and ask the question of whether the capabilities of current and future display designs combined with efficient perception-inspired content optimizations can be used to improve human task performance beyond the human capabilities in the natural world. Based on an in-depth analysis of previous literature, we hypothesize here that such enhancements can be achieved when the human visual system is provided with content that optimizes the oculomotor responses. To further investigate possible gains, we present a series of perceptual experiments that built upon this idea. More specifically, we focus on speeding up accommodation response, which significantly contributes to the eye-adaptation when a new stimulus is shown. Through our experiments, we demonstrate that such speedups canbe achieved, and more importantly, they can lead to significant improvements in human task performance. While not all of our results give definite answers, we believe that they reveal plentiful opportunities for further enhancing the human experience and task performance when using new augmented-reality displays.},
  keywords={Visualization;Switches;Visual systems;Software;Sparks;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR50242.2020.00029},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284683,
  author={Takegawa, Yoshinari and Tokuda, Yutaka and Umezawa, Akino and Suzuki, Katsuhiro and Masai, Katsutoshi and Sugiura, Yuta and Sugimoto, Maki and Plasencia, Diego Martinez and Subramanian, Sriram and Hirata, Keiji},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Digital Full-Face Mask Display with Expression Recognition using Embedded Photo Reflective Sensor Arrays}, 
  year={2020},
  volume={},
  number={},
  pages={101-108},
  abstract={This paper presents a thin digital full-face mask display that can reflect an entire facial expression of a user onto an avatar to support augmented face-to-face communication in real environments. Although camera-based facial expression recognition technology has enabled people to augment their faces with avatars, application was limited to face-to-face communication in virtual environments. To enable digital facial augmentation with an avatar in a real space, we propose a digital face mask display system that integrates a lightweight flexible display with a thin facial expression recognition system. The thin wearable facial expression recognition system was implemented with photo reflective sensor arrays which can measure facial expressions at 40 feature points distributed across an entire face. We investigated a ten-class facial expression identification model based on an SVM training algorithm. The trained model achieved an average accuracy of 79% when identifying the facial expressions of multiple users. User experiments indicated that the proposed thin digital full-face mask display allows the wearer to control the facial expression of the avatar with a fast response rate and create a positive sense of self-agency and self-ownership toward the augmented avatar face.},
  keywords={Support vector machines;Training;Face recognition;Avatars;Virtual environments;Faces;Sensor arrays;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/ISMAR50242.2020.00030},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284653,
  author={Sasaki, Hisayuki and Okaichi, Naoto and Watanabe, Hayato and Omura, Takuya and Kano, Masanori and Kawakita, Masahiro},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Color Moiré Reduction and Resolution Improvement for Integral 3D Displays Using Multiple Wobbling Optics}, 
  year={2020},
  volume={},
  number={},
  pages={109-116},
  abstract={The integral three-dimensional (3D) display is an ideal visual 3D user interface. It is a display method that fulfills many of the physiological factors of human vision. However, in integral 3D displays for mobile applications that use direct-view flat panels to display elemental images, color moiré is a problem that occurs because of the sampling of subpixels by elemental lenses and the insufficient resolution and depth reproduction of the reconstructed 3D image. In the conventional moiré reduction method, the degree of defocus of elemental lenses has to be set to a large value, which is one of the factors that reduces the performance in terms of depth reproduction. In contrast, only one-step optics can be installed and the installation positions are limited in the conventional wobbling method. This is because, in the method, which uses a birefringent optical element, two-step optics are thicker than the focal length of the lens array. For this reason, it was difficult to achieve ideal moiré reduction and depth reproduction performance improvements. To solve these problems, we propose a method that utilizes multiple optical wobbling spatiotemporal multiplexing using polarization diffractive elements and liquid-crystal polarization controllers. Using the proposed method, the wobbling optics can be designed to be thin, allowing two-step optics to be installed between the display panel and lens array. When the moiré modulation degree without wobbling is normalized as 100%, it decreases to 25% with wobbling. The proposed method not only achieves effective color moiré reduction without deteriorating the 3D image quality, but also can double the resolution of the elemental images to improve the depth reproduction.},
  keywords={Three-dimensional displays;Optical polarization;Optical diffraction;Image color analysis;Optical imaging;Lenses;Optical arrays;Integral photography;3D display;color moiré;depth reproduction;wobbling optics;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers;Hardware;Communication hardware;interfaces and storage;Displays and imagers},
  doi={10.1109/ISMAR50242.2020.00031},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284794,
  author={Li, Richard and Whitmire, Eric and Stengel, Michael and Boudaoud, Ben and Kautz, Jan and Luebke, David and Patel, Shwetak and Akşit, Kaan},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors}, 
  year={2020},
  volume={},
  number={},
  pages={117-126},
  abstract={Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of 2.67° at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of 1.57° at 250 Hz using 800 mW.},
  keywords={Solid modeling;Wearable computers;Prototypes;Gaze tracking;Light emitting diodes;Photodiodes;Augmented reality;Human-centered computing;Ubiquitous and mobile computing;Ubiquitous and mobile devices;Computer systems organization;Embedded and cyber-physical systems;Sensors and actuators},
  doi={10.1109/ISMAR50242.2020.00033},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284657,
  author={Dubeau, Etienne and Garon, Mathieu and Debaque, Benoit and Charette, Raoul de and Lalonde, Jean-François},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={RGB-D-E: Event Camera Calibration for Fast 6-DOF object Tracking}, 
  year={2020},
  volume={},
  number={},
  pages={127-135},
  abstract={Augmented reality devices require multiple sensors to perform various tasks such as localization and tracking. Currently, popular cameras are mostly frame-based (e.g. RGB and Depth) which impose a high data bandwidth and power usage. With the necessity for low power and more responsive augmented reality systems, using solely frame-based sensors imposes limits to the various algorithms that needs high frequency data from the environement. As such, event-based sensors have become increasingly popular due to their low power, bandwidth and latency, as well as their very high frequency data acquisition capabilities. In this paper, we propose, for the first time, to use an event-based camera to increase the speed of 3D object tracking in 6 degrees of freedom. This application requires handling very high object speed to convey compelling AR experiences. To this end, we propose a new system which combines a recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We develop a deep learning approach, which combines an existing RGB-D network along with a novel event-based network in a cascade fashion, and demonstrate that our approach significantly improves the robustness of a state-of-the-art frame-based 6-DOF object tracker using our RGB-D-E pipeline. Our code and our RGB-D-E evaluation dataset are available at https://github.com/lvsn/rgbde-tracking.},
  keywords={Bandwidth;Cameras;Sensor systems;Sensors;Object tracking;High frequency;Augmented reality;Event camera;Calibration;6-DOF Object tracking;Augmented reality},
  doi={10.1109/ISMAR50242.2020.00034},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284700,
  author={Zhou, Lipu and Koppel, Daniel and Ju, Hul and Steinbruecker, Frank and Kaess, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Efficient Planar Bundle Adjustment Algorithm}, 
  year={2020},
  volume={},
  number={},
  pages={136-145},
  abstract={This paper presents an efficient algorithm for the least-squares problem using the point-to-plane cost, which aims to jointly optimize depth sensor poses and plane parameters for 3D reconstruction. We call this least-squares problem Planar Bundle Adjustment (PBA), due to the similarity between this problem and the original Bundle Adjustment (BA) in visual reconstruction. As planes ubiquitously exist in the man-made environment, they are generally used as landmarks in SLAM algorithms for various depth sensors. PBA is important to reduce drift and improve the quality of the map. However, directly adopting the well-established BA framework in visual reconstruction will result in a very inefficient solution for PBA. This is because a 3D point only has one observation at a camera pose. In contrast, a depth sensor can record hundreds of points in a plane at a time, which results in a very large nonlinear least-squares problem even for a small-scale space. The main contribution of this paper is an efficient solution for the PBA problem using the point-to-plane cost. We introduce a reduced Jacobian matrix and a reduced residual vector, and prove that they can replace the original Jacobian matrix and residual vector in the generally adopted Levenberg-Marquardt (LM) algorithm. This significantly reduces the computational cost. Besides, when planes are combined with other features for 3D reconstruction, the reduced Jacobian matrix and residual vector can also replace the corresponding parts derived from planes. Our experimental results show that our algorithm can significantly reduce the computational time compared to the solution using the traditional BA framework. In addition, our algorithm is faster, more accurate, and more robust to initialization errors compared to the start-of-the-art solution using the plane-to-plane cost [3].},
  keywords={Jacobian matrices;Bundle adjustment;Visualization;Simultaneous localization and mapping;Runtime;Computational efficiency;Optimization;Bundle Adjustment;Nonlinear Optimization;SLAM;Depth Sensor},
  doi={10.1109/ISMAR50242.2020.00035},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284705,
  author={Yu, Hailin and Ye, Weicai and Feng, Youji and Bao, Hujun and Zhang, Guofeng},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Learning Bipartite Graph Matching for Robust Visual Localization}, 
  year={2020},
  volume={},
  number={},
  pages={146-155},
  abstract={2D-3D matching is an essential step for visual localization, where the accuracy of the camera pose is mainly determined by the quality of 2D-3D correspondences. The matching is typically achieved by the nearest neighbor search of local features. Many existing works have shown impressive results on both the efficiency and accuracy. Recently emerged learning-based features further improve the robustness compared to the traditional hand-crafted ones. However, it is still hard to establish enough correct matches in challenging scenes with illumination changes or repetitive patterns due to the intrinsic local properties of local features. In this work, we propose a novel method to deal with 2D-3D matching in a very robust way. We first establish as many potential correct matches as possible using the local similarity. Then we construct a bipartite graph and use a deep neural network, referred to as Bipartite Graph Network (BGNet), to extract the global geometric information. The network predicts the likelihood of being an inlier for each edge and outputs the globally optimal one-to-one correspondences with a Hungarian pooling layer. The experiments show that the proposed method can find more correct matches and improves localization on both the robustness and accuracy. The results on multiple visual localization datasets are obviously better than the existing state-of-the-arts, which demonstrates the effectiveness of the proposed method.},
  keywords={Location awareness;Visualization;Feature extraction;Cameras;Robustness;Bipartite graph;Trajectory;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Mixed / augmented reality;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR50242.2020.00036},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284738,
  author={Li, Xiang and Tian, Yuan and Zhang, Fuyao and Quan, Shuxue and Xu, Yi},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Object Detection in the Context of Mobile Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={156-163},
  abstract={In the past few years, numerous Deep Neural Network (DNN) models and frameworks have been developed to tackle the problem of real-time object detection from RGB images. Ordinary object detection approaches process information from the images only, and they are oblivious to the camera pose with regard to the environment and the scale of the environment. On the other hand, mobile Augmented Reality (AR) frameworks can continuously track a camera's pose within the scene and can estimate the correct scale of the environment by using Visual-Inertial Odometry (VIO). In this paper, we propose a novel approach that combines the geometric information from VIO with semantic information from object detectors to improve the performance of object detection on mobile devices. Our approach includes three components: (1) an image orientation correction method, (2) a scale-based filtering approach, and (3) an online semantic map. Each component takes advantage of the different characteristics of the VIO-based AR framework. We implemented the AR-enhanced features using ARCore and the SSD Mobilenet model on Android phones. To validate our approach, we manually labeled objects in image sequences taken from 12 room-scale AR sessions. The results show that our approach can improve on the accuracy of generic object detectors by 12% on our dataset.},
  keywords={Performance evaluation;Two dimensional displays;Semantics;Object detection;Detectors;Real-time systems;Augmented reality;Computer Graphics;Graphics systems and interfaces;Mixed / Augmented Reality;Artificial intelligence;Computer vision;Computer vision problems;Object detection},
  doi={10.1109/ISMAR50242.2020.00037},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284678,
  author={Fukamizu, Kentaro and Miyashita, Leo and Ishikawa, Masatoshi},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ElaMorph Projection: Deformation of 3D Shape by Dynamic Projection Mapping}, 
  year={2020},
  volume={},
  number={},
  pages={164-173},
  abstract={We propose a projector-based method that provides an illusion of geometry change, similar to that caused by a change in physical properties such as elasticity, in response to inertia caused by physical motion. The proposed method is named “ElaMorph projection.” Although several projection mapping methods capable of deforming targets have been previously proposed, these methods require the preparation of animations in advance. Moreover, these methods are unable to deform the shape in real-time according to actual movements of the object. To address these issues, we perform real-time geometry deformation and rendering based on the spatial motion of the object. To render the projection image, we extend the conventional method of deformation for 2D pictures or static 3D objects to include dynamic 3D objects. This study involves projection onto a dynamic 3D object; however, the projection quality decreases if a part of the rendered image extends beyond the projection target. To address this issue, the proposed algorithm ensures that the vertices after deformation always remain within the projection target. In addition, we develop a robust algorithm to generate projection images under dynamic illuminative conditions, through real-time estimation of the environmental lighting required for rendering. Moreover, using an elasticity map that can be easily constructed using a UV map, our method enables users to specify the vertices to be deformed, using an elasticity map. We present projections under several different sets of elasticity maps, environmental lighting, and elasticities. Finally, we evaluate the latency and throughput of our system.},
  keywords={Three-dimensional displays;Shape;Heuristic algorithms;Lighting;Elasticity;Real-time systems;Strain;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality;Computing methodologies;Computer graphics;Animation},
  doi={10.1109/ISMAR50242.2020.00038},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284714,
  author={Kurth, Philipp and Lange, Vanessa and Stamminger, Marc and Bauer, Frank},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Real-Time Adaptive Color Correction in Dynamic Projection Mapping}, 
  year={2020},
  volume={},
  number={},
  pages={174-184},
  abstract={Projection mapping augments a real-world object's appearance by projecting digital content on its surface. However, a remaining obstacle to immersive projection mapping is the limitation to white Lambertian surfaces and uniform neutral environment light, if any. Violating one of these assumptions results in a discernible difference between the source material and the appearance of the projected content. For example, some colors may not be visible due to intense environment lighting or pronounced surface colors. We present a system that actively subdues many of those real-world influences, especially environment lighting. Our system supports dynamic (i.e., movable) target objects as well as changing lighting conditions while requiring no prior color calibration of the projector nor any precomputed environment probing. We automatically and continuously estimate these influences during runtime in a real-time feedback-loop and adjust the projected colors accordingly.},
  keywords={Runtime;Lighting;Color;Cameras;Real-time systems;Calibration;Systems support;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception},
  doi={10.1109/ISMAR50242.2020.00039},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284734,
  author={Wang, Miao and Li, Yi-Jun and Zhang, Wen-Xuan and Richardt, Christian and Hu, Shi-Min},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Transitioning360: Content-aware NFoV Virtual Camera Paths for 360° Video Playback}, 
  year={2020},
  volume={},
  number={},
  pages={185-194},
  abstract={Despite the increasing number of head-mounted displays, many 360° VR videos are still being viewed by users on existing 2D displays. To this end, a subset of the 360° video content is often shown inside a manually or semi-automatically selected normal-field-of-view (NFoV) window. However, during the playback, simply watching an NFoV video can easily miss concurrent off-screen content. We present Transitioning360, a tool for 360° video navigation and playback on 2D displays by transitioning between multiple NFoV views that track potentially interesting targets or events. Our method computes virtual NFoV camera paths considering content awareness and diversity in an offline preprocess. During playback, the user can watch any NFoV view corresponding to a precomputed camera path. Moreover, our interface shows other candidate views, providing a sense of concurrent events. At any time, the user can transition to other candidate views for fast navigation and exploration. Experimental results including a user study demonstrate that the viewing experience using our method is more enjoyable and convenient than previous methods.},
  keywords={Visualization;Target tracking;Navigation;Two dimensional displays;Tools;Cameras;Videos;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Computing methodologies;Computer graphics;Image manipulation;Image processing},
  doi={10.1109/ISMAR50242.2020.00040},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284771,
  author={Feick, Martin and Bateman, Scott and Tang, Anthony and Miede, André and Marquardt, Nicolai},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Tangi: Tangible Proxies For Embodied Object Exploration And Manipulation In Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={195-206},
  abstract={Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi's proxies outperform traditional controllers and were generally favored by participants.},
  keywords={Shape;Manipulators;Augmented reality;Virtual Reality;Tangible Interfaces;VR Object Exploration and Manipulation;Tangible Proxy Objects},
  doi={10.1109/ISMAR50242.2020.00042},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284719,
  author={Martin-Gomez, Alejandro and Fotouhi, Javad and Eck, Ulrich and Navab, Nassir},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Gain A New Perspective: Towards Exploring Multi-View Alignment in Mixed Reality}, 
  year={2020},
  volume={},
  number={},
  pages={207-216},
  abstract={Manufacturing, maintenance, assembly, and training tasks represent some of the human activities that have captured special interest for Mixed Reality (MR) applications. For most of these scenarios, accurate object alignment constitutes a requirement to ensure the desired outcome. This task has proved to be especially challenging in egocentric approaches, frequently leading to estimation errors in depth. While traditional MR methods provide virtual guides such as text, arrows, or animations to assist users during alignment, this work explores the feasibility of using additional views generated by virtual cameras and mirrors. Presenting additional views from different perspectives can help to mitigate the estimation errors and show information that is not directly visible to users.To explore the benefits of using additional views for alignment tasks, and to collect reliable data and diminish external factors, we conducted a user study in a controlled virtual environment where participants aligned objects supported by additional views from a top-down camera and virtual mirrors. Data regarding alignment error, time to completion, user's interaction and attention, distance traveled, average head velocity, usability, and mental effort were collected. Our results show that using additional views reduces the mental effort and distance traveled by users and increases acceptance without negatively affecting the alignment accuracy. Therefore, we believe that users will also benefit from integrating these techniques during alignment tasks in MR environments.},
  keywords={Human computer interaction;Estimation error;Design methodology;Virtual environments;Cameras;Mirrors;Task analysis;Human-centered computing [Human computer interaction (HCI)]: HCI design and evaluation methods—User studies;Human-centered computing [Human computer interaction (HCI)]: Interaction paradigms—Virtual reality;Human-centered computing [Visualization]: Visualization design and evaluation methods},
  doi={10.1109/ISMAR50242.2020.00044},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284798,
  author={Martin-Gomez, Alejandro and Winkler, Alexander and Yu, Kevin and Roth, Daniel and Eck, Ulrich and Navab, Nassir},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmented Mirrors}, 
  year={2020},
  volume={},
  number={},
  pages={217-226},
  abstract={A recurrent problem in egocentric Augmented Reality (AR) applications is the misestimation of depth. Providing alternative views from non-egocentric perspectives can convey useful information for applications that require the correct judgment of depth as it is in the case of placement and alignment of virtual and real content, but also for exploration and visualization tasks.In this paper, we introduce Augmented Mirrors. Through the integration of a real mirror, our approach is capable to reflect changes of the real and virtual content of an AR application while users benefit from the perceptual advantages of using mirrors. Our concept, simple yet effective, only requires tracking the user and mirror poses with the accuracy demanded by a specific application. To showcase the potential and flexibility of the Augmented Mirrors, we present and discuss multiple examples ranging from alignment, exploration, spatial understanding, and selective content visualization using different AR-enabled devices and tracking technologies. We envision the Augmented Mirrors as a new and valuable concept that can be used in applications that benefit from additional viewpoints and require the simultaneous visualization of real and virtual content.},
  keywords={Manifolds;Visualization;Tools;Mirrors;Synchronization;Task analysis;Augmented reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms Mixed / augmented reality—; Human-centered computing—Human computer interaction (HCI)— HCI theory;concepts and models—;Human-centered computing Visualization—Visualization—Visualization theory;concepts and paradigms—},
  doi={10.1109/ISMAR50242.2020.00045},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284697,
  author={Kraus, Matthias and Schäfer, Hanna and Meschenmoser, Philipp and Schweitzer, Daniel and Keim, Daniel A. and Sedlmair, Michael and Fuchs, Johannes},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation}, 
  year={2020},
  volume={},
  number={},
  pages={227-238},
  abstract={Movement-compensating interactions like teleportation are commonly deployed techniques in virtual reality environments. Although practical, they tend to cause disorientation while navigating. Previous studies show the effectiveness of orientation-supporting tools, such as trails, in reducing such disorientation and reveal different strengths and weaknesses of individual tools. However, to date, there is a lack of a systematic comparison of those tools when teleportation is used as a movement-compensating technique, in particular under consideration of different tasks. In this paper, we compare the effects of three orientation-supporting tools, namely minimap, trail, and heatmap. We conducted a quantitative user study with 48 participants to investigate the accuracy and efficiency when executing four exploration and search tasks. As dependent variables, task performance, completion time, space coverage, amount of revisiting, retracing time, and memorability were measured. Overall, our results indicate that orientation-supporting tools improve task completion times and revisiting behavior. The trail and heatmap tools were particularly useful for speed-focused tasks, minimal revisiting, and space coverage. The minimap increased memorability and especially supported retracing tasks. These results suggest that virtual reality systems should provide orientation aid tailored to the specific tasks of the users.},
  keywords={Visualization;Virtual environments;Tools;Teleportation;Time measurement;Task analysis;Space heating;Human-centered computing—;Human computer interaction (HCI)—;Interaction paradigms—;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00046},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284696,
  author={Aziz, K M Arafat and Luo, Hu and Asma, Lehiany and Xu, Weiliang and Zhang, Yuru and Wang, Dangxiao},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Haptic Handshank – A Handheld Multimodal Haptic Feedback Controller for Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={239-250},
  abstract={Compared to wearable devices, handheld haptic devices are promising for large scale virtual reality applications because of their portability and capability of supporting large workspace haptic interaction. However, it remains a challenge to render multimodal haptic stimuli in handheld devices due to space confinement. In this paper, we present a modular approach to build a Multimodal Handheld Haptic Controller called “Haptic Handshank” that includes a thumb feedback component, a palm feedback component, and a motion tracking component. In the thumb feedback component, a compact pneumatically-driven silicone airbag is utilized to simulate softness, and a flexible membrane based on the electro-vibration principle which covers the top portion of the airbag for rendering virtual textures. In the palm feedback component, vibrational motors and Peltier devices are embedded into the device's body for rendering vibrotactile flow and distributing thermal stimuli. In the motion tracking component, an HTC-Vive tracker is mounted on the bottom of the controller's handle to enable 6-DOF palm motion tracking. The performance of the handheld device is evaluated through quantitative experimental studies, which validate the ability of the device to simulate multimodal haptic sensations in accordance with diverse hand manipulation gestures such as enclosure, static contact, rubbing, squeezing and shaking of a cup of cold drink in 3D virtual space.},
  keywords={Tracking;Handheld computers;Wearable computers;Thumb;Aerospace electronics;Rendering (computer graphics);Haptic interfaces;Multimodal;handheld device;haptic feedback;controller;virtual reality},
  doi={10.1109/ISMAR50242.2020.00047},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284802,
  author={Cho, Woojin and Park, Gabyong and Woo, Woontack},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object}, 
  year={2020},
  volume={},
  number={},
  pages={251-259},
  abstract={We propose a 3D hand tracking system using bare-hand depth inpainting from an RGB-depth image for a hand interacting with an object. The effectiveness of most existing hand-object tracking methods is impeded by the insufficiency of data, which do not include hand data occluded by the object, and their reliance on the information inferred from assuming the specific object type. We generate a sufficiently accurate bare-hand depth image from a hand interacting with an object using a conditional generative adversarial network, which is trained using the synthesized 2D silhouettes of the object to learn the morphology of the hand. We evaluate the proposed approach using a hierarchical particle filter-based hand tracker and prove that our approach utilizing the bare-hand tracker in the hand-object interaction dataset achieve state-of-the-art performance. The generalization of our work will enable visual-tactile interaction that is more natural in various wearable augmented reality applications.},
  keywords={Geometry;Three-dimensional displays;Two dimensional displays;Morphology;Semisupervised learning;Object tracking;Augmented reality;Artificial intelligence;Computer vision;Computer vision problems;Tracking;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR50242.2020.00048},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284746,
  author={Ricca, Aylen and Chellali, Amine and Otmane, Samir},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Influence of hand visualization on tool-based motor skills training in an immersive VR simulator}, 
  year={2020},
  volume={},
  number={},
  pages={260-268},
  abstract={Immersive VR technologies offer versatile training tools by recreating real-world situations in a virtual and safe environment and allowing users to have a first-person experience. The design of such training systems requires defining the most critical components to simulate, and to what extent they can be simulated successfully. One open research question for designing such systems is how to represent the user in the virtual environment, and which is the added value of this representation for training purposes. In this work, we focus on how the user's hand representation in an immersive virtual environment can impact the training of tool-based motor skills.To investigate this question, we have designed a VR trainer for a simple tool-based pick and place task. A user experiment was conducted to evaluate how the movements of the users' real hand representation influence their performance and subjective experience in the virtual environment. For that purpose, the participants performed the task on the VR simulator with two conditions: the presence or absence of their animated virtual hands representation. The results of this study show that, although users prefer to have a visual representation of their hands, they achieved similar and correlated performance in the VR system regardless of the hand representation condition. These results suggest that the presence of the user's hand representation is not necessary when performing a tool-based motor skill task in a VR trainer. These findings have practical implications for the design of VR simulators for training motor skills tasks since adding users' hand representation may require cumbersome and expensive additional devices.},
  keywords={Training;Performance evaluation;Visualization;Virtual environments;Tools;Task analysis;Augmented reality},
  doi={10.1109/ISMAR50242.2020.00049},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284804,
  author={Benda, Brett and Esmaeili, Shaghayegh and Ragan, Eric D.},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Determining Detection Thresholds for Fixed Positional Offsets for Virtual Hand Remapping in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={269-278},
  abstract={Virtual reality commonly makes use of tracked hand interactions for user input. Interaction techniques sometimes alter the mapping between the real and virtual coordinate systems to modify interaction possibilities. This paper studies fixed positional offsets applied to the location of the virtual hand. We present a controlled experiment in which users' hands were subject to fixed positional offsets of varying magnitudes while completing target-touching tasks. The study provides estimations for detection thresholds for positional hand offsets in six directions relative to the real-world location of the hand and provides evidence performance using offset virtual hands can vary based on offset parameters. Significant differences in offset detection were identified based on offset direction, indicating that positional adjustments made to virtual hands should consider directionality when limiting techniques rather than just a constant value. Hand offsets kept within the threshold value resulted in comparable performance to unmodified hand registration, while offsets beyond the threshold resulted in larger completion times.},
  keywords={Limiting;Estimation;Task analysis;Augmented reality;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction;Interaction techniques},
  doi={10.1109/ISMAR50242.2020.00050},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284688,
  author={Syiem, Brandon Victor and Kelly, Ryan M. and Velloso, Eduardo and Goncalves, Jorge and Dingler, Tilman},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing Visitor Experience or Hindering Docent Roles: Attentional Issues in Augmented Reality Supported Installations}, 
  year={2020},
  volume={},
  number={},
  pages={279-288},
  abstract={Studies using augmented reality (AR) technology have suggested that users focus excessively on the virtual content in the AR environment at the expense of the physical world around them. This has implications related to the design of installations that aim to incorporate the user's physical environment as part of the AR experience. To better understand how user attention is managed in an AR environment, we present an observational study of Rewild Our Planet, a multi-modal installation that combined video, audio, a human docent and mobile AR to promote awareness about environmental issues. We found that, while AR was successful in engaging visitors, it drew attention away from other modalities within the installation. This impacts the work of the human docent and affects how visitors absorb information presented in the installation. Based on these observations, we present guidelines to inform the design of future AR-supported installations with the aim of minimizing or taking advantage of the observed attentional issues.},
  keywords={Planets;User experience;Augmented reality;Guidelines;Human-centered computing;Mixed / augmented reality;Human-centered computing;Empirical studies in HCI;Human-centered computing;Field studies},
  doi={10.1109/ISMAR50242.2020.00053},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284793,
  author={Andersen, Daniel and Popescu, Voicu},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AR Interfaces for Mid-Air 6-DoF Alignment: Ergonomics-Aware Design and Evaluation}, 
  year={2020},
  volume={},
  number={},
  pages={289-300},
  abstract={Aligning hand-held objects into mid-air positions and orientations is important for many applications. Task performance depends on speed and accuracy, and also on minimizing the user's physical exertion. Augmented reality head-mounted displays (AR HMDs) can guide users during mid-air alignments by tracking an object's pose and delivering visual instruction directly into the user's field of view (FoV). However, it is unclear which AR HMD interfaces are most effective for mid-air alignment guidance, and how the form factor of current AR HMD hardware (such as heaviness and low FoV) affects how users put themselves into tiring body poses during mid-air alignment. We defined a set of design requirements for mid-air alignment interfaces that target reduction of high-exertion body poses during alignment. We then designed, implemented, and tested several interfaces in a user study in which novice participants performed a sequence of mid-air alignments using each interface.Results show that interfaces that rely on visual guidance located near the hand-held object reduce acquisition times and translation errors, while interfaces that involve aiming at a faraway virtual object reduce rotation errors. Users tend to avoid focus shifts and to position the head and arms to maximize how much AR visualization is contained within a single FoV without moving the head. We found that changing the size of visual elements affected how far out the user extends the arm, which affects torque forces. We also found that dynamically adjusting where visual guidance is placed relative to the mid-air pose can help keep the head level during alignment, which is important for distributing the weight of the AR HMD.},
  keywords={Visualization;Torque;Resists;Hardware;Space exploration;Task analysis;Augmented reality;Human-centered computing—User studies;Computing methodologies—Mixed / augmented reality},
  doi={10.1109/ISMAR50242.2020.00055},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284767,
  author={Zhang, Yan and Isoyama, Naoya and Sakata, Nobuchika and Kiyokawa, Kiyoshi and Hua, Hong},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Super Wide-view Optical See-through Head Mounted Displays with Per-pixel Occlusion Capability}, 
  year={2020},
  volume={},
  number={},
  pages={301-311},
  abstract={Augmented reality (AR) has been widely used that combines human with the digital world tightly at an unprecedented level, and various types of optical see-through head-mounted displays (OSTHMDs) have been actively developed to meet the requirement of everyday AR use. Correct mutual occlusion between real and virtual objects is often necessary for displaying realistic virtual images for users in the AR application scenarios. Some optical designs have been proposed to realize mutual occlusion by means of an OSTHMD in recent years. However, all of them support a limited field of view (FOV) that is much narrower than that of a natural human. The main limiting factor of the FOV of general OSTHMDs is the limited numerical aperture (NA) of the lenses. To address the problem, we propose an OSTHMD based on the double ellipsoidal mirror structure to avoid a stack of lenses and to achieve a wide FOV close to that of the naked eye with small distortion. A pair of imaging lenses are carefully arranged, then assembled between the two ellipsoidal mirrors with a pinhole mask to improve image quality. An experiment using a monocular prototype shows that a sharp see through view is achieved which remains clear regardless of the focus of the eye-simulating camera. An image undistortion algorithm is also developed to obtain a full-view display, enabling a virtual image to be displayed spanning a super wide FOV of H160°×V74°. Finally, per-pixel mutual occlusion of a wide FOV of H122°×V74° is realized by placing a spatial light modulator (SLM) in front of the entrance pupil.},
  keywords={Optical design;Prototypes;Optical distortion;Optical imaging;Mirrors;Optical sensors;Lenses;Computing methodologies;Computer graphics;Graphics system and interface-Mixed/Augmented reality},
  doi={10.1109/ISMAR50242.2020.00056},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284652,
  author={Xia, Xinxing and Guan, Yunqing and State, Andrei and Chakravarthula, Praneeth and Cham, Tat-Jen and Fuchs, Henry},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Eyeglass-style Holographic Near-eye Displays with Statically}, 
  year={2020},
  volume={},
  number={},
  pages={312-319},
  abstract={Holography is perhaps the only method demonstrated so far that can achieve a wide field of view (FOV) and a compact eyeglass-style form factor for augmented reality (AR) near-eye displays (NEDs). Unfortunately, the eyebox of such NEDs is impractically small (~ <; 1mm). In this paper, we introduce and demonstrate a design for holographic NEDs with a practical, wide eyebox of ~ 10mm and without any moving parts, based on holographic lenslets. In our design, a holographic optical element (HOE) based on a lenslet array was fabricated as the image combiner with expanded eyebox. A phase spatial light modulator (SLM) alters the phase of the incident laser light projected onto the HOE combiner such that the virtual image can be perceived at different focus distances, which can reduce the vergence-accommodation conflict (VAC). We have successfully implemented a bench-top prototype following the proposed design. The experimental results show effective eyebox expansion to a size of ~ 10mm. With further work, we hope that these design concepts can be incorporated into eyeglass-size NEDs.},
  keywords={Phase modulation;Optical device fabrication;Prototypes;Holography;Optical imaging;Optical modulation;Augmented reality;Near-eye displays;Augmented reality;Holographic displays;Expanded eyebox Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Com-puting methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality;Hardware—Communication hardware;interfaces and storage—Displays and imagers},
  doi={10.1109/ISMAR50242.2020.00057},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284672,
  author={Lu, Conny and Chakravarthula, Praneeth and Tao, Yujie and Chen, Steven and Fuchs, Henry},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Improved vergence and accommodation via Purkinje Image tracking with multiple cameras for AR glasses}, 
  year={2020},
  volume={},
  number={},
  pages={320-331},
  abstract={We present a personalized, comprehensive eye-tracking solution based on tracking higher-order Purkinje images, suited explicitly for eyeglasses-style AR and VR displays. Existing eye-tracking systems for near-eye applications are typically designed to work for an on-axis configuration and rely on pupil center and corneal reflections (PCCR) to estimate gaze with an accuracy of only about 0.5°to 1°. These are often expensive, bulky in form factor, and fail to estimate monocular accommodation, which is crucial for focus adjustment within the AR glasses.Our system independently measures the binocular vergence and monocular accommodation using higher-order Purkinje reflections from the eye, extending the PCCR based methods. We demonstrate that these reflections are sensitive to both gaze rotation and lens accommodation and model the Purkinje images’ behavior in simulation. We also design and fabricate a user-customized eye tracker using cheap off-the-shelf cameras and LEDs. We use an end-to-end convolutional neural network (CNN) for calibrating the eye tracker for the individual user, allowing for robust and simultaneous estimation of vergence and accommodation. Experimental results show that our solution, specifically catering to individual users, outperforms state-of-the-art methods for vergence and depth estimation, achieving an accuracy of 0.3782° and 1.108cm respectively.},
  keywords={Estimation;Cameras;Light emitting diodes;Rendering (computer graphics);Reflection;Pupils;Augmented reality;Augmented Reality;Eye tracking;Eye tracking Techniques;Purkinje images;Human-Computer Interaction},
  doi={10.1109/ISMAR50242.2020.00058},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284722,
  author={Brasier, Eugenie and Chapuis, Olivier and Ferey, Nicolas and Vezien, Jeanne and Appert, Caroline},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ARPads: Mid-air Indirect Input for Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={332-343},
  abstract={Interacting efficiently and comfortably with Augmented Reality (AR) headsets remains a major issue. We investigate the concept of mid-air pads as an alternative to gaze or direct hand input to control a cursor in windows anchored in the environment. ARPads allow users to control the cursor displayed in the headset screen through movements on a mid-air plane, which is not spatially aligned with the headset screen. We investigate a design space for ARPads, which takes into account the position of the pad relative to the user's body, and the orientation of the pad relative to that of the headset screen. Our study suggests that 1) indirect input can achieve the same performance as direct input while causing less fatigue than hand raycast, 2) an ARPad should be attached to the wrist or waist rather than to the thigh, and 3) the ARPad and the screen should have the same orientation.},
  keywords={Headphones;Wrist;Thigh;Aerospace electronics;Fatigue;Augmented reality;AR headset;Indirect input;Small scale gestures Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—Interaction techniques—Gestural input},
  doi={10.1109/ISMAR50242.2020.00060},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284687,
  author={Lu, Xueshi and Yu, Difeng and Liang, Hai-Ning and Xu, Wenge and Chen, Yuzheng and Li, Xiang and Hasan, Khalad},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploration of Hands-free Text Entry Techniques For Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={344-349},
  abstract={Text entry is a common activity in virtual reality (VR) systems. There is a limited number of available hands-free techniques, which allow users to carry out text entry when users’ hands are busy such as holding items or hand-based devices are not available. The most used hands-free text entry technique is DwellType, where a user selects a letter by dwelling over it for a specific period. However, its performance is limited due to the fixed dwell time for each character selection. In this paper, we explore two other hands-free text entry mechanisms in VR: BlinkType and NeckType, which leverage users’ eye blinks and neck’s forward and backward movements to select letters. With a user study, we compare the performance of the two techniques with DwellType. Results show that users can achieve an average text entry rate of 13.47, 11.18 and 11.65 words per minute with BlinkType, NeckType, and DwellType, respectively. Users’ subjective feedback shows BlinkType as the preferred technique for text entry in VR.},
  keywords={Performance evaluation;Head-mounted displays;Resists;Gaze tracking;Indexes;Augmented reality;Virtual reality;Text Entry;Dwelling;Eye Blinking;NeckType;Head-Mounted Display;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Text input},
  doi={10.1109/ISMAR50242.2020.00061},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284742,
  author={Gupta, Aakar and Samad, Majed and Kin, Kenrick and Kristensson, Per Ola and Benko, Hrvoje},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating Remote Tactile Feedback for Mid-Air Text-Entry in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={350-360},
  abstract={In this paper, we investigate the utility of remote tactile feedback for freehand text-entry on a mid-air Qwerty keyboard in VR. To that end, we use insights from prior work to design a virtual keyboard along with different forms of tactile feedback, both spatial and non-spatial, for fingers and for wrists. We report on a multi-session text-entry study with 24 participants where we investigated four vibrotactile feedback conditions: on-fingers, on-wrist spatialized, on-wrist non-spatialized, and audio-visual only. We use micro-metrics analyses and participant interviews to analyze the mechanisms underpinning the observed performance and user experience. The results show comparable performance across feedback types. However, participants overwhelmingly prefer the tactile feedback conditions and rate on-fingers feedback as significantly lower in mental demand, frustration, and effort. Results also show that spatialization of vibrotactile feedback on the wrist as a way to provide finger-specific feedback is comparable in performance and preference to a single vibration location. The micro-metrics analyses suggest that users compensated for the lack of tactile feedback with higher visual and cognitive attention, which ensured similar performance but higher user effort.},
  keywords={Wrist;Vibrations;Measurement;Visualization;Tactile sensors;Keyboards;User experience;Human-centered computing—Human Computer Interaction—;——Human-centered computing—Keyboards—;Human-centered computing—Haptics——},
  doi={10.1109/ISMAR50242.2020.00062},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284682,
  author={Gesslein, Travis and Biener, Verena and Gagel, Philipp and Schneider, Daniel and Kristensson, Per Ola and Ofek, Eyal and Pahud, Michel and Grubert, Jens},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Pen-based Interaction with Spreadsheets in Mobile Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={361-373},
  abstract={Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.},
  keywords={Productivity;Headphones;Switches;Debugging;Tools;Mobile handsets;Sensors},
  doi={10.1109/ISMAR50242.2020.00063},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284736,
  author={Masai, Katsutoshi and Kunze, Kai and Sakamoto, Daisuke and Sugiura, Yuta and Sugimoto, Maki},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Face Commands - User-Defined Facial Gestures for Smart Glasses}, 
  year={2020},
  volume={},
  number={},
  pages={374-386},
  abstract={We propose the use of face-related gestures involving the movement of the face, eyes, and head for augmented reality (AR). This technique allows us to use computer systems via hands-free, discreet interactions. In this paper, we present an elicitation study to explore the proper use of facial gestures for daily tasks in the context of a smart home. We used Amazon Mechanical Turk to conduct this study (N=37). Based on the proposed gestures, we report usage scenarios and complexity, proposed associations between gestures/tasks, a user-defined gesture set, and insights from the participants. We also conducted a technical feasibility study (N=13) with participants using smart eyewear to consider their uses in daily life. The device has 16 optical sensors and an inertial measurement unit (IMU). We can potentially integrate the system into optical see-through displays or other smart glasses. The results demonstrate that the device can detect eight temporal face-related gestures with a mean F1 score of 0.911 using a convolutional neural network (CNN). We also report the results of user-independent training and a one-hour recording of the experimenter testing two of the gestures.},
  keywords={Training;User experience;Optical sensors;Faces;Augmented reality;Smart glasses;Testing;Human-centered computing;Interaction techniques;Human-centered computing;Ubiquitous and mobile computing design and evaluation methods},
  doi={10.1109/ISMAR50242.2020.00064},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284654,
  author={Martin, Nicolas and Mathieu, Nicolas and Pallamin, Nico and Ragot, Martin and Diverrez, Jean-Marc},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Virtual reality sickness detection: an approach based on physiological signals and machine learning}, 
  year={2020},
  volume={},
  number={},
  pages={387-399},
  abstract={Virtual Reality (VR) is spreading to the general public but still has a major issue: VR sickness. To take it into consideration and minimize its occurrence, evaluation methods are required. The current methods are mainly based on subjective measurements and therefore have several drawbacks (e.g., non-continuous, intrusive). Physiological signals combined with Machine Learning (ML) methods seem an interesting approach to go beyond these limits. In this paper, we present a large-scale experimentation (103 participants) where physiological data (cardiac and electrodermal activities) and subjective data (perceived VR sickness) were gathered during 30-minute VR video game sessions. Using ML methods, models were trained to predict VR sickness level (based on the physiological data labeled with the subjective data). Results showed an explained variance up to 75% (in a regression approach) and an accuracy up to 91% (in a classification approach). Despite generalization issues, this method seems promising and valuable for a real time, automatic and continuous evaluation of VR sickness, based on physiological signals and ML models.},
  keywords={Solid modeling;Machine learning;Games;Predictive models;Particle measurements;Physiology;Real-time systems;H.1.2 [Models and principles]: User/Machine Systems;Human factors;I.3.6 [Computer graphics]: Methodology and Techniques;Ergonomics},
  doi={10.1109/ISMAR50242.2020.00065},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284761,
  author={Islam, Rifatul and Lee, Yonggun and Jaloli, Mehrad and Muhammad, Imtiaz and Zhu, Dakai and Rad, Paul and Huang, Yufei and Quarles, John},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Automatic Detection and Prediction of Cybersickness Severity using Deep Neural Networks from user’s Physiological Signals}, 
  year={2020},
  volume={},
  number={},
  pages={400-411},
  abstract={Cybersickness is one of the primary challenges to the usability and acceptability of virtual reality (VR). Cybersickness can cause motion sickness-like discomforts, including disorientation, headache, nausea, and fatigue, both during and after the VR immersion. Prior research suggested a significant correlation between physiological signals and cybersickness severity, as measured by the simulator sickness questionnaire (SSQ). However, SSQ may not be suitable for automatic detection of cybersickness severity during immersion, as it is usually reported before and after the immersion. In this study, we introduced an automated approach for the detection and prediction of cybersickness severity from the user's physiological signals. We collected heart rate, breathing rate, heart rate variability, and galvanic skin response data from 31 healthy participants while immersed in a VR roller coaster simulation. We found a significant difference in the participants' physiological signals during their cybersickness state compared to their resting baseline. We compared a support vector machine classifier and three deep neural classifiers for cybersickness severity detection and prediction in two minutes' future, given the previous two minutes of physiological signals. Our proposed simplified convolutional long short-term memory classifier achieved an accuracy of 97.44% for detecting current cybersickness severity and 87.38% for predicting future cybersickness severity from the physiological signals.},
  keywords={Support vector machines;Solid modeling;Neural networks;Physiology;Skin;Usability;Heart rate variability;Virtual reality;automated cybersickness detection;visually induced motion sickness;deep neural network},
  doi={10.1109/ISMAR50242.2020.00066},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284807,
  author={George, Ceenu and Tien, An Ngo and Hussmann, Heinrich},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Seamless, Bi-directional Transitions along the Reality-Virtuality Continuum: A Conceptualization and Prototype Exploration}, 
  year={2020},
  volume={},
  number={},
  pages={412-424},
  abstract={With head mounted displays, consumers are able to transition from the real world to virtual realities. However, this requires frequent transitions between the two realities to maintain their physical integrity and awareness of the real world while in the virtual space. We completed two consecutive studies to investigate the dimensions of a system that supports seamless transition between realities without requiring the user to remove the headset. Our results are twofold: First, based on the the analysis of structured interviews (n=20), we present a conceptualization of existing solutions (n=37) and novel ideas (n=9) in the form of a design space. Second, we present the results of a user study (n=36) in which we tested two exemplary prototypes that evolved from the design space, called “Sky Portal” and “Virtual Phone.” Our exploration shows that our “Virtual Phone” metaphor has the potential to support HMD users in completing bidirectional transitions along Milgram's reality-virtuality continuum. Users are also enabled to complete micro-interactions across the realities, even without performance loss.},
  keywords={Human computer interaction;Headphones;Prototypes;Resists;Space exploration;Interviews;Augmented reality;Human-centered computing Mixed / augmented reality},
  doi={10.1109/ISMAR50242.2020.00067},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284745,
  author={Luong, Tiffany and Martin, Nicolas and Raison, Anaïs and Argelaguet, Ferran and Diverrez, Jean-Marc and Lécuyer, Anatole},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Real-Time Recognition of Users Mental Workload Using Integrated Physiological Sensors Into a VR HMD}, 
  year={2020},
  volume={},
  number={},
  pages={425-437},
  abstract={This paper describes an “all-in-one” solution for the real-time recognition of users' mental workloads in virtual reality through the customization of a commercial HMD with physiological sensors. First, we describe the hardware and software solution employed to build the system. Second, we detail the machine learning methods used for the automatic recognition of the users' mental workload, which are based on the well-known Random Forest algorithm. In order to gather data to train the system, we conducted an extensive user study with 75 participants using a VR flight simulator to induce different levels of mental workload. In contrast to previous works which label the data based on a standardized task (e.g., n-back task) or on a pre-defined task-difficulty, participants were asked about their perceived mental workload level along the experiment. With the data collected, we were able to train the system in order to classify four different levels of mental workload with an accuracy up to 65%. In addition, we discuss the role of the signal normalization procedures, the contribution of the different physiological signals on the recognition accuracy and compare the results obtained with the sensors embedded in the HMD with commercial grade systems. Preliminary results show our pipeline is able to recognize mental workload in real-time. Taken together, our results suggest that such all-in-one approach, with physiological sensors directly embedded in the HMD, is a promising path for VR applications in which the real-time or off-line estimation of Mental Workload assessment is beneficial.},
  keywords={Software algorithms;Resists;Real-time systems;Software;Sensors;Task analysis;Biomedical monitoring;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00068},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284762,
  author={Merino, Leonel and Schwarzl, Magdalena and Kraus, Matthias and Sedlmair, Michael and Schmalstieg, Dieter and Weiskopf, Daniel},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)}, 
  year={2020},
  volume={},
  number={},
  pages={438-451},
  abstract={We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.},
  keywords={Systematics;Benchmark testing;User interfaces;Rendering (computer graphics);Calibration;Usability;Augmented reality;Mixed and Augmented Reality;Evaluation;Systematic Literature Review;I.3.7 [Computing Methodologies];Three-Dimensional Graphics and Realism;A.1 [General Literature];Computer Graphics;Introductory and Survey},
  doi={10.1109/ISMAR50242.2020.00069},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284694,
  author={Dewez, Diane and Hoyet, Ludovic and Lécuyer, Anatole and Argelaguet, Ferran},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying the Inter-Relation Between Locomotion Techniques and Embodiment in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={452-461},
  abstract={Virtual embodiment and navigation are two topics widely studied in the virtual reality community. Despite the potential inter-relation between embodiment and locomotion, studies on virtual navigation rarely supply users with a virtual representation, while studies on virtual embodiment rarely allow users to virtually navigate. This paper therefore explores this potential inter-relation by focusing on the two following questions: Does the locomotion technique have an impact on the user's sense of embodiment? Does embodying an avatar have an impact on the user's preference and performance depending on the locomotion technique?To address these questions, we conducted a user study (N=60) exploring the relationship between the locomotion technique and the user's sense of embodiment over a virtual avatar seen from a first-person perspective. Three widely used locomotion techniques were evaluated: real walking, walking-in-place and virtual steering. All participants performed four different tasks involving a different awareness of their virtual avatar. Participants also performed the same tasks without being embodied in an avatar. The results show that participants had a comparable sense of embodiment with all techniques when embodied in an avatar, and that the presence or absence of the virtual avatar did not alter their performance while navigating, independently of the technique. Taken together, our results represent a first attempt to qualify the inter-relation between virtual navigation and virtual embodiment, and suggest that the 3D locomotion technique used has little influence on the user's sense of embodiment in VR.},
  keywords={Legged locomotion;Three-dimensional displays;Navigation;Avatars;Virtual environments;Focusing;Task analysis;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00070},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284711,
  author={Wolf, Erik and Döllinger, Nina and Mal, David and Wienrich, Carolin and Botsch, Mario and Latoschik, Marc Erich},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Body Weight Perception of Females using Photorealistic Avatars in Virtual and Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={462-473},
  abstract={The appearance of avatars can potentially alter changes in their users’ perception and behavior. Based on this finding, approaches to support the therapy of body perception disturbances in eating or body weight disorders by mixed reality (MR) systems gain in importance. However, the methodological heterogeneity of previous research has made it difficult to assess the suitability of different MR systems for therapeutic use in these areas. The effects of MR system properties and related psychometric factors on body-related perceptions have so far remained unclear. We developed an interactive virtual mirror embodiment application to investigate the differences between an augmented reality see-through head-mounted-display (HMD) and a virtual reality HMD on the before-mentioned factors. Additionally, we considered the influence of the participant’s body-mass-index (BMI) and the BMI difference between participants and their avatars on the estimations. The 54 normal-weight female participants significantly underestimated the weight of their photorealistic, generic avatar in both conditions. Body weight estimations were significantly predicted by the participants’ BMI and the BMI difference. We also observed partially significant differences in presence and tendencies for differences in virtual body ownership between the systems. Our results offer new insights into the relationships of body weight perception in different MR environments and provide new perspectives for the development of therapeutic applications.},
  keywords={Avatars;Medical treatment;Estimation;Resists;Mirrors;Indexes;Augmented reality;Mixed reality;immersion;presence;embodiment;virtual body ownership;agency;body image;eating disorders;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Human-centered computing;Human computer interaction (HCI);Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Virtual reality},
  doi={10.1109/ISMAR50242.2020.00071},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284723,
  author={Dey, Arindam and Phoon, Jane and Saha, Shuvodeep and Dobbins, Chelsea and Billinghurst, Mark},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Neurophysiological Approach for Measuring Presence in Immersive Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={474-485},
  abstract={Presence, the feeling of being there, is an important factor that affects the overall experience of Virtual Reality (VR). Higher presence commonly provides a better experience in VR than lower presence. However, presence is commonly measured subjectively through postexperience questionnaires, which can suffer from participant biases, dishonest answers, and fatigue. It can also be difficult for subjects to accurately remember their feelings of presence after they have left the VR experience. In this paper, we measured the effects of different levels of presence (high and low) in VR using physiological and neurological signals. The experiment involved 24 participants in a between-subjects design. Results indicated a significant effect of presence on both physiological and neurological signals. We noticed that higher presence results in higher heart rate, less visual stress, higher theta and beta activities in the frontal region, and higher alpha activities in the parietal region. These findings and insights could lead to an alternative objective measure of presence.},
  keywords={Heart rate;Visualization;Atmospheric measurements;Virtual environments;Particle measurements;Physiology;Stress;Human–centered computing–Human computer interaction (HCI)–Empirical studies in HCI;Human–centered computing–Visualization–Visualization design and evaluation methods},
  doi={10.1109/ISMAR50242.2020.00072},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284693,
  author={Tainaka, Keishi and Fujimoto, Yuichiro and Kanbara, Masayuki and Kato, Hirokazu and Moteki, Atsunori and Kuraki, Kensuke and Osamura, Kazuki and Yoshitake, Toshiyuki and Fukuoka, Toshiyuki},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Guideline and Tool for Designing an Assembly Task Support System Using Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={486-497},
  abstract={Augmented reality (AR) systems support complex tasks like assembly by overlaying task-related content onto the real world. In recent years, the effort of designing and developing assembly task support systems in AR decreased with the availability of high potential head-mounted displays and provision of integrated development environments. Nevertheless, problems still arise when companies craft an effective AR task support system, particularly in the difficulty of selecting appropriate techniques and information-presentation methods, and the requirements that vary with each use case. In this study, we formulated a corresponding guideline, developed a selection aid tool that incorporates filtering based on the categorization of subtasks and the degree of freedom of available tracking, and evaluated their effectiveness in two experiments. First, to confirm effects on system design, we asked 18 participants to perform the design action of the AR system with the guideline for two tasks (PC assembly and rope work). Consequently, to verify the quality of the designed AR systems from Experiment 1, we asked another set of 20 participants to perform the same tasks with those systems. The results confirm that using the guideline can considerably lower efforts creating media and alleviate the error for a specific process. We envision our guideline and tool to be accessible as an online web page, assisting AR assembly task support system designer/developers worldwide.},
  keywords={Web pages;Tools;Media;Systems support;Task analysis;Augmented reality;Guidelines;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Human-centered computing;Visualization;Visualization techniques},
  doi={10.1109/ISMAR50242.2020.00077},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284669,
  author={Lee, Gun A. and Ahn, Seungjun and Hoff, William and Billinghurst, Mark},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing First-Person View Task Instruction Videos with Augmented Reality Cues}, 
  year={2020},
  volume={},
  number={},
  pages={498-508},
  abstract={This research investigates enhancing first-person view (FPV) task instruction videos by applying Augmented Reality (AR) visualisation of spatial cues. With personal mobile devices, recording and sharing a video clip has become very easy, and how-to videos are becoming popular on social video sharing services. Instructional videos are actively used not only in formal education and training, but also in everyday life. However, video clips are limited to two-dimensional representation of the task space, making it hard for the viewer to follow and match the objects in the video to those in the real world task space. We propose augmenting task instruction videos with AR visualisation of spatial cues to overcome this problem, focusing on creating and viewing FPV instruction videos. We designed and implemented a prototype system, AR Tips, which allows users to capture and share augmented FPV instruction videos on a wearable AR device. We conducted a user study to evaluate the benefit of our approach, and the results showed that with the help of augmented spatial cues users better understood the instructions, performed the tasks faster with fewer errors, and had lower mental effort.},
  keywords={Training;Performance evaluation;Visualization;Prototypes;Task analysis;Augmented reality;Videos;Augmented Reality;instruction video;task guide},
  doi={10.1109/ISMAR50242.2020.00078},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284661,
  author={Wang, Lili and Wu, Wentao and Zhou, Zijing and Popescu, Voicu},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={View Splicing for Effective VR Collaboration}, 
  year={2020},
  volume={},
  number={},
  pages={509-519},
  abstract={In a co-located multi-user collaborative virtual reality (VR) application a collaborator should be able to indicate a workspace location to the user, such that they can refer to it simultaneously as they work together. Due to their different viewpoints, the collaborator sees some parts of the virtual environment (VE) that the user does not, and communication breaks down when the collaborator's reference is not visible to the user. The conventional solutions of asking the user to move around to gain line of sight to the collaborator's reference, or of asking the user to toggle back and forth between their view and that of the collaborator can be inefficient and ineffective. This paper proposes a method for improving collaboration in VR by alleviating the disparity between the user and the collaborator views of the VE. The user is shown a multiperspective visualization of the VE that transitions smoothly from the user to the collaborator's perspective. The multiperpsective visualization is based on the switch camera, a novel camera model with curved rays that splice together the user and collaborator views. The multiperspective visualization is computed first by warping the VE geometry, through projection with the switch camera followed by unprojection with a conventional camera, and then by rendering the warped VE conventionally, for each user eye. A controlled user study with three tasks shows that VR collaboration using the switch camera multiperpsective visualization is faster, more reliable, and less taxing on the user than the conventional approaches of viewpoint translation or view toggling.},
  keywords={Visualization;Splicing;Collaboration;Virtual environments;Switches;Cameras;Task analysis;Virtual reality;Collaborative;Occlusion management;Multiperspective visualization},
  doi={10.1109/ISMAR50242.2020.00079},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284778,
  author={Yoon, Boram and Kim, Hyung-il and Oh, Seo Young and Woo, Woontack},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating Remote Virtual Hands Models on Social Presence in Hand-based 3D Remote Collaboration}, 
  year={2020},
  volume={},
  number={},
  pages={520-532},
  abstract={This study investigates the effects of a virtual hand representation on the user experience including social presence during hand-based 3D remote collaboration. Although a remote hand appearance is a critical parts of a hand-based telepresence, it has been rarely studied in comparison to studies on the self-embodiment of virtual hands in a 3D environment. Thus, we conducted a user study comparing the three virtual hands models (Skeleton, Low Polygon and Realistic) while performing a remote collaborative task based on the American Sign Language (ASL) using both Augmented Reality (AR) and Virtual Reality (VR) environments. We found that the realistic type was perceived as the most sense of being together, human-like, and trustable representation. The low polygon model could also convey a clear sign and moderate level of social presence. Although the system was configured asymmetrically in AR and VR, little difference in perception was found except for the participant’s mental load and message understanding. We then discuss the results and suggest design implications for future hand-based 3D telepresence systems.},
  keywords={Solid modeling;Three-dimensional displays;Telepresence;Collaboration;User experience;Augmented reality;Load modeling;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/augmented reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/ISMAR50242.2020.00080},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284751,
  author={Platinsky, Lukas and Szabados, Michal and Hlasek, Filip and Hemsley, Ross and Pero, Luca Del and Pancik, Andrej and Baum, Bryan and Grimmett, Hugo and Ondruska, Peter},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps}, 
  year={2020},
  volume={},
  number={},
  pages={533-541},
  abstract={In this paper we present the first published end-to-end production computer-vision system for powering city-scale shared augmented reality experiences on mobile devices. In doing so we propose a new formulation for an experience-based mapping framework as an effective solution to the key issues of city-scale SLAM scalability, robustness, map updates and all-time all-weather performance required by a production system. Furthermore, we propose an effective way of synchronising SLAM systems to deliver seamless real-time localisation of multiple edge devices at the same time. All this in the presence of network latency and bandwidth limitations. The resulting system is deployed and tested at scale in San Francisco where it delivers AR experiences in a mapped area of several hundred kilometers. To foster further development of this area we offer the data set to the public, constituting the largest of this kind to date.},
  keywords={Production systems;Simultaneous localization and mapping;Scalability;Collaboration;Robustness;Augmented reality;Smart phones;Computer vision;Augmented reality;Structure from motion;Large-scale SLAM;Computing methodologies;Artificial intelligence;Computer vision;Tracking and Reconstruction;Computing methodologies-Mixed/augmented Reality},
  doi={10.1109/ISMAR50242.2020.00081},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284659,
  author={He, Zhenyi and Du, Ruofei and Perlin, Ken},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={CollaboVR: A Reconfigurable Framework for Creative Collaboration in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={542-554},
  abstract={Writing or sketching on whiteboards is an essential part of collaborative discussions in business meetings, reading groups, design sessions, and interviews. However, prior work in collaborative virtual reality (VR) systems has rarely explored the design space of multi-user layouts and interaction modes with virtual whiteboards. In this paper, we present CollaboVR, a reconfigurable framework for both co-located and geographically dispersed multi-user communication in VR. Our system unleashes users’ creativity by sharing freehand drawings, converting 2D sketches into 3D models, and generating procedural animations in real-time. To minimize the computational expense for VR clients, we leverage a cloud architecture in which the computational expensive application (Chalktalk) is hosted directly on the servers, with results being simultaneously streamed to clients. We have explored three custom layouts – integrated, mirrored, and projective – to reduce visual clutter, increase eye contact, or adapt different use cases. To evaluate CollaboVR, we conducted a within-subject user study with 12 participants. Our findings reveal that users appreciate the custom configurations and real-time interactions provided by CollaboVR. We have open sourced CollaboVR at https://github.com/snowymo/CollaboVR to facilitate future research and development of natural user interfaces and real-time collaborative systems in virtual and augmented reality.},
  keywords={Visualization;Layout;Collaboration;Writing;Real-time systems;Space exploration;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Collaborative and social computing;Collaborative and social computing systems and tools},
  doi={10.1109/ISMAR50242.2020.00082},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284728,
  author={Yakura, Hiromu and Goto, Masataka},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing Participation Experience in VR Live Concerts by Improving Motions of Virtual Audience Avatars}, 
  year={2020},
  volume={},
  number={},
  pages={555-565},
  abstract={While participating in live concerts is a promising application of virtual reality (VR), it falls short of our participation experience in the real world. In particular, to increase the engagement of participants, previous studies emphasized the importance of social experience among audience members, such as the sense of co-presence elicited by sharing physical reactions or body movements synchronized with music. In this respect, a common strategy in existing platforms is to present avatars of remote human participants in a VR venue and make every avatar imitate movements of the corresponding participant. However, this strategy implicitly assumes that a not small number of users connect simultaneously to watch the same content and thus is not applicable when only a few users gather or a user is watching alone. Therefore, with the aim of providing better experience to a user who participates in live concerts as one of the audience, we examine computational approaches to enhancing the sense of co-presence through virtual audience avatars. We propose four methods of presenting avatar movements: copying the user’s own movements, copying other users’ movements, repeating beat-synchronous movements, and synthesizing machine-learning-based movements. We compare their effectiveness in a user experiment and discuss application scenarios and design implications that open up new ways of active media consumption in VR environments.},
  keywords={Art;Avatars;Immersive experience;Media;Synchronization;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interactive systems and tools;Applied computing;Arts and humanities;Performing arts},
  doi={10.1109/ISMAR50242.2020.00083},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284702,
  author={Liu, Huimin and Wang, Zhiquan and Mousas, Christos and Kao, Dominic},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Virtual Reality Racket Sports: Virtual Drills for Exercise and Training}, 
  year={2020},
  volume={},
  number={},
  pages={566-576},
  abstract={We have developed a modular virtual reality gaming application that can be used to synthesize exercise drills for racket sports. By defining cost terms that are related to the gameplay and the mechanics of the game, as well as by allowing a user to control the parameters of the cost terms, users can easily adjust the objectives and the intensity levels of the exercise drills. Based on the user-defined exercise objectives, a Markov chain Monte Carlo optimization method called “simulated annealing” was used to optimize the exercise drill. The effectiveness of the developed virtual reality gaming application was measured in two studies by using virtual reality table tennis as the evaluation tool. The first study investigated the potential usefulness of the developed virtual reality gaming application as an exercise tool by comparing its workout effectiveness at three intensity levels (low, medium, and high) through the collection of heart rate readings. The second study explored the potential utility of the virtual reality gaming application as a training tool by exploring whether there was any improvement in participants' performance across the three conditions (no training, virtual reality training, and real-world training). The results indicate that a virtual reality gaming application, such as the examined virtual reality table tennis exergame, could indeed be used effectively as both an exercise and a training tool. Limitations and future research directions are discussed further below.},
  keywords={Training;Monte Carlo methods;Optimization methods;Virtual reality;Games;Tools;Sports;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Applied computing;Computers in other domains;Personal computers and PC applications;Computer games},
  doi={10.1109/ISMAR50242.2020.00084},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284729,
  author={Yu, Xingyao and Angerbauer, Katrin and Mohr, Peter and Kalkofen, Denis and Sedlmair, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perspective Matters: Design Implications for Motion Guidance in Mixed Reality}, 
  year={2020},
  volume={},
  number={},
  pages={577-587},
  abstract={We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.},
  keywords={Visualization;Atmospheric measurements;Prototypes;Particle measurements;Timing;Task analysis;Sports;Information Interfaces and Presentation—User Interfaces—Evaluation/Methodology;Computer Graphics—ThreeDimensional Graphics and Realism—Virtual Reality},
  doi={10.1109/ISMAR50242.2020.00085},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284791,
  author={Park, Gabyong and Kim, Tae-Kyun and Woo, Woontack},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={3D Hand Pose Estimation with a Single Infrared Camera via Domain Transfer Learning}, 
  year={2020},
  volume={},
  number={},
  pages={588-599},
  abstract={Previous methods successfully estimated 3D hand poses from unblurred depth images with slow and smooth hand motions. However, the performance drops when the depth images are contaminated by motion blur due to fast hand motion. In this paper, we exploit an infrared (IR) image input, which is weakly blurred under fast hand motion. The proposed method is based on domain transfer learning from depth to infrared images. Note we do not have IR images with hand skeletons, thus proposing self-supervision rather than direct supervision using the skeleton labels. We train a Hand Image Generator (HIG) and two Hand Pose Estimators (HPEs) on paired depth and infrared images via self-supervision using a consistency loss, guided by an existing HPE trained on paired depth and hand skeleton entries. The IR-based HPE is then refined on the weakly blurred infrared images. The qualitative and quantitative experiments demonstrate that the proposed method accurately estimates 3D hand poses under motion blur by fast hand motion, while existing depth-based methods fail. Our solution therefore supports fast 3D manipulation of virtual objects for augmented reality applications. Our model and dataset are publicly available for future research.1},
  keywords={Solid modeling;Three-dimensional displays;Pose estimation;Network architecture;Cameras;Skeleton;Augmented reality;Computing methodologies;Computer vision problems;Deep learning},
  doi={10.1109/ISMAR50242.2020.00086},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284715,
  author={Cao, Chong and Shi, Zhaowei and Yu, Miao},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Automatic Generation of Diegetic Guidance in Cinematic Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={600-607},
  abstract={One of the advantages of Cinematic Virtual Reality over traditional movies is that viewers can freely explore the virtual space. However, such freedom leads to a problem of missing some key events during watching. Therefore, directional visual guidance in the virtual space is vital for the viewer to follow the storyline and capture key events. Generating moving objects is one of the most commonly used diegetic guidance techniques which can implicitly guide the viewer's attention in the virtual space. In this paper, we investigate the formulation of key events in Cinematic Virtual Reality with event-of-interest script. Based on the formulation, we analyze the factors influencing diegetic guidance and propose an automatic-generating approach named Dynamic Diegetic Guidance. User study showed Dynamic Diegetic Guidance has a higher degree of immersion and takes less time to redirect viewers to key events compared with fixed guidance techniques, which makes the viewing experience more informative and entertaining.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Image synthesis;Immersive experience;Motion pictures;Space exploration;Human-centered computing—;Interaction paradigms—;Virtual reality—},
  doi={10.1109/ISMAR50242.2020.00087},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284797,
  author={Sayyad, Ehsan and Sra, Misha and Höllerer, Tobias},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Walking and Teleportation in Wide-area Virtual Reality Experiences}, 
  year={2020},
  volume={},
  number={},
  pages={608-617},
  abstract={Location-based or Out-of-Home Entertainment refers to experiences such as theme and amusement parks, laser tag and paintball arenas, roller and ice skating rinks, zoos and aquariums, or science centers and museums among many other family entertainment and cultural venues. More recently, location-based VR has emerged as a new category of out-of-home entertainment. These VR experiences can be likened to social entertainment options such as laser tag, where physical movement is an inherent part of the experience versus at-home VR experiences where physical movement often needs to be replaced by artificial locomotion techniques due to tracking space constraints. In this work, we present the first VR study to understand the impact of natural walking in a large physical space on presence and user preference. We compare it with teleportation in the same large space, since teleportation is the most commonly used locomotion technique for consumer, at-home VR. Our results show that walking was overwhelmingly preferred by the participants and teleportation leads to significantly higher self-reported simulator sickness. The data also shows a trend towards higher self-reported presence for natural walking.},
  keywords={Legged locomotion;Headphones;Laser theory;Virtual environments;Teleportation;Market research;Sports;Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00088},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284676,
  author={Englmeier, David and Fan, Fan and Butz, Andreas},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Rock or Roll – Locomotion Techniques with a Handheld Spherical Device in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={618-626},
  abstract={We investigate the use of a handheld spherical object as a controller for locomotion in VR. Rotating the object controls avatar movement in two different ways: As a zero order controller, it is continuously rotated to the target position as if rolling a ball on the floor. As a first order controller, it is tilted like a joystick to determine the direction and speed of movement. We describe how our prototype was built from low-cost commercially available hardware and discuss our design decisions. Then we evaluate both locomotion techniques in a user study (N=20) and compare them to established methods using handheld VR controllers. Our prototype matched and in some cases outperformed these methods regarding task time and accuracy. All results were obtained without any usage instructions, indicating easy learnability. Some of our insights may transfer to interaction with other naturally shaped objects in VR experiences.},
  keywords={Avatars;Prototypes;Rocks;Hardware;Task analysis;Floors;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00089},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284750,
  author={Lai, Chengyuan and McMahan, Ryan P.},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Cognitive Load and Usability of Three Walking Metaphors for Consumer Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={627-638},
  abstract={Walking metaphors have been extensively researched for travel in virtual reality (VR) applications. However, only a few walking metaphors are feasible for most consumer VR systems. In this paper, we present a study that compares three of these suitable metaphors: Scaled Walking, Human Joystick, and Walking-In-Place. Our study empirically assesses the cognitive loads and travel performances of these three walking metaphors by employing a novel dual-task methodology. We also evaluated their effects on simulator sickness, presence, and perceived usability. The results of our study indicate that Scaled Walking afforded significantly better travel performances and perceived usability than Human Joystick and Walking-In-Place. Our results also indicate that Walking-In-Place required the worst cognitive loads and that Human Joystick induced the worst simulator sickness. Given these results, we discuss the implications of using a high-fidelity, full-gait walking metaphor.},
  keywords={Legged locomotion;Usability;Augmented reality;Cognitive load;3D travel techniques;virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR50242.2020.00091},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284673,
  author={Gao, Peizhong and Matsumoto, Keigo and Narumi, Takuji and Hirose, Michitaka},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Visual-Auditory Redirection: Multimodal Integration of Incongruent Visual and Auditory Cues for Redirected Walking}, 
  year={2020},
  volume={},
  number={},
  pages={639-648},
  abstract={In this paper, we present a study of redirected walking (RDW) that shifts the positional relationship between visual and auditory cues during curvature manipulation. It has been shown that, when presented with incongruent visual and auditory spatial cues during a localization task, human observers integrate that information based on each cue's relative reliability, which determines their final perception of the target object's location. This multi-modal integration model is known as maximum likelihood estimation (MLE). By altering the visual location of objects that users perceive in virtual reality (VR) through auditory cues during redirection manipulation, we expect fewer users to notice the manipulation, which helps increase the usable curvature gain. Most existing studies on MLE in multi-modal integration have used random-dot stereograms as visual cues under stable motion states. In the present study, we first investigated whether this model holds while walking in VR environment. Our results indicate that in a walking state, users' perceptions of the target object's location shift toward auditory cue as the reliability of vision decreases, in keeping with the trend shown in previous studies on MLE. Based on this result, we then investigated the detection threshold of curvature gains during redirection manipulation under a condition with congruent visual-auditory cues as well as a condition in which users' location perceptions of the target object are considered to be affected by the incongruent auditory cue. We found that the detection threshold of curvature gains was higher with incongruent visual-auditory cues than with congruent cues. These results show that incongruent multimodal cues in VR may have a promising application in the area of redirected walking.},
  keywords={Legged locomotion;Visualization;Maximum likelihood estimation;Solid modeling;Observers;Reliability;Task analysis;Human-centered computing-Visualization-Visualization techniques-Treemaps;Human-centered computing-Visualization-Visualization design and evaluation methods},
  doi={10.1109/ISMAR50242.2020.00092},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284656,
  author={Fukushima, Shogo and Hamada, Takeo and Hautasaari, Ari},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparing World and Screen Coordinate Systems in Optical See-Through Head-Mounted Displays for Text Readability while Walking}, 
  year={2020},
  volume={},
  number={},
  pages={649-658},
  abstract={Augmented reality (AR) optical-see-through (OST) head-mounted displays (HMD) have developed to a point where browsing information on the go is possible. In this paper, we investigate the readability of text on an AR HMD while the user is walking. There are two common methods of displaying text on a HMD: anchoring the text on the screen coordinate system or the world coordinate system. We report on the results of two laboratory experiments comparing text readability when the text is displayed in these two coordinate systems, and while the participants walked on a treadmill. In the first experiment, the participants read letter strings comprising Sloane letters, whereas the second experiment used English words. In addition to evaluating the text readability and workload experienced by participants, we employed IMU sensors to compare the effects of the text display method on the participants' head movement and gait. In both experiments, the reading speed and head movement were significantly higher and mental workload significantly lower for the world coordinate system than for the screen coordinate system. These results suggest that text readability while walking is better on the world coordinate system, and displaying text with the screen coordinate system results in an unnatural gait owing to the user trying to keep their head still in an effort to stabilize the HMD screen.},
  keywords={Legged locomotion;Head;Head-mounted displays;Resists;Optical imaging;Optical sensors;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers},
  doi={10.1109/ISMAR50242.2020.00093},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284724,
  author={Leuze, Christoph and Sathyanarayana, Supriya and Daniel, Bruce L and McNab, Jennifer A},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Landmark-based mixed-reality perceptual alignment of medical imaging data and accuracy validation in living subjects}, 
  year={2020},
  volume={},
  number={},
  pages={659-664},
  abstract={Medical augmented reality (AR) applications where virtual renderings are aligned with the real world allow to visualize internal anatomy of the patient to a medical caregiver wearing an AR headset. Accurate alignment of virtual and real content is important for applications where the virtual rendering is used to guide the medical procedure such as a surgery. Compared to 2D AR applications, where the alignment accuracy can be directly measured on the 2D screen, 3D medical AR applications require alignment measurements using phantoms and external tracking systems. In this paper we present an approach for landmark-based alignment, validation and accuracy measurement of a 3D AR overlay of medical images on the real-world subject. This is done by performing an initial MRI of a subject’s head, an AR alignment task of the virtual rendering of the head MRI data to the subject’s real-world head using virtual fiducials, and a second MRI scan to test the accuracy of the AR alignment task. We have performed these 3D medical AR alignment measurements on seven volunteers using a MagicLeap AR head-mounted display. Across all seven volunteers we measured an alignment accuracy of $4.7 \pm 2.6$ mm. These results suggest that such an AR application can be a valuable tool for guiding non-invasive transcranial magnetic brain stimulation treatment. The presented MRI-based accuracy validation will furthermore be an important versatile tool to establish the safety of medical AR techniques.},
  keywords={Three-dimensional displays;Head;Magnetic resonance imaging;Rendering (computer graphics);Magnetic heads;Task analysis;Biomedical imaging;MRI;medical imaging;medical AR;accuracy;FDA;Mixed / augmented reality;Visualization design and evaluation methods},
  doi={10.1109/ISMAR50242.2020.00095},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284735,
  author={Krauß, Veronika and Uzun, Yücel},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Supporting Medical Auxiliary Work: The Central Sterile Services Department as a Challenging Environment for Augmented Reality Applications}, 
  year={2020},
  volume={},
  number={},
  pages={665-671},
  abstract={This paper reports on the central sterile services department (CSSD) as a potential new design space for future research in Human-Computer Interaction (HCI) and Augmented Reality (AR). Within the last 2 years, we explored processes, tools and user needs in this field to identify use cases with the capability of enhancing everyday work using AR head-mounted displays (HMD). The conducted research was focused on the potential of applying AR technology as a proof of concept in which 8 problem-driven use cases were identified. These use cases enable interesting aspects for future investigation and utilization of new technologies. In addition to that, this paper describes the insights into user groups, their tasks, challenges, and needs for work support in this specific domain. Furthermore, a sample application is introduced which demonstrates the possibilities of HMD-based AR in the CSSD.},
  keywords={Head-mounted displays;Resists;Tools;Task analysis;Augmented reality;Human-centered computing;Human computer Interaction (HCI);Applied Computing;Life and medical sciences},
  doi={10.1109/ISMAR50242.2020.00096},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284707,
  author={Zhao, Shang and Xiao, Xiao and Wang, Qiyue and Zhang, Xiaoke and Li, Wei and Soghier, Lamia and Hahn, James},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Intelligent Augmented Reality Training Framework for Neonatal Endotracheal Intubation}, 
  year={2020},
  volume={},
  number={},
  pages={672-681},
  abstract={Neonatal Endotracheal Intubation (ETI) is a critical resuscitation skill that requires tremendous practice of trainees before clinical exposure. However, current manikin-based training regimen is ineffective in providing satisfactory real-time procedural guidance for accurate assessment due to the lack of see-through visualization within the manikin. The training efficiency is further reduced by the limited availability of expert instructors, which inevitably results in a long learning curve for trainees. To this end, we propose an intelligent Augmented Reality (AR) training framework that provides trainees with a complete visualization of the ETI procedure for real-time guidance and assessment. Specifically, the proposed framework is capable of capturing the motions of the laryngoscope and the manikin and offer 3D see-through visualization rendered to the head-mounted display (HMD). Furthermore, an attention-based Convolutional Neural Network (CNN) model is developed to automatically assess the ETI performance from the captured motions as well as identify regions of motions that significantly contribute to the performance evaluation. Lastly, augmented user-friendly feedback is delivered with interpretable results with the ETI scoring rubric through the color-coded motion trajectory that classifies highlighted regions that need more practice. The classification accuracy of our machine learning model is 84.6%.},
  keywords={Training;Performance evaluation;Visualization;Pediatrics;Real-time systems;Trajectory;Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computing methodologies;Modeling and simulation;Simulation types and techniques;Real-time simulation;Computing methodologies;Machine learning;Learning paradigms;Supervised learning;Human-centered computing;Visualization;Visualization techniques;Heat maps},
  doi={10.1109/ISMAR50242.2020.00097},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284775,
  author={Krösl, Katharina and Elvezio, Carmine and Luidolt, Laura R. and Hürbe, Matthias and Karst, Sonja and Feiner, Steven and Wimmer, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={CatARact: Simulating Cataracts in Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={682-693},
  abstract={For our society to be more inclusive and accessible, the more than 2.2 billion people worldwide with limited vision should be considered more frequently in design decisions, such as architectural planning. To help architects in evaluating their designs and give medical personnel some insight on how patients experience cataracts, we worked with ophthalmologists to develop the first medically-informed, pilot-studied simulation of cataracts in eye-tracked augmented reality (AR). To test our methodology and simulation, we conducted a pilot study with cataract patients between surgeries of their two cataract-affected eyes. Participants compared the vision of their corrected eye, viewing through simulated cataracts, to that of their still affected eye, viewing an unmodified AR view. In addition, we conducted remote experiments via video call, live adjusting our simulation and comparing it to related work, with participants who had cataract surgery a few months before. We present our findings and insights from these experiments and outline avenues for future work.},
  keywords={Cataracts;Design methodology;Computational modeling;Surgery;Planning;Personnel;Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception;Applied computing;Life and medical sciences;Health informatics;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR50242.2020.00098},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284660,
  author={Moore, Alec G. and McMahan, Ryan P. and Dong, Hailiang and Ruozzi, Nicholas},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Extracting Velocity-Based User-Tracking Features to Predict Learning Gains in a Virtual Reality Training Application}, 
  year={2020},
  volume={},
  number={},
  pages={694-703},
  abstract={Virtual Reality (VR) for training and education of real-world tasks has been researched extensively and has growing use in industry. The data generated by trainees in VR could be leveraged to improve the ability to evaluate learning beyond that which is possible in traditional training scenarios. In this paper, we present a machine learning approach that is able to classify users into participants with low-learning (LL) and high-learning (HL) gains, based on a knowledge test, using only the linear and angular velocities of the head-mounted display (HMD) and handheld controllers. To collect this data, we conduct a VR training user study. We demonstrate that even with a limited data set, it is possible to train a machine learning classifier to predict a trainee's learning performance for a given task with high degrees of accuracy and confidence. We investigate three different sets of velocity-based input features and two feature representations in a machine learning experiment. Our results indicate that all feature combinations resulted in high degrees of accuracy and confidence for predicting learning gains in our testing data. By employing a novel visualization technique, we were able to determine that participants with HL gains moved with greater velocities and fewer changes in direction than those with LL gains. These results indicate that it may be feasible to create VR training applications that can predict a user's learning gains and dynamically adapt the training to better support the user's learning, based on commonly available tracking data.},
  keywords={Training;Solid modeling;Resists;Machine learning;Feature extraction;Task analysis;Testing;Feature extraction;machine learning;virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Machine learning;Machine learning algorithms;Feature selection},
  doi={10.1109/ISMAR50242.2020.00099},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284665,
  author={Whitlock, Matt and Szafir, Danielle Albers and Gruchalla, Kenny},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={HydrogenAR: Interactive Data-Driven Presentation of Dispenser Reliability}, 
  year={2020},
  volume={},
  number={},
  pages={704-712},
  abstract={When delivering presentations to a co-located audience, we typically use slides with text and 2D graphics to complement the spoken narrative. Though presentations have largely been explored on 2D media, augmented reality (AR) allows presentation designers to add data and augmentations to existing physical infrastructure on display. This coupling could provide a more engaging experience to the audience and support comprehension. With HydrogenAR, we present a novel application that leverages the benefits of data-driven storytelling with those of AR to explain the unique challenges of hydrogen dispenser reliability. Utilizing physical props, situated data, and virtual augmentations and animations, HydrogenAR serves as a unique presentation tool, particularly critical for stakeholders, tour groups, and VIPs. HydrogenAR is a product of multiple collaborative design iterations with a local Hydrogen Fuel research team and is evaluated through interviews with team members and a user study with end-users to evaluate the usability and quality of the interactive AR experience. Through this work, we provide design considerations for AR data-driven presentations and discuss how AR could be used for innovative content delivery beyond traditional slide-based presentations.},
  keywords={Hydrogen;Two dimensional displays;Media;Reliability;Fuels;Usability;Augmented reality},
  doi={10.1109/ISMAR50242.2020.00101},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284718,
  author={Monteiro, Diego and Liang, Hai-Ning and Wang, Jialin and Chen, Hao and Baghaei, Nilufar},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An In-Depth Exploration of the Effect of 2D/3D Views and Controller Types on First Person Shooter Games in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={713-724},
  abstract={The amount of interest in Virtual Reality (VR) research has significantly increased over the past few years, both in academia and industry. The release of commercial VR Head-Mounted Displays (HMDs) has been a major contributing factor. However, there is still much to be learned, especially how views and input techniques, as well as their interaction, affect the VR experience. There is little work done on First-Person Shooter (FPS) games in VR, and those few studies have focused on a single aspect of VR FPS. They either focused on the view, e.g., comparing VR to a typical 2D display or on the controller types. To the best of our knowledge, there are no studies investigating variations of 2D/3D views in HMDs, controller types, and their interactions. As such, it is challenging to distinguish findings related to the controller type from those related to the view. If a study does not control for the input method and finds that 2D displays lead to higher performance than VR, we cannot generalize the results because of the confounding variables. To understand their interaction, we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the way it is controlled that gives the platforms their respective advantages. To study the effects of the 2D/3D views, we created a 2D visual technique, PlaneFrame, that was applied inside the VR headset. Our results show that the controller type can have a significant positive impact on performance, immersion, and simulator sickness when associated with a 2D view. They further our understanding of the interactions that controllers and views have and demonstrate that comparisons are highly dependent on how both factors go together. Further, through a series of three experiments, we developed a technique that can lead to a substantial performance, a good level of immersion, and can minimize the level of simulator sickness.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Two dimensional displays;Keyboards;Games;Turning;Virtual Reality;2D/3D Views;Controller types;First Person Shooter;Gaming;Head-Mounted Displays},
  doi={10.1109/ISMAR50242.2020.00102},
  ISSN={1554-7868},
  month={Nov},}@INPROCEEDINGS{9284777,
  author={},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Author Index}, 
  year={2020},
  volume={},
  number={},
  pages={725-729},
  abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
  keywords={},
  doi={10.1109/ISMAR50242.2020.00103},
  ISSN={1554-7868},
  month={Nov},}
