@INPROCEEDINGS{8613745,
  author={Kunert, Christian and Schwandt, Tobias and Broll, Wolfgang},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Efficient Point Cloud Rasterization for Real Time Volumetric Integration in Mixed Reality Applications}, 
  year={2018},
  volume={},
  number={},
  pages={1-9},
  abstract={Real-time capable simultaneous localization and mapping (SLAM) approaches applying consumer hardware have been extensively researched in recent years. Their 3D reconstruction typically applies voxel volumes stored in regular grid hierarchies, sparse voxel octrees or voxel hash tables. They represent the model implicitly in the form of a truncated signed distance function (TSDF). Data integration is usually achieved by stepping through the reconstruction hierarchy from top to bottom and checking voxel grids against the new input data or by rasterizing input data to find associated voxels. For hierarchical representations, a major challenge remains the efficient determination of relevant portions of the reconstruction to be modified by new input data. We present a novel approach efficiently rasterizing input point clouds into intermediate volumes by the GPU. Our technique performs a simple preprocessing step on the input data to properly account for the TSDF representation, allowing for an accurate and hole-free reconstruction. We show that our approach is well suited for a fast integration of new input data into the hierarchical 3D reconstruction, allowing for real-time performance while only slightly increasing memory consumption.},
  keywords={Three-dimensional displays;Simultaneous localization and mapping;Cameras;Real-time systems;Surface reconstruction;Hardware;Pipelines;Simultaneous localization and mapping;3D reconstruction;Voxelization;real time;volumetric surface integration},
  doi={10.1109/ISMAR.2018.00023},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613746,
  author={Runz, Martin and Buffier, Maud and Agapito, Lourdes},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects}, 
  year={2018},
  volume={},
  number={},
  pages={10-20},
  abstract={We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera. As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable realtime object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic. Code will be made available.},
  keywords={Semantics;Simultaneous localization and mapping;Real-time systems;Three-dimensional displays;Image reconstruction;Image segmentation;Cameras},
  doi={10.1109/ISMAR.2018.00024},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613747,
  author={Moser, Kenneth R. and Arefin, Mohammed Safayet and Swan, J. Edward},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Impact of Alignment Point Distance and Posture on SPAAM Calibration of Optical See-Through Head-Mounted Displays}, 
  year={2018},
  volume={},
  number={},
  pages={21-30},
  abstract={The use of Optical See-Through (OST) technology for presenting Augmented Reality (AR) experiences is becoming more common. However, OST-AR displays require a calibration procedure, in order to determine the location of the users eyes. Currently, the predominantly cited manual calibration technique is the Single Point Active Alignment Method (SPAAM). However, with the SPAAM technique, there remains uncertainty about the causes of poor calibration results. This paper reports an experiment which examined the influence of two factors on SPAAM accuracy and precision: alignment point distribution, and user posture. Alignment point distribution is examined at user-centered reaching distances, 0.15 to 0.3 meters, as well as environment-centered room-scale distances, 0.5 to 2.0 meters. User posture likely contributes to misalignment error, and is examined at the levels of sitting and standing. In addition, a control condition replaces the user with a rigidly-mounted camera, and mounts the OST display on a precisely-adjustable tripod. The experiment finds that user-centric distributions are more accurate than environment-centric distributions, and, somewhat surprisingly, that the users posture has no effect. The control condition replicates these findings. The implication is that alignment point distribution is the predominant mode for induction of calibration error for SPAAM calibration procedures.},
  keywords={Calibration;Cameras;Resists;Adaptive optics;Manuals;Optical imaging},
  doi={10.1109/ISMAR.2018.00025},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613748,
  author={Rojtberg, Pavel and Kuijper, Arjan},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Efficient Pose Selection for Interactive Camera Calibration}, 
  year={2018},
  volume={},
  number={},
  pages={31-36},
  abstract={The choice of poses for camera calibration with planar patterns is only rarely considered - yet the calibration precision heavily depends on it. This work presents a pose selection method that finds a compact and robust set of calibration poses and is suitable for interactive calibration. Consequently, singular poses that would lead to an unreliable solution are avoided explicitly, while poses reducing the uncertainty of the calibration are favoured. For this, we use uncertainty propagation. Our method takes advantage of a self-identifying calibration pattern to track the camera pose in real-time. This allows to iteratively guide the user to the target poses, until the desired quality level is reached. Therefore, only a sparse set of key-frames is needed for calibration. The method is evaluated on separate training and testing sets, as well as on synthetic data. Our approach performs better than comparable solutions while requiring 30% less calibration frames.},
  keywords={Calibration;Cameras;Distortion;Three-dimensional displays;Estimation;Image analysis;Uncertainty;interactive;calibration;vision},
  doi={10.1109/ISMAR.2018.00026},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613749,
  author={Domínguez-Conti, Javier and Yin, Jianfeng and Alami, Yacine and Civera, Javier},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Visual-Inertial SLAM Initialization: A General Linear Formulation and a Gravity-Observing Non-Linear Optimization}, 
  year={2018},
  volume={},
  number={},
  pages={37-45},
  abstract={The initialization is one of the less reliable pieces of Visual-Inertial SLAM (VI-SLAM) and Odometry (VI-O). The estimation of the initial state (camera poses, IMU states and landmark positions) from the first data readings lacks the accuracy and robustness of other parts of the pipeline, and most algorithms have high failure rates and/or initialization delays up to tens of seconds. Such initialization is critical for AR systems, as the failures and delays of the current approaches can ruin the user experience or mandate impractical guided calibration. In this paper we address the state initialization problem using a monocular-inertial sensor setup, the most common in AR platforms. Our contributions are 1) a general linear formulation to obtain an initialization seed, and 2) a non-linear optimization scheme, including gravity, to refine the seed. Our experimental results, in a public dataset, show that our approach improves the accuracy and robustness of current VI state initialization schemes.},
  keywords={Cameras;Feature extraction;Simultaneous localization and mapping;Gravity;Mathematical model;Optimization;Robustness;Visual Inertial SLAM;Visual Inertial Localization;Visual Inertial Mapping;Visual Inertial Initialization;Sensor Fusion},
  doi={10.1109/ISMAR.2018.00027},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613750,
  author={Wisely Babu, Benzun Pious and Yan, Zhixin and Ye, Mao and Ren, Liu},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={On Exploiting Per-Pixel Motion Conflicts to Extract Secondary Motions}, 
  year={2018},
  volume={},
  number={},
  pages={46-56},
  abstract={Ubiquitous Augmented Reality requires robust localization in complex daily environments. The combination of camera and Inertial Mersurement Unit (IMU) has shown promising results for robust localization due to the complementary characteristics of the visual and inertial modalities. However, there exists many cases where the measurements from visual and inertial modalities do not provide a single consistent motion estimate thus causing disagreement on the estimated motion. Limited literature has addressed this problem associated with sensor fusion for localization. Since the disagreement is not a result of measurement noises, existing outlier rejection techniques are not suitable to address this problem. In this paper, we propose a novel approach to handle the disagreement as motion conflict with two key components. The first one is a generalized Hidden Markov Model (HMM) that formulates the tracking and management of the primary motion and the secondary motion as a single estimation problem. The second component is an epipolar constrained Deep Neural Network that generates a per-pixel motion conflict probability map. Experimental evaluations demonstrate significant improvement to the tracking accuracy in cases of strong motion conflict compared to previous state-of-the-art algorithms for localization. Moreover, as a consequence of motion tracking on the secondary maps, our solution enables augmentation of virtual content attached to secondary motions, which brings us one step closer to Ubiquitous Augmented Reality.},
  keywords={Tracking;Visualization;Dynamics;Simultaneous localization and mapping;Hidden Markov models;Cameras;Visual Inertial Odometry;Deep Neural Network;Camera Pose Tracking;Motion Conflict;Sensor Fusion;Augmented Reality},
  doi={10.1109/ISMAR.2018.00028},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613751,
  author={Park, Gabyong and Woo, Woontack},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Hybrid 3D Hand Articulations Tracking Guided by Classification and Search Space Adaptation}, 
  year={2018},
  volume={},
  number={},
  pages={57-69},
  abstract={We propose a novel method for model-based 3D tracking of hand articulations that is effective even for fastmoving hand postures in depth images. A large number of augmented reality (AR) and virtual reality (VR) studies have used model-based approaches for estimating hand postures and tracking movements. However, these approaches exhibit limitations if the hand moves rapidly or into the camera's field of view. To overcome these problems, researchers attempted a hybrid strategy that uses multiple initializations for 3D tracking of articulations. However, this strategy also exhibits limitations. For example, in genetic optimization, the hypotheses generated from the previous solution may search for a solution in an incorrect search space in a fast-moving hand gesture. This problem also occurs if the search space selected from the results of a trained model does not cover the true solution although the tracked hand moves slowly. Our proposed method estimates the hand pose based on model-based tracking guided by classification and search space adaptation. From the classification by a convolutional neural network (CNN), a data-driven prior is included in the objective function and additional hypotheses are generated in particle swarm optimization (PSO). In addition, the search spaces of the two sets of the hypotheses, generated by the data-driven prior and the previous solution, are adaptively updated using the distribution of each set of the hypotheses. We demonstrated the effectiveness of the proposed method by applying it to an American Sign Language (ASL) dataset consisting of fast-moving hand postures. The experimental results demonstrate that the proposed algorithm exhibits more accurate tracking results compared to other state-of-the-art tracking algorithms.},
  keywords={Adaptation models;Tracking;Solid modeling;Three-dimensional displays;Estimation;Optimization;Assistive technology;Computing methodologies;Computer vision problems;Tracking},
  doi={10.1109/ISMAR.2018.00029},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613752,
  author={Jung, Jinki and Lee, Hyeopwoo and Choi, Jeehye and Nanda, Abhilasha and Gruenefeld, Uwe and Stratmann, Tim and Heuten, Wilko},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Ensuring Safety in Augmented Reality from Trade-off Between Immersion and Situation Awareness}, 
  year={2018},
  volume={},
  number={},
  pages={70-79},
  abstract={Although the mobility and emerging technology of augmented reality (AR) have brought significant entertainment and convenience in everyday life, the use of AR is becoming a social problem as the accidents caused by a shortage of situation awareness due to an immersion of AR are increasing. In this paper, we address the trade-off between immersion and situation awareness as the fundamental factor of the AR-related accidents. As a solution against the trade-off, we propose a third-party component that prevents pedestrian-vehicle accidents in a traffic environment based on vehicle position estimation (VPE) and vehicle position visualization (VPV). From a RGB image sequence, VPE efficiently estimates the relative 3D position between a user and a car using generated convolutional neural network (CNN) model with a region-of-interest based scheme. VPV shows the estimated car position as a dot using an out-of-view object visualization method to alert the user from possible collisions. The VPE experiment with 16 combinations of parameters showed that the InceptionV3 model, fine-tuned on activated images yields the best performance with a root mean squared error of 0.34 m in 2.1 ms. The user study of VPV showed the inversely proportional relationship between the immersion controlled by the difficulty of the AR game and the frequency of situation awareness in both quantitatively and qualitatively. Additional VPV experiment assessing two out-of-view object visualization methods (EyeSee360 and Radar) showed no significant effect on the participants' activity, while EyeSee360 yielded faster responses and Radar engendered participants' preference on average. Our field study demonstrated an integration of VPE and VPV which has potentials for safety-ensured immersion when the proposed component is used for AR in daily uses. We expect that when the proposed component is developed enough to be used in real world, it will contribute to the safety-ensured AR, as well as to the population of AR.},
  keywords={Estimation;Accidents;Visualization;Three-dimensional displays;Safety;Automobiles;Two dimensional displays;augmented reality;safety;warning system;evaluation;user study},
  doi={10.1109/ISMAR.2018.00032},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613753,
  author={Kanamori, Kohei and Sakata, Nobuchika and Tominaga, Tomu and Hijikata, Yoshinori and Harada, Kensuke and Kiyokawa, Kiyoshi},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Obstacle Avoidance Method in Real Space for Virtual Reality Immersion}, 
  year={2018},
  volume={},
  number={},
  pages={80-89},
  abstract={Typical Head-Mounted Displays (HMDs) that provide a highly immersive Virtual Reality (VR) experience make any interaction between a user and real space difficult by occluding the user's entire field of view. Video see-through type HMDs can solve this problem by superimposing real-space information on the VR environment. The existing method of supporting interactions with the real space is superimposition of boundary lines of the real space on the virtual space in the HMD. However, overlaying the boundary lines on the entire field of view may reduce the user's immersive feeling. In this paper, we propose two methods to support interactions with the real world while playing immersive VR games without reducing the user's immersive feeling as much as possible, even when the user wanders. The first method is to superimpose a 3D point cloud of real space around the user on the virtual space in the HMD. The second method is to deploy familiar objects (e.g., furniture in his/her room) in the virtual space in the HMD. The user traces the familiar objects as subgoals to reach the goal. We implement the two methods and conduct a user study to compare interaction performance. As a result of the user study, we find that the second method provides better spatial information about the real space without reducing the user's immersive feeling, compared to the existing method.},
  keywords={Resists;Three-dimensional displays;Legged locomotion;Virtual environments;Switches;Image color analysis;Virtual Reality;immersive feeling;walking assistance},
  doi={10.1109/ISMAR.2018.00033},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613754,
  author={Geronazzo, Michele and Sikström, Erik and Kleimola, Jari and Avanzini, Federico and de Götzen, Amalia and Serafin, Stefania},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Impact of an Accurate Vertical Localization with HRTFs on Short Explorations of Immersive Virtual Reality Scenarios}, 
  year={2018},
  volume={},
  number={},
  pages={90-97},
  abstract={Achieving a full 3D auditory experience with head-related transfer functions (HRTFs) is still one of the main challenges of spatial audio rendering. HRTFs capture the listener's acoustic effects and personal perception, allowing immersion in virtual reality (VR) applications. This paper aims to investigate the connection between listener sensitivity in vertical localization cues and experienced presence, spatial audio quality, and attention. Two VR experiments with head-mounted display (HMD) and animated visual avatar are proposed: (i) a screening test aiming to evaluate the participants' localization performance with HRTFs for a non-visible spatialized audio source, and (ii) a 2 minute free exploration of a VR scene with five audiovisual sources in a both non-spatialized (2D stereo panning) and spatialized (free-field HRTF rendering) listening conditions. The screening test allows a distinction between good and bad localizers. The second one shows that no biases are introduced in the quality of the experience (QoE) due to different audio rendering methods; more interestingly, good localizers perceive a lower audio latency and they are less involved in the visual aspects.},
  keywords={Rendering (computer graphics);Acoustics;Virtual environments;Visualization;Solid modeling;Ear;virtual reality;spatial audio rendering;head related transfer function;auditory vertical localization;personalization;quality of the experience},
  doi={10.1109/ISMAR.2018.00034},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613755,
  author={Liu, Chang and Plopski, Alexander and Kiyokawa, Kiyoshi and Ratsamee, Photchara and Orlosky, Jason},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={IntelliPupil: Pupillometric Light Modulation for Optical See-Through Head-Mounted Displays}, 
  year={2018},
  volume={},
  number={},
  pages={98-104},
  abstract={In practical use of optical see-through head-mounted displays, users often have to adjust the brightness of virtual content to ensure that it is at the optimal level. Automatic adjustment is still a challenging problem, largely due to the bidirectional nature of the structure of the human eye, complexity of real world lighting, and user perception. Allowing the right amount of light to pass through to the retina requires a constant balance of incoming light from the real world, additional light from the virtual image, pupil contraction, and feedback from the user. While some automatic light adjustment methods exist, none have completely tackled this complex input-output system. As a step towards overcoming this issue, we introduce IntelliPupil, an approach that uses eye tracking to properly modulate augmentation lighting for a variety of lighting conditions and real scenes. We first take the data from a small form factor light sensor and changes in pupil diameter from an eye tracking camera as passive inputs. This data is coupled with user-controlled brightness selections, allowing us to fit a brightness model to user preference using a feed-forward neural network. Using a small amount of training data, both scene luminance and pupil size are used as inputs into the neural network, which can then automatically adjust to a user's personal brightness preferences in real time. Experiments in a high dynamic range AR scenario with varied lighting show that pupil size is just as important as environment light for optimizing brightness and that our system outperforms linear models.},
  keywords={Brightness;Lighting;Cameras;Gaze tracking;Dynamic range;Resists;Three-dimensional displays;Head mounted displays, optical see through, eye tracking, augmented reality, lighting adjustment},
  doi={10.1109/ISMAR.2018.00037},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613756,
  author={Kim, Kangsoo and Boelling, Luke and Haesler, Steffen and Bailenson, Jeremy and Bruder, Gerd and Welch, Greg F.},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Does a Digital Assistant Need a Body? The Influence of Visual Embodiment and Social Behavior on the Perception of Intelligent Virtual Agents in AR}, 
  year={2018},
  volume={},
  number={},
  pages={105-114},
  abstract={Intelligent Virtual Agents (IVAs) are becoming part of our everyday life, thanks to artificial intelligence technology and Internet of Things devices. For example, users can control their connected home appliances through natural voice commands to the IVA. However, most current-state commercial IVAs, such as Amazon Alexa, mainly focus on voice commands and voice feedback, and lack the ability to provide non-verbal cues which are an important part of social interaction. Augmented Reality (AR) has the potential to overcome this challenge by providing a visual embodiment of the IVA. In this paper we investigate how visual embodiment and social behaviors influence the perception of the IVA. We hypothesize that a user's confidence in an IVA's ability to perform tasks is improved when imbuing the agent with a human body and social behaviors compared to the agent solely depending on voice feedback. In other words, an agent's embodied gesture and locomotion behavior exhibiting awareness of the surrounding real world or exerting influence over the environment can improve the perceived social presence with and confidence in the agent. We present a human-subject study, in which we evaluated the hypothesis and compared different forms of IVAs with speech, gesturing, and locomotion behaviors in an interactive AR scenario. The results show support for the hypothesis with measures of confidence, trust, and social presence. We discuss implications for future developments in the field of IVAs.},
  keywords={Visualization;Internet of Things;Robots;Avatars;Augmented reality;Face;Atmospheric measurements;Intelligent virtual agents;digital assistants;social interaction;presence;confidence;trust in technology;augmented reality},
  doi={10.1109/ISMAR.2018.00039},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613757,
  author={Rietzler, Michael and Gugenheimer, Jan and Hirzle, Teresa and Deubzer, Martin and Langbehn, Eike and Rukzio, Enrico},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Rethinking Redirected Walking: On the Use of Curvature Gains Beyond Perceptual Limitations and Revisiting Bending Gains}, 
  year={2018},
  volume={},
  number={},
  pages={115-122},
  abstract={Redirected walking (RDW) allows virtual reality (VR) users to walk infinitely while staying inside a finite physical space through subtle shifts (gains) of the scene to redirect them back inside the volume. All prior approaches measure the feasibility of RDW techniques based on if the user perceives the manipulation, leading to rather small applicable gains. However, we treat RDW as an interaction technique and therefore use visually perceivable gains instead of using the perception of manipulation. We revisited prior experiments with focus on applied gains and additionally tested higher gains on the basis of applicability in a user study. We found that users accept curvature gains up to 20°/m, which reduces the necessary physical volume down to approximately 6x6m for virtually walking infinitely straight ahead. Our findings strife to rethink the usage of redirection from being unperceived to being applicable and natural.},
  keywords={Legged locomotion;Measurement;Meters;Navigation;Virtual environments;Teleportation;Augmented reality},
  doi={10.1109/ISMAR.2018.00041},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613758,
  author={Normand, Erwan and McGuffin, Michael J.},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enlarging a Smartphone with AR to Create a Handheld VESAD (Virtually Extended Screen-Aligned Display)}, 
  year={2018},
  volume={},
  number={},
  pages={123-133},
  abstract={We investigate using augmented reality to extend the screen of a smartphone beyond its physical limits with a virtual surface that is co-planar with the phone and that follows as the phone is moved. We call this extension a VESAD, or Virtually Extended Screen-Aligned Display. We illustrate and describe several ways that a VESAD could be used to complement the physical screen of a phone, and describe two novel interaction techniques: one where the user performs a quick rotation of the phone to switch the information shown in the VESAD, and another called "slide-and-hang" whereby the user can detach a VESAD and leave it hanging in mid-air, using the phone to establish the initial position and orientation of the virtual window. We also report an experiment that compared three interfaces used for an abstract classification task: the first using only a smartphone, the second using the phone for input but with a VESAD for output, and the third where the user performed input in mid-air on the VESAD (as detected by a Leap Motion). The second user interface was found to be superior in time and selection count (a metric of mistakes committed by users) and was also subjectively preferred over the other two interfaces. This demonstrates the added value of a VESAD for output over a phone's physical screen, and also demonstrates that input on the phone's screen was better than input in mid-air in our experiment.},
  keywords={Microsoft Windows;Navigation;Resists;Switches;Task analysis;Hardware;User interfaces;augmented reality;HMD;smartphone},
  doi={10.1109/ISMAR.2018.00043},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613759,
  author={Werrlich, Stefan and Daniel, Austino and Ginger, Alexandra and Nguyen, Phuc-Anh and Notni, Gunther},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparing HMD-Based and Paper-Based Training}, 
  year={2018},
  volume={},
  number={},
  pages={134-142},
  abstract={Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ.},
  keywords={Task analysis;Training;Engines;Software;Resists;Atmospheric measurements;Particle measurements;Augmented Reality;Evaluation;Head Mounted Displays;Training},
  doi={10.1109/ISMAR.2018.00046},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613760,
  author={Lu, Feiyu and Yu, Difeng and Liang, Hai-Ning and Chen, Wenjun and Papangelis, Konstantinos and Ali, Nazlena Mohamad},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating Engagement Level and Analytical Support of Interactive Visualizations in Virtual Reality Environments}, 
  year={2018},
  volume={},
  number={},
  pages={143-152},
  abstract={Interactive visualizations are external cognitive artifacts aimed at supporting users' exploratory and sense-making activities. In recent years, there has been an explosion of commercial virtual reality (VR) head-mounted displays (HMD). These VR devices are meant to offer high levels of engagement and improve users' analytical exploration of the displayed content. However, given their rapid market introduction, the possible influences and usefulness that VR could bring in terms of supporting users' exploration with interactive visualizations remain largely underexplored. We attempt to fill this gap and provide results of an empirical study of an interactive visualization tool that we have developed for a VR HMD system. This tool is aimed at facilitating exploratory and analytical reasoning activities with 3D shapes and their transformational processes. Overall, the results show that the tool is supportive of users' exploratory and analytical activities based on the significant improvement in their post-experiment test scores (when compared to their pre-experiment ones) and their engagement level measured via a user engagement questionnaire and participants' comments. The results shed a positive light on the use of visualizations in VR environments and can inform the design of these tools of domains beyond 3D transformational geometry.},
  keywords={Tools;Visualization;Three-dimensional displays;Solids;Shape;Virtual reality;Geometry;Virtual reality;Interactive visualizations;User engagement;3D geometry;Analytical Reasoning;Sense making},
  doi={10.1109/ISMAR.2018.00050},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613761,
  author={Lee, Gun A. and Teo, Theophilus and Kim, Seungwon and Billinghurst, Mark},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A User Study on MR Remote Collaboration Using Live 360 Video}, 
  year={2018},
  volume={},
  number={},
  pages={153-164},
  abstract={Sharing and watching live 360 panorama video is available on modern social networking platforms, yet the communication is often a passive one-directional experience. This research investigates how to further improve live 360 panorama based remote collaborative experiences by adding Mixed Reality (MR) cues. SharedSphere is a wearable MR remote collaboration system that enriches a live captured immersive panorama based collaboration through MR visualisation of non-verbal communication cues (e.g., view awareness and gestures cues). We describe the design and implementation details of the prototype system, and report on a user study investigating how MR live panorama sharing affects the user's collaborative experience. The results showed that providing view independence through sharing live panorama enhances co-presence in collaboration, and the MR cues help users understanding each other. Based on the study results we discuss design implications and future research direction.},
  keywords={Collaboration;Streaming media;Virtual reality;Visualization;Cameras;Three-dimensional displays;Two dimensional displays;Mixed Reality;Augmented Reality;remote collaboration;live panorama sharing;view independence},
  doi={10.1109/ISMAR.2018.00051},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613762,
  author={Dey, Arindam and Chen, Hao and Zhuang, Chang and Billinghurst, Mark and Lindeman, Robert W.},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of Sharing Real-Time Multi-Sensory Heart Rate Feedback in Different Immersive Collaborative Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={165-173},
  abstract={Collaboration is an important application area for virtual reality (VR). However, unlike in the real world, collaboration in VR misses important empathetic cues that can make collaborators aware of each other’s emotional states. Providing physiological feedback, such as heart rate or respiration rate, to users in VR has been shown to create a positive impact in single user environments. In this paper, through a rigorous mixed-factorial user experiment, we evaluated how providing heart rate feedback to collaborators influences their collaboration in three different environments requiring different kinds of collaboration. We have found that when provided with real-time heart rate feedback participants felt the presence of the collaborator more and felt that they understood their collaborator’s emotional state more. Heart rate feedback also made participants feel more dominant when performing the task. We discuss the implication of this research for collaborative VR environments, provide design guidelines, and directions for future research.},
  keywords={Heart rate;Collaboration;Real-time systems;Physiology;Avatars;Task analysis;Virtual environments},
  doi={10.1109/ISMAR.2018.00052},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613763,
  author={},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Author Index}, 
  year={2018},
  volume={},
  number={},
  pages={175-176},
  abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
  keywords={},
  doi={10.1109/ISMAR.2018.00054},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8613764,
  author={},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={[Publisher's information]}, 
  year={2018},
  volume={},
  number={},
  pages={178-178},
  abstract={Provides a listing of current committee members and society officers.},
  keywords={},
  doi={10.1109/ISMAR.2018.00055},
  ISSN={1554-7868},
  month={Oct},}
