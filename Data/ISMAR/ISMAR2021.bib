@INPROCEEDINGS{9583738,
  author={Liu, Jingyu and Mantel, Claire and Forchhammer, Søren},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perception-Driven Hybrid Foveated Depth of Field Rendering for Head-Mounted Displays}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={In this paper, we present a novel perception-driven hybrid rendering method leveraging the limitation of the human visual system (HVS). Features accounted in our model include: foveation from the visual acuity eccentricity (VAE), depth of field (DOF) from vergence & accommodation, and longitudinal chromatic aberration (LCA) from color vision. To allocate computational workload efficiently, first we apply a gaze-contingent geometry simplification. Then we convert the coordinates from screen space to polar space with a scaling strategy coherent with VAE. Upon that, we apply a stochastic sampling based on DOF. Finally, we post-process the Bokeh for DOF, which can at the same time achieve LCA and anti-aliasing. A virtual reality (VR) experiment on 6 Unity scenes with a head-mounted display (HMD) HTC VIVE Pro Eye yields frame rates range from 25.2 to 48.7 fps. Objective evaluation with FovVideoVDP - a perceptual based visible difference metric - suggests that the proposed method gives satisfactory just-objectionable-difference (JOD) scores across 6 scenes from 7.61 to 8.69 (in a 10 unit scheme). Our method achieves better performance compared with the existing methods while having the same or better level of quality scores.},
  keywords={Geometry;Measurement;Visualization;Solid modeling;Head-mounted displays;Stochastic processes;Resists;Computing methodologies;Rendering;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00014},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583797,
  author={Wang, Zhimin and Zhao, Yuxin and Liu, Yunfei and Lu, Feng},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Edge-Guided Near-Eye Image Analysis for Head Mounted Displays}, 
  year={2021},
  volume={},
  number={},
  pages={11-20},
  abstract={Eye tracking provides an effective way for interaction in Augmented Reality (AR) Head Mounted Displays (HMDs). Current eye tracking techniques for AR HMDs require eye segmentation and ellipse fitting under near-infrared illumination. However, due to the low contrast between sclera and iris regions and unpredictable reflections, it is still challenging to accomplish accurate iris/pupil segmentation and the corresponding ellipse fitting tasks. In this paper, inspired by the fact that most essential information is encoded in the edge areas, we propose a novel near-eye image analysis method with edge maps as guidance. Specifically, we first utilize an Edge Extraction Network ($E^{2}-$Net) to predict high-quality edge maps, which only contain eyelids and iris/pupil contours without other undesired edges. Then we feed the edge maps into an Edge-Guided Segmentation and Fitting Network (ESF-Net) for accurate segmentation and ellipse fitting. Extensive experimental results demonstrate that our method outperforms current state-of-the-art methods in near-eye image segmentation and ellipse fitting tasks, based on which we present applications of eye tracking with AR HMD.},
  keywords={Image segmentation;Iris;Image analysis;Head;Image edge detection;Fitting;Gaze tracking;Augmented Reality;Eye tracking;Near-eye image analysis;Edge Extraction;Human Computer Interaction (HCI)},
  doi={10.1109/ISMAR52148.2021.00015},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583831,
  author={Li, Yi-Jun and Wang, Miao and Steinicke, Frank and Zhao, Qinping},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={OpenRDW: A Redirected Walking Library and Benchmark with Multi-User, Learning-based Functionalities and State-of-the-art Algorithms}, 
  year={2021},
  volume={},
  number={},
  pages={21-30},
  abstract={Redirected walking (RDW) is a locomotion technique that guides users on virtual paths, which might vary from the paths they physically walk in the real world. Thereby, RDW enables users to explore a virtual space that is larger than the physical counterpart with near-natural walking experiences. Several approaches have been proposed and developed; each using individual platforms and evaluated on a custom dataset, making it challenging to compare between methods. However, there are seldom public toolkits and recognized benchmarks in this field. In this paper, we introduce OpenRDW, an open-source library and benchmark for developing, deploying and evaluating a variety of methods for walking path redirection. The OpenRDW library provides application program interfaces to access the attributes of scenes, to customize the RDW controllers, to simulate and visualize the navigation process, to export multiple formats of the results, and to evaluate RDW techniques. It also supports the deployment of multi-user real walking, as well as reinforcement learning-based models exported from TensorFlow or PyTorch. The OpenRDW benchmark includes multiple testing conditions, such as walking in size varied tracking spaces or shape varied tracking spaces with obstacles, multiple user walking, etc. On the other hand, procedurally generated paths and walking paths collected from user experiments are provided for a comprehensive evaluation. It also contains several classic and state-of-the-art RDW techniques, which include the above mentioned functionalities.},
  keywords={Legged locomotion;Performance evaluation;Visualization;Shape;Navigation;Process control;Benchmark testing;Redirected Walking;Open-Source Library;Benchmark},
  doi={10.1109/ISMAR52148.2021.00016},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583838,
  author={Islam, Rifatul and Desai, Kevin and Quarles, John},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Cybersickness Prediction from Integrated HMD’s Sensors: A Multimodal Deep Fusion Approach using Eye-tracking and Head-tracking Data}, 
  year={2021},
  volume={},
  number={},
  pages={31-40},
  abstract={Cybersickness prediction is one of the significant research challenges for real-time cybersickness reduction. Researchers have proposed different approaches for predicting cybersickness from bio-physiological data (e.g., heart rate, breathing rate, electroencephalogram). However, collecting bio-physiological data often requires external sensors, limiting locomotion and 3D-object manipulation during the virtual reality (VR) experience. Limited research has been done to predict cybersickness from the data readily available from the integrated sensors in head-mounted displays (HMDs) (e.g., head-tracking, eye-tracking, motion features), allowing free locomotion and 3D-object manipulation. This research proposes a novel deep fusion network to predict cybersickness severity from heterogeneous data readily available from the integrated HMD sensors. We extracted 1755 stereoscopic videos, eye-tracking, and head-tracking data along with the corresponding self-reported cybersickness severity collected from 30 participants during their VR gameplay. We applied several deep fusion approaches with the heterogeneous data collected from the participants. Our results suggest that cybersickness can be predicted with an accuracy of 87.77% and a root-mean-square error of 0.51 when using only eye-tracking and head-tracking data. We concluded that eye-tracking and head-tracking data are well suited for a standalone cybersickness prediction framework.},
  keywords={Heart rate;Limiting;Cybersickness;Stereo image processing;Resists;Sensor fusion;Sensor phenomena and characterization;Cybersickness Prediction;Visually induced motion sickness;Eye-tracking;Multimodal Deep Fusion Network;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR52148.2021.00017},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583812,
  author={Zhou, Kanglei and Cheng, Zhiyuan and Shum, Hubert P. H. and Li, Frederick W. B. and Liang, Xiaohui},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={STGAE: Spatial-Temporal Graph Auto-Encoder for Hand Motion Denoising}, 
  year={2021},
  volume={},
  number={},
  pages={41-49},
  abstract={Hand object interaction in mixed reality (MR) relies on the accurate tracking and estimation of human hands, which provide users with a sense of immersion. However, raw captured hand motion data always contains errors such as joints occlusion, dislocation, high-frequency noise, and involuntary jitter. Denoising and obtaining the hand motion data consistent with the user’s intention are of the utmost importance to enhance the interactive experience in MR. To this end, we propose an end-to-end method for hand motion denoising using the spatial-temporal graph auto-encoder (STGAE). The spatial and temporal patterns are recognized simultaneously by constructing the consecutive hand joint sequence as a spatial-temporal graph. Considering the complexity of the articulated hand structure, a simple yet effective partition strategy is proposed to model the physic-connected and symmetry-connected relationships. Graph convolution is applied to extract structural constraints of the hand, and a self-attention mechanism is to adjust the graph topology dynamically. Combining graph convolution and temporal convolution, a fundamental graph encoder or decoder block is proposed. We finally establish the hourglass residual auto-encoder to learn a manifold projection operation and a corresponding inverse projection through stacking these blocks. In this work, the proposed framework has been successfully used in hand motion data denoising with preserving structural constraints between joints. Extensive quantitative and qualitative experiments show that the proposed method has achieved better performance than the state-of-the-art approaches.},
  keywords={Manifolds;Convolution;Network topology;Tracking;Noise reduction;Stacking;Estimation;Motion data cleanup;Hand motion denoising;Graph convolutional network;Spatial-temporal graph auto-encoder},
  doi={10.1109/ISMAR52148.2021.00018},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583733,
  author={Benda, Brett and Ragan, Eric D.},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Virtual Avatar Visibility on Pointing Interpretation by Observers in 3D Environments}, 
  year={2021},
  volume={},
  number={},
  pages={50-59},
  abstract={Avatars are often used to provide representations of users in 3D environments, such as desktop games or VR applications. While full-body avatars are often sought to be used in applications, low visibility avatars (i.e., head and hands) are often used in a variety of contexts, either as intentional design choices, for simplicity in contexts where full-body avatars are not needed, or due to external limitations. Avatar style can also vary from more simplistic and abstract to highly realistic depending on application context and user choices. We present the results of two desktop experiments that examine avatar visibility, style, and observer view on accuracy in a pointing interpretation task. Significant effects of visibility were found, with effects varying between horizontal and vertical components of error, and error amounts not always worsening as a result of lowering visibility. Error due to avatar visibility was much smaller than error resulting from avatar style or observer view. Our findings suggest that humans are reasonably able to understand pointing gestures with a limited observable body.},
  keywords={Context;Visualization;Three-dimensional displays;Avatars;Virtual environments;Games;Observers;Human-centered computing;Human computer interaction;Interaction techniques;Pointing;Interaction paradigms;Collaborative interaction},
  doi={10.1109/ISMAR52148.2021.00019},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583833,
  author={Wang, Lili and Liu, Xiaolong and Li, Xiangyu},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VR Collaborative Object Manipulation Based on Viewpoint Quality}, 
  year={2021},
  volume={},
  number={},
  pages={60-68},
  abstract={We introduce a collaborative manipulation method to improve the efficiency and accuracy of object manipulation in virtual reality applications with multiple users. When multiple users manipulate an object in collaboration, a certain user may have a better perspective than other users at a certain moment, and can clearly observe the object to be manipulated and the target position, and it is more efficient and accurate for him to manipulate the object. We construct a viewpoint quality function and evaluate the viewpoints of multiple users by calculating its three components: the visibility of the object need to be manipulated, the visibility of target, the depth and distance combined of the target. By comparing the viewpoint quality of multiple users, the user with the highest viewpoint quality is determined as the dominant manipulator, who can manipulate the object at the moment. A temporal filter is proposed to filter the dominant sequence generated by the previous frames and the current frame, which reduces the dominant manipulator jumping back and forth between multiple users in a short time slice, making the determination of the dominant manipulator more stable. We have designed a user study and tested our method with three multi-user collaborative manipulation tasks. Compared to the previous methods, our method showed significant improvement in task completion time, rotation accuracy, user participation and task load.},
  keywords={Collaboration;Manipulators;Task analysis;Augmented reality;Virtual reality;Collaboration;Object manipulation;Viewpoint quality},
  doi={10.1109/ISMAR52148.2021.00020},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583783,
  author={Kari, Mohamed and Grosse-Puppendahl, Tobias and Coelho, Luis Falconeri and Fender, Andreas Rene and Bethge, David and Schütte, Reinhard and Holz, Christian},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={TransforMR: Pose-Aware Object Substitution for Composing Alternate Mixed Realities}, 
  year={2021},
  volume={},
  number={},
  pages={69-79},
  abstract={Despite the advances in machine perception, semantic scene understanding is still a limiting factor in mixed reality scene composition. In this paper, we present TransforMR, a video see-through mixed reality system for mobile devices that performs 3D-pose-aware object substitution to create meaningful mixed reality scenes. In real-time and for previously unseen and unprepared real-world environments, TransforMR composes mixed reality scenes so that virtual objects assume behavioral and environment-contextual properties of replaced real-world objects. This yields meaningful, coherent, and humaninterpretable scenes, not yet demonstrated by today’s augmentation techniques. TransforMR creates these experiences through our novel pose-aware object substitution method building on different 3D object pose estimators, instance segmentation, video inpainting, and pose-aware object rendering. TransforMR is designed for use in the real-world, supporting the substitution of humans and vehicles in everyday scenes, and runs on mobile devices using just their monocular RGB camera feed as input. We evaluated TransforMR with eight participants in an uncontrolled city environment employing different transformation themes. Applications of TransforMR include real-time character animation analogous to motion capturing in professional film making, however without the need for preparation of either the scene or the actor, as well as narrative-driven experiences that allow users to explore fictional parallel universes in mixed reality. We make all of our source code and assets available1.1TransforMR code release: https://github.com/MohamedKari/transformr},
  keywords={Performance evaluation;Three-dimensional displays;Codes;Semantics;Pose estimation;Neural networks;Mixed reality;Human-centered computing—;Mixed / augmented reality—},
  doi={10.1109/ISMAR52148.2021.00021},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583806,
  author={Li, Hai and Fan, Tianxing and Zhai, Hongjia and Cui, Zhaopeng and Bao, Hujun and Zhang, Guofeng},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={BDLoc: Global Localization from 2.5D Building Map}, 
  year={2021},
  volume={},
  number={},
  pages={80-89},
  abstract={Robust and accurate global 6DoF localization is essential for many applications, i.e., augmented reality and autonomous driving. Most existing 6DoF visual localization approaches need to build a dense texture model in advance, which is computationally extensive and almost infeasible in the global range. In this work, we propose BDLoc, a hierarchical global localization framework via the 2.5D building map, which is able to estimate the accurate pose of the query street-view image without using detailed dense 3D model and texture information. Specifically speaking, we first extract the 3D building information from the street-view image and surrounding 2.5D building map, and then solve a coarse relative pose by local to global registration. In order to improve the feature extraction, we propose a novel SPG-Net which is able to capture both local and global features. Finally, an iterative semantic alignment is applied to obtain a finner result with the differentiable rendering and the cross-view semantic constraint. Except for a coarse longitude and latitude from GPS, BDLoc doesn’t need any additional information like altitude and orientation that are necessary for many previous works. We also create a large dataset to explore the performance of the 2.5D map-based localization task. Extensive experiments demonstrate the superior performance of our method.},
  keywords={Location awareness;Visualization;Three-dimensional displays;Computational modeling;Buildings;Semantics;Feature extraction;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Mixed / augmented reality;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR52148.2021.00022},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583830,
  author={Lu, Yao and Mayol-Cuevas, Walterio W.},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions}, 
  year={2021},
  volume={},
  number={},
  pages={90-98},
  abstract={In this paper, we concern with the problem of how to automatically extract the steps that compose real-life hand activities. This is a key competence towards processing, monitoring and providing video guidance in Mixed Reality systems. We use egocentric vision to observe hand-object interactions in real-world tasks and automatically decompose a video into its constituent steps. Our approach combines hand-object interaction (HOI) detection, object similarity measurement and a finite state machine (FSM) representation to automatically edit videos into steps. We use a combination of Convolutional Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments while observing real hand activities. We evaluate quantitatively and qualitatively our algorithm on two datasets: the GTEA [19], and a new dataset we introduce for Chinese Tea making. Results show our method is able to segment hand-object interaction videos into key step segments with high levels of precision.},
  keywords={Mixed reality;Automata;Cognition;Convolutional neural networks;Task analysis;Monitoring;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Applied computing;Education;Computer-assisted instruction},
  doi={10.1109/ISMAR52148.2021.00023},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583792,
  author={Zheng, Xiaozheng and Ren, Pengfei and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={SAR: Spatial-Aware Regression for 3D Hand Pose and Mesh Reconstruction from a Monocular RGB Image}, 
  year={2021},
  volume={},
  number={},
  pages={99-108},
  abstract={3D hand reconstruction is a popular research topic in recent years, which has great potential for VR/AR applications. However, due to the limited computational resource of VR/AR equipment, the reconstruction algorithm must balance accuracy and efficiency to make the users have a good experience. Nevertheless, current methods are not doing well in balancing accuracy and efficiency. Therefore, this paper proposes a novel framework that can achieve a fast and accurate 3D hand reconstruction. Our framework relies on three essential modules, including spatial-aware initial graph building (SAIGB), graph convolutional network (GCN) based belief maps regression (GBBMR), and pose-guided refinement (PGR). At first, given image feature maps extracted by convolutional neural networks, SAIGB builds a spatial-aware and compact initial feature graph. Each node in this graph represents a vertex of the mesh and has vertex-specific spatial information that is helpful for accurate and efficient regression. After that, GBBMR first utilizes adaptive-GCN to introduce interactions between vertices to capture short-range and long-range dependencies between vertices efficiently and flexibly. Then, it maps vertices’ features to belief maps that can model the uncertainty of predictions for more accurate predictions. Finally, we apply PGR to compress the redundant vertices’ belief maps to compact-joints’ belief maps with the pose guidance and use these joints’ belief maps to refine previous predictions better to obtain more accurate and robust reconstruction results. Our method achieves state-of-the-art performance on four public benchmarks, FreiHAND, HO-3D, RHD, and STB. Moreover, our method can run at a speed of two to three times that of previous state-of-the-art methods. Our code is available at https://github.com/zxz267/SAR.},
  keywords={Convolutional codes;Three-dimensional displays;Uncertainty;Benchmark testing;Reconstruction algorithms;Predictive models;Feature extraction;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR52148.2021.00024},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583807,
  author={Khan, Farzana Alam and Muvva, Veera Venkata Ram Murali Krishna Rao and Wu, Dennis and Arefin, Mohammed Safayet and Phillips, Nate and Swan, J. Edward},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Measuring the Perceived Three-Dimensional Location of Virtual Objects in Optical See-Through Augmented Reality}, 
  year={2021},
  volume={},
  number={},
  pages={109-117},
  abstract={For optical see-through augmented reality (AR), a new method for measuring the perceived three-dimensional location of virtual objects is presented, where participants verbally report a virtual object’s location relative to both a vertical and horizontal grid. The method is tested with a small (1.95 × 1.95 × 1.95 cm) virtual object at distances of 50 to 80 cm, viewed through a Microsoft HoloLens 1st generation AR display. Two experiments examine two different virtual object designs, whether turning in a circle between reported object locations disrupts HoloLens tracking, and whether accuracy errors, including a rightward bias and underestimated depth, might be due to systematic errors that are restricted to a particular display. Turning in a circle did not disrupt HoloLens tracking, and testing with a second display did not suggest systematic errors restricted to a particular display. Instead, the experiments are consistent with the hypothesis that, when looking downwards at a horizontal plane, HoloLens 1st generation displays exhibit a systematic rightward perceptual bias. Precision analysis suggests that the method could measure the perceived location of a virtual object within an accuracy of less than 1 mm.},
  keywords={Systematics;Atmospheric measurements;Optical variables measurement;Turning;Particle measurements;Adaptive optics;Augmented reality;Depth Perception;Augmented Reality;Optical see-through display;Perceived Location;HoloLens;Human Subject Analysis},
  doi={10.1109/ISMAR52148.2021.00025},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583741,
  author={Shi, Rongkai and Zhu, Nan and Liang, Hai-Ning and Zhao, Shengdong},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Head-based Mode-Switching in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={118-127},
  abstract={Mode-switching supports multilevel operations using a limited number of input methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches for mode-switching use buttons, controllers, and users’ hands. However, they are inefficient and challenging to do with tasks that require both hands (e.g., when users need to use two hands during drawing operations). Using head gestures for mode-switching can be an efficient and cost-effective way, allowing for a more continuous and smooth transition between modes. In this paper, we explore the use of head gestures for mode-switching especially in scenarios when both users’ hands are performing tasks. We present a first user study that evaluated eight head gestures that could be suitable for VR HMD with a dual-hand line-drawing task. Results show that move forward, move backward, roll left, and roll right led to better performance and are preferred by participants. A second study integrating these four gestures in Tilt Brush, an open-source painting VR application, is conducted to further explore the applicability of these gestures and derive insights. Results show that Tilt Brush with head gestures allowed users to change modes with ease and led to improved interaction and user experience. The paper ends with a discussion on some design recommendations for using head-based mode-switching in VR HMD.},
  keywords={Head-mounted displays;Brushes;Systematics;Resists;Switches;User experience;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Empirical studies in HCI},
  doi={10.1109/ISMAR52148.2021.00026},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583837,
  author={Tang, Man To and Zhu, Victor Long and Popescu, Voicu},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AlterEcho: Loose Avatar-Streamer Coupling for Expressive VTubing}, 
  year={2021},
  volume={},
  number={},
  pages={128-137},
  abstract={VTubers are live streamers who embody computer animation virtual avatars. VTubing is a rapidly rising form of online entertainment in East Asia, most notably in Japan and China, and it has been more recently introduced in the West. However, animating an expressive VTuber avatar remains a challenge due to budget and usability limitations of current solutions, i.e., high-fidelity motion capture is expensive, while keyboard-based VTubing interfaces impose a cognitive burden on the streamer. This paper proposes a novel approach for VTubing animation based on the key principle of loosening the coupling between the VTuber and their avatar, and it describes a first implementation of the approach in the AlterEcho VTubing animation system. AlterEcho generates expressive VTuber avatar animation automatically, without the streamer’s explicit intervention; it breaks the strict tethering of the avatar from the streamer, allowing the avatar’s nonverbal behavior to deviate from that of the streamer. Without the complete independence of a true alter ego, but also without the constraint of mirroring the streamer with the fidelity of an echo, AlterEcho produces avatar animations that have been rated significantly higher by VTubers and viewers (N = 315) compared to animations created using simple motion capture, or using VMagicMirror, a state-of-the-art keyboard-based VTubing system. Our work also opens the door to personalizing the avatar persona for individual viewers.},
  keywords={Couplings;Avatars;Asia;Entertainment industry;Tools;Animation;Usability;VTuber;automatic avatar animation;live streaming;Human-centered computing;Int. systems and tools},
  doi={10.1109/ISMAR52148.2021.00027},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583795,
  author={Monteiro, Diego and Liang, Hai-Ning and Tang, Xiaohang and Irani, Pourang},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Using Trajectory Compression Rate to Predict Changes in Cybersickness in Virtual Reality Games}, 
  year={2021},
  volume={},
  number={},
  pages={138-146},
  abstract={Identifying cybersickness in virtual reality (VR) applications such as games in a fast, precise, non-intrusive, and non-disruptive way remains challenging. Several factors can cause cybersickness, and their identification will help find its origins and prevent or minimize it. One such factor is virtual movement. Movement, whether physical or virtual, can be represented in different forms. One way to represent and store it is with a temporally annotated point sequence. Because a sequence is memory-consuming, it is often preferable to save it in a compressed form. Compression allows redundant data to be eliminated while still preserving changes in speed and direction. Since changes in direction and velocity in VR can be associated with cybersickness, changes in compression rate can likely indicate changes in cybersickness levels. In this research, we explore whether quantifying changes in virtual movement can be used to estimate variation in cybersickness levels of VR users. We investigate the correlation between changes in the compression rate of movement data in two VR games with changes in players’ cybersickness levels captured during gameplay. Our results show (1) a clear correlation between changes in compression rate and cybersickness, and (2) that a machine learning approach can be used to identify these changes. Finally, results from a second experiment show that our approach is feasible for cybersickness inference in games and other VR applications that involve movement.},
  keywords={Correlation;Cybersickness;Design methodology;Neural networks;Games;Machine learning;Hardware;Human-centered computing;Empirical studies in HCI;Virtual reality Human-centered computing;HCI design and evaluation methods},
  doi={10.1109/ISMAR52148.2021.00028},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583793,
  author={Kahl, Denise and Ruble, Marc and Krüger, Antonio},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigation of Size Variations in Optical See-through Tangible Augmented Reality}, 
  year={2021},
  volume={},
  number={},
  pages={147-155},
  abstract={Optical see-through AR headsets are becoming increasingly attractive for many applications. Interaction with the virtual content is usually achieved via hand gestures or with controllers. A more seamless interaction between the real and virtual world can be achieved by using tangible objects to manipulate the virtual content. Instead of interacting with detailed physical replicas, working with abstractions allows a single physical object to represent a variety of virtual objects. These abstractions would differ from their virtual representations in shape, size, texture and material. This paper investigates for the first time in optical see-through AR whether size variations are possible without major losses in performance, usability and immersion. The conducted study shows that size can be varied within a limited range without significantly affecting task completion times as well as feelings of disturbance and presence. Stronger size deviations are possible for physical objects smaller than the virtual object than for larger physical objects.},
  keywords={Optical losses;Headphones;Shape;Fitting;Lighting;Estimation;Task analysis;Tangible augmented reality;optical see-through augmented reality;tangible interaction;haptic devices;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR52148.2021.00029},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583800,
  author={Wang, Kangkan and Zheng, Huayu and Zhang, Guofeng and Yang, Jian},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Parametric Model Estimation for 3D Clothed Humans from Point Clouds}, 
  year={2021},
  volume={},
  number={},
  pages={156-165},
  abstract={This paper presents a novel framework to estimate parametric model- s for 3D clothed humans from partial point clouds. It is a challenging problem due to factors such as arbitrary human shape and pose, large variations in clothing details, and significant missing data. Existing methods mainly focus on estimating the parametric model of undressed bodies or reconstructing the non-parametric 3D shapes from point clouds. In this paper, we propose a hierarchical regression framework to learn the parametric model of detailed human shapes from partial point clouds of a single depth frame. Benefiting from the favorable ability of deep neural networks to model nonlinearity, the proposed framework cascades several successive regression networks to estimate the parameters of detailed 3D human body models in a coarse-to-fine manner. Specifically, the first global regression network extracts global deep features of point clouds to obtain an initial estimation of the undressed human model. Based on the initial estimation, the local regression network then refines the undressed human model by using the local features of neighborhood points of human joints. Finally, the clothing details are inferred as an additive displacement on the refined undressed model using the vertex-level regression network. The experimental results demonstrate that the proposed hierarchical regression approach can accurately predict detailed human shapes from partial point clouds and outperform prior works in the recovery accuracy of 3D human models.},
  keywords={Deep learning;Solid modeling;Three-dimensional displays;Shape;Clothing;Estimation;Predictive models;Detailed human shape estimation;parametric model;point clouds},
  doi={10.1109/ISMAR52148.2021.00030},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583740,
  author={Wolf, Julian and Lohmeyer, Quentin and Holz, Christian and Meboldt, Mirko},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Gaze Comes in Handy: Predicting and Preventing Erroneous Hand Actions in AR-Supported Manual Tasks}, 
  year={2021},
  volume={},
  number={},
  pages={166-175},
  abstract={Emerging Augmented Reality headsets incorporate gaze and hand tracking and can, thus, observe the user’s behavior without interfering with ongoing activities. In this paper, we analyze hand-eye coordination in real-time to predict hand actions during target selection and warn users of potential errors before they occur. In our first user study, we recorded 10 participants playing a memory card game, which involves frequent hand-eye coordination with little task-relevant information. We found that participants’ gaze locked onto target cards 350ms before the hands touched them in 73.3% of all cases, which coincided with the peak velocity of the hand moving to the target. Based on our findings, we then introduce a closed-loop support system that monitors the user’s fingertip position to detect the first card turn and analyzes gaze, hand velocity and trajectory to predict the second card before it is turned by the user. In a second study with 12 participants, our support system correctly displayed color-coded visual alerts in a timely manner with an accuracy of 85.9%. The results indicate the high value of eye and hand tracking features for behavior prediction and provide a first step towards predictive real-time user support.},
  keywords={Headphones;Visualization;Wearable computers;Manuals;Games;Real-time systems;Trajectory;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR52148.2021.00031},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583813,
  author={Yamaguchi, Shoma and Ogawa, Nami and Narumi, Takuji},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Now I’m Not Afraid: Reducing Fear of Missing Out in 360° Videos on a Head-Mounted Display using a Panoramic Thumbnail}, 
  year={2021},
  volume={},
  number={},
  pages={176-183},
  abstract={Cinematic virtual reality, or 360° video, provides viewers with an immersive experience, allowing them to enjoy a video while moving their head to watch in any direction. However, there is an inevitable problem of feeling fear of missing out (FOMO) when viewing a 360° video, as only a part of the video is visible to the viewer at any given time. To solve this problem, we developed a technique to present a panoramic thumbnail of a full 360° video to users through a head-mounted display. With this technique, the user can grasp the overall view of the video as needed. We conducted an experiment to evaluate the FOMO, presence, and quality of viewing experience while using this technique compared to normal viewing without it. The results of the experiment show that the proposed technique relieved FOMO, the quality of viewing experience was improved, and there was no difference in presence. We also investigated how users interacted with this new interface based on eye tracking and head tracking data during viewing, which suggested that users used the panoramic thumbnail to actively explore outside their field of view.},
  keywords={Head-mounted displays;Resists;Immersive experience;Gaze tracking;Watches;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00032},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583811,
  author={Chen, Ze-Yin and Li, Yi-Jun and Wang, Miao and Steinicke, Frank and Zhao, Qinping},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Reinforcement Learning Approach to Redirected Walking with Passive Haptic Feedback}, 
  year={2021},
  volume={},
  number={},
  pages={184-192},
  abstract={Various redirected walking (RDW) techniques have been proposed, which unwittingly manipulate the mapping from the user’s physical locomotion to motions of the virtual camera. Thereby, RDW techniques guide users on physical paths with the goal to keep them inside a limited tracking area, whereas users perceive the illusion of being able to walk infinitely in the virtual environment. However, the inconsistency between the user’s virtual and physical location hinders passive haptic feedback when the user interacts with virtual objects, which are represented by physical props in the real environment.In this paper, we present a novel reinforcement learning approach towards RDW with passive haptics. With a novel dense reward function, our method learns to jointly consider physical boundary avoidance and consistency of user-object positioning between virtual and physical spaces. The weights of reward and penalty terms in the reward function are dynamically adjusted to adaptively balance term impacts during the walking process. Experimental results demonstrate the advantages of our technique in comparison to previous approaches. Finally, the code of our technique is provided as an open-source solution.},
  keywords={Legged locomotion;Tracking;Design methodology;Virtual environments;Reinforcement learning;Real-time systems;Haptic interfaces;Redirected Walking;Reinforcement Learning},
  doi={10.1109/ISMAR52148.2021.00033},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583782,
  author={Tyagi, Abhishek and Liang, Yangwen and Wang, Shuangquan and Bai, Dongwoon},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DVIO: Depth-Aided Visual Inertial Odometry for RGBD Sensors}, 
  year={2021},
  volume={},
  number={},
  pages={193-201},
  abstract={In past few years we have observed an increase in the usage of RGBD sensors in mobile devices. These sensors provide a good estimate of the depth map for the camera frame, which can be used in numerous augmented reality applications. This paper presents a new visual inertial odometry (VIO) system, which uses measurements from a RGBD sensor and an inertial measurement unit (IMU) sensor for estimating the motion state of the mobile device. The resulting system is called the depth-aided VIO (DVIO) system. In this system we add the depth measurement as part of the nonlinear optimization process. Specifically, we propose methods to use the depth measurement using one-dimensional (1D) feature parameterization as well as three-dimensional (3D) feature parameterization. In addition, we propose to utilize the depth measurement for estimating time offset between the unsynchronized IMU and the RGBD sensors. Last but not least, we propose a novel block-based marginalization approach to speed up the marginalization processes and maintain the real-time performance of the overall system. Experimental results validate that the proposed DVIO system outperforms the other state-of-the-art VIO systems in terms of trajectory accuracy as well as processing time.},
  keywords={Visualization;Three-dimensional displays;Runtime;Sensor systems;Time measurement;Mobile handsets;Sensors;VIO;localization;marginalization;RGBD sensor;IMU sensor;SLAM;Sliding window;Nonlinear optimization;3D reconstruction},
  doi={10.1109/ISMAR52148.2021.00034},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583815,
  author={Cabric, Florent and Dubois, Emmanuel and Serrano, Marcos},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Predictive Performance Model for Immersive Interactions in Mixed Reality}, 
  year={2021},
  volume={},
  number={},
  pages={202-210},
  abstract={The design of immersive interaction for mixed reality based on head-mounted displays (HMDs), hereafter referred to as Mixed Reality (MR), is still a tedious task which can hinder the advent of such devices. Indeed, the effects of the interface design on task performance are difficult to anticipate during the design phase: the spatial layout of virtual objects and the interaction techniques used to select those objects can have an impact on task completion time. Besides, testing such interfaces with users in controlled experiments requires considerable time and efforts. To overcome this problem, predictive models, such as the Keystroke-Level Model (KLM), can be used to predict the time required to complete an interactive task at an early stage of the design process. However, so far these models have not been properly extended to address the specific interaction techniques of MR environments. In this paper we propose an extension of the KLM model to interaction performed in MR. First, we propose new operators and experimentally determine the unit times for each of them with a HoloLens v1. Then, we perform experiments based on realistic interaction scenarios to consolidate our model. These experiments confirm the validity of our extension of KLM to predict interaction time in mixed reality environments..},
  keywords={Performance evaluation;Head-mounted displays;Computational modeling;Layout;Mixed reality;Predictive models;Task analysis;Human-centered computing;Human-Computer Interaction;HCI theory;concepts and models},
  doi={10.1109/ISMAR52148.2021.00035},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583820,
  author={O’Hagan, Joseph and Williamson, Julie R. and McGill, Mark and Khamis, Mohamed},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Safety, Power Imbalances, Ethics and Proxy Sex: Surveying In-The-Wild Interactions Between VR Users and Bystanders}, 
  year={2021},
  volume={},
  number={},
  pages={211-220},
  abstract={VR users and bystanders must sometimes interact, but our understanding of these interactions - their purpose, how they are accomplished, attitudes toward them, and where they break down - is limited. This current gap inhibits research into managing or supporting these interactions, and preventing unwanted or abusive activity. We present the results of the first survey (N=100) that investigates stories of actual emergent in-the-wild interactions between VR users and bystanders. Our analysis indicates VR user and bystander interactions can be categorised into one of three categories: coexisting, demoing, and interrupting. We highlight common interaction patterns and impediments encountered during these interactions. Bystanders play an important role in moderating the VR user’s experience, for example intervening to save the VR user from potential harm. However, our stories also suggest that the occlusive nature of VR introduces the potential for bystanders to exploit the vulnerable state of the VR user; and for the VR user to exploit the bystander for enhanced immersion, introducing significant ethical concerns.},
  keywords={Ethics;Safety;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00036},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583839,
  author={Moore, Alec G. and McMahan, Ryan P. and Dong, Hailiang and Ruozzi, Nicholas},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Personal Identifiability and Obfuscation of User Tracking Data From VR Training Sessions}, 
  year={2021},
  volume={},
  number={},
  pages={221-228},
  abstract={Recent research indicates that user tracking data from virtual reality (VR) experiences can be used to personally identify users with degrees of accuracy as high as 95%. However, these results indicating that VR tracking data should be understood as personally identifying data were based on observing 360° videos. In this paper, we present results based on sessions of user tracking data from an ecologically valid VR training application, which indicate that the prior claims may not be as applicable for identifying users beyond the context of observing 360° videos. Our results indicate that the degree of identification accuracy notably decreases between VR sessions. Furthermore, we present results indicating that user tracking data can be obfuscated by encoding positional data as velocity data, which has been successfully used to predict other user experience outcomes like simulator sickness and knowledge acquisition. These results, which show identification accuracies were reduced by more than half, indicate that velocity-based encoding can be used to reduce identifiability and help protect personal identifying data.},
  keywords={Training;Tracking;Knowledge acquisition;Encoding;User experience;Libraries;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;Computing methodologies;Machine learning;Cross-validation},
  doi={10.1109/ISMAR52148.2021.00037},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583828,
  author={Zhao, Lizhi and Lu, Xuequan and Zhao, Min and Wang, Meili},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Classifying In-Place Gestures with End-to-End Point Cloud Learning}, 
  year={2021},
  volume={},
  number={},
  pages={229-238},
  abstract={Walking in place for moving through virtual environments has attracted noticeable attention recently. Recent attempts focused on training a classifier to recognize certain patterns of gestures (e.g., standing, walking, etc) with the use of neural networks like CNN or LSTM. Nevertheless, they often consider very few types of gestures and/or induce less desired latency in virtual environments. In this paper, we propose a novel framework for accurate and efficient classification of in-place gestures. Our key idea is to treat several consecutive frames as a “point cloud”. The HMD and two VIVE trackers provide three points in each frame, with each point consisting of 12-dimensional features (i.e., three-dimensional position coordinates, velocity, rotation, angular velocity). We create a dataset consisting of 9 gesture classes for virtual in-place locomotion. In addition to the supervised point-based network, we also take unsupervised domain adaptation into account due to inter-person variations. To this end, we develop an end-to-end joint framework involving both a supervised loss for supervised point learning and an unsupervised loss for unsupervised domain adaptation. Experiments demonstrate that our approach generates very promising outcomes, in terms of high overall classification accuracy (95.0%) and real-time performance (192ms latency). We will release our dataset and source code to the community.},
  keywords={Legged locomotion;Training;Supervised learning;Pose estimation;Neural networks;Virtual environments;Resists;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00038},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583787,
  author={Ye, Zi-Ming and Chen, Jun-Long and Wang, Miao and Yang, Yong-Liang},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PAVAL: Position-Aware Virtual Agent Locomotion for Assisted Virtual Reality Navigation}, 
  year={2021},
  volume={},
  number={},
  pages={239-247},
  abstract={Virtual agents are typical assistance tools for navigation and interaction in Virtual Reality (VR) tour, training, education, etc. It has been demonstrated that the gaits, gestures, gazes, and positions of virtual agents are major factors that affect the user’s perception and experience for seated and standing VR. In this paper, we present a novel position-aware virtual agent locomotion method, called PAVAL, that can perform virtual agent positioning (position+orientation) in real time for room-scale VR navigation assistance. We first analyze design guidelines for virtual agent locomotion and model the problem using the positions of the user and the surrounding virtual objects. Then we conduct a one-off preliminary study to collect subjective data and present a model for virtual agent positioning prediction with fixed user position. Based on the model, we propose an algorithm to optimize the object of interest, virtual agent position, and virtual agent orientation in sequence for virtual agent locomotion. As a result, during user navigation in a virtual scene, the virtual agent automatically moves in real time and introduces virtual object information to the user. We evaluate PAVAL and two alternative methods via a user study with humanoid virtual agents in various scenes, including virtual museum, factory, and school gym. The results reveal that our method is superior to the baseline condition.},
  keywords={Training;Solid modeling;Navigation;Design methodology;Humanoid robots;Tools;Predictive models;Virtual Agent;Navigation;optimization},
  doi={10.1109/ISMAR52148.2021.00039},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583788,
  author={Sun, Zhoutao and Hu, Yong and Shen, Xukun},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Two-hand Pose Estimation from the non-cropped RGB Image with Self-Attention Based Network}, 
  year={2021},
  volume={},
  number={},
  pages={248-255},
  abstract={Estimating the pose of two hands is a crucial problem for many human-computer interaction applications. Since most of the existing works utilize cropped images to predict the hand pose, they require a hand detection stage before pose estimation or input cropped images directly. In this paper, we propose the first real-time one-stage method for pose estimation from a single RGB image without hand tracking. Combining the self-attention mechanism with convolutional layers, the network we proposed is able to predict the 2.5D hand joints coordinate while locating the two hands regions. And to reduce the extra memory and computational consumption caused by self-attention, we proposed a linear attention structure with a spatial reduction attention block called SRAN block. We demonstrate the effectiveness of each component in our network through the ablation study. And experiments on public datasets showed the competitive result with the state-of-the-art method.},
  keywords={Human computer interaction;Visualization;Pose estimation;Memory management;Real-time systems;Augmented reality;Artificial intelligence;Computer version;Pose estimation;Human computer interaction(HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR52148.2021.00040},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583799,
  author={Hu, Xue and Baena, Ferdinando Rodriguez y and Cutolo, Fabrizio},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Rotation-constrained optical see-through headset calibration with bare-hand alignment}, 
  year={2021},
  volume={},
  number={},
  pages={256-264},
  abstract={The inaccessibility of user-perceived reality remains an open issue in pursuing the accurate calibration of optical see-through (OST) head-mounted displays (HMDs). Manual user alignment is usually required to collect a set of virtual-to-real correspondences, so that a default or an offline display calibration can be updated to account for the user’s eye position(s). Current alignment-based calibration procedures usually require point-wise alignments between rendered image point(s) and associated physical landmark(s) of a target calibration tool. As each alignment can only provide one or a few correspondences, repeated alignments are required to ensure calibration quality. This work presents an accurate and tool-less online OST calibration method to update an offline-calibrated eye-display model. The user’s bare hand is markerlessly tracked by a commercial RGBD camera anchored to the OST headset to generate a user-specific cursor for correspondence collection. The required alignment is object-wise, and can provide thousands of unordered corresponding points in tracked space. The collected correspondences are registered by a proposed rotation-constrained iterative closest point (rcICP) method to optimise the viewpoint-related calibration parameters. We implemented such a method for the Microsoft HoloLens 1. The resiliency of the proposed procedure to noisy data was evaluated through simulated tests and real experiments performed with an eye-replacement camera. According to the simulation test, the rcICP registration is robust against possible user-induced rotational misalignment. With a single alignment, our method achieves 8.81 arcmin (1.37 mm) positional error and 1. 76° rotational error by camera-based tests in the arm-reach distance, and 10.79 arcmin (7.71 pixels) reprojection error by user tests.},
  keywords={Headphones;Solid modeling;Target tracking;Tools;Cameras;Optical imaging;Adaptive optics;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems;Artificial;Augmented and Virtual realities;H.5.2 [Information interfaces and presentation]: User Interfaces;Ergonomics;Evaluation/methodology;Screen design},
  doi={10.1109/ISMAR52148.2021.00041},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583840,
  author={Ng, Alexander and Medeiros, Daniel and McGill, Mark and Williamson, Julie and Brewster, Stephen},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Passenger Experience of Mixed Reality Virtual Display Layouts in Airplane Environments}, 
  year={2021},
  volume={},
  number={},
  pages={265-274},
  abstract={Augmented / Mixed Reality headsets will in-time see adoption and use in a variety of mobility and transit contexts, allowing users to view and interact with virtual content and displays for productivity and entertainment. However, little is known regarding how multi-display virtual workspaces should be presented in a transit context, nor to what extent the unique affordances of transit environments (e.g. the social presence of others) might influence passenger perception of virtual display layouts. Using a simulated VR passenger airplane environment, we evaluated three different AR-driven virtual display configurations (Horizontal, Vertical, and Focus main display with smaller secondary windows) at two different depths, exploring their usability, user preferences, and the underlying factors that influenced those preferences. We found that the perception of invading other’s personal space significantly influenced preferred layouts in transit contexts. Based on our findings, we reflect on the unique challenges posed by passenger contexts, provide recommendations regarding virtual display layout in the confined airplane environment, and expand on the significant benefits that AR offers over physical displays in said environments.},
  keywords={Productivity;Headphones;Airplanes;Layout;Entertainment industry;Mixed reality;Virtual environments;Mixed Reality;Virtual reality;Augmented reality;Multi-display layouts;Virtual Workspace;[Human-centered computing];Human computer interaction (HCI);Interaction paradigms},
  doi={10.1109/ISMAR52148.2021.00042},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583805,
  author={Chen, Danpeng and Wang, Nan and Xu, Runsen and Xie, Weijian and Bao, Hujun and Zhang, Guofeng},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={RNIN-VIO: Robust Neural Inertial Navigation Aided Visual-Inertial Odometry in Challenging Scenes}, 
  year={2021},
  volume={},
  number={},
  pages={275-283},
  abstract={In this work, we propose a tightly-coupled EKF framework for visual-inertial odometry with NIN (Neural Inertial Navigation) aided. Traditional VIO systems are fragile in challenging scenes with weak or confusing visual information, such as weak/repeated texture, dynamic environment, fast camera motion with serious motion blur, etc. It is extremely difficult for a vision-based algorithm to handle these problems. So we firstly design a robust deep learning based inertial network (called RNIN), using only IMU measurements as input. RNIN is significantly more robust in challenging scenes than traditional VIO systems. In order to take full advantage of vision-based algorithms in AR/VR areas, we further develop a multi-sensor fusion system RNIN-VIO, which tightly couples the visual, IMU and NIN measurements. Our system performs robustly in extremely challenging conditions, with high precision both in trajectories and AR effects. The experimental results of evaluation on dataset evaluation and online AR demo demonstrate the superiority of the proposed system in robustness and accuracy.},
  keywords={Deep learning;Visualization;Fuses;Heuristic algorithms;Neural networks;Dynamics;Inertial navigation;VIO;SLAM;INS;IMU;AR-6DoF;Motion Tracking},
  doi={10.1109/ISMAR52148.2021.00043},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583789,
  author={Li, Jie and Subramanyam, Shishir and Jansen, Jack and Mei, Yanni and Reimat, Ignacio and Ławicka, Kinga and Cesar, Pablo},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating the user Experience of a Photorealistic Social VR Movie}, 
  year={2021},
  volume={},
  number={},
  pages={284-293},
  abstract={We all enjoy watching movies together. However, this is not always possible if we live apart. While we can remotely share our screens, the experience differs from being together. We present a social Virtual Reality (VR) system that captures, reconstructs, and transmits multiple users’ volumetric representations into a commercially produced 3D virtual movie, so they have the feeling of “being there” together. We conducted a 48-user experiment where we invited users to experience the virtual movie either using a Head Mounted Display (HMD) or using a 2D screen with a game controller. In addition, we invited 14 VR experts to experience both the HMD and the screen version of the movie and discussed their experiences in two focus groups. Our results showed that both end-users and VR experts found that the way they navigated and interacted inside a 3D virtual movie was novel. They also found that the photorealistic volumetric representations enhanced feelings of co-presence. Our study lays the groundwork for future interactive and immersive VR movie co-watching experiences.},
  keywords={Human computer interaction;Three-dimensional displays;Navigation;Two dimensional displays;Virtual environments;Resists;Games;Human-centered computing [Human computer interaction (HCI)]: HCI design and evaluation methods;User studies;Human-centered computing [Human computer interaction (HCI)]: Interaction paradigms;Virtual reality.},
  doi={10.1109/ISMAR52148.2021.00044},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583804,
  author={Chen, Mengyu and Monroy-Hernández, Andrés and Sra, Misha},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={SceneAR: Scene-based Micro Narratives for Sharing and Remixing in Augmented Reality}, 
  year={2021},
  volume={},
  number={},
  pages={294-303},
  abstract={Short-form digital storytelling has become a popular medium for millions of people to express themselves. Traditionally, this medium uses primarily 2D media such as text (e.g., memes), images (e.g., Instagram), GIFs (e.g., Giphy), and videos (e.g., TikTok, Snapchat). To expand the modalities from 2D to 3D media, we present SceneAR, a smartphone application for creating sequential scene-based micro narratives in augmented reality (AR). What sets SceneAR apart from prior work is its ability to share the scene-based stories as AR content. No longer limited to sharing images or videos, users can now experience narratives in their own physical environments. Additionally, SceneAR affords users the ability to remix AR content, empowering them to collectively build on others’ creations. We asked 18 people to use SceneAR in a three-day study, and based on user interviews, analyses of screen recordings, and the stories they created, we extracted three themes. From these themes and the study overall, we derived six strategies for designers interested in supporting short-form AR narratives.},
  keywords={Three-dimensional displays;Visual communication;Social networking (online);Multimedia Web sites;Lighting;Media;Reliability engineering;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/ISMAR52148.2021.00045},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583818,
  author={Sorli, Suzanne and Casas, Dan and Verschoor, Mickeal and Tajadura-Jiménez, Ana and Otaduy, Miguel A.},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Fine Virtual Manipulation with Hands of Different Sizes}, 
  year={2021},
  volume={},
  number={},
  pages={304-309},
  abstract={Natural interaction with virtual objects relies on two major technology components: hand tracking and hand-object physics simulation. There are functional solutions for these two components, but their hand representations may differ in size and skeletal morphology, hence making the connection non-trivial. In this paper, we introduce a pose retargeting strategy to connect the tracked and simulated hand representations, and we have formulated and solved this hand retargeting as an optimization problem. We have also carried out a user study that demonstrates the effectiveness of our approach to enable fine manipulations that are slow and awkward with naïve approaches.},
  keywords={Morphology;Physics;Optimization;Augmented reality;Hand simulation;hand tracking;pose retargeting.},
  doi={10.1109/ISMAR52148.2021.00046},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583786,
  author={Gottsacker, Matt and Norouzi, Nahal and Kim, Kangsoo and Bruder, Gerd and Welch, Greg},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Diegetic Representations for Seamless Cross-Reality Interruptions}, 
  year={2021},
  volume={},
  number={},
  pages={310-319},
  abstract={The closed design of virtual reality (VR) head-mounted displays substantially limits users’ awareness of their real-world surroundings. This presents challenges when another person in the same physical space needs to interrupt the VR user for a brief conversation. Such interruptions, e.g., tapping a VR user on the shoulder, can cause a disruptive break in presence (BIP), which affects their place and plausibility illusions, and may cause a drop in performance of their virtual activity. Recent findings related to the concept of diegesis, which denotes the internal consistency of an experience/story, suggest potential benefits of integrating registered virtual representations for physical interactors, especially when these appear internally consistent in VR. In this paper, we present a human-subject study we conducted to compare and evaluate five different diegetic and non-diegetic methods to facilitate cross-reality interruptions in a virtual office environment, where a user’s task was briefly interrupted by a physical person. We created a Cross-Reality Interaction Questionnaire (CRIQ) to capture the quality of the interaction from the VR user’s perspective. Our results show that the diegetic representations afforded reasonably high senses of co-presence, the highest quality interactions, the highest place illusions, and caused the least disruption of the participants’ virtual experiences. We discuss our findings as well as implications for practical applications that aim to leverage virtual representations to ease cross-reality interruptions.},
  keywords={Head-mounted displays;Avatars;Design methodology;Teleworking;User experience;Task analysis;Augmented reality;Virtual Reality;Cross-Reality;Diegesis;Interruptions;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00047},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583796,
  author={Fabre, Emilie and Verhulst, Adrien and Balandra, Alfonso and Sugimoto, Maki and Inami, Masahiko},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating Textual Visual Sound Effects in a Virtual Environment and their impacts on Object Perception and Sound Perception}, 
  year={2021},
  volume={},
  number={},
  pages={320-328},
  abstract={In comics, Textual Sound Effects (TE) can describe sounds, but also actions, events, etc. TE could be used in Virtual Environment to efficiently create an easily recognizable scene and add more information to objects at a relatively low design cost. We investigate the impact of TE in a Virtual Environment on objects’ material perception (on category and properties) and on sound perception (on volume [dB] and spatial position). Participants (N=13, repeated measures) categorized metallic and wooden spheres and significantly changed their reaction time depending on the TE congruence with the spheres’ material/sound. They then rated a sphere’s properties (i.e., wetness, warmness, softness, smoothness, and dullness) and significantly changed their rating depending on the TE. When comparing 2 sound volumes, they perceived a sound associated with a shrinking TE as less loud and a sound associated with a growing TE as louder. When locating an audio source location, they located it significantly closer to a TE.},
  keywords={Visualization;Costs;Atmospheric measurements;Virtual environments;Color;Tools;Position measurement;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00048},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583810,
  author={Liu, Zhihao and Zhang, Fanxing and Cheng, Zhanglin},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={BuildingSketch: Freehand Mid-Air Sketching for Building Modeling}, 
  year={2021},
  volume={},
  number={},
  pages={329-338},
  abstract={Advancements in virtual reality (VR) technology enable us to rethink the way of interactive 3D modeling - intuitively creating 3D content directly in 3D space. However, conventional VR-based modeling is laborious and tedious to generate a detailed 3D model in full manual mode since users need to carefully draw almost the entire surface. In this paper, we present a freehand mid-air sketching system with the aid of deep learning techniques for modeling structured buildings, where the user freely draws a few key strokes in mid-air using his/her fingers to represent the desired shapes and our system automatically interprets the strokes using a deep neural network and generates a detailed building model based on a procedural modeling method. After creating several building blocks one by one, the user can freely move, rotate, and combine the blocks to form a complex building model. We demonstrate the ease of use for novice users, effectiveness, and efficiency of our sketching system, BuildingSketch, by presenting a variety of building models.},
  keywords={Deep learning;Solid modeling;Three-dimensional displays;Shape;Computational modeling;Buildings;Fingers;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Computer graphics;Shape modeling},
  doi={10.1109/ISMAR52148.2021.00049},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583816,
  author={Chung, SeungA and Lee, Kyungyeon and Oh, Uran},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Understanding the Two-Step Nonvisual Omnidirectional Guidance for Target Acquisition in 3D Spaces}, 
  year={2021},
  volume={},
  number={},
  pages={339-346},
  abstract={Providing directional guidance is important especially for exploring unfamiliar environments. However, most studies are limited to two-dimensional guidance when many interactions happen in 3D spaces. Moreover, visual feedback that is often used to communicate the 3D position of a particular object may not be available in situations when the target is occluded by other objects or located outside of one’s field of view, or due to visual overload or light conditions. Inspired by a prior finding that showed users’ tendency of scanning a 3D space in one direction at a time, we propose two-step nonvisual omnidirectional guidance feedback designs varying the searching order where the guidance for the vertical location of the target (the altitude) is offered to the users first, followed by the horizontal direction of the target (the azimuth angle) and visa versa. To investigate its effect, we conducted the user study with 12 blind-folded sighted participants. Findings suggest that our proposed two-step guidance outperforms the default condition with no order in terms of task completion time and travel distance, particularly when the guidance in the horizontal direction is presented first. We plan to extend this work to assist with finding a target in 3D spaces in a real-world environment.},
  keywords={Visualization;Three-dimensional displays;Azimuth;Space exploration;Task analysis;Augmented reality;Human-centered computing;Empirical studies in interaction design;User interface design},
  doi={10.1109/ISMAR52148.2021.00050},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583825,
  author={Raeburn, Gideon and Tokarchuk, Laurissa},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Varying user agency and interaction opportunities in a home mobile augmented virtuality story}, 
  year={2021},
  volume={},
  number={},
  pages={347-356},
  abstract={New opportunities for immersive storytelling experiences have arrived through the technology in mobile phones, including the ability to overlay or register digital content on a user’s real world surroundings, to greater immerse the user in the world of the story. This raises questions around the methods and freedom to interact with the digital elements, that will lead to a more immersive and engaging experience. To investigate these areas the Augmented Virtuality (AV) mobile phone application Home Story was developed for iOS devices. It allows a user to move and interact with objects in a virtual environment displayed on their phone, by physically moving in the real world, completing particular actions to progress a story. A mixed methods study with Home Story either guided participants to the next interaction, or offered them increased agency to choose what object to interact with next. Virtual objects could also be interacted with in one of three ways; imagining the interaction, an embodied interaction using the user’s free hand, or a virtual interaction performed on the phone’s touchscreen. Similar levels of immersion were recorded across both study conditions suggesting both can be effective, though highlighting different issues in each case. The embodied free hand interactions proved particularly memorable, though further work is required to improve their implementation, arising from their novelty and lack of familiarity.},
  keywords={Art;Augmented virtuality;Navigation;Design methodology;Virtual environments;Mobile handsets;Registers;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Applied computing;Arts and humanities;Media arts},
  doi={10.1109/ISMAR52148.2021.00051},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583798,
  author={Quintero, Luis and Muñoz, John E. and Mooij, Jeroen de and Gaebler, Michael},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Excite-O-Meter: Software Framework to Integrate Heart Activity in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={357-366},
  abstract={Bodily signals can complement subjective and behavioral measures to analyze human factors, such as user engagement or stress, when interacting with virtual reality (VR) environments. Enabling widespread use of (also the real-time analysis) of bodily signals in VR applications could be a powerful method to design more user-centric, personalized VR experiences. However, technical and scientific challenges (e.g., cost of research-grade sensing devices, required coding skills, expert knowledge needed to interpret the data) complicate the integration of bodily data in existing interactive applications. This paper presents the design, development, and evaluation of an open-source software framework named Excite-O-Meter. It allows existing VR applications to integrate, record, analyze, and visualize bodily signals from wearable sensors, with the example of cardiac activity (heart rate and its variability) from the chest strap Polar H10. Survey responses from 58 potential users determined the design requirements for the framework. Two tests evaluated the framework and setup in terms of data acquisition/analysis and data quality. Finally, we present an example experiment that shows how our tool can be an easy-to-use and scientifically validated tool for researchers, hobbyists, or game designers to integrate bodily signals in VR applications.},
  keywords={Data visualization;Systems architecture;Kinematics;Tools;Real-time systems;Physiology;Open source software;Bodily Signals;Heart Activity;Software;Architecture;Experiment;Virtual Reality;Open Source;Human-centered computing;Interactive systems and tools;Virtual reality;Computer systems organization;Real-time system architecture},
  doi={10.1109/ISMAR52148.2021.00052},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583803,
  author={Fuhl, Wolfgang and Kasneci, Gjergji and Kasneci, Enkelejda},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={TEyeD: Over 20 Million Real-World Eye Images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types}, 
  year={2021},
  volume={},
  number={},
  pages={367-375},
  abstract={We present TEyeD, the world’s largest unified public data set of eye images taken with head-mounted devices. TEyeD was acquired with seven different head-mounted eye trackers. Among them, two eye trackers were integrated into virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD were obtained from various tasks, including car rides, simulator rides, outdoor sports activities, and daily indoor activities. The data set includes 2D&3D landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and eye movement types for all images. Landmarks and semantic segmentation are provided for the pupil, iris and eyelids. Video lengths vary from a few minutes to several hours. With more than 20 million carefully annotated images, TEyeD provides a unique, coherent resource and a valuable foundation for advancing research in the field of computer vision, eye tracking and gaze estimation in modern VR and AR applications. Data and code at DOWNLOAD LINK.},
  keywords={Image segmentation;Iris;Computer vision;Three-dimensional displays;Annotations;Tracking;Semantics;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems;Machine learning General and reference;Document types;Surveys and overviews},
  doi={10.1109/ISMAR52148.2021.00053},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583791,
  author={Tadeja, Sławomir K. and Langdon, Patrick and Kristensson, Per Ola},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Supporting Iterative Virtual Reality Analytics Design and Evaluation by Systematic Generation of Surrogate Clustered Datasets}, 
  year={2021},
  volume={},
  number={},
  pages={376-385},
  abstract={Virtual Reality (VR) is a promising technology platform for immersive visual analytics. However, the design space of VR analytics interface design is vast and difficult to explore using traditional A/B comparisons in formal or informal controlled experiments— a fundamental part of an iterative design process. A key factor that complicates such comparisons is the dataset. Exposing participants to the same dataset in all conditions introduces an unavoidable learning effect. On the other hand, using different datasets for all experimental conditions introduces the dataset itself as an uncontrolled variable, which reduces internal validity to an unacceptable degree. In this paper, we propose to rectify this problem by introducing a generative process for synthesizing clustered datasets for VR analytics experiments. This process generates datasets that are distinct while simultaneously allowing systematic comparisons in experiments. A key advantage is that these datasets can then be used in iterative design processes. In a two-part experiment, we show the validity of the generative process and demonstrate how new insights in VR-based visual analytics can be gained using synthetic datasets.},
  keywords={Systematics;Codes;Visual analytics;Design methodology;Process control;Tools;Aerospace electronics;Human-centered computing;Visualization;Visualization design and evaluation methods;Visualization application domains;Visual Analytics;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00054},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583794,
  author={Che, Yunlong and Qi, Yue},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Detection-Guided 3D Hand Tracking for Mobile AR Applications}, 
  year={2021},
  volume={},
  number={},
  pages={386-392},
  abstract={Interaction using bare hands is experiencing a growing interest in mobile-based Augmented Reality (AR). Existing RGB-based works fail to provide a practical solution to identifying rich details of the hand. In this paper, we present a detection-guided method capable of recovery 3D hand posture with a color camera. The proposed method consists of key-point detectors and 3D pose optimizer. The detectors first locate the 2D hand bounding box and then apply a lightweight network on the hand region to provide a pixel-wise like-hood of hand joints. The optimizer lifts the 3D pose from the estimated 2D joints in a model-fitting manner. To ensure the result plausibly, we encode the hand shape into the objective function. The estimated 3D posture allows flexible hand-to-mobile interaction in AR applications. We extensively evaluate the proposed approach on several challenging public datasets. The experimental results indicate the efficiency and effectiveness of the proposed method.},
  keywords={Performance evaluation;Solid modeling;Three-dimensional displays;Shape;Detectors;Real-time systems;Mobile handsets;Mobile AR;3D Hand Pose Estimation;Hand Tracking},
  doi={10.1109/ISMAR52148.2021.00055},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583739,
  author={Shen, Junxiao and Dudley, John and Kristensson, Per Ola},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Simulating Realistic Human Motion Trajectories of Mid-Air Gesture Typing}, 
  year={2021},
  volume={},
  number={},
  pages={393-402},
  abstract={The eventual success of many AR and VR intelligent interactive systems relies on the ability to collect user motion data at large scale. Realistic simulation of human motion trajectories is a potential solution to this problem. Simulated user motion data can facilitate prototyping and speed up the design process. There are also potential benefits in augmenting training data for deep learning-based AR/VR applications to improve performance. However, the generation of realistic motion data is nontrivial. In this paper, we examine the specific challenge of simulating index finger movement data to inform mid-air gesture keyboard design. The mid-air gesture keyboard is deployed on an optical see-through display that allows the user to enter text by articulating word gesture patterns with their physical index finger in the vicinity of a visualized keyboard layout. We propose and compare four different approaches to simulating this type of motion data, including a Jerk-Minimization model, a Recurrent Neural Network (RNN)-based generative model, and a Generative Adversarial Network (GAN)-based model with two modes: style transfer and data alteration. We also introduce a procedure for validating the quality of the generated trajectories in terms of realism and diversity. The GAN-based model shows significant potential for generating synthetic motion trajectories to facilitate design and deep learning for advanced gesture keyboards deployed in AR and VR.},
  keywords={Training;Solid modeling;Adaptation models;Layout;Fingers;Keyboards;Training data;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Text input},
  doi={10.1109/ISMAR52148.2021.00056},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583823,
  author={Wieland, Jonathan and Zagermann, Johannes and Müller, Jens and Reiterer, Harald},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Separation, Composition, or Hybrid? – Comparing Collaborative 3D Object Manipulation Techniques for Handheld Augmented Reality}, 
  year={2021},
  volume={},
  number={},
  pages={403-412},
  abstract={Augmented Reality (AR) supported collaboration is a popular topic in HCI research. Previous work has shown the benefits of collaborative 3D object manipulation and identified two possibilities: Either separate or compose users’ inputs. However, their experimental comparison using handheld AR displays is still missing. We, therefore, conducted an experiment in which we tasked 24 dyads with collaboratively positioning virtual objects in handheld AR using three manipulation techniques: 1) Separation – performing only different manipulation tasks (i. e., translation or rotation) simultaneously, 2) Composition – performing only the same manipulation tasks simultaneously and combining individual inputs using a merge policy, and 3) Hybrid – performing any manipulation tasks simultaneously, enabling dynamic transitions between Separation and Composition. While all techniques were similarly effective, Composition was least efficient, with higher subjective workload and worse user experience. Preferences were polarized between clear work division (Separation) and freedom of action (Hybrid). Based on our findings, we offer research and design implications.},
  keywords={Human computer interaction;Three-dimensional displays;Handheld computers;Collaboration;User experience;Object recognition;Task analysis;Augmented reality;mobile devices;collaborative 3D object manipulation.;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / Augmented Reality;Collaborative Interaction},
  doi={10.1109/ISMAR52148.2021.00057},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583734,
  author={Kim, Youngwook and Ko, Yunmin and Ihm, Insung},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Selective Foveated Ray Tracing for Head-Mounted Displays}, 
  year={2021},
  volume={},
  number={},
  pages={413-421},
  abstract={Although ray tracing produces significantly more realistic images than traditional rasterization techniques, it is still considered computationally burdensome when implemented on a head-mounted display (HMD) system that demands both wide field of view and high rendering rate. A further challenge is that to present high-quality images on an HMD screen, a sufficient number of ray samples should be taken per pixel for effective antialiasing to reduce visually annoying artifacts. In this paper, we present a novel foveated real-time rendering framework that realizes classic Whitted-style ray tracing on an HMD system. In particular, our method proposes combining the selective supersampling technique by Jin et al. [8] with the foveated rendering scheme, resulting in perceptually highly efficient pixel sampling suitable for HMD ray tracing. We show that further enhanced by foveated temporal antialiasing, our ray tracer renders nontrivial 3D scenes in real time on commodity GPUs at high sampling rates as effective as up to 36 samples per pixel (spp) in the foveal area, gradually reducing to at least 1 spp in the periphery.},
  keywords={Head-mounted displays;Three-dimensional displays;Resists;Ray tracing;Rendering (computer graphics);Real-time systems;Augmented reality;Computing methodologies;Computer graphics;Rendering;Ray tracing;Human-centered computing;Visualization;Visualization technique},
  doi={10.1109/ISMAR52148.2021.00058},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583808,
  author={Someya, Kiyosato and Itoh, Yuta},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Blending Shadows: Casting Shadows in Virtual and Real using Occlusion-Capable Augmented Reality Near-Eye Displays}, 
  year={2021},
  volume={},
  number={},
  pages={422-430},
  abstract={The fundamental goal of augmented reality (AR) is to integrate virtual objects into the user’s perceived reality seamlessly. However, various issues hinder this integration. In particular, Optical See Through (OST) AR is hampered by the need for light subtraction due to its see-through nature, making some basic rendering harder to realize. In this paper, we realize mutual shadows between real and virtual objects in OST AR to improve this virtual-real integration. Shadows are a classic problem in computer graphics, virtual reality, and video see-through AR, yet they have not been fully explored in OST AR due to the light subtraction requirement. We build a proof-of-concept system that combines a custom occlusion-capable OST display, global light source estimation, 3D registration, and ray-tracing-based rendering. We will demonstrate mutual shadows using a prototype and demonstrate its effectiveness by quantitatively evaluating shadows with the real environment using a perceptual visual metric.},
  keywords={Measurement;Visualization;Three-dimensional displays;Multimedia systems;Estimation;Prototypes;Lighting;Augmented reality;optical see-through displays;occlusion;shadow rendering},
  doi={10.1109/ISMAR52148.2021.00059},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583814,
  author={Hertel, Julia and Karaosmanoglu, Sukran and Schmidt, Susanne and Bräker, Julia and Semmann, Martin and Steinicke, Frank},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Taxonomy of Interaction Techniques for Immersive Augmented Reality based on an Iterative Literature Review}, 
  year={2021},
  volume={},
  number={},
  pages={431-440},
  abstract={Developers of interactive systems have a variety of interaction techniques to choose from, each with individual strengths and limitations in terms of the considered task, context, and users. While there are taxonomies for desktop, mobile, and virtual reality applications, augmented reality (AR) taxonomies have not been established yet. However, recent advances in immersive AR technology (i.e., head-worn or projection-based AR), such as the emergence of untethered headsets with integrated gesture and speech sensors, have enabled the inclusion of additional input modalities and, therefore, novel multimodal interaction methods have been introduced. To provide an overview of interaction techniques for current immersive AR systems, we conducted a literature review of publications between 2016 and 2021. Based on 44 relevant papers, we developed a comprehensive taxonomy focusing on two identified dimensions – task and modality. We further present an adaptation of an iterative taxonomy development method to the field of human-computer interaction. Finally, we discuss observed trends and implications for future work.},
  keywords={Human computer interaction;Headphones;Bibliographies;Interactive systems;Taxonomy;Market research;Sensors;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Interaction design theory;concepts and paradigms},
  doi={10.1109/ISMAR52148.2021.00060},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583736,
  author={Meng, Ming and Xiao, Likai and Zhou, Yi and Li, Zhaoxin and Zhou, Zhong},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Distortion-Aware Room Layout Estimation from A Single Fisheye Image}, 
  year={2021},
  volume={},
  number={},
  pages={441-449},
  abstract={Omnidirectional images of 180° or 360° field of view provide the entire visual content around the capture cameras, giving rise to more sophisticated scene understanding and reasoning and bringing broad application prospects for VR/AR/MR. As a result, researches on omnidirectional image layout estimation have sprung up in recent years. However, existing layout estimation methods designed for panorama images cannot perform well on fisheye images, mainly due to lack of public fisheye dataset as well as the significantly differences in the positions and degree of distortions caused by different projection models. To fill theses gaps, in this work we first reuse the released large-scale panorama datasets and reproduce them to fisheye images via projection conversion, thereby circumventing the challenge of obtaining high-quality fisheye datasets with ground truth layout annotations. Then, we propose a distortion-aware module according to the distortion of the orthographic projection (i.e., OrthConv) to perform effective features extraction from fisheye images. Additionally, we exploit bidirectional LSTM with two-dimensional step mode for horizontal and vertical prediction to capture the long-range geometric pattern of the object for the global coherent predictions even with occlusion and cluttered scenes. We extensively evaluate our deformable convolution for room layout estimation task. In comparison with state-of-the-art approaches, our approach produces considerable performance gains in real-world dataset as well as in synthetic dataset. This technology provides high-efficiency and low-cost technical implementations for VR house viewing and MR video surveillance. We present an MR-based building video surveillance scene equipped with nine fisheye lens can achieve an immersive hybrid display experience, which can be used for intelligent building management in the future.},
  keywords={Training;Visualization;Convolution;Layout;Buildings;Estimation;Network architecture;Layout estimation;Deformable convolution;Fisheye image dataset;Orthographic projection},
  doi={10.1109/ISMAR52148.2021.00061},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583784,
  author={Zhao, Jiayan and Simpson, Mark and Sajjadi, Pejman and Wallgrün, Jan Oliver and Li, Ping and Bagher, Mahda M. and Oprean, Danielle and Padilla, Lace and Klippel, Alexander},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={CrowdXR - Pitfalls and Potentials of Experiments with Remote Participants}, 
  year={2021},
  volume={},
  number={},
  pages={450-459},
  abstract={Although the COVID-19 pandemic has made the need for remote data collection more apparent than ever, progress has been slow in the virtual reality (VR) research community, and little is known about the quality of the data acquired from crowdsourced participants who own a head-mounted display (HMD), which we call crowdXR. To investigate this problem, we report on a VR spatial cognition experiment that was conducted both in-lab and out-of-lab. The in-lab study was administered as a traditional experiment with undergraduate students and dedicated VR equipment. The out-of-lab study was carried out remotely by recruiting HMD owners from VR-related research mailing lists, VR subreddits in Reddit, and crowdsourcing platforms. Demographic comparisons show that our out-of-lab sample was older, included more males, and had a higher sense of direction than our in-lab sample. The results of the involved spatial memory tasks indicate that the reliability of the data from out-of-lab participants was as good as or better than their in-lab counterparts. Additionally, the data for testing our research hypotheses were comparable between in- and out-of-lab studies. We conclude that crowdsourcing is a feasible and effective alternative to the use of university participant pools for collecting survey and performance data for VR research, despite potential design issues that may affect the generalizability of study results. We discuss the implications and future directions of running VR studies outside the laboratory and provide a set of practical recommendations.},
  keywords={Crowdsourcing;Three-dimensional displays;Social networking (online);Resists;Cognition;Spatial databases;Reliability;Virtual reality;crowdsourcing;remote experiments;spatial cognition;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Empirical studies in HCI},
  doi={10.1109/ISMAR52148.2021.00062},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583829,
  author={Masopust, Lukas Maximilian and Bauer, David and Yao, Siyuan and Ma, Kwan-Liu},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Comparison of the Fatigue Progression of Eye-Tracked and Motion-Controlled Interaction in Immersive Space}, 
  year={2021},
  volume={},
  number={},
  pages={460-469},
  abstract={Eye-tracking enabled virtual reality (VR) headsets have recently become more widely available. This opens up opportunities to incorporate eye gaze interaction methods in VR applications. However, studies on the fatigue-induced performance fluctuations of these new input modalities are scarce and rarely provide a direct comparison with established interaction methods. We conduct a study to compare the selection-interaction performance between commonly used handheld motion control devices and emerging eye interaction technology in VR. We investigate each interaction’s unique fatigue progression pattern in study sessions with ten minutes of continuous engagement. The results support and extend previous findings regarding the progression of fatigue in eye-tracked interaction over prolonged periods. By directly comparing gaze-with motion-controlled interaction, we put the emerging eye-trackers into perspective with the state-of-the-art interaction method for immersive space. We then discuss potential implications for future extended reality (XR) interaction design based on our findings.},
  keywords={Performance evaluation;Headphones;Fluctuations;Shape;Extended reality;Aerospace electronics;Fatigue;Human-centered computing;Empirical studies in interaction design;Interaction design theory;concepts and paradigms;Virtual reality;Mixed / augmented reality},
  doi={10.1109/ISMAR52148.2021.00063},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583834,
  author={Fribourg, Rebecca and Peillard, Etienne and McDonnell, Rachel},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mirror, Mirror on My Phone: Investigating Dimensions of Self-Face Perception Induced by Augmented Reality Filters}, 
  year={2021},
  volume={},
  number={},
  pages={470-478},
  abstract={The main use of Augmented Reality (AR) today for the general public is in applications for smartphones. In particular, social network applications allow the use of many AR filters, modifying users’ environments but also their own image. These AR filters are increasingly and frequently being used and can distort in many ways users’ facial traits. Yet, we still do not know clearly how users perceive their faces augmented by these filters. In this paper, we present a study that aims to evaluate the impact of different filters, modifying several facial features such as the size or position of the eyes, the shape of the face or the orientation of the eyebrows, or adding virtual content such as virtual glasses. These filters are evaluated via a self-evaluation questionnaire, asking the participants about the personality, emotion, appeal and intelligence traits that their distorted face conveys. Our results show relative effects between the different filters in line with previous results regarding the perception of others. However, they also reveal specific effects on self-perception, showing, inter alia, that facial deformation decreases participants’ credence towards their image. The findings of this study covering multiple factors allow us to highlight the impact of face deformation on user perception but also the specificity related to this use in AR, paving the way for new works focusing on the psychological impact of such filters.},
  keywords={Social networking (online);Shape;Pandemics;Psychology;Mirrors;Eyebrows;Faces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR52148.2021.00064},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583841,
  author={Pereira, Nuno and Rowe, Anthony and Farb, Michael W and Liang, Ivan and Lu, Edward and Riebling, Eric},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ARENA: The Augmented Reality Edge Networking Architecture}, 
  year={2021},
  volume={},
  number={},
  pages={479-488},
  abstract={Many have predicted the future of the Web to be the integration of Web content with the real-world through technologies such as Augmented Reality (AR). This has led to the rise of Extended Reality (XR) Web Browsers used to shorten the long AR application development and deployment cycle of native applications especially across different platforms. As XR Browsers mature, we face new challenges related to collaborative and multi-user applications that span users, devices, and machines. These collaborative XR applications require: (1) networking support for scaling to many users, (2) mechanisms for content access control and application isolation, and (3) the ability to host application logic near clients or data sources to reduce application latency. In this paper, we present the design and evaluation of the AR Edge Networking Architecture (ARENA) which is a platform that simplifies building and hosting collaborative XR applications on WebXR capable browsers. ARENA provides a number of critical components including: a hierarchical geospatial directory service that connects users to nearby servers and content, a token-based authentication system for controlling user access to content, and an application/service runtime supervisor that can dispatch programs across any network connected device. All of the content within ARENA exists as endpoints in a PubSub scene graph model that is synchronized across all users. We evaluate ARENA in terms of client performance as well as benchmark end-to-end response-time as load on the system scales. We show the ability to horizontally scale the system to Internet-scale with scenes containing hundreds of users and latencies on the order of tens of milliseconds. Finally, we highlight projects built using ARENA and showcase how our approach dramatically simplifies collaborative multi-user XR development compared to monolithic approaches.},
  keywords={Performance evaluation;Runtime;Collaboration;Computer architecture;Browsers;Geospatial analysis;Synchronization;Computer systems organization;Architectures;Distributed architectures;Computing methodologies;Computer graphics;Graphics systems and interfaces Mixed / augmented reality},
  doi={10.1109/ISMAR52148.2021.00065},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583832,
  author={Lu, Edward and Miller, John and Pereira, Nuno and Rowe, Anthony},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={FLASH: Video-Embeddable AR Anchors for Live Events}, 
  year={2021},
  volume={},
  number={},
  pages={489-497},
  abstract={Public spaces like concert stadiums and sporting arenas are ideal venues for AR content delivery to crowds of mobile phone users. Unfortunately, these environments tend to be some of the most challenging in terms of lighting and dynamic staging for vision-based relocalization. In this paper, we introduce FLASH1, a system for delivering AR content within challenging lighting environments that uses active tags (i.e., blinking) with detectable features from passive tags (quads) for marking regions of interest and determining pose. This combination allows the tags to be detectable from long distances with significantly less computational overhead per frame, making it possible to embed tags in existing video displays like large jumbotrons. To aid in pose acquisition, we implement a gravity-assisted pose solver that removes the ambiguous solutions that are often encountered when trying to localize using standard passive tags. We show that our technique outperforms similarly sized passive tags in terms of range by 20-30% and is fast enough to run at 30 FPS even within a mobile web browser on a smartphone.},
  keywords={Visualization;Codes;Pose estimation;Lighting;Cameras;Mobile handsets;Browsers},
  doi={10.1109/ISMAR52148.2021.00066},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583785,
  author={Matsumoto, Keigo and Aoyama, Kazuma and Narumi, Takuji and Kuzuoka, Hideaki},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Redirected Walking using Noisy Galvanic Vestibular Stimulation}, 
  year={2021},
  volume={},
  number={},
  pages={498-507},
  abstract={In this study, considering the characteristics of multisensory integration, we examined a method for improving redirected walking (RDW) by adding noise to the vestibular system to reduce the effects of vestibular inputs on self-motion perception. In RDW, the contradiction between vestibular inputs and visual sensations may make users notice the RDW manipulation, resulting in discomfort throughout the experience. Because humans integrate multisensory information by considering the reliability of each modality, by reducing the effects of vestibular inputs on self-motion perception, it is possible to suppress awareness of and discomfort during RDW manipulation and improve the effectiveness of the manipulation. Therefore, we hypothesized that adding noise to the vestibular inputs would reduce the reliability of the vestibular sensations and enhances the effectiveness of RDW by improving the relative reliability of vision. In this study, we used noisy galvanic vestibular stimulation (GVS) to reduce the reliability of vestibular inputs. GVS is a method of stimulating vestibular organs and nerves by applying small electrical currents to the bilateral mastoid. To reduce the reliability of vestibular inputs, we employed noisy GVS whose current pattern is white noise. We experimented with comparing the threshold of curvature gains between noisy GVS conditions and a control condition.},
  keywords={Legged locomotion;Visualization;Sensitivity;Pain;Pacemakers;Reliability theory;White noise;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR52148.2021.00067},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583801,
  author={Mandl, David and Roth, Peter M. and Langlotz, Tobias and Ebner, Christoph and Mori, Shohei and Zollmann, Stefanie and Mohr, Peter and Kalkofen, Denis},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering}, 
  year={2021},
  volume={},
  number={},
  pages={508-516},
  abstract={Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user’s real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using neural networks. Our system allows for adding new cameras to the framework by learning the visual properties from a database of images that has been captured using the physical camera. We present qualitative and quantitative results and discuss future direction for research that emerge from using Neural Cameras.},
  keywords={Visualization;Runtime;Three-dimensional displays;Pipelines;Mixed reality;Coherence;Tools;Visual Coherence;Mixed Reality;Cameras},
  doi={10.1109/ISMAR52148.2021.00068},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583790,
  author={Klein, Vanessa and Leuschner, Markus and Langen, Tobias and Kurth, Philipp and Stamminger, Marc and Bauer, Frank},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Scan&Paint: Image-based Projection Painting}, 
  year={2021},
  volume={},
  number={},
  pages={517-525},
  abstract={We present a pop-up projection painting system that projects onto an unknown three-dimensional surface, while the user creates the projection content on the fly. The digital paint is projected immediately and follows the object if it is moved. If unexplored surface areas are thereby exposed, an automated trigger system issues new depth recordings that expand and refine the surface estimate. By intertwining scanning and projection painting we scan the exposed surface at the appropriate time and only if needed. Like image-based rendering, multiple automatically recorded depth maps are fused in screen space to synthesize novel views of the object, making projection poses independent from the scan positions. Since the user’s digital paint is also stored in images, we eliminate the need to reconstruct and parametrize a single full mesh, which makes geometry and color updates simple and fast.},
  keywords={Geometry;Surface reconstruction;Three-dimensional displays;Image color analysis;Rendering (computer graphics);Paints;Image reconstruction;Computing methodologies;Mixed / augmented reality;Image-based rendering;Reconstruction},
  doi={10.1109/ISMAR52148.2021.00069},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9583802,
  author={},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Author Index}, 
  year={2021},
  volume={},
  number={},
  pages={527-529},
  abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
  keywords={},
  doi={10.1109/ISMAR52148.2021.00070},
  ISSN={1554-7868},
  month={Oct},}
