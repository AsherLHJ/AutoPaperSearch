@INPROCEEDINGS{10316512,
  author={Lim, Seoyeon and Dong, Suh-Yeon},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of Interaction with Virtual Pets on Self-Disclosure in Mixed Reality}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Self-disclosure involves revealing information about oneself to others and is critical in relationship formation to develop trust and understanding, leading to emotional intimacy. In psychotherapy, inducing self-disclosure is critical for a therapist to clearly understand clients and suggest relevant solutions. Companion animals have been known to increase human self-disclosure; hence, we hypothesized that virtual animals could have the same effect, which can be strengthened through interaction. To verify this hypothesis, we implemented a mixed-reality-based interaction between humans and virtual cats through Unity. Participants could interact with the virtual cat using hand gestures and voice commands. Psychological states related to self-disclosure were evaluated using questionnaires after the interaction. Furthermore, participants’ responses to the virtual cats were compared with their responses to non-interactive virtual contents. Participants exhibited higher willingness for self-disclosure with virtual cats compared with virtual humans. Interacting with virtual cats was also found to encourage more self-disclosure than interactions with non-interactive states content. Therefore, virtual animals can induce self-disclosure and can be used in psychotherapy. Our findings demonstrate that virtual animals can be used to provide solutions to mental health problems and can be widely applied in the field of psychotherapy.},
  keywords={Surveys;Animals;Mixed reality;Mental health;Standards;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction design;Interaction design theory, concepts and paradigms},
  doi={10.1109/ISMAR59233.2023.00014},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316411,
  author={Kovacs, Balint Istvan and Erb, Ingrid and Kaufmann, Hannes and Ferschin, Peter},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MR.Sketch. Immediate 3D Sketching via Mixed Reality Drawing Canvases}, 
  year={2023},
  volume={},
  number={},
  pages={10-19},
  abstract={Sketching is a fundamental technique for early design and form finding. Digital 3D sketching can improve the early design phase by improving spatial understanding and enriching the design with additional information. However, the tools used for sketching should not hinder the expression and thought process inherent in form finding. Methods already exist for using 2D pen-on-tablet input for 3D sketching via stroke projection onto 3D drawing canvases. However, positioning the canvas and sketching lines are separate work steps. This breaks the flow of the designer’s thought process. We propose a novel technique for mixed reality 3D sketching that involves the use of viewport-attached drawing canvases, spatial meshing and intersection canvas visualisation. By combining the inside-out tracking capabilities of current portable consumer devices with stylus-on-tablet freehand drawing input, we transform 2D to 3D projective sketching into a more seamless experience. Results of a pilot user study with 16 participants show significant user preference for our technique, as well as increased sketching speed and immediacy.},
  keywords={Visualization;Three-dimensional displays;Buildings;Mixed reality;Transforms;Augmented reality;Human-centered computing;Interaction paradigms;Mixed / augmented reality;Applied computing;Arts and humanities;Architecture (buildings)},
  doi={10.1109/ISMAR59233.2023.00015},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316445,
  author={Owaki, Koichi and Techasarntikul, Nattaon and Shimonishi, Hideyuki},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Human Behavior Analysis in Human-Robot Cooperation with AR Glasses}, 
  year={2023},
  volume={},
  number={},
  pages={20-28},
  abstract={To achieve efficient human-robot cooperation, it is necessary to work in close proximity while ensuring safety. However, in conventional robot control, maintaining a certain distance between humans and robots is required for safety, owing to control uncertainties and unexpected human actions, which can limit the efficiency of robot operations. Therefore, this study aims to establish a human-robot cooperation aiding system that concerns both safety and efficiency in a close proximity situation. We propose two Augmented Reality (AR) interfaces to display robot information via AR glasses, allowing workers to see the robot information while focusing on their task and avoiding collisions with the robot. AR glasses can give hands-free communication required for a work environment like warehouses or convenience store backyards, and multiple information levels, simple or informative, to balance accuracy and easiness of human recognition ability. We conducted a comparative evaluation experiments with 24 participants and found that both safety and efficiency were improved using the proposed user interfaces (UIs). We also collected the position, head motion, and eye-tracking data from the AR glasses to gain insight into human behavior during the tasks for each UI. Consequently, we clarified the behavior of the participants under each condition and how they contributed to safety and efficiency.},
  keywords={Visualization;Navigation;Glass;Gaze tracking;Safety;Behavioral sciences;Task analysis;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR59233.2023.00016},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316467,
  author={Kreber, Lucas and Diehl, Stephan},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Comparative Evaluation of Tabs and Linked Panels for Program Understanding in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={29-38},
  abstract={Integrated development environments (IDEs) commonly employ a tab-based interface for displaying source code, which often poses challenges in efficient code navigation and retrieval. Previous research has proposed several novel approaches that have in common that they place code fragments on a 2D canvas and draw visual connections between them. In this paper, we investigate the extension of such interfaces to augmented reality (AR) environments. As AR allows to display information in three dimensions, the restriction to a 2D canvas for placing code fragments is not justified, and we implement it by allowing users to place code panels freely in 3D space. We call the resulting interface linked panels. We present the results of a quantitative user study conducted with 24 participants, aiming to explore whether the benefits observed for the canvas-based approach in traditional 2D screen environments can be replicated with linked panels in augmented reality. The participants were given tasks to identify and resolve two bugs in two different software projects using the traditional tab-based and the panel-based approaches in AR. To find possible explanations of our quantitative results we also conducted a qualitative analysis evaluating participants’ comments and different placement strategies of panels in the panel-based approach. Our results indicate that participants found more bugs with the tabs-version, but were equally fast with both tools. We also found that less skilled participants were faster with the tabs, while more skilled ones were faster with the panels. Although, participants experienced problems with the cluttered spatial arrangement of the panels, they preferred the panels version over the tabs version as it made better use of AR.},
  keywords={Visualization;Codes;Three-dimensional displays;Source coding;Computer bugs;Two dimensional displays;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Software and its engineering;Software notations and tools;Development frameworks and environments;Integrated and visual development environments},
  doi={10.1109/ISMAR59233.2023.00017},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316443,
  author={Zhao, Junhong and Tran, Kien T. P. and Chalmers, Andrew and Hoh, Weng Khuan and Yao, Richard and Dey, Arindam and Wilmott, James and Lin, James and Billinghurst, Mark and Lindeman, Robert W. and Rhee, Taehyun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Deep Learning-based Simulator Sickness Estimation from 3D Motion}, 
  year={2023},
  volume={},
  number={},
  pages={39-48},
  abstract={This paper presents a novel solution for estimating simulator sickness in HMDs using machine learning and 3D motion data, informed by user-labeled simulator sickness data and user analysis. We conducted a novel VR user study, which decomposed motion data and used an instant dial-based sickness scoring mechanism. We were able to emulate typical VR usage and collect user simulator sickness scores. Our user analysis shows that translation and rotation differently impact user simulator sickness in HMDs. In addition, users’ demographic information and self-assessed simulator sickness susceptibility data are collected and show some indication of potential simulator sickness. Guided by the findings from the user study, we developed a novel deep learning-based solution to better estimate simulator sickness with decomposed 3D motion features and user profile information. The model was trained and tested using the 3D motion dataset with user-labeled simulator sickness and profiles collected from the user study. The results show higher estimation accuracy when using the 3D motion data compared with methods based on optical flow extracted from the recorded video, as well as improved accuracy when decomposing the motion data and incorporating user profile information.},
  keywords={Solid modeling;Image motion analysis;Computer vision;Three-dimensional displays;Estimation;Resists;Machine learning;Virtual reality;machine learning;simulator sickness;estimation;user study;motion analysis},
  doi={10.1109/ISMAR59233.2023.00018},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316504,
  author={Jarrell, Marie A. and Peillard, Etienne},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Using Identification with AR Face Filters to Predict Explicit & Implicit Gender Bias}, 
  year={2023},
  volume={},
  number={},
  pages={49-58},
  abstract={Augmented Reality (AR) filters, such as those used by social media platforms like Snapchat and Instagram, are perhaps the most commonly used AR technology. As with fully immersive Virtual Reality (VR) systems, individuals can use AR to embody different people. This experience in VR has been able to influence real world biases such as sexism. However, there is little to no comparative research on AR embodiment’s impact on societal biases. This study aims to set groundwork by examining possible connections between using gender changing Snapchat AR face filters and a person’s predicted implicit and explicit gender biases. We discovered that participants who experienced identification with gender manipulated versions of themselves showed both greater and lesser amounts of bias against men and women. These results depended the user’s gender, the filter applied, and the level of identification users reported with their AR manipulated selves. The results were similar to past VR findings but offered unique AR observations that could be useful for future bias intervention efforts.},
  keywords={Social networking (online);Design methodology;Multimedia Web sites;Emulation;Oral communication;Faces;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR59233.2023.00019},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316440,
  author={Krauß, Veronika and Berkholz, Jenny and Recki, Lena and Boden, Alexander},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Beyond Well-Intentioned: An HCI Students’ Ethical Assessment of Their Own XR Designs}, 
  year={2023},
  volume={},
  number={},
  pages={59-68},
  abstract={Foreseeing the impact of augmented and virtual reality applications on users and society is challenging. Thus, efforts to establish an ethical mindset and include technology assessment techniques in HCI education are increasing. However, XR educational courses fostering students’ reasoning and perceived responsibility in designing ethical applications are lacking. We, therefore, developed the explorative design method Reality Composer to investigate and foster the students’ assessment of the ethical impact of and responsibilities in XR application design. We conducted a workshop with 40 international HCI master students applying this method and analyzed the resulting application concepts regarding the students’ ethical assessment. Our findings show that they critically discussed their concepts’ impact and identified potential countermeasures for negative social implications. However, they overestimated the users’ responsibility to securely use XR applications as well as a positive design intention. We contribute with our findings and developed method to understand students’ potential and derive future course design implications.},
  keywords={Human computer interaction;Training;Ethics;Conferences;Design methodology;Educational courses;Cognition;Human-centered computing;Human computer interaction (HCI);Mixed / augmented reality;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00020},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316522,
  author={Zhou, Xuemei and Viola, Irene and Alexiou, Evangelos and Jansen, Jack and Cesar, Pablo},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={QAVA-DPC: Eye-Tracking Based Quality Assessment and Visual Attention Dataset for Dynamic Point Cloud in 6 DoF}, 
  year={2023},
  volume={},
  number={},
  pages={69-78},
  abstract={Perceptual quality assessment of Dynamic Point Cloud (DPC) contents plays an important role in various Virtual Reality (VR) applications that involve human beings as the end user, understanding and modeling perceptual quality assessment is greatly enriched by insights from visual attention. However, incorporating aspects of visual attention in DPC quality models is largely unexplored, as ground-truth visual attention data is scarcely available. This paper presents a dataset containing subjective opinion scores and visual attention maps of DPCs, collected in a VR environment using eye-tracking technology. The data was collected during a subjective quality assessment experiment, in which subjects were instructed to watch and rate DPCs at various degradation levels under 6 degrees-of-freedom inspection, using a head-mounted display. The dataset comprises 5 reference DPC contents, with each reference encoded at 3 distortion levels using 3 different codecs, amounting to a total of 9 degraded DPC contents. Moreover, it includes 1,000 gaze trials from 40 participants, resulting in 15,000 visual attention maps in total. The curated dataset can serve as authentic benchmark data for assessing the performance of objective DPC quality metrics. Additionally, it establishes a link between quality assessment and visual attention within the context of DPC. This work deepens our understanding of DPC quality and visual attention, driving progress in the realm of VR experiences and perception.},
  keywords={Point cloud compression;Measurement;Visualization;Solid modeling;Head-mounted displays;Gaze tracking;Inspection;Volumetric video;Dynamic point cloud;Visual saliency;Visual attention;Subjective quality assessment;Objective quality metrics;Eye tracking;6DoF},
  doi={10.1109/ISMAR59233.2023.00021},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316499,
  author={Wang, Junyi and Qi, Yue},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Scene-independent Localization by Learning Residual Coordinate Map with Cascaded Localizers}, 
  year={2023},
  volume={},
  number={},
  pages={79-88},
  abstract={Visual localization plays an essential role in a variety of different fields. The indirect learning based method obtains an excellent performance, but it requests a training process in the target scene before the localization. To achieve deep scene-independent localization, we start by proposing the representation called residual coordinate map between a pair of images. Based on the structure, we put forward a network called SILocNet with the proposed residual coordinate map as the output. The network consists of feature extraction, multi-level feature fusion and transformer based coordinate decoder. Moreover, considering the dynamic scene, we introduce an additional segmentation branch that distinguishes fixed and dynamic parts to promote network perception. With SILocNet in place, a cascaded localizer design is presented for reducing the accumulative error. Meanwhile, the simple mathematical analysis behind the cascaded localizers is also provided. To verify how well our algorithm could perform, we conduct experiments on static 7 Scenes, ScanNet and dynamic TUM RGB-D. In particular, we train the network on ScanNet and test it on 7 Scenes and TUM RGB-D to demonstrate the generality performance. All experiments demonstrate superior performance to other existing methods. Additionally, the effects of the cascaded localizer design, feature fusion, transformer based coordinate decoder and segmentation loss are also discussed.},
  keywords={Location awareness;Training;Learning systems;Visualization;Image segmentation;Heuristic algorithms;Mathematical analysis;Scene-independent localization;residual coordinate map;cascaded localizer;dynamic scene},
  doi={10.1109/ISMAR59233.2023.00022},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316473,
  author={Song, Hail and Yoon, Boram and Cho, Woojin and Woo, Woontack},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={RC-SMPL: Real-time Cumulative SMPL-based Avatar Body Generation}, 
  year={2023},
  volume={},
  number={},
  pages={89-98},
  abstract={We present a novel method for avatar body generation that cumulatively updates the texture and normal map in real-time. Multiple images or videos have been broadly adopted to create detailed 3D human models that capture more realistic user identities in both Augmented Reality (AR) and Virtual Reality (VR) environments. However, this approach has a higher spatiotemporal cost because it requires a complex camera setup and extensive computational resources. For lightweight reconstruction of personalized avatar bodies, we design a system that progressively captures the texture and normal values using a single RGBD camera to generate the widely-accepted 3D parametric body model, SMPL-X. Quantitatively, our system maintains real-time performance while delivering reconstruction quality comparable to the state-of-the-art method. Moreover, user studies reveal the benefits of real-time avatar creation and its applicability in various collaborative scenarios. By enabling the production of high-fidelity avatars at a lower cost, our method provides more general way to create personalized avatar in AR/VR applications, thereby fostering more expressive self-representation in the metaverse.},
  keywords={Solid modeling;Three-dimensional displays;Avatars;Computational modeling;Cameras;Real-time systems;Neck;Computing methodologies;Computer graphics;Image manipulation;Texturing;Artificial intelligence;Computer vision;Reconstruction},
  doi={10.1109/ISMAR59233.2023.00023},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316461,
  author={Martin-Gomez, Alejandro and Merkl, Felix and Winkler, Alexander and Heiliger, Christian and Eck, Ulrich and Karcz, Konrad and Navab, Nassir},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Closer Look at Dynamic Medical Visualization Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={99-108},
  abstract={In navigated surgery, physicians perform complex tasks assisted by virtual representations of anatomical structures and surgical tools. Integrating Augmented Reality (AR) in these scenarios enriches the information presented to the surgeon through a range of visualization techniques. Their selection is a crucial task as they represent the primary interface between the system and the surgeon.In this work, we present a novel approach to conveying augmented content using dynamic visualization techniques, allowing users to gather depth and shape information from both pictorial and kinetic cues. We conducted user studies comparing two novel dynamic methods – Object Flow and Wave Propagation – and three state-of-the-art static visualization techniques among medical experts. Our studies provide a detailed comparison of the visualization techniques’ efficacy in conveying shape and depth information from medical data, as well as task load and usability reported by the participants and post hoc analyses. We found that kinetic cues can assist users in understanding complex anatomical structures in medical AR.},
  keywords={Shape;Propagation;Surgery;Data visualization;Anatomical structure;Kinetic theory;Sparks;Human-centered computing;Visualization;Visualization techniques;Empirical studies in visualization},
  doi={10.1109/ISMAR59233.2023.00024},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316515,
  author={Lu, Feiyu and Xu, Yan and Xu, Xuhai and Jones, Brennan and Malamed, Laird},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Impact of User and System Factors on Human-AI Interactions in Head-Worn Displays}, 
  year={2023},
  volume={},
  number={},
  pages={109-118},
  abstract={Empowered by the rich sensory capabilities and the advancements in artificial intelligence (AI), head-worn displays (HWD) could understand the user’s contexts and provide just-in-time assistance to users’ tasks to augment their everyday lives. However, there has been limited understanding of how users perceive interacting with AI services, and how different factors impact the user experience in HWD applications. In this research, we investigated broadly what user and system factors play important roles in human-AI experiences during an AI-assisted spatial task. We conducted a user study to simulate an everyday scenario where augmented reality (AR) glasses could provide suggestions/assistance. We researched three AI system factors (performance, initiation, transparency) with multiple user factors (personality traits, trust propensity, and prior trust with AI). We not only identified the impact of user traits such as the levels of conscientiousness and prior trust with the AI, but also found interesting interactions between them and system factors such as AI’s performance and initiation strategy. Based on the findings, we suggest that future AI assistance on HWD needs to take users’ individual characteristics into account and customize the system design accordingly.},
  keywords={Head-mounted displays;Wearable computers;Glass;User experience;Artificial intelligence;Task analysis;Augmented reality;Human-centered computing;Interaction paradigms;Mixed/augmented reality;Interaction techniques},
  doi={10.1109/ISMAR59233.2023.00025},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316441,
  author={Ablett, Daniel and Cunningham, Andrew and Lee, Gun A. and Thomas, Bruce H.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Point & Portal: A New Action at a Distance Technique For Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={119-128},
  abstract={This paper introduces Point & Portal, a novel Virtual Reality (VR) interaction technique, inspired by Point & Teleport. This new technique enables users to configure portals using pointing actions, and supports seamless action at a distance and navigation without requiring line of sight. By supporting multiple portals, Point & Portalenables users to create dynamic portal configurations to manage multiple remote tasks. Additionally, this paper introduces Relative Portal Positioning for reliable portal interactions, and the concept of maintaining Level Portals. In a comparative user study, Point & Portal demonstrated significant advantages over the traditional Point & Teleport technique to bring interaction devices within-arm’s reach. In the presence of obstacles, Point & Portal exhibited faster speed, lower cognitive load and was preferred by participants. Overall, participants required less physical movement, pointing actions, and reported higher involvement and “good” usability.},
  keywords={Navigation;Teleportation;Cognitive load;Reliability;Usability;Task analysis;Portals;Human-centered computing-Human computer interaction (HCI)-Interaction techniques},
  doi={10.1109/ISMAR59233.2023.00026},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316480,
  author={Derksen, Melanie and Becker, Julia and Elahi, Mohammad Fazleh and Maier, Angelika and Maile, Marius and Pätzold, Ingo and Penningroth, Jonas and Reglin, Bettina and Rothgänger, Markus and Cimiano, Philipp and Schubert, Erich and Schwandt, Silke and Kuhlen, Torsten and Botsch, Mario and Weissker, Tim},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Who Did What When? Discovering Complex Historical Interrelations in Immersive Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={129-137},
  abstract={Traditional digital tools for exploring historical data mostly rely on conventional 2D visualizations, which often cannot reveal all relevant interrelationships between historical fragments (e.g., persons or events). In this paper, we present a novel interactive exploration tool for historical data in VR, which represents fragments as spheres in a 3D environment and arranges them around the user based on their temporal, geo, categorical and semantic similarity. Quantitative and qualitative results from a user study with 29 participants revealed that most participants considered the virtual space and the abstract fragment representation well-suited to explore historical data and to discover complex interrelationships. These results were particularly underlined by high usability scores in terms of attractiveness, stimulation, and novelty, while researching historical facts with our system did not impose unexpectedly high task loads. Additionally, the insights from our post-study interviews provided valuable suggestions for future developments to further expand the possibilities of our system.},
  keywords={Three-dimensional displays;Semantics;Data visualization;Usability;Task analysis;Interviews;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Visualization systems and tools},
  doi={10.1109/ISMAR59233.2023.00027},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316426,
  author={Vanukuru, Rishi and Weng, Suibi Che-Chuan and Ranjan, Krithik and Hopkins, Torin and Banic, Amy and Gross, Mark D. and Do, Ellen Yi-Luen},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DualStream: Spatially Sharing Selves and Surroundings using Mobile Devices and Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={138-147},
  abstract={In-person human interaction relies on our spatial perception of each other and our surroundings. Current remote communication tools partially address each of these aspects. Video calls convey real user representations but without spatial interactions. Augmented and Virtual Reality (AR/VR) experiences are immersive and spatial but often use virtual environments and characters instead of real-life representations. Bridging these gaps, we introduce DualStream, a system for synchronous mobile AR remote communication that captures, streams, and displays spatial representations of users and their surroundings. DualStream supports transitions between user and environment representations with different levels of visuospatial fidelity, as well as the creation of persistent shared spaces using environment snapshots. We demonstrate how DualStream can enable spatial communication in real-world contexts, and support the creation of blended spaces for collaboration. A formative evaluation of DualStream revealed that users valued the ability to interact spatially and move between representations, and could see DualStream fitting into their own remote communication practices in the near future. Drawing from these findings, we discuss new opportunities for designing more widely accessible spatial communication tools, centered around the mobile phone.},
  keywords={Headphones;Fitting;Collaboration;Virtual environments;Cameras;Mobile handsets;Sensors;Human-centered computing;Mixed/augmented reality;Collaborative Interaction;Mobile computing},
  doi={10.1109/ISMAR59233.2023.00028},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316469,
  author={Akiyoshi, Takuto and Shimizu, Yuki and Takahama, Yusaku and Nagata, Koki and Sawabe, Taishi},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Hype D-Live: XR Live Music System to Entertain Passengers for Anxiety Reduction in Autonomous Vehicles}, 
  year={2023},
  volume={},
  number={},
  pages={148-156},
  abstract={Passengers in autonomous vehicles enjoy the comfort of being free from driving tasks, but they inevitably experience anxiety caused by autonomous vehicle stress (AVS). AVS encompasses vehicle behavior stress due to unpredictable acceleration, and external environmental stress due to potential collisions. Past research has explored approaches to improve passengers’ comfort through behavior control and information presentation. However, methods that utilize stressful vehicle behavior in Extended Reality (XR) entertainment to distract from AVS-related anxiety are limited. Hence, the goal of this study was to maximize passenger comfort in automated vehicles. To achieve this goal, we implemented an XR entertainment system that utilizes vehicle behavior and evaluated its effect on reducing anxiety. In this study, we proposed “Hype D-Live”, an XR live music system designed to reduce anxiety by providing multimodal visual, auditory, force, and vestibular stimuli using a hemispherical display and motion platform mounted on a vehicle. We developed system functions to adjust the force and vestibular senses according to the excitement level of the music and the direction of stressful acceleration and to reproduce moshing, a characteristic behavior at live music venues. However, we hypothesized that passengers might not fully enjoy the entertainment and could experience anxiety if the video content makes them aware of the external environment. Therefore, we conducted an experiment with a within-participant design, involving 24 participants (14 males and 10 females), comparing 3 types of video content for XR entertainment inside the autonomous vehicle: a real external environment, a virtual simulation of the external environment, and a virtual live music venue. The Wilcoxon signed rank test with the Bonferroni correction after the Friedman test revealed that, without the moshing function, the virtual live music venue video significantly enhanced enjoyment and reduced anxiety, compared to the real one.},
  keywords={Visualization;Extended reality;Design methodology;Anxiety disorders;Force;Entertainment industry;Behavioral sciences;Human-centered computing;Interaction paradigms;Virtual reality;HCI design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00029},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316508,
  author={Stanescu, Ana and Mohr, Peter and Kozinski, Mateusz and Mori, Shohei and Schmalstieg, Dieter and Kalkofen, Denis},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={State-Aware Configuration Detection for Augmented Reality Step-by-Step Tutorials}, 
  year={2023},
  volume={},
  number={},
  pages={157-166},
  abstract={Presenting tutorials in augmented reality is a compelling application area, but previous attempts have been limited to objects with only a small numbers of parts. Scaling augmented reality tutorials to complex assemblies of a large number of parts is difficult, because it requires automatically discriminating many similar-looking object configurations, which poses a challenge for current object detection techniques. In this paper, we seek to lift this limitation. Our approach is inspired by the observation that, even though the number of assembly steps may be large, their order is typically highly restricted: Some actions can only be performed after others. To leverage this observation, we enhance a state-of-the-art object detector to predict the current assembly state by conditioning on the previous one, and to learn the constraints on consecutive states. This learned ‘consecutive state prior’ helps the detector disambiguate configurations that are otherwise too similar in terms of visual appearance to be reliably discriminated. Via the state prior, the detector is also able to improve the estimated probabilities that a state detection is correct. We experimentally demonstrate that our technique enhances the detection accuracy for assembly sequences with a large number of steps and on a variety of use cases, including furniture, Lego and origami. Additionally, we demonstrate the use of our algorithm in an interactive augmented reality application.},
  keywords={Visualization;Tutorials;Detectors;Object detection;Reliability engineering;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Machine learning;Learning settings;Learning from demonstrations},
  doi={10.1109/ISMAR59233.2023.00030},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316428,
  author={Zhou, Kanglei and Chen, Chen and Ma, Yue and Leng, Zhiying and Shum, Hubert P. H. and Li, Frederick W. B. and Liang, Xiaohui},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Mixed Reality Training System for Hand-Object Interaction in Simulated Microgravity Environments}, 
  year={2023},
  volume={},
  number={},
  pages={167-176},
  abstract={As human exploration of space continues to progress, the use of Mixed Reality (MR) for simulating microgravity environments and facilitating training in hand-object interaction holds immense practical significance. However, hand-object interaction in microgravity presents distinct challenges compared to terrestrial environments due to the absence of gravity. This results in heightened agility and inherent unpredictability of movements that traditional methods struggle to simulate accurately. To this end, we propose a novel MR-based hand-object interaction system in simulated microgravity environments, leveraging physics-based simulations to enhance the interaction between the user’s real hand and virtual objects. Specifically, we introduce a physics-based hand-object interaction model that combines impulse-based simulation with penetration contact dynamics. This accurately captures the intricacies of hand-object interaction in microgravity. By considering forces and impulses during contact, our model ensures realistic collision responses and enables effective object manipulation in the absence of gravity. The proposed system presents a cost-effective solution for users to simulate object manipulation in microgravity. It also holds promise for training space travelers, equipping them with greater immersion to better adapt to space missions. The system reliability and fidelity test verifies the superior effectiveness of our system compared to the state-of-the-art CLAP system.},
  keywords={Training;Adaptation models;Space missions;Dynamics;Mixed reality;Reliability;Augmented reality;Human-centered computing-Human computer interaction-Interaction paradigms-Mixed/augmented reality},
  doi={10.1109/ISMAR59233.2023.00031},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316431,
  author={Chi, Ting-Hsun and Perng, Wen and Chen, Homer},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perceptual Tolerance of Split-Up Effect for Near-Eye Light Field Display}, 
  year={2023},
  volume={},
  number={},
  pages={177-184},
  abstract={When the light field of a scene is generated with a finite number of subviews, the defocused regions would appear to be split-up if a camera is used to capture the light field. Yet, the split-up effect is unnoticeable when the light field is viewed directly through a human eye. In this paper, we attribute the unobservability of the split-up effect to the decrease in visual acuity as a function of retinal eccentricity and to the low-pass filtering property of visual attention. Theoretical and experimental results are provided to support our claim. Furthermore, we set an observability criterion for the split-up effect and discuss design strategies for performance improvement of light field displays.},
  keywords={Measurement;Image quality;Visualization;Image resolution;Focusing;Cameras;Retina;Light field display;augmented reality;retinal eccentricity;split-up effect;visual perception;Hardware;Communication hardware;Interfaces and storage;Mixed/augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception},
  doi={10.1109/ISMAR59233.2023.00032},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316419,
  author={Pochwatko, Grzegorz and Kopec, Wieslaw and Swidrak, Justyna and Jaskulska, Anna and Skorupska, Kinga H. and Karpowicz, Barbara and Masłyk, Rafał and Grzeszczuk, Maciej and Barnes, Steven and Borkiewicz, Paulina and Kobyliński, Paweł and Pabiś-Orzeszyna, Michał and Balas, Robert and Lazarek, Jagoda and Dufresne, Florian and Bensch, Leonie and Nilsson, Tommy},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Well-being in Isolation: Exploring Artistic Immersive Virtual Environments in a Simulated Lunar Habitat to Alleviate Asthenia Symptoms}, 
  year={2023},
  volume={},
  number={},
  pages={185-194},
  abstract={Revived interest in lunar and planetary exploration is heralding a new era for human spaceflight, characterized by frequent strain on astronaut’s mental well-being, which stems from increased exposure to isolated, confined, and extreme (ICE) conditions. Whilst Immersive Virtual Reality (IVR) has been employed to facilitate self-help interventions to mitigate challenges caused by isolated environments in several domains, its applicability in support of future space expeditions remains largely unexplored. To address this limitation, we administered the use of distinct IVR environments to crew members $(n=5)$ partaking in a simulated lunar habitat study. Utilizing a Bayesian approach to scrutinize small group data, we discovered a significant relationship between IVR usage and a reduction in perceived stress-related symptoms, particularly those associated with asthenia (syndrome often linked to chronic fatigue and weakness; a condition characterized by feelings of energy depletion or exhaustion that can be amplified in ICE conditions). The reductions were most prominent with the use of interactive virtual environments. The ’Aesthetic Realities’ - virtual environments conceived as art exhibits - received exceptional praise from our participants. These environments mark a fascinating convergence of art and science, holding promise to mitigate effects related to isolation in spaceflight training and beyond},
  keywords={Training;Art;Moon;Habitats;Virtual environments;Fatigue;Bayes methods;Human-centered computing;Virtual reality;Applied computing;Fine arts;Psychology},
  doi={10.1109/ISMAR59233.2023.00033},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316439,
  author={Theelke, Luisa and Metzler, Fynn-Lennardt and Kreimeier, Julian and Hauer, Christopher and Binder, Johannes and Roth, Daniel},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating the Effects of Selective Information Presentation in Intensive Care Units Using Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={195-204},
  abstract={Medical personnel working in intensive care units (ICUs) are continuously exposed to a multitude of alarms emanating from various monitoring devices, such as cardiac monitors, ventilators, or infusion pumps. The sheer volume of alarms, coupled with high false positive rates, can lead to alarm fatigue. This phenomenon compromises patient safety and places an additional burden on nurses who must diligently prioritize and respond to alarms in the highly dynamic environment. While the testing of stress-reducing strategies in a real ICU is challenging, virtual reality (VR) represents a powerful tool and methodology to simulate an ICU environment and test optimization scenarios for alarm display strategies. For example, redistributing alarms to responsible individuals (personalized information presentation) has been proposed as a solution, but testing in real ICU environments is not applicable due to critical patient safety. In this paper, we present a VR simulation of an ICU to simulate comparable stress situations, as well as to assess the impact of a selective and personalized alarm representation strategy in an evaluation study in two conditions. A stress condition mirrors the current ubiquitous audible alarm distribution in most ICUs, where alarms are heard non-patient-specific throughout the ward. In an experimental condition, alarms are filtered patient-specific to reduce information overload and noise pollution. Our user study with medical personnel and novices shows that stress levels can be simulated with our system as indicated by physiological responses. Further, we show that the perceived task load can be reduced with selective information presentation. We discuss the potential benefits of ICU simulations as a methodology and personalized alarm distribution as a first potential strategy for future technologies in ICUs.},
  keywords={Wearable computers;Virtual environments;Alarm systems;Safety;Personnel;Task analysis;Biomedical monitoring;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Human computer interaction (HCI)- Interaction paradigms-Virtual reality},
  doi={10.1109/ISMAR59233.2023.00034},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316430,
  author={Hertel, Julia and Schmidt, Susanne and Briede, Marc and Anders, Oliver and Thies, Thomas and Steinicke, Frank},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Welcome AboARd! Evaluating Augmented Reality as a Skipper’s Navigator}, 
  year={2023},
  volume={},
  number={},
  pages={205-213},
  abstract={Augmented Reality (AR) technology has been widely investigated to support various navigation tasks, including initial approaches that suggest its potential use on ships. For maritime navigation, skippers use a variety of information displayed on a ship’s bridge. However, the constant shift of focus between this information and the outside view of the ship might pose cognitive as well as safety challenges. Here, AR could facilitate the navigation of ships by overlaying the real-world view with spatially anchored visual navigation aids. Despite this potential, previous work mainly presents conceptual approaches, technical tests, or user studies performed in ship simulators only.In this paper, we evaluate an AR-based assistance system in the actual real-world water environment, where technical issues and varying physical conditions could influence the system’s usability. In collaboration with hydrographic experts following a user-centered design approach, a functional AR system was developed that virtually displays navigation aids on the water surface. In a field study, ten skippers used the system to navigate a ship along a path through a port area. We assessed the accuracy, perceived workload, and user experience of participants. In addition, qualitative feedback was thematically analyzed to retrieve insights about the skippers’ attitude regarding using AR on actual ships. We report lessons learned about aspects such as ergonomics, perceived safety challenges, as well as envisioned further use cases and extended data integration.},
  keywords={Surveys;Visualization;Navigation;User centered design;User experience;Safety;Marine vehicles;Human-centered computing;Empirical studies in HCI;Mixed / augmented reality},
  doi={10.1109/ISMAR59233.2023.00035},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316478,
  author={Chen, Yang-Sheng and Hsieh, Chiao-En and Jie, Miguel Then Ying and Han, Ping-Hsuan and Hung, Yi-Ping},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Leap to the Eye: Implicit Gaze-based Interaction to Reveal Invisible Objects for Virtual Environment Exploration}, 
  year={2023},
  volume={},
  number={},
  pages={214-222},
  abstract={Cinematic virtual reality (CVR) brings viewers a novel and immersive movie-watching experience. However, they may miss story events and scene transitions that the director has designed as key points hidden in the VR scene. In this paper, we introduce implicit gaze-based interaction for enhancing the exploration experience in CVR. In contrast to most research on gaze-based selection of objects or explicit guidance of attention using visual cues, we focus on implicit interaction that utilizes the user’s natural gaze and attention to explore the scene. We design and implement different gaze trigger methods for implicit interaction, making the interaction more intuitive and natural when users reveal the invisible objects. We implemented the adaptive collider technique, offering users a better sense of exploration than raycasting and spotlight techniques. We have also conducted user studies to compare animation sequences for visual feedback, with each animation sequence offering different storytelling techniques. One of the sequences is better suited for describing spaces in the virtual world, while the other sequence offers users the feeling of constructing a world through their gaze.},
  keywords={Visualization;Virtual environments;Data visualization;Animation;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/ISMAR59233.2023.00036},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316363,
  author={Qiu, Mengyu and Rong, Quan and Liang, Dong and Tu, Huawei},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Visual ScanPath Transformer: Guiding Computers to See the World}, 
  year={2023},
  volume={},
  number={},
  pages={223-232},
  abstract={We propose to exploit the scanpath prediction technology to simulate human visual system to automatically generate gaze scanpaths for VR/AR applications, to alleviate the equipment and computational cost in foveated rendering. Specifically, we propose a novel deep learning-based scanpath prediction model called Visual ScanPath Transformer (VSPT), to predict human gaze scanpaths in both free viewing and task-driven viewing situations, based on which the VR/AR systems can execute foveated rendering rapidly and cheaply. The proposed VSPT first extracts highly task-related image features from the visual scene, and then explores the global dependency relationships among all the image regions to generate each image region a global feature. Next, VSPT simulates the human visual working memory to consider all the previous fixations’ influences when predicting each fixation. Experimental findings confirm that our model exhibits adherence to classical visual principles during saccadic decision-making, surpassing the current state-of-the-art performance in free-viewing and task-driven (goal-driven and question-driven) visual scenarios.},
  keywords={Computers;Visualization;Computational modeling;Decision making;Predictive models;Rendering (computer graphics);Feature extraction;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Pointing;Computing methodologies;Artificial intelligence;Computer vision;Scene understanding},
  doi={10.1109/ISMAR59233.2023.00037},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316523,
  author={Kang, Seongjun and Kim, Gwangbin and Kim, SeungJun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Giant Finger: A Novel Visuo-Somatosensory Approach to Simulating Lower Body Movements in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={233-242},
  abstract={Surreal experience in virtual reality (VR) occurs when visual experience is accompanied by congruent somatosensation. Thus, VR contents that require physical actions are often bounded to our physical capabilities to maintain somatosensory consistency. Alternatively, users often choose less immersive but safer interfaces that offer a wider action variability. In either case, this situation compromises the potential for a hyper-realistic experience. To address this, we introduce “Giant Finger,” a concept that replicates human lower body movements through two enlarged virtual fingers in VR. Through a user study, we affirmed Giant Finger’s ownership using proprioceptive drift and questionnaire responses. We also compared Giant Finger’s capability to perform a variety of tasks with existing methods. Despite its minimalistic approach, Giant Finger demonstrated a high level of efficacy in supporting lower body movements, with ownership and presence comparable to those of the body-leaning method with whole-body motion. Giant Finger can replace the sensations of real legs or support locomotion in confined spaces by providing proprioceptive illusions to the virtual lower body. The applications showcased in this paper suggest that Giant Finger can enable new forms of movement with high action variability and immersion in various fields such as gaming, industry, and accessibility.},
  keywords={Legged locomotion;Somatosensory;Industries;Visualization;Three-dimensional displays;Navigation;Fingers;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input},
  doi={10.1109/ISMAR59233.2023.00038},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316466,
  author={Rolff, Tim and Schmidt, Susanne and Li, Ke and Steinicke, Frank and Frintrop, Simone},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VRS-NeRF: Accelerating Neural Radiance Field Rendering with Variable Rate Shading}, 
  year={2023},
  volume={},
  number={},
  pages={243-252},
  abstract={Recent advancements in Neural Radiance Fields (NeRF) provide enormous potential for a wide range of Mixed Reality (MR) applications. However, the applicability of NeRF to real-time MR systems is still largely limited by the rendering performance of NeRF. In this paper, we present a novel approach for Variable Rate Shading for Neural Radiance Fields (VRS-NeRF). In contrast to previous techniques, our approach does not require training multiple neural networks or re-training of already existing ones, but instead utilizes the raytracing properties of NeRF. This is achieved by merging rays depending on a variable shading rate, which reduces the overall number of queries to the neural network. We demonstrate the generalizability of our approach by implementing three alternative functions for the determination of the shading rate. The first method uses the gaze of users to effectively implement a foveated rendering technique in NeRF. For the other two techniques, we utilize shading rates based on edges and saliency. Based on a psychophysical experiment and multiple image-based metrics, we suggest a set of parameters for each technique, yielding an optimal tradeoff between rendering performance gain and perceived visual quality.},
  keywords={Training;Measurement;Visualization;Image edge detection;Neural networks;Merging;Mixed reality;neural radiance fields;variable rate shading;virtual reality;psychophysical experiment;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00039},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316420,
  author={Doula, Achref and Schmidt, Lennart and Mühlhäuser, Max and Guinea, Alejandro Sanchez},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={“Can You Handle the Truth?”: Investigating the Effects of AR-Based Visualization of the Uncertainty of Deep Learning Models on Users of Autonomous Vehicles}, 
  year={2023},
  volume={},
  number={},
  pages={253-262},
  abstract={The recent advances in deep learning have paved the way for autonomous vehicles (AVs) to take charge of more complex tasks in the navigation process. However, predictions of deep learning models are subject to different types of uncertainty that may put the user and the surrounding environment in danger. In this paper, we investigate the effects that AR-based visualizations of 3 types of uncertainties in deep learning modules for path planning in AVs may have on drivers. The uncertainty types of the deep learning models that we consider are: the waypoint uncertainty, the situation uncertainty, and the path uncertainty. We propose 3 concepts to visualize the 3 uncertainty types on a Windshield display. We evaluate our AR-based concepts with a user study $(\mathrm{N}=20)$ using a VR-based immersive environment, to ensure the security of the participants. The results of our evaluation reveal that the absence of uncertainty visualization leads to lower driver engagement. More importantly, the combination of situation uncertainty and path uncertainty visualizations leads to higher driver engagement, and higher trust in the automated vehicle, while inducing an acceptable mental load for the drive.},
  keywords={Deep learning;Visualization;Uncertainty;Predictive models;Security;Usability;Task analysis;Deep learning;uncertainty;autonomous driving},
  doi={10.1109/ISMAR59233.2023.00040},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316360,
  author={Hmaiti, Yahya and Maslych, Mykola and Taranta, Eugene M. and LaViola, Joseph J.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Exploration of The Effects of Head-Centric Rest Frames On Egocentric Distance Judgments in VR}, 
  year={2023},
  volume={},
  number={},
  pages={263-272},
  abstract={Users tend to underestimate distances in virtual reality (VR), and several efforts have been directed toward finding the causes and developing tools that mitigate this phenomenon. One hypothesis that stands out in the field of spatial perception is the rest frame hypothesis (RFH), which states that visual frames of reference (RFs), defined as fixed reference points of view in a virtual environment (VE), contribute to minimizing sensory mismatch. RFs have been shown to promote better eye-gaze stability and focus, reduce VR sickness, and improve visual search, along with other benefits. However, their effect on distance perception in VEs has not been evaluated. In this paper, we use a blind walking task to explore the effect of three head-centric RFs (mesh mask, nose, and hat) on egocentric distance estimation. We found that at near and mid-field distances, certain RFs can improve the user’s distance estimation accuracy and reduce distance underestimation. These findings mean that the addition of head-centric RFs, a simple avatar augmentation method, can lead to meaningful improvements in distance judgments, user experience, and task performance in VR.},
  keywords={Radio frequency;Legged locomotion;Human computer interaction;Visualization;Estimation;Nose;Virtual environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Empirical studies in HCI},
  doi={10.1109/ISMAR59233.2023.00041},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316359,
  author={Xin, Yingye and Zuo, Xingxing and Lu, Dongyue and Leutenegger, Stefan},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={SimpleMapping: Real-Time Visual-Inertial Dense Mapping with Deep Multi-View Stereo}, 
  year={2023},
  volume={},
  number={},
  pages={273-282},
  abstract={We present a real-time visual-inertial dense mapping method capable of performing incremental 3D mesh reconstruction with high quality using only sequential monocular images and inertial measurement unit (IMU) readings. 6-DoF camera poses are estimated by a robust feature-based visual-inertial odometry (VIO), which also generates noisy sparse 3D map points as a by-product. We propose a sparse point aided multi-view stereo neural network (SPA-MVSNet) that can effectively leverage the informative but noisy sparse points from the VIO system. The sparse depth from VIO is firstly completed by a single-view depth completion network. This dense depth map, although naturally limited in accuracy, is then used as a prior to guide our MVS network in the cost volume generation and regularization for accurate dense depth prediction. Predicted depth maps of keyframe images by the MVS network are incrementally fused into a global map using TSDF-Fusion. We extensively evaluate both the proposed SPA-MVSNet and the entire dense mapping system on several public datasets as well as our own dataset, demonstrating the system’s impressive generalization capabilities and its ability to deliver high-quality 3D reconstruction online. Our proposed dense mapping system achieves a 39.7% improvement in F-score over existing systems when evaluated on the challenging scenarios of the EuRoC dataset.},
  keywords={Tracking loops;Three-dimensional displays;Simultaneous localization and mapping;Pose estimation;Real-time systems;6-DOF;Robustness;Real-Time System;Dense Mapping;Depth Completion;Multi-view Stereo},
  doi={10.1109/ISMAR59233.2023.00042},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316417,
  author={Scargill, Tim and Chen, Ying and Hu, Tianyi and Gorlatova, Maria},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={SiTAR: Situated Trajectory Analysis for In-the-Wild Pose Error Estimation}, 
  year={2023},
  volume={},
  number={},
  pages={283-292},
  abstract={Virtual content instability caused by device pose tracking error remains a prevalent issue in markerless augmented reality (AR), especially on smartphones and tablets. However, when examining environments which will host AR experiences, it is challenging to determine where those instability artifacts will occur; we rarely have access to ground truth pose to measure pose error, and even if pose error is available, traditional visualizations do not connect that data with the real environment, limiting their usefulness. To address these issues we present SiTAR (Situated Trajectory Analysis for Augmented Reality), the first situated trajectory analysis system for AR that incorporates estimates of pose tracking error. We start by developing the first uncertainty-based pose error estimation method for visual-inertial simultaneous localization and mapping (VI-SLAM), which allows us to obtain pose error estimates without ground truth; we achieve an average accuracy of up to 96.1% and an average FI score of up to 0.77 in our evaluations on four VI-SLAM datasets. Next, we present our SiTAR system, implemented for ARCore devices, combining a backend that supplies uncertainty-based pose error estimates with a frontend that generates situated trajectory visualizations. Finally, we evaluate the efficacy of SiTAR in realistic conditions by testing three visualization techniques in an in-the-wild study with 15 users and 13 diverse environments; this study reveals the impact both environment scale and the properties of surfaces present can have on user experience and task performance.},
  keywords={Simultaneous localization and mapping;Limiting;Error analysis;Measurement uncertainty;Data visualization;User experience;Trajectory;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR59233.2023.00043},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316501,
  author={Ibrahim, Muhammad Twaha and Gopi, M. and Majumder, Aditi},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Self-Calibrating Dynamic Projection Mapping System for Dynamic, Deformable Surfaces with Jitter Correction and Occlusion Handling}, 
  year={2023},
  volume={},
  number={},
  pages={293-302},
  abstract={Dynamic projection mapping (DPM) is becoming increasingly popular, enabling viewers to visualize information on moving and deformable surfaces. Examples include large data visualization on the moving walls of tents deployed in austere remote locations during emergency management or defense operations. A DPM system typically comprises a RGB-D camera and a projector. In this paper, we present the first fully functional DPM system that auto-calibrates (without any physical props like planar checkerboard or rigid 3D objects) and creates a comprehensible display in the presence of large and fast movements by managing jitter and occlusion by passing objects.Prior DPM systems need specific calibration props, manual inputs and in order to deliver sub-pixel calibration accuracy. Recalibration in the face of movement or change in system setup becomes a time consuming process where the calibration prop needs to be brought back. When rendering content using DPM, errors in calibration are exacerbated and the noise in the depth camera leads to jitter, making the projection unreadable or incomprehensible. Occlusion may disrupt operations completely by jumbling up even the unoccluded parts of the display.In this paper we propose key hardware-agnostic methods for DPM calibration and rendering to make DPM systems easily deployable, stable and legible. First, we present a novel projector-camera calibration that does not need synchronization of the devices and leverages the moving surface itself, a counter-intuitive proposition. We project ArUCo markers on the moving surface and use corresponding detected features of these markers in the RGB and depth camera over multiple frames to accurately estimate the intrinsics and extrinsics of both the projector and the RGB-D camera. Second, we present a DPM rendering method that uses Kalman filtering models to reduce jitter and predict the surface shape in the presence of short term occlusions by other static objects. This results in the first DPM system, to the best of our knowledge, that can auto-calibrate in minutes and can render high resolution content like high-resolution text or images comprehensible even in the presence of fast movements, deformations and occlusions. We compare and evaluate the accuracy with prior methods and analyze the effect of surface movement on the calibration accuracy.},
  keywords={Three-dimensional displays;Shape;Data visualization;Manuals;Jitter;Predictive models;Cameras;Computing Methodologies;Artificial Intelligence;Computer Vision;Image and Video Acquisition},
  doi={10.1109/ISMAR59233.2023.00044},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316495,
  author={Rodrigues, Francielly and Giovannelli, Alexander and Pavanatto, Leonardo and Miao, Haichao and Oliveira, Jauvane C. de and Bowman, Doug A.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AMP-IT and WISDOM: Improving 3D Manipulation for High-Precision Tasks in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={303-311},
  abstract={Precise 3D manipulation in virtual reality (VR) is essential for effectively aligning virtual objects. However, state-of-the-art VR manipulation techniques have limitations when high levels of precision are required, including the unnaturalness caused by scaled rotations and the increase in time due to degree-of-freedom (DoF) separation in complex tasks. We designed two novel techniques to address these issues: AMP-IT, which offers direct manipulation with an adaptive scaled mapping for implicit DoF separation, and WISDOM, which offers a combination of Simple Virtual Hand and scaled indirect manipulation with explicit DoF separation. We compared these two techniques against baseline and state-of-the-art manipulation techniques in a controlled experiment. Results indicate that WISDOM and AMP-IT have significant advantages over best-practice techniques regarding task performance, usability, and user preference.},
  keywords={Three-dimensional displays;Switches;Market research;Task analysis;Usability;X reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques},
  doi={10.1109/ISMAR59233.2023.00045},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316520,
  author={Lu, Edward and Bharadwaj, Sagar and Dasari, Mallesham and Smith, Connor and Seshan, Srinivasan and Rowe, Anthony},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={RenderFusion: Balancing Local and Remote Rendering for Interactive 3D Scenes}, 
  year={2023},
  volume={},
  number={},
  pages={312-321},
  abstract={Many modern-day XR devices (e.g. mobile headsets, phones, etc.) lack the computing resources required to render complex 3D scenes in real-time. Typically, to render a high-resolution scene on a lightweight XR device, 3D designers arduously decimate and fine-tune the objects. As an alternative, remote rendering systems can utilize powerful nearby servers to stream rendering results to a client. While this is a promising solution, it can introduce a variety of latency and reliability issues, especially under variable network conditions. In this paper, we present a distributed rendering system that combines both remote rendering and on-device, “local” rendering to add robustness to network fluctuations and device workloads. To maximize user QoE, our approach dynamically swaps an object’s rendering medium, adjusting for client workload, low frame rates, and several perceptual characteristics. To model these characteristics, we perform a study under simulated conditions to measure how users perceive latency and complexity differences between objects in a scene. Using the results of the study, we then provide an algorithm for choosing the optimal object rendering medium, based on rendering complexity as well as network and latency models, ensuring that a target frame rate will be met. Finally, we evaluate this algorithm on a prototype implementation that can provide cross-platform split rendering using web technologies.},
  keywords={Performance evaluation;Three-dimensional displays;Runtime;Rendering (computer graphics);Robustness;Real-time systems;Complexity theory;Computer systems organization;Architectures;Distributed architectures;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality},
  doi={10.1109/ISMAR59233.2023.00046},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316494,
  author={Ghamandi, Ryan K. and Hmaiti, Yahya and Nguyen, Tam T. and Ghasemaghaei, Amirpouya and Kattoju, Ravi Kiran and Taranta, Eugene M. and LaViola, Joseph J.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={What And How Together: A Taxonomy On 30 Years Of Collaborative Human-Centered XR Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={322-335},
  abstract={We present a taxonomy of human-centered collaborative XR tasks. XR technologies have extended into the realm of collaboration, improving the quality and accessibility of teamwork. However, after a comprehensive assessment of the literature on the interaction between XR technologies and collaboration, no comprehensive method that emphasizes task actions and properties exists to classify collaborative tasks. Thus, our suggested taxonomy represents a classification system for collaborative tasks. After conducting a thorough literature review across different research venues, we conducted several exhaustive classification and review cycles for over 800 papers collected, which resulted in 148 papers retained to create the taxonomy. We dissected the actions and properties that the collaborative endeavors and tasks of these papers encompass as well as the types of categorizations and relations these papers illustrate. We expand on the design choices and usage of our taxonomy, followed by its limitations and future work. We built this taxonomy in order to reduce ambiguities and confusion regarding the design and comprehension of human-based collaborative tasks that use XR technology, which could prove useful in aiding the development and understanding of these tasks. Our taxonomy reveals a framework for understanding how collaborative tasks are designed and a systematic way of classifying different methods by which people can collaborate and interact in environments that involve XR, while still promoting efficient communication, teamwork, goal achievement and productivity.},
  keywords={Productivity;Social computing;Systematics;Design methodology;Bibliographies;Taxonomy;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Collaborative and social computing design and evaluation methods;HCI design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00047},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316365,
  author={Wang, Haokun and Singhal, Yatharth and Kim, Jin Ryong},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Fabric Thermal Display using Ultrasonic Waves}, 
  year={2023},
  volume={},
  number={},
  pages={336-345},
  abstract={This paper presents a fabric-based thermal display of a polyester fabric material combined with thermally-conductive materials using an ultrasound haptic display. We first empirically test the thermal generation process in five fabric materials by applying 40 kHz ultrasonic waves to the fabric materials. We also examine their thermal characteristics by applying different frequencies and amplitudes of ultrasonic cues. We show that polyester demonstrates the best thermal performance. We then combine it with thermally-conductive materials, including copper and aluminum, and compare them with the fabric-only condition. Two user studies show that our approach of combining a fabric material with copper and aluminum outperforms fabric-only conditions in thermal perception and thermal level identification. We integrate polyester with aluminum into a glove to explore the use cases in VR and share our findings, insights, limitations, and future works.},
  keywords={Ultrasonic imaging;Aluminum;Fabrics;Acoustics;Haptic interfaces;Copper;Augmented reality;Haptics;Thermo-tactile Feedback;VR},
  doi={10.1109/ISMAR59233.2023.00048},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316484,
  author={Lutwak, Hope and Murdison, T. Scott and Rio, Kevin W.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={User Self-Motion Modulates the Perceptibility of Jitter for World-locked Objects in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={346-355},
  abstract={A key feature of augmented reality (AR) is the ability to display virtual content that appears stationary as users move throughout the physical world (‘world-locked rendering’). Imperfect world-locked rendering gives rise to perceptual artifacts that can negatively impact user experience. One example is random variation in the position of virtual objects that are intended to be stationary (jitter’). The human visual system is highly attuned to detect moving objects, and moreover it can disambiguate between the retinal velocities that arise from object motion and self-motion, respectively. In this study, we investigated how the perceptibility of AR object jitter varies as a function of user self-motion. Using a commercially available AR HMD to display a 3D textured cube, we measured sensitivity to added jitter versus a no-jitter reference using a two-interval forced choice task. Three user motion conditions (stationary, head rotation, and walking) and three object placement conditions (floating in free space, on a desk, and against a wall) were tested in a full factorial design. We hypothesized that (1) as users move their head and eyes during self-motion, their sensitivity to jitter will decrease, due to added retinal velocity; and (2) rendering virtual objects near physical surfaces will increase sensitivity to jitter, by providing proximal veridical visual cues. Psychometric thresholds indicated that users were significantly less sensitive to jitter during self-motion than when they were stationary, consistent with hypothesis (1). Users were also more sensitive to jitter in one of the two object placement conditions, providing partial support for hypothesis (2). To generalize beyond distinct user motion and object placement conditions, we also analyzed eye tracking data. The amount of retinal slip (i.e. how much gaze drifted across the virtual object) predicted jitter thresholds better than recorded head movements alone, suggesting a retinally-driven decrease in jitter sensitivity during self-motion. These results can be used to inform requirements for AR world-locked rendering systems, as well as how these may be updated dynamically using online measurement of user head and eye movements.},
  keywords={Legged locomotion;Visualization;Sensitivity;Three-dimensional displays;Tracking;Jitter;Visual systems;Augmented reality;jitter;visual perception;world-locked rendering},
  doi={10.1109/ISMAR59233.2023.00049},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316518,
  author={Liu, Jen-Shuo and Tversky, Barbara and Feiner, Steven},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Cueing Sequential 6DoF Rigid-Body Transformations in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={356-365},
  abstract={Augmented reality (AR) has been used to guide users in multi-step tasks, providing information about the current step (cueing) or future steps (precueing). However, existing work exploring cueing and precueing a series of rigid-body transformations requiring rotation has only examined one-degree-of-freedom (DoF) rotations alone or in conjunction with 3DoF translations. In contrast, we address sequential tasks involving 3DoF rotations and 3DoF translations. We built a testbed to compare two types of visualizations for cueing and precueing steps. In each step, a user picks up an object, rotates it in 3D while translating it in 3D, and deposits it in a target 6DoF pose. Action-based visualizations show the actions needed to carry out a step and goal-based visualizations show the desired end state of a step. We conducted a user study to evaluate these visualizations and the efficacy of precueing. Participants performed better with goal-based visualizations than with action-based visualizations, and most effectively with goal-based visualizations aligned with the Euler axis. However, only a few of our participants benefited from precues, most likely because of the cognitive load of 3D rotations.},
  keywords={Visualization;Three-dimensional displays;Design methodology;Maintenance engineering;Cognitive load;Task analysis;Augmented reality;Human-centered computing;Human-centered computing (HCI);Interaction paradigms;Mixed / augmented reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR59233.2023.00050},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316358,
  author={Lie, Hing and Studer, Kachina and Zhao, Zhen and Thomson, Ben and Turakhia, Dishita G and Liu, John},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Training for Open-Ended Drilling through a Virtual Reality Simulation}, 
  year={2023},
  volume={},
  number={},
  pages={366-375},
  abstract={Virtual Reality (VR) can support effective and scalable training of psychomotor skills in manufacturing. However, many industry training modules offer experiences that are close-ended and do not allow for human error. We aim to address this gap in VR training tools for psychomotor skills training by exploring an open-ended approach to the system design. We designed a VR training simulation prototype to perform open-ended practice of drilling using a 3-axis milling machine. The simulation employs near “endto-end” instruction through a safety module, a setup and drilling tutorial, open-ended practice complete with warnings of mistakes and failures, and a function to assess the geometries and locations of drilled holes against an engineering drawing. We developed and conducted a user study within an undergraduate-level introductory fabrication course to investigate the impact of open-ended VR practice on learning outcomes. Study results reveal positive trends, with the VR group successfully completing the machining task of drilling at a higher rate (75% vs 64%), with fewer mistakes (1.75 vs 2.14 score), and in less time (17.67 mins vs 21.57 mins) compared to the control group. We discuss our findings and limitations and implications for the design of open-ended VR training systems for learning psychomotor skills.},
  keywords={Training;Drilling;Industries;Geometry;Prototypes;Tutorials;Market research;virtual reality;open-ended practice;learning by mistakes;drilling;Human-centered computing;Interaction design;Interaction design process and methods;User centered design;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00051},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316442,
  author={Kim, Somin and Jung, Myeongul and Heo, Jiwoong and Kim, Kwanguk Kenny},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Interaction between AR Cue Types and Environmental Conditions in Autonomous Vehicles}, 
  year={2023},
  volume={},
  number={},
  pages={376-385},
  abstract={As one of autonomous vehicles, conditional autonomous vehicles is expected to become popular in the near future. Conditional autonomous vehicles can send a take-over request (TOR) to a driver, and if they are immersed in non-driving-related tasks (NDRT), they will struggle to accommodate this request. Previous studies have shown that providing augmented reality (AR) information on traffic situations (status cues) or driver actions (command cues) can improve TOR performance. However, we are not aware of any studies comparing the types of AR cues (state versus command cues) and their interactions with environmental factors. Therefore, the current study investigated this and evaluated the TOR performance of 42 drivers. We used a 2 (environments: day and night) $\times$ 4 (AR cue types: without, status, command, and combined cues) mixed-subject experimental design, and dependent measures included driving, cognitive, and NDRT performances. The results suggest that overall driving and cognitive performance were significantly improved by the command AR cue. In contrast, the status AR cue improved the TOR performance in nighttime environments. The performance of AR cues can vary depending on environmental factors, and AR cue designs for autonomous vehicles should consider this interaction for successful collaboration between drivers and vehicles.},
  keywords={Visualization;Design methodology;Current measurement;Collaboration;Environmental factors;Copper;Task analysis;Automated driving;take-over request;augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Virtual reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR59233.2023.00052},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316415,
  author={Benjamin, Juanita and Bruder, Gerd and Neumann, Carsten and Reiners, Dirk and Cruz-Neira, Carolina and Welch, Gregory F.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perception and Proxemics with Virtual Humans on Transparent Display Installations in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={386-395},
  abstract={It is not uncommon for science fiction movies to portray futuristic user interfaces that can only be realized decades later with state-of-the-art technology. In this work, we present a prototypical augmented reality (AR) installation that was inspired by the movie The Time Machine (2002). It consists of a transparent screen that acts as a window through which users can see the stereoscopic projection of a three-dimensional virtual human (VH). However, there are some key differences between the vision of this technology and the way VHs on these displays are actually perceived. In particular, the additive light model of these displays causes darker VHs to appear more transparent, while light in the physical environment further increases transparency, which may affect the way VHs are perceived, to what degree they are trusted, and the distances one maintains from them in a spatial setting. In this paper, we present a user study in which we investigate how transparency in the scope of transparent AR screens affects the perception of a VH’s appearance, social presence with the VH, and the social space around users as defined by proxemics theory. Our results indicate that appearances are comparatively robust to transparency, while social presence improves in darker physical environments, and proxemic distances to the VH largely depend on one’s distance from the screen but are not noticeably affected by transparency. Overall, our results suggest that such transparent AR screens can be an effective technology for facilitating social interactions between users and VHs in a shared physical space.},
  keywords={Additives;Stereo image processing;Prototypes;User interfaces;Motion pictures;Augmented reality;Augmented reality;virtual humans;appearance;proxemics;social presence.},
  doi={10.1109/ISMAR59233.2023.00053},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316362,
  author={Moore, Alec G. and Do, Tiffany D. and Ruozzi, Nicholas and McMahan, Ryan P.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Identifying Virtual Reality Users Across Domain-Specific Tasks: A Systematic Investigation of Tracked Features for Assembly}, 
  year={2023},
  volume={},
  number={},
  pages={396-404},
  abstract={Recently, there has been much interest in using virtual reality (VR) tracking data to authenticate or identify users. Most prior research has relied on task-specific characteristics but newer studies have begun investigating task-agnostic, domain-specific approaches. In this paper, we present one of the first systematic investigations of how different combinations of VR tracked devices (i.e., the headset, dominant hand controller, and non-dominant hand controller) and their spatial representations (i.e., position and/or rotation as Euler angles, quaternions, or 6D) affect identification accuracy for domain-specific approaches. We conducted a user study $( n =45)$ involving participants learning how to assemble two distinct full-scale constructions. Our results indicate that more tracked devices improve identification accuracies for the same assembly task, but only headset features afford the best accuracies across the domain-specific tasks. Our results also indicate that spatial features involving position and any rotation yield better accuracies than either alone.},
  keywords={Headphones;Systematics;Quaternions;Object recognition;Task analysis;Rotation measurement;Augmented reality;Virtual reality;identification;obfuscation},
  doi={10.1109/ISMAR59233.2023.00054},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316468,
  author={Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing Perception and Immersion in Pre-Captured Environments through Learning-Based Eye Height Adaptation}, 
  year={2023},
  volume={},
  number={},
  pages={405-414},
  abstract={Pre-captured immersive environments using omnidirectional cameras provide a wide range of virtual reality applications. Previous research has shown that manipulating the eye height in egocentric virtual environments can significantly affect distance perception and immersion. However, the influence of eye height in pre-captured real environments has received less attention due to the difficulty of altering the perspective after finishing the capture process. To explore this influence, we first propose a pilot study that captures real environments with multiple eye heights and asks participants to judge the egocentric distances and immersion. If a significant influence is confirmed, an effective image-based approach to adapt pre-captured real-world environments to the user’s eye height would be desirable. Motivated by the study, we propose a learning-based approach for synthesizing novel views for omnidirectional images with altered eye heights. This approach employs a multitask architecture that learns depth and semantic segmentation in two formats, and generates high-quality depth and semantic segmentation to facilitate the inpainting stage. With the improved omnidirectional-aware layered depth image, our approach synthesizes natural and realistic visuals for eye height adaptation. Quantitative and qualitative evaluation shows favorable results against state-of-the-art methods, and an extensive user study verifies improved perception and immersion for pre-captured real-world environments.},
  keywords={Visualization;Semantic segmentation;Virtual environments;Computer architecture;Rendering (computer graphics);Cameras;Finishing;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality;Image manipulation;Image-based rendering},
  doi={10.1109/ISMAR59233.2023.00055},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316493,
  author={Zhao, Ziyue and Li, Yue and Liang, Hai-Ning},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={LeanOn: Simulating Balance Vehicle Locomotion in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={415-424},
  abstract={Locomotion plays a critical role in user experience in Virtual Reality (VR). This work presents a novel locomotion device, LeanOn, which aims to enhance immersion and feedback experience in VR. Inspired by balance vehicles, LeanOn is a leaning-based locomotion device that allows users to control their location by tilting a board on two balance wheels, with rotation enabled by two buttons near users’ feet. To create a more realistic riding experience, LeanOn is equipped with a terrain vibration system that generates varying levels of vibration based on the roughness of the terrain. We conducted a within-subjects experiment $(\mathrm{N}=24)$ and compared the use of LeanOn and joystick steering in four aspects: cybersickness, spatial presence, feedback experience, and task performance. Participants used LeanOn with and without the vibration system to investigate the necessity of tactile feedback. The results showed that LeanOn significantly improved users’ feedback experience, including autotelic, expressivity, harmony, and immersion, and maintained similar levels of cybersickness and spatial presence, compared to joystick steering. Our work contributes to the field of VR locomotion by validating a leaning-based steering prototype and showing its positive effect on improving users’ feedback experience in VR. We also showed that tactile feedback in locomotion is necessary to further enhance immersion in VR.},
  keywords={Vibrations;Performance evaluation;Cybersickness;Tactile sensors;Wheels;Virtual environments;Prototypes;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00056},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316355,
  author={Batmaz, Anil Ufuk and Turkmen, Rumeysa and Sarac, Mine and Machuca, Mayra Donaji Barrera and Stuerzlinger, Wolfgang},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effect of Grip Style on Peripersonal Target Pointing in VR Head Mounted Displays}, 
  year={2023},
  volume={},
  number={},
  pages={425-433},
  abstract={When working in Virtual Reality (VR), the user’s performance is affected by how the user holds the input device (e.g., controller), typically using either a precision or a power grip. Previous work examined these grip styles for 3D pointing at targets at different depths in peripersonal space and found that participants had a lower error rate with the precision grip but identified no difference in movement speed, throughput, or interaction with target depth. Yet, this previous experiment was potentially affected by tracking differences between devices. This paper reports an experiment that partially replicates and extends the previous study by evaluating the effect of grip style on the 3D selection of nearby targets with the same device. Furthermore, our experiment re-investigates the effect of the vergence-accommodation conflict (VAC) present in current stereo displays on 3D pointing in peripersonal space. Our results show that grip style significantly affects user performance. We hope that our results are useful for researchers and designers when creating virtual environments.},
  keywords={Performance evaluation;Three-dimensional displays;Target tracking;Error analysis;Virtual environments;Input devices;Aerospace electronics;Human-centered computing;Human Computer Interaction (HCI);Virtual Reality;Pointing},
  doi={10.1109/ISMAR59233.2023.00057},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316528,
  author={Sun, Yitong and Zhou, Zijian and Diels, Cyriel and Asadipour, Ali},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DeepMetricEye: Metric Depth Estimation in Periocular VR Imagery}, 
  year={2023},
  volume={},
  number={},
  pages={434-443},
  abstract={Despite the enhanced realism and immersion provided by VR headsets, users frequently encounter adverse effects such as digital eye strain (DES), dry eye, and potential long-term visual impairment due to excessive eye stimulation from VR displays and pressure from the mask. Recent VR headsets are increasingly equipped with eye-oriented monocular cameras to segment ocular feature maps. Yet, to compute the incident light stimulus and observe periocular condition alterations, it is imperative to transform these relative measurements into metric dimensions. To bridge this gap, we propose a lightweight framework derived from the U-Net 3 + deep learning backbone that we re-optimised, to estimate measurable periocular depth maps. Compatible with any VR headset equipped with an eye-oriented monocular camera, our method reconstructs three-dimensional periocular regions, providing a metric basis for related light stimulus calculation protocols and medical guidelines. Navigating the complexities of data collection, we introduce a Dynamic Periocular Data Generation (DPDG) environment based on UE MetaHuman, which synthesises thousands of training images from a small quantity of human facial scan data. Evaluated on a sample of 36 participants, our method exhibited notable efficacy in the periocular global precision evaluation experiment, and the pupil diameter measurement.},
  keywords={Headphones;Training;Estimation;Glass;Cameras;Particle measurements;Pupils;Computing methodologies;Artificial intelligence;Computer vision;Computer vision tasks;Applied computing;Life and medical sciences;Consumer health},
  doi={10.1109/ISMAR59233.2023.00058},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316429,
  author={Rahman, Yitoshee and Kulshreshth, Arun K and Borst, Christoph W},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparing Visualizations to Help a Teacher Effectively Monitor Students in a VR Classroom}, 
  year={2023},
  volume={},
  number={},
  pages={444-453},
  abstract={Educational virtual reality (VR) applications are the most recent addition to the learning management tools in this modern age. Due to health concerns, financial concerns, and convenience, people are looking for alternate ways to teach and learn. An efficient VR-based teaching interface could enhance student engagement, learning outcomes, and overall educational experience. Typically, teachers in a VR classroom do not have a way to know what students are doing since students are not visible. An efficient teaching interface should include some mechanism for a teacher to monitor students and alert the teacher if a student is trying to catch the attention of the teacher. An ideal interface would be one, which helps a teacher effectively monitor students while teaching without increasing the cognitive load of the teacher. In this paper, we present a comparative study of two such student monitoring interfaces. In the first interface, the student activity related information is shown using icons near the student avatar (representing a student in the VR environment). While in the second interface, a set of centrally-arranged emoticon-like visual indicators are present in addition to the student avatar, and the student activity related information is shown near the student emoticon. We present a detailed user experiment comparing the two interfaces in terms of teaching management, student monitoring capability, cognitive load, and user preference. Participants preferred and performed better with Indicator-located interface over avatar-located interface.},
  keywords={Visualization;Avatars;Design methodology;Education;Cognitive load;Monitoring;Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Visualization;Visualization design and evaluation methods;Applied computing;Education;Distance learning},
  doi={10.1109/ISMAR59233.2023.00059},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316500,
  author={Deng, Hanchen and Li, Jin and Gao, Yang and Liang, Xiaohui and Wu, Hongyu and Hao, Aimin},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PhyVR: Physics-based Multi-material and Free-hand Interaction in VR}, 
  year={2023},
  volume={},
  number={},
  pages={454-462},
  abstract={The realistic interaction with physical phenomena is a crucial aspect of human-computer interaction (HCI) in virtual reality (VR). However, the real-time performance of physical simulation, interactive computation, and rendering is the bottleneck of physics-based VR HCI. To address these challenges, we propose a novel physics-oriented framework for multi-material objects and free-hand interaction, termed PhyVR. This framework enables users to interact with diverse virtual phenomena dynamically. At the algorithm level, we develop a unified particle system to describe both the virtual multi-materials and the user’s avatar for the efficiency issue, optimize collision detection, and accelerate the HCI algorithms with a variable fine-coarse particle sampling scheme. At the rendering level, we introduce a hybrid particle-grid anisotropic algorithm for surface reconstruction, enabling real-time and visually convincing fluid rendering. Comprehensive experiments and user studies demonstrate that our framework effectively captures various physical interaction phenomena, providing an enhanced user experience and paving the way for expanding VR-related HCI applications.},
  keywords={Human computer interaction;Surface reconstruction;Fluids;Heuristic algorithms;Reconstruction algorithms;Propulsion;Rendering (computer graphics);Human-centered computing;Human computer interaction (HCI);Interaction devices;Computing methodologies;Computer graphics;Animation;Physical simulation},
  doi={10.1109/ISMAR59233.2023.00060},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316477,
  author={Kalamkar, Snehanjali and Biener, Verena and Beck, Fabian and Grubert, Jens},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Remote Monitoring and Teleoperation of Autonomous Vehicles—Is Virtual Reality an Option?}, 
  year={2023},
  volume={},
  number={},
  pages={463-472},
  abstract={While the promise of autonomous vehicles has led to significant scientific and industrial progress, fully automated, SAE level 5 conform cars will likely not see mass adoption anytime soon. Instead, in many applications, human supervision, such as remote monitoring and teleoperation, will be required for the foreseeable future. While Virtual Reality (VR) has been proposed as one potential interface for teleoperation, its benefits and drawbacks over physical monitoring and teleoperation solutions have not been thoroughly investigated. To this end, we contribute three user studies, comparing and quantifying the performance of and subjective feedback for a VR-based system with an existing monitoring and teleoperation system, which is in industrial use today. Through these three user studies, we contribute to a better understanding of future virtual monitoring and teleoperation solutions for autonomous vehicles. The results of our first user study (n= 16) indicate that a VR interface replicating the physical interface does not outperform the physical interface. It also quantifies the negative effects that combined monitoring and teleoperating tasks have on users irrespective of the interface being used. The results of the second user study (n= 24) indicate that the perceptual and ergonomic issues caused by VR outweigh its benefits, like better concentration through isolation. The third follow-up user study (n= 24) specifically targeted the perceptual and ergonomic issues of VR; the subjective feedback of this study indicates that newer-generation VR headsets have the potential to catch up with the current physical displays.},
  keywords={Headphones;Degradation;Costs;Ergonomics;Stereo image processing;Layout;Automobiles;Human-centered computing;Virtual Reality;Visualization;Teleoperation;Monitoring;Autonomous Vehicles},
  doi={10.1109/ISMAR59233.2023.00061},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316414,
  author={Chen, Xin and Guo, Dongliang and Feng, Li and Chen, Bo and Liu, Wei},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Compass+Ring: A Multimodal Menu to Improve Interaction Performance and Comfortability in One-handed Scenarios}, 
  year={2023},
  volume={},
  number={},
  pages={473-482},
  abstract={In numerous applications, an excellent interface design should allow users to perform secondary tasks as naturally as possible without affecting the main task. Multimodal handheld menus are regularly the preferred user interface that meets the natural switching of primary and secondary tasks. However, existing multimodal handheld menus have some limitations under single-handed conditions, or the comfort needs improvement. To address these issues, this paper proposes a novel multimodal handheld menu: Compass+Ring. The “compass” integrates gesture, gaze, and speech into a pie menu, whereas the “ring” serves as a shortcut menu. The Compass menu improves interaction performance and comfortability in one-handed scenarios, and the Ring menu alleviates eye fatigue when both hands are free. We evaluated five handheld menus: Touch, Gaze+Pinch, Speech+Pinch, Bangles, and Compass+Ring. We first analyze the usability of these menus in three different scenarios, and then conduct a user study about these menus in geometry matching and line drawing tasks. The results show that the Bangles menu and the Compass+Ring menu are more suitable for one-handed scenarios than the other three menus, and the Compass+Ring menu is superior to the Bangles menu in terms of efficiency and hand fatigue. In addition, participants indicate that the Ring menu can reduce eye strain for the Compass menu in two-handed scenarios and increase haptic perception.},
  keywords={Geometry;Switches;User interfaces;Fatigue;Haptic interfaces;Compass;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR59233.2023.00062},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316447,
  author={Döllinger, Nina and Beck, Matthias and Wolf, Erik and Mal, David and Botsch, Mario and Latoschik, Marc Erich and Wienrich, Carolin},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={“If It’s Not Me It Doesn’t Make a Difference” - The Impact of Avatar Personalization on user Experience and Body Awareness in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={483-492},
  abstract={Body awareness is relevant for the efficacy of psychotherapy. However, previous work on virtual reality (VR) and avatar-assisted therapy has often overlooked it. We investigated the effect of avatar individualization on body awareness in the context of VR-specific user experience, including sense of embodiment (SoE), plausibility, and sense of presence (SoP). In a between-subject design, 86 participants embodied three avatar types and engaged in VR movement exercises. The avatars were (1) generic and gender-matched, (2) customized from a set of pre-existing options, or (3) personalized photorealistic scans. Compared to the other conditions, participants with personalized avatars reported increased SoE, yet higher eeriness and reduced body awareness. Further, SoE and SoP positively correlated with body awareness across conditions. Our results indicate that VR user experience and body awareness do not always dovetail and do not necessarily predict each other. Future research should work towards a balance between body awareness and SoE.},
  keywords={Avatars;Medical treatment;User experience;Augmented reality;Testing;Virtual reality;embodiment;personalization;body awareness;virtual body ownership;avatars;user experience;Human-centered computing;Empirical studies in HCI;Usability testing},
  doi={10.1109/ISMAR59233.2023.00063},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316453,
  author={Peng, Hao-Lun and Nishida, Shin’Ya and Watanabe, Yoshihiro},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying User Perceptible Misalignment in Simulated Dynamic Facial Projection Mapping}, 
  year={2023},
  volume={},
  number={},
  pages={493-502},
  abstract={High-speed dynamic facial projection mapping (DFPM) is an advanced technology that aims to create perceptual changes in facial appearance by overlapping images based on facial position and shape. Compared to traditional monitor-based augmented reality systems, DFPM offers a higher level of immersion because users can directly observe digital content on their faces. However, DFPM suffers from misalignment issues owing to a slight temporal delay from sensing to projection, which reduces the level of immersion. To the best of our knowledge, no previous study has established the necessary latency requirements to avoid perceptible misalignment and achieve an immersive experience. Furthermore, conventional DFPM works followed latency requirements that were not reported for the DFPM scenario. Therefore, this study measured the latency that provided a just-noticeable difference (JND) in DFPM under different facial motion conditions, using the weighted up-down two-alternative forced-choice method. The results showed that user-perceptible misalignment was influenced by facial motion types and their velocities. Additionally, it was found that an average latency of 3.87 ms was necessary to avoid perceptible misalignment in the DFPM system when the translation speed was 0.5 m/s, which contradicts the commonly held belief regarding the required latency threshold.},
  keywords={Weight measurement;Image resolution;Force measurement;Shape;Immersive experience;Jitter;Sensors;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR59233.2023.00064},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316503,
  author={Bhat, Srikrishna S. and Dobbins, Chelsea and Dey, Arindam and Sharma, Ojaswa},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Multi-modal classification of cognitive load in a VR-based training system}, 
  year={2023},
  volume={},
  number={},
  pages={503-512},
  abstract={Training systems are used in many industries, ranging from surgery to space missions to rehabilitation. Virtual Reality (VR) is a technology that has been incorporated as an effective tool in such training systems to simulate the environment, especially in situations where the training can’t take place in the actual environment. For a training environment and task to be effective, it must sufficiently challenge the trainee. One parameter that can be used to measure this is cognitive load (CL), which is defined as the amount of working memory used while performing a learning task. This parameter needs to be sufficiently high to maximize learning but not too high as to overload the trainee. However, the challenge is to detect this state using objective physiological measures, which can be collected during the entire task. This paper presents a study to classify CL using a combination of Electroencephalogram (EEG) and Electrodermal Activity (EDA) signals during a procedural VR training task. Thirty participants undertook a study where they built a designated model within a given time over multiple levels that were constructed to induce low to high CL. Features generated from the data were subject to feature selection (FS), which was undertaken using the Mutual Information (MI) technique. Binary classification models were developed using Support Vector Machines (SVM), Random Forest (RF), k-Nearest Neighbors (kNN), Extreme Gradient Boosting (Xgboost) and Multi-Layer Perceptrons (MLP). Results illustrated that the Xgboost classifier performed the best with an F1-score of $0.831 \pm 0.030$ and accuracy of $0.805 \pm 0.033.$ SHAP analysis of the features illustrated greater contributions from the frontal and occipital regions of the brain and frequency domain features from tonic skin conductance.},
  keywords={Training;Support vector machines;Solid modeling;Space missions;Surgery;Cognitive load;Brain modeling;EEG;EDA;Machine Learning;Virtual Reality;Cognitive Load},
  doi={10.1109/ISMAR59233.2023.00065},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316491,
  author={Han, Dongyun and Cho, Isaac},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating 3D User Interaction Techniques on Spatial Working Memory for 3D Scatter Plot Exploration in Immersive Analytics}, 
  year={2023},
  volume={},
  number={},
  pages={513-522},
  abstract={This work evaluates three 3D user interaction techniques to investigate their visuo-spatial working memory support for users’ data exploration in immersive analytics. Two techniques are the common VR locomotion technique, Walking and Teleportation, while the other one is Grab, an object manipulation technique. We present two formal user studies in VR and AR. Our study is designed based on the Corsi block-tapping Task, a psychological test for assessing visuo-spatial working memory. Our study results show that Walking supports spatial memory best, and Grab follows. Though Teleportation is found to support it the least, participants rated Teleportation as the easiest way to move in the VR study. We also compare the Walking and Grab results in the VR and AR studies and discuss differences. At last, we discuss our limitations and future work.},
  keywords={Legged locomotion;Human computer interaction;Three-dimensional displays;Psychology;Teleportation;Task analysis;Augmented reality;Human-centered computing;Visualization;Empirical studies in HCI},
  doi={10.1109/ISMAR59233.2023.00066},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316434,
  author={Kim, Irene and Azimi, Ehsan and Kazanzides, Peter and Huang, Chien-Ming},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Active Engagement with Virtual Reality Reduces Stress and Increases Positive Emotions}, 
  year={2023},
  volume={},
  number={},
  pages={523-532},
  abstract={Stress, anxiety, and depression negatively affect productivity and the global economy with an estimated annual cost of ${\$}$1 trillion U.S. dollars, according to the World Health Organization. Moreover, prolonged daily stress—even if minor—can lead to severe health consequences, including cancer and various mental disorders. Virtual reality (VR) has been shown to be a promising tool for relieving daily stressors given its accessibility and its projected availability as compared to visiting with mental health professionals. Prior work in this area has mostly focused on the restorative effects of nature simulations, demonstrating that passively experiencing immersive nature scenes improves positive affect. However, aside from providing opportunities for exercise, little is known about how active VR engagement can improve one’s mental health. To address this research gap, this paper presents a new, active form of VR therapy and assesses its effectiveness as compared to passive VR experiences. We developed VR Drawing—inspired by art therapy, which promotes positive emotions through artistic creation—and VR Throwing—inspired by “rage rooms”, which allow people to release negative emotions via intentional destruction. In a between- participants study (n = 64), we found that both VR Drawing and VR Throwing significantly reduced participants’ stress levels and increased positive affect when compared to passively watching nature scenes in VR. Linear regression models suggest that the total number of user interactions positively affects improvement in positive emotions for VR Drawing, but has a negative impact on positive emotions for VR Throwing. This study provides empirical evidence of how active VR experiences may reduce stress and offers guidelines for creating future VR applications to promote psychological well-being.},
  keywords={Productivity;Solid modeling;Art;Costs;Linear regression;Medical treatment;Mental health;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Applied computing;Law;social;behavioral sciences;Psychology},
  doi={10.1109/ISMAR59233.2023.00067},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316517,
  author={Das, Satabdi and Nasser, Arshad and Hasan, Khalad},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={FingerButton: Enabling Controller-Free Transitions between Real and Virtual Environments}, 
  year={2023},
  volume={},
  number={},
  pages={533-542},
  abstract={With the recent Virtual Reality (VR) Head-Mounted Displays (HMDs), users can seamlessly transition between the virtual and real worlds with techniques such as passthrough. These techniques leverage on-device cameras to capture the real world and show users a view of their physical surroundings while wearing the HMDs. However, they often require users to hold a controller or frequently tap on the HMD, limiting the potential for hands-free interaction and thereby hindering a truly immersive and natural VR experience. To address this limitation, we designed FingerButton, a finger-worn push button device that enables seamless transitions between real and virtual environments. We conducted two studies, where the first one explored a set of hand gestures for transitioning between two environments that are commonly used for “mode switching” within realities. In the second study, we compared FingerButton with the best two-hand gesture identified in the first study and other commercially available solutions (e.g., double tap) for a between-reality selection task. The results show that the physical finger button is faster and user-preferred than other techniques for the transition tasks. Overall, this research contributes to understanding and improving the interaction techniques for fluid switching between the real and virtual worlds, thereby enhancing VR user experiences.},
  keywords={Limiting;Head-mounted displays;Fluids;Virtual environments;Thumb;Switches;Resists;Cross-Reality;Transition techniques;FingerButton;RealWorld;VirtualWorld},
  doi={10.1109/ISMAR59233.2023.00068},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316454,
  author={Shahidi, Ali and Alabood, Lorans and Kaufman, Kate M. and Jaswal, Vikram K. and Krishnamurthy, Diwakar and Wang, Mea},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AR-Based Educational Software for Nonspeaking Autistic People - A Feasibility Study}, 
  year={2023},
  volume={},
  number={},
  pages={543-552},
  abstract={Approximately one-third of individuals with autism are nonspeaking: They cannot communicate effectively using speech. Some traditional accounts suggest that these individuals cannot talk because they lack the symbolic capacity for language. And yet, recent studies have shown that these individuals’ cognitive abilities are vastly underestimated by standardized tests, and that difficulties with motor skills and movement contribute to their difficulty with speech. One consequence of the traditional accounts of nonspeaking autism is that life skills (rather than academic content) tend to be emphasized in schooling. Without access to meaningful academic content, their educational and vocational opportunities are significantly limited. Recent studies have proposed the use of head-mounted Augmented Reality (AR) applications as a means of providing engaging, customizable, and age-appropriate content to this population. Specifically, such applications can address the unique sensory and motor needs of nonspeaking autistic students, e.g., allow them to move freely around the room as they interact with lessons in the application. This paper describes the design and evaluation of the first AR application aimed to facilitate tailored educational experiences for nonspeaking autistic students. After extensive consultations with nonspeaking people, parents, and professionals, we developed our application to run on HoloLens 2 offering lessons and multiple-choice comprehension and spelling questions. We conducted a study involving five nonspeaking autistic participants and two specialized educators. Through a design critique process and an iterative design refinement approach, we show that most of our participants successfully interacted with the application and completed different types of lesson tasks. Based on quantitative data from the study sessions and qualitative feedback from participants and educators, we provide recommendations for UI and UX design that will promote the development and use of such software for this under-served and under-researched population.},
  keywords={Autism;Sociology;Software;Iterative methods;Statistics;Task analysis;Augmented reality;Human-centered computing;Usability testing},
  doi={10.1109/ISMAR59233.2023.00069},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316502,
  author={Jannat, Marium-E and Hasan, Khalad},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Effects of Virtually-Augmented Display Sizes on Users’ Spatial Memory in Smartwatches}, 
  year={2023},
  volume={},
  number={},
  pages={553-562},
  abstract={The small display size of the smartwatches makes it difficult to display large amounts of information on the device. Prior work explored leveraging a second device (e.g., Head-mounted displays) to extend the space where users can access large information space with virtual displays anchored on their wrists. Though researchers showed that having an additional virtual screen increased information bandwidth, little is known about the effect of virtual display sizes on users’ performance. In this paper, we examined the impact of display sizes on spatial memory, workload, and user experience to better understand the prospects of virtually-augmented displays for smartwatches. Results from a user study revealed that a 4.8 inches display size can be the “sweet spot” for the virtually-augmented displays to ensure improved spatial memory performance and better user experience with less workload. Finally, we provided a set of design guidelines focusing to display size, spatial memory, user experience, and workload for designing virtually augmented user interfaces for smartwatches.},
  keywords={Wrist;Performance evaluation;Head-mounted displays;Navigation;Design methodology;Memory management;Focusing;Human-centered computing;Human computer interaction;Interaction paradigms;Mixed / augmented reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR59233.2023.00070},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316412,
  author={Song, Tianyu and Eck, Ulrich and Navab, Nassir},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Leveraging Motion Tracking for Intuitive Interactions in a Tablet-Based 3D Scene Annotation System}, 
  year={2023},
  volume={},
  number={},
  pages={563-572},
  abstract={In the rapidly evolving field of computer vision, efficient and accurate annotation of 3D scenes plays a crucial role. While automation has streamlined this process, manual intervention is still essential for obtaining precise annotations. Existing annotation tools often lack intuitive interactions and efficient interfaces, particularly when it comes to annotating complex elements such as 3D bounding boxes, 6D human poses, and semantic relationships in a 3D scene. Therefore, it is often time-consuming and error-prone. Emerging technologies such as augmented reality (AR) and virtual reality (VR) have shown potential to provide an immersive and interactive environment for annotators to label objects and their relationships. However, the cost and accessibility of these technologies can be a barrier to their widespread adoption. This work introduces a novel tablet-based system that utilizes built-in motion tracking to facilitate an efficient and intuitive 3D scene annotation process. The system supports a variety of annotation tasks and leverages the tracking and mobility features of the tablet to enhance user interactions. Through a thorough user study investigating three distinct tasks - creating bounding boxes, adjusting human poses, and annotating scene relationships - we evaluate the effectiveness and usability of two interaction methods: touch-based interactions and hybrid interactions that utilize both touch and device motion tracking. Our results suggest that leveraging the tablet’s motion tracking feature could lead to more intuitive and efficient annotation processes. This work contributes to the understanding of tablet-based interaction and the potential it holds for annotating complex 3D scenes.},
  keywords={Three-dimensional displays;Annotations;Tracking;Design methodology;Semantics;Touch sensitive screens;Systems support;Human-centered computing;Interaction design;Empirical studies in interaction design;Interaction design process and methods;Ubiquitous and mobile computing design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00071},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316460,
  author={Mahmud, M. Rasel and Cordova, Alberto and Quarles, John},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Auditory, Vibrotactile, or Visual? Investigating the Effective Feedback Modalities to Improve Standing Balance in Immersive Virtual Reality for People with Balance Impairments Due to Type 2 Diabetes}, 
  year={2023},
  volume={},
  number={},
  pages={573-582},
  abstract={Immersive Virtual Reality (VR) users often experience difficulties with maintaining their balance. This issue poses a significant challenge to the widespread usability and accessibility of VR, particularly for individuals with balance impairments. Previous studies have confirmed the existence of balance problems in VR, but little attention has been given to addressing them. To investigate the impact of different feedback modalities (auditory, vibrotactile, and visual) on balance in immersive VR, we conducted a study with 50 participants, consisting of 25 individuals with balance impairments due to type 2 diabetes and 25 without balance impairments. Participants were asked to perform standing reach and grasp tasks. Our findings indicated that auditory and vibrotactile techniques improved balance significantly (p<.001) in immersive VR for participants with and without balance impairments, while visual techniques only improved balance significantly for participants with balance impairments. Also, auditory and vibrotactile feedback techniques improved balance significantly more than visual techniques. Spatial auditory feedback outperformed other conditions significantly for all people. This study presents implementations and comparisons of potential strategies that can be implemented in future VR environments to enhance standing balance and promote the broader adoption of VR.},
  keywords={Visualization;Virtual environments;Diabetes;Task analysis;Usability;Augmented reality;Virtual Reality;balance;postural stability;auditory feedback;vibrotactile feedback;visual feedback;VR accessibility;VR usability;Head-Mounted Display;diabetes},
  doi={10.1109/ISMAR59233.2023.00072},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316436,
  author={Xu, Xuanhui and Puggioni, Antonella and Kilroy, David and Campbell, Abraham G.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={User Experience of Collaborative Co-located Mixed Reality: a User Study in Teaching Veterinary Radiation Safety Rules}, 
  year={2023},
  volume={},
  number={},
  pages={583-590},
  abstract={As part of the clinical training during their degree course, veterinary students learn how to safely obtain radiographs in horses. However, this can sometimes be challenging due to ethical considerations related to the use of live animals and the fact that it is strictly dependent on the caseload of the veterinary teaching hospital. This networked setup allows the lecturer to guide students in equine radiographic techniques without requiring actual horses or an X-ray machine. A study involving veterinary students showed promising results regarding the effectiveness of using MR to teach radiation safety while performing radiographic techniques on horses. In addition to performance metrics, we employed questionnaires, including MREQ, VRSQ, and UEQ, to collect demographic data and participant feedback. Participants praised the system’s pedagogical effectiveness and overall user experience. The immersive MR experience created a sense of presence and co-presence, underscoring the potential for broader applications of co-located MR in radiology and other areas.},
  keywords={Radiography;Training;Ethics;Visualization;Radiation safety;Collaboration;Virtual reality;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques;Gestural input},
  doi={10.1109/ISMAR59233.2023.00073},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316498,
  author={Zaman, Faisal and Anslow, Craig and Chalmers, Andrew and Rhee, Taehyun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MRMAC: Mixed Reality Multi-user Asymmetric Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={591-600},
  abstract={We present MRMAC, a Mixed Reality Multi-user Asymmetric Collaboration system that allows remote users to teleport virtually into a real-world collaboration space to communicate and collaborate with local users. Our system enables telepresence for remote users by live-streaming the physical environment of local users using a 360° camera while blending 3D virtual assets into the mixed-reality collaboration space. Our novel client-server architecture enables asymmetric collaboration for multiple AR and VR users and incorporates avatars, view controls, as well as synchronized low-latency audio, video, and asset streaming. We evaluated our implementation with two baseline conditions: conventional 2D and standard 360° videoconferencing. Results show that MRMAC outperformed both baselines in inducing a sense of presence, improving task performance, usability, and overall user preference, demonstrating its potential for immersive multi-user telecollaboration.},
  keywords={Telepresence;Three-dimensional displays;Collaboration;Mixed reality;Teleportation;Video conferencing;User experience;Mixed Reality;Telecollaboration;Telepresence},
  doi={10.1109/ISMAR59233.2023.00074},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316470,
  author={Zeidler, Conrad and Klug, Matthias and Woeckner, Gerrit and Clausen, Urte and Schöning, Johannes},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ARCHIE2: An Augmented Reality Interface with Plant Detection for Future Planetary Surface Greenhouses}, 
  year={2023},
  volume={},
  number={},
  pages={601-610},
  abstract={More than 50 years after the last human set foot on the Moon during the Apollo 17 mission, humans aim to return to the Moon in this decade. This time, humanity plans to establish lunar habitats for a sustainable longer presence. An integrated part of these lunar habitats will be planetary surface greenhouses. These greenhouses will produce food, process air, recycle water, and improve the psychological well-being of humans. Past research has shown that a large amount of crew time, a scarce and valuable resource in spaceflight, is needed for maintenance and repairs in a planetary surface greenhouse, leaving less time for crop cultivation and science activities. In this paper, we present the concept of an augmented reality interface named ARCHIE2 to reduce crew time and the workload of astronauts and remote support teams on Earth to operate a planetary surface greenhouse. ARCHIE2 allows users to visualize status information on plants, technical systems, and environmental parameters in the greenhouse or other features supporting the greenhouse operations using an augmented reality headset. In particular, we report on the implementation and performance of the ARCHIE2 plant detection module that runs locally on the augmented reality headset. Using images with a resolution of 320$\times$l92 pixels, arugula selvatica plants were detected using an artificial neural network (based on a YOLOv5s model) from a horizontal distance up to 50 cm with an average inference time of 602 ms and an average of 48 FPS. Based on that, the plants were augmented with labels to visualize relevant plant-specific information supporting astronauts in the maintenance of the plants.},
  keywords={Headphones;Visualization;Moon;Habitats;Greenhouses;Maintenance engineering;Water conservation;EDEN ISS;Augmented reality;ARCHIE2;Greenhouse;Space analog;Operations;Crew time;Workload;J.2 [Physical Sciences and Engineering]: Aerospace;C.1.3 [Processor Architectures]: Other Architecture Styles;Neural nets;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems;Artificial;augmented;and virtual realities},
  doi={10.1109/ISMAR59233.2023.00075},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316472,
  author={Hoang, Thuong and Aggarwal, Deepti and Wood-Bradley, Guy and Lee, Tsz-Kwan and Wang, Rui and Ferdous, Hasan and Baldwin, Alexander},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Systematic Review of Immersive Technologies for Physical Training in Fitness and Sports}, 
  year={2023},
  volume={},
  number={},
  pages={611-621},
  abstract={The increased availability of immersive headsets in the games industry has promoted wide adoption of immersive technology amongst consumers. The benefits of spatial freedom and agency for body movements in virtual reality pave the way for everyday fitness and physical training applications. We conducted a systematic review through ACM Digital Library, IEEEXplore, and Scopus, to investigate how immersive technologies have been applied for physical training in fitness and sports. Our review included all peer-reviewed papers, published and written in English, discussing all forms of hardware for VR, AR, and MR technologies, targeted towards healthy population. We excluded applications for clinical populations and treatment of specific diseases, and all non-peer reviewed articles like position papers, workshops, tutorials, and magazine columns. From the shortlisted 144 papers, we summarize the development trends of immersive technologies and highlight the challenges of designing immersive technologies for everyday fitness. We also provide recommendations for future work and highlight the need to support better collaboration with industry partners, trainer-trainee experiences, and social dynamics of sports for designing better experiences.},
  keywords={Training;Industries;Systematics;Sociology;Market research;Libraries;Object recognition;Virtual Reality;Augmented Reality;Fitness;Physical Training;Systematic Review;[Interaction Paradigms] - Virtual Reality;Surveys and Overviews},
  doi={10.1109/ISMAR59233.2023.00076},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316475,
  author={Guo, Zixuan and Xu, Wenge and Zhang, Jialin and Wang, Hongyu and Lo, Cheng-Hung and Liang, Hai-Ning},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Who’s Watching Me?: Exploring the Impact of Audience Familiarity on Player Performance, Experience, and Exertion in Virtual Reality Exergames}, 
  year={2023},
  volume={},
  number={},
  pages={622-631},
  abstract={Familiarity with audiences plays a significant role in shaping individual performance and experience across various activities in everyday life. This study delves into the impact of familiarity with non-playable character (NPC) audiences on player performance and experience in virtual reality (VR) exergames. By manipulating of NPC appearance (face and body shape) and voice familiarity, we explored their effect on game performance, experience, and exertion. The findings reveal that familiar NPC audiences have a positive impact on performance, creating a more enjoyable gaming experience, and leading players to perceive less exertion. Moreover, individuals with higher levels of self-consciousness exhibit heightened sensitivity to the familiarity with NPC audiences. Our results shed light on the role of familiar NPC audiences in enhancing player experiences and provide insights for designing more engaging and personalized VR exergame environments.},
  keywords={Sensitivity;Shape;Virtual environments;Atmosphere;Games;Organizations;Faces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Software and its engineering;Software organization and properties;Virtual worlds software;Interactive games},
  doi={10.1109/ISMAR59233.2023.00077},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316357,
  author={Wang, Zhimin and Gu, Xiangyuan and Lu, Feng},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DEAMP: Dominant-Eye-Aware Foveated Rendering with Multi-Parameter optimization}, 
  year={2023},
  volume={},
  number={},
  pages={632-641},
  abstract={The increasing use of high-resolution displays and the demand for interactive frame rates presents a major challenge to widespread adoption of virtual reality. Foveated rendering address this issue by lowering pixel sampling rate at the periphery of the display. How-ever, existing techniques do not fully exploit the feature of human binocular vision, i.e., the dominant eye. In this paper, we propose a Dominant-Eye-Aware foveated rendering method optimized with Multi-Parameter foveation (DEAMP). Specifically, we control the level of foveation for both eyes with two distinct sets of foveation parameters. To achieve this, each eye’s visual field is divided into three nested layers based on eccentricity. Multiple parameters govern the level of foveation of each layer, respectively. We conduct user studies to evaluate our method. Experimental results demonstrate that DEAMP is superior in terms of rendering time and reduces the disparity between pixel sampling rate and the visual acuity fall-off model while maintaining the perceptual quality.},
  keywords={Visualization;Solid modeling;Rendering (computer graphics);Optimization;Augmented reality;Virtual Reality;Foveated Rendering;Eye tracking;Eye dominance},
  doi={10.1109/ISMAR59233.2023.00078},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316410,
  author={Bataille, Guillaume and Lammini, Abdelhadi and Chardonnet, Jean-Rémy},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ARPuzzle: Evaluating the Effectiveness of Collaborative Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={642-651},
  abstract={Collaborative Augmented Reality (CAR) offers disruptive ways for people to collaborate. However, this emerging technology must improve its acceptance, efficiency, and usability to scale up and, for example, support augmented operations executed by technicians. This paper presents our CAR system and its experimentation during a cooperative puzzle-solving task. Our system provides collaborators with a shared virtual space allowing verbal and non-verbal interpersonal communications, and intuitive interactions with shared virtual replicas of real objects. Our system also integrates avatars embodied by remote users. We conducted a dual-user study comparing colocated and remote solving of a puzzle virtual replica with its real solving. We evaluated task performance, collaboration, mutual awareness, spatial presence, and copresence, usability, and preference. We found that, if real is preferred and more efficient than our CAR system, CAR is reaching favorable usability levels. We also found that remote augmented reality including full-body avatars offers similar results to colocated augmented reality. This preliminary work paves the way for future research aiming to support and enhance the design and making of Collaborative Augmented Reality systems dedicated to augmented operations.},
  keywords={Avatars;Collaboration;Automobiles;Usability;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Collaborative interaction},
  doi={10.1109/ISMAR59233.2023.00079},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316514,
  author={Li, Junjie and Wang, Yumei and Liu, Yu},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Meta360: Exploring User-Specific and Robust Viewport Prediction in360-Degree Videos through Bi-Directional LSTM and Meta-Adaptation}, 
  year={2023},
  volume={},
  number={},
  pages={652-661},
  abstract={Viewport prediction is a critical aspect of virtual reality (VR) video streaming, directly impacting user experience in adaptive streaming. However, most existing algorithms treat users as homogeneous entities and overlook the variations in user behaviors and video content. Additionally, they often struggle with long-term predictions and intense movement. Our research sheds light on the importance of considering user behavior variations and leveraging advanced techniques to optimize robust viewport prediction in VR video streaming. First, we address these limitations by conducting a comprehensive feature analysis on existing datasets to uncover distinctive user behaviors. Building upon these findings, we propose a novel approach that utilizes the power of Bidirectional Long Short-Term Memory (BiLSTM) networks and meta-learning. The BiLSTM architecture effectively captures long-term dependencies, which can strengthen the robustness of viewport prediction especially in longterm prediction and intense movement. Additionally, meta-learning enables personalized adaptation to individual users’ viewing behaviors. Through extensive evaluations on diverse datasets, our algorithm Meta360 demonstrates superior performance in terms of accuracy and robustness compared to state-of-the-art methods.},
  keywords={Metalearning;Adaptation models;Solid modeling;Streaming media;Predictive models;Prediction algorithms;Robustness;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR59233.2023.00080},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316516,
  author={Yu, Kevin and Roth, Daniel and Strak, Robin and Pankratz, Frieder and Reichling, Julia and Kraetsch, Clemens and Weidert, Simon and Lazarovici, Marc and Navab, Nassir and Eck, Ulrich},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mixed Reality 3D Teleconsultation for Emergency Decompressive Craniotomy: An Evaluation with Medical Residents}, 
  year={2023},
  volume={},
  number={},
  pages={662-671},
  abstract={Enabling collaborative telepresence in healthcare, especially surgical procedures, presents a critical challenge. The decompressive craniotomy procedure stands out as particularly complex and time-sensitive. The current teleconsultation approach relies on 2D color cameras, often offering only a fixed view and limited visual capabilities between experts and surgeons. However, teleconsultation can be addressed with Mixed Reality and immersive technology to potentially enable a better consultation of the procedure. We conducted an extensive user study focusing on decompressive craniotomy to investigate the advantages and challenges of our 3D teleconsultation system compared to a 2D video-based consultation system. Our 3D teleconsultation system leverages real-time 3D reconstruction of the patient and environment to empower experts to provide guidance and create virtual 3D annotations. The study utilized 3D-printed head models to perform a lifelike surgical intervention. It involved 14 medical residents and demonstrated an in-vitro 17% improvement in accurately describing the incision size on the patient’s head, contributing to potentially improved patient outcomes.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Telepresence;Annotations;Scalp;Mixed reality;3D Telepresence;Medical Consultation;Evaluation},
  doi={10.1109/ISMAR59233.2023.00081},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316479,
  author={Kruse, Lucie and Hertel, Julia and Mostajeran, Fariba and Schmidt, Susanne and Steinicke, Frank},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Would You Go to a Virtual Doctor? A Systematic Literature Review on User Preferences for Embodied Virtual Agents in Healthcare}, 
  year={2023},
  volume={},
  number={},
  pages={672-682},
  abstract={Medical virtual agents (VAs) hold great potential to support patients in achieving their health goals, especially at times or in regions where the demand for physiological and psychological therapy exceeds the capacity of medical services. To create an accepted complement to on-site diagnosis, treatment, and counseling, it is critical to understand the impact of factors such as the agent’s visual representation, behavior, and responsibilities on creating a trustworthy human-agent relationship. To gain insights into these factors, we conducted a systematic literature review including 59 papers on embodied VAs in the medical domain. Our review focused on the application fields and the role of VAs in medicine, as well as the technology used to display them. Using thematic analysis, we discuss our findings in terms of user preferences, as well as potentials and barriers faced in the interaction with medical VAs. Concerning the visual representation, the users’ wish for customization in terms of appearance and communication modalities was pointed out. It was also important that the agent’s information builds up on trustworthy sources, that they are motivating and adapted to the users’ knowledge. Finally, our results identify research gaps, in particular regarding the technological implementation and the use of artificial intelligence.},
  keywords={Employee welfare;Visualization;Solid modeling;Systematics;Three-dimensional displays;Bibliographies;Medical treatment;artificial intelligence;healthcare;agents;telemedicine;virtual doctors;Human-centered computing;HCI theory;concepts and models;Applied computing;Health care information systems},
  doi={10.1109/ISMAR59233.2023.00082},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316424,
  author={Gao, Hong and Bozkir, Efe and Stark, Philipp and Goldberg, Patricia and Meixner, Gerrit and Kasneci, Enkelejda and Göllner, Richard},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Detecting Teacher Expertise in an Immersive VR Classroom: Leveraging Fused Sensor Data with Explainable Machine Learning Models}, 
  year={2023},
  volume={},
  number={},
  pages={683-692},
  abstract={Currently, VR technology is increasingly being used in applications to enable immersive yet controlled research settings. One such area of research is expertise assessment, where novel technological approaches to collecting process data, specifically eye tracking, in combination with explainable models, can provide insights into assessing and training novices, as well as fostering expertise development. We present a machine learning approach to predict teacher expertise by leveraging data from an off-the-shelf VR device collected in a VirATec study. By fusing eye-tracking and controller-tracking data, teachers’ recognition and handling of disruptive events in the classroom are taken into account or considered. Three classification models were compared, including SVM, Random Forest, and LightGBM, with Random Forest achieving the best ROC-AUC score of 0.768 in predicting teacher expertise. The SHAP approach to model interpretation revealed informative features (e.g., fixations on identified disruptive students) for distinguishing teacher expertise. Our study serves as a pioneering effort in assessing teacher expertise using eye tracking within an interactive virtual setting, paving the way for future research and advancements in the field.},
  keywords={Training;Support vector machines;Visualization;Computational modeling;Gaze tracking;Predictive models;Data models;Computing methodologies;Machine learning;Machine learning approaches;Classification and regression trees;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00083},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316505,
  author={Lyu, Junfeng and Xu, Feng},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Eyeglasses Refraction in Appearance-based Gaze Estimation}, 
  year={2023},
  volume={},
  number={},
  pages={693-702},
  abstract={For myopia and hyperopia subjects, eyeglasses would change the position of objects in their views, leading to different eyeball rotations for the same gaze target (Fig. 1). Existing appearance-based gaze estimation methods ignore this effect, while this paper investigates it and proposes an effective method to consider it in gaze estimation, achieving noticeable improvements. Specifically, we discover that the appearance-gaze mapping differs for spectacled and unspectacled conditions, and the deviations are nearly consistent with the physical laws of the ideal lens. Based on this discovery, we propose a novel multi-task training strategy that encourages networks to regress gaze and classify the wearing conditions simultaneously. We apply the proposed strategy to some popular methods, including supervised and unsupervised ones, and evaluate them on different datasets with various backbones. The results show that the multi-task training strategy could be used on the existing methods to improve the performance of gaze estimation. To the best of our knowledge, we are the first to clearly reveal and explicitly consider eyeglasses refraction in appearance-based gaze estimation. Data and code are available at https://github.com/StoryMY/RefractionGaze.},
  keywords={Training;Codes;Estimation;Multitasking;Augmented reality;Lenses;Computing methodologies;Gaze estimation;Eyeglasses refraction;Multi-task learning},
  doi={10.1109/ISMAR59233.2023.00084},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316519,
  author={Richard, Grégoire and Pietrzak, Thomas and Argelaguet, Ferran and Lécuyer, Anatole and Casiez, Géry},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MultiVibes: What if your VR Controller had 10 Times more Vibrotactile Actuators?}, 
  year={2023},
  volume={},
  number={},
  pages={703-712},
  abstract={Consumer-grade virtual reality (VR) controllers are typically equipped with one vibrotactile actuator, allowing to create simple and non-spatialized tactile sensations through the vibration of the entire controller. Leveraging the funneling effect, an illusion in which multiple vibrations are perceived as a single one, we propose MultiVibes, a VR controller capable of rendering spatialized sensations at different locations on the user’s hand and fingers. The designed prototype includes ten vibrotactile actuators, directly in contact with the skin of the hand, limiting the propagation of vibrations through the controller. We evaluated MultiVibes through two controlled experiments. The first one focused on the ability of users to recognize spatio-temporal patterns, while the second one focused on the impact of MultiVibes on the users’ haptic experience when interacting with virtual objects they can feel. Taken together, the results show that MultiVibes is capable of providing accurate spatialized feedback and that users prefer MultiVibes over recent VR controllers.},
  keywords={Vibrations;Actuators;Prototypes;Valves;Three-dimensional printing;Rendering (computer graphics);User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR59233.2023.00085},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316529,
  author={Tahmid, Ibrahim A. and Lisle, Lee and Davidson, Kylie and Whitley, Kirsten and North, Chris and Bowman, Doug A.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating the Feasibility of Predicting Information Relevance During Sensemaking with Eye Gaze Data}, 
  year={2023},
  volume={},
  number={},
  pages={713-722},
  abstract={Eye gaze patterns vary based on reading purpose and complexity, and can provide insights into a reader’s perception of the content. We hypothesize that during a complex sensemaking task with many text-based documents, we will be able to use eye-tracking data to predict the importance of documents and words, which could be the basis for intelligent suggestions made by the system to an analyst. We introduce a novel eye-gaze metric called ‘GazeScore’ that predicts an analyst’s perception of the relevance of each document and word when they perform a sensemaking task. We conducted a user study to assess the effectiveness of this metric and found strong evidence that documents and words with high GazeScores are perceived as more relevant, while those with low GazeScores were considered less relevant. We explore potential real-time applications of this metric to facilitate immersive sensemaking tasks by offering relevant suggestions.},
  keywords={Measurement;Analytical models;Gaze tracking;Information retrieval;Real-time systems;Data models;Complexity theory;Immersive Analytics;Sensemaking;Augmented Reality;Human-Computer Interaction;Relevance Perception;Predicted Relevance;Gaze-Based Metric;Multiple Documents},
  doi={10.1109/ISMAR59233.2023.00086},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316530,
  author={Voisard, Laurent and Hatira, Amal and Sarac, Mine and Kersten-Oertel, Marta and Batmaz, Anil Ufuk},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of Opaque, Transparent and Invisible Hand Visualization Styles on Motor Dexterity in a Virtual Reality Based Purdue Pegboard Test}, 
  year={2023},
  volume={},
  number={},
  pages={723-731},
  abstract={The virtual hand interaction technique is one of the most common interaction techniques used in virtual reality (VR) systems. A VR application can be designed with different hand visualization styles, which might impact motor dexterity. In this paper, we aim to investigate the effects of three different hand visualization styles — transparent, opaque, and invisible — on participants’ performance through a VR-based Purdue Pegboard Test (PPT). A total of 24 participants were recruited and instructed to place pegs on the board as quickly and accurately as possible. The results indicated that using the invisible hand visualization significantly increased the number of task repetitions completed compared to the opaque hand visualization. However, no significant difference was observed in participants’ preference for the hand visualization styles. These findings suggest that an invisible hand visualization may enhance performance in the VR-based PPT, potentially indicating the advantages of a less obstructive hand visualization style. We hope our results can guide developers, researchers, and practitioners when designing novel virtual hand interaction techniques.},
  keywords={Visualization;Design methodology;Task analysis;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00087},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316421,
  author={Rowden, Alexander and Krokos, Eric and Whitley, Kirsten and Varshney, Amitabh},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Effective Immersive Approaches to Visualizing WiFi}, 
  year={2023},
  volume={},
  number={},
  pages={732-740},
  abstract={WiFi networks are essential to our daily lives, but their signals are not visible to us. Therefore, it is challenging to evaluate the health of a network or make changes to ensure an optimal configuration. Traditional visualization approaches, such as contour lines, are not intuitive and lead to challenges in the analysis and comprehension of networks. In this paper, we introduce two novel visualizations: Wavelines and Stacked Bars. We then compared these visualizations to the state-of-the-art visualization technique of contour lines. We carried out a user study with 32 participants to validate that our novel visualizations can improve user confidence, accuracy, and completion time for the tasks of router localization, ranking of signal strengths, channel interference, and router coverage. We selected these tasks after extensive discussions with domain experts. We believe that our findings will assist network analysts in visually understanding our increasingly rich signal environments.},
  keywords={Location awareness;Visualization;Data visualization;Interference;Task analysis;Wireless fidelity;Augmented reality;Scalar Field Data;Application Motivated Visualization;Human-Subjects Qualitative Studies;Human-Subjects Quantitative Studies;Data Analysis;Virtual Reality;Immersive},
  doi={10.1109/ISMAR59233.2023.00088},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316432,
  author={Nakagawa, Soran and Watanabe, Yoshihiro},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={High-Frame-Rate Projection with Thousands of Frames Per Second Based on the Multi-Bit Superimposition Method}, 
  year={2023},
  volume={},
  number={},
  pages={741-750},
  abstract={The growing need for high-frame-rate projectors in the fields of dynamic projection mapping (DPM) and three-dimensional (3D) displays has increased. Conventional methods allow for an increase in the frame rate to as much as 2,841 frames per second (fps) for 8-bit image projection, using digital light processing (DLP) technology when the minimum digital mirror device (DMD) control time is $44 \mu \mathrm{s}$. However, this rate needs to be further augmented to suit specific applications. In this study, we developed a novel high-frame-rate projection method, which divides the bit depth of an image among multiple projectors and simultaneously projects them in synchronization. The simultaneously projected bit images are superimposed such that a high-bit-depth image is generated within a reduced single-frame duration. Additionally, we devised an optimization process to determine the system parameters necessary for attaining maximum brightness. We constructed a prototype system utilizing two high-frame-rate projectors and validated the feasibility of using our system to project 8-bit images at a rate of 5,600 fps. Furthermore, the quality assessment of our projected image exhibited superior performance in comparison to a dithered image.},
  keywords={Three-dimensional displays;Brightness;Prototypes;Process control;Optical imaging;Quality assessment;Synchronization;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Hardware;Communication hardware;interfaces;storage;Displays and imagers},
  doi={10.1109/ISMAR59233.2023.00089},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316364,
  author={Zhu, Jiarui and Kumaran, Radha and Xu, Chengyuan and Höllerer, Tobias},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Free-form Conversation with Human and Symbolic Avatars in Mixed Reality}, 
  year={2023},
  volume={},
  number={},
  pages={751-760},
  abstract={The integration of large language models and mixed reality technologies has enabled users to engage in free-form conversations with virtual agents across different “realities”. However, if and how the agent’s visual representation, especially when combined with mixed reality environments, will affect the conversation content or user experience is not yet fully understood. In this work, we design and conduct a user study involving two types of visual representations (a human avatar and a symbolic avatar) and two mixed reality environments (virtual reality and augmented reality), facilitating a free-form conversation experience with GPT-3 powered agents. We found evidence that the use of virtual or augmented realities can influence conversation content. Users chatting with avatars in virtual reality made significantly more references to the location or the space, suggesting they tended to perceive conversations as occurring in the agent’s space, whereas the physical AR environment was perhaps more perceived as the user’s space. Conversations with the human avatar improve user recall of the conversation, even though there is no evidence of increased information extracted during the conversation. These observations and our analysis of post-study questionnaires suggest that human avatars can positively impact user memory and experience. We hope our findings and the open-source implementation will help facilitate future research on free-form conversational agents in mixed reality.},
  keywords={Visualization;Solid modeling;Avatars;Mixed reality;Oral communication;User experience;Data mining;Artificial intelligence (AI), large language models (LLMs), mixed reality Human-centered computing;Empirical studies in HCI;Computing methodologies;Mixed / augmented reality Computing methodologies;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00090},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316465,
  author={Wang, Yifan and Li, Yue and Liang, Hai-Ning},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparative Analysis of Artefact Interaction and Manipulation Techniques in VR Museums: A Study of Performance and User Experience}, 
  year={2023},
  volume={},
  number={},
  pages={761-770},
  abstract={For museums in Virtual Reality (VR), various interaction and manipulation techniques could be employed for users to engage with artefact interactions. This study examined four combinations of interaction (controller-based and hand-tracking) and manipulation (direct and indirect) techniques, assessing user performance and experience with these interaction techniques in a virtual museum environment. We conducted a within-subjects experiment and asked participants to perform a series of transform manipulation tasks using the four techniques. Participants’ task completion time was measured. They also provided feedback on acceptance, learnability, presence, sickness, and fatigue, and gave an overall ranking through post-experiment questionnaires and interviews. The results revealed that controller-based direct manipulation outperformed the other techniques in terms of task performance and user experience, with hand-tracking indirect manipulation being the least efficient and the least preferred option. The study offers insights for future research and development in refining interaction and manipulation techniques and designing more user-friendly VR museum experiences.},
  keywords={Refining;Transforms;Fatigue;Virtual museums;Particle measurements;User experience;Time measurement;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality Human-centered computing;Empirical studies in HCI Human-centered computing;Interaction techniques;Gestural input},
  doi={10.1109/ISMAR59233.2023.00091},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316485,
  author={Dyrda, Daniel and Klusmann, Jack and Rudolph, Linda and Stieglbauer, Felix and Amougou, Maximilian and Pantförder, Dorothea and Vogel-Heuser, Birgit and Klinker, Gudrun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Specifying Volumes of Interest for Industrial Use Cases}, 
  year={2023},
  volume={},
  number={},
  pages={771-779},
  abstract={Creating digital representations of industrial facilities presents substantial opportunities for the industry. The development of digital twins opens up many prospective applications for industrial plants, such as facilitating virtual training environments, streamlining change management, and providing real-time understanding of remote scenes. However, creating and maintaining these digital twins involves complex tasks, including scanning industrial plants and annotating their subsystems. Many use cases necessitate specifying a Volume of Interest (VoI) for further processing while presenting severe environmental challenges for segmentation quality and worker safety. Besides that, widespread adoption also depends on creating methods that are easy to use, even for workers with limited 3D interaction knowledge. Currently, no commonly implemented method fulfills all these diverse requirements. This paper introduces an approach for specifying a VoI in industrial scenarios. Our approach defines a volume by intersecting the projection of hand-drawn surroundings on a small number of pictures of the target volume, utilizing Augmented Reality and Voxel Carving. It can successfully handle various sizes of target volumes and delivers an appropriately detailed result. Applying qualitative discussions and a quantitative evaluation, we ensure our application meets all requirements posed by the scenarios. This simple interaction metaphor, tailored to specific use cases, can serve as a versatile pattern for various digital twin scenarios. It offers a valuable alternative to 3D primitive-based segmentation methods.},
  keywords={Training;Industries;Three-dimensional displays;Industrial facilities;Real-time systems;Digital twins;Industrial plants;Scenario-based design;Touch screens;Mixed / augmented reality},
  doi={10.1109/ISMAR59233.2023.00092},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316489,
  author={Garrido, Daniel and Jacob, João and Silva, Daniel Castro},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Performance Impact of Immersion and Collaboration in Visual Data Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={780-789},
  abstract={Immersive Analytics is a recent field of study that focuses on utilizing emerging extended reality technologies to bring visual data analysis from the 2D screen to the real/virtual world. The effectiveness of Immersive Analytics, when compared to traditional systems, has been widely studied in this field’s corpus, usually concluding that the immersive solution is superior. However, when it comes to comparing collaborative to single-user immersive analytics, the literature is lacking in user studies. As such, we developed a comprehensive experimental study with the objective of quantifying and analysing the impact that both immersion and collaboration have on the visual data analysis process. A two-variable (immersion: desktop/virtual reality; number of users: solo/pair) full factorial study was conceived with a mixed design (within-subject for immersion and between subject for number of users). Each of the 24 solo and 24 pairs of participants solved five visual data analysis tasks in both a head-mounted display-based virtual world and a desktop computer environment. The results show that, in terms of task time to completion, there were no significant differences between desktop and virtual reality, or between the solo and pair conditions. However, it was possible to conclude that collaboration is more beneficial the more complex the task is in both desktop and virtual reality, and that for less complex tasks, collaboration can be a hindrance. System Usability Scale scores were significantly better in the virtual reality condition than the desktop one, especially when working in pairs. As for user preference, the virtual reality system was significantly more favoured both as a visual data analysis platform and a collaborative data analysis platform over the desktop system. All supplemental materials are available at https://osf.io/k94u5/.},
  keywords={Visualization;Data analysis;Extended reality;Collaboration;Data visualization;Task analysis;Usability;Human-centered computing;Visualization;Empirical studies in visualization;Interaction paradigms;Virtual reality;Collaborative interaction},
  doi={10.1109/ISMAR59233.2023.00093},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316509,
  author={Joos, Lucas and Klein, Karsten and Fischer, Maximilian T. and Dennig, Frederik L. and Keim, Daniel A. and Krone, Michael},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Trajectory Data in Augmented Reality: A Comparative Study of Interaction Modalities}, 
  year={2023},
  volume={},
  number={},
  pages={790-799},
  abstract={The visual exploration of trajectory data is crucial in domains such as animal behavior, molecular dynamics, and transportation. With the emergence of immersive technology, trajectory data, which is often inherently three-dimensional, can be analyzed in stereoscopic 3D, providing new opportunities for perception, engagement, and understanding. However, the interaction with the presented data remains a key challenge. While most applications depend on hand tracking, we see eye tracking as a promising yet under-explored interaction modality, while challenges such as imprecision or inadvertently triggered actions need to be addressed. In this work, we explore the potential of eye gaze interaction for the visual exploration of trajectory data within an AR environment. We integrate hand- and eye-based interaction techniques specifically designed for three common use cases and address known eye tracking challenges. We refine our techniques and setup based on a pilot user study (n=6) and find in a follow-up study (n=20) that gaze interaction can compete with hand-tracked interaction regarding effectiveness, efficiency, and task load for selection and cluster exploration tasks. However, time step analysis comes with higher answer times and task load. In general, we find the results and preferences to be user-dependent. Our work contributes to the field of immersive data exploration, underscoring the need for continued research on eye tracking interaction.},
  keywords={Visualization;Three-dimensional displays;Stereo image processing;Animal behavior;Transportation;Gaze tracking;Trajectory;Augmented reality;trajectories;gaze interaction;eye tracking;user evaluation;data analysis;immersive analytics;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR59233.2023.00094},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316366,
  author={Chaffangeon Caillet, Adrien and Goguey, Alix and Nigay, Laurence},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={3D Selection in Mixed Reality: Designing a Two-Phase Technique To Reduce Fatigue}, 
  year={2023},
  volume={},
  number={},
  pages={800-809},
  abstract={Mid-air pointing is widely used for 3D selection in Mixed Reality but leads to arm fatigue. In a first exploratory experiment we study a two-phase design and compare modalities for each phase: mid-air gestures, eye-gaze and microgestures. Results suggest that eye-gaze and microgestures are good candidates to reduce fatigue and improve interaction speed. We therefore propose two 3D selection techniques: Look&MidAir and Look&Micro. Both techniques include a first phase during which users control a cone directed along their eye-gaze. Using the flexion of their non-dominant hand index finger, users pre-select the objects intersecting this cone. If several objects are pre-selected, a disambiguation phase is performed using direct mid-air touch for Look&MidAir or thumb to finger microgestures for Look&Micro. In a second study, we compare both techniques to the standard raycasting technique. Results show that Look&MidAir and Look&Micro perform similarly. However they are 55% faster, perceived easier to use and are less tiring than the baseline. We discuss how the two techniques could be combined for greater flexibility and for object manipulation after selection.},
  keywords={Three-dimensional displays;Thumb;Mixed reality;Fatigue;Standards;Augmented reality;eye-gaze},
  doi={10.1109/ISMAR59233.2023.00095},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316446,
  author={Plümer, Jan Hendrik and Tatzgern, Markus},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards a Framework for Validating XR Prototyping for Performance Evaluations of Simulated User Experiences}, 
  year={2023},
  volume={},
  number={},
  pages={810-819},
  abstract={Extended Reality (XR) technology has matured in recent years, leading to increased use of XR simulations for prototyping novel human-centered interfaces, approximating advanced display hardware, or exploring future user experiences, before realising them in real-world scenarios. However, the validity of utilizing XR prototyping (XRP) as a method for gathering performance data on novel user experiences is still underexplored, i.e, it is not clear if results gathered in simulations can be transferred to a real experience. To address this gap, we propose a validation framework that supports establishing equivalence of performance measures gathered with real products and simulated products and, thus, improve ecological validity of XRP. To demonstrate the utility of the framework, we conduct an exemplary validation study using a Varjo XR-3, a state-of-the-art XR head-mounted display (HMD). The study focuses on steering a small drone and comparing it to interactions with its real-world counterpart. We identify functional fidelity, i.e., functional similarity between real and simulated product, as well as simulation overhead from wearing an HMD as major confounding factors for XRP.},
  keywords={Performance evaluation;Head-mounted displays;Extended reality;Biological system modeling;Resists;Hardware;Data models;Extended Reality;Prototyping;Augmented Reality;Mixed Reality;Virtual Reality;Product Development},
  doi={10.1109/ISMAR59233.2023.00096},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316427,
  author={Khan, Humayun and Nilsson, Daniel},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Smell of Fire Increases Behavioural Realism in Virtual Reality: A Case Study on a Recreated MGM Grand Hotel Fire}, 
  year={2023},
  volume={},
  number={},
  pages={820-828},
  abstract={Virtual reality allows creating highly immersive visual and auditory experiences, making users feel physically present in the environment. This makes it an ideal platform to simulate dangerous scenarios, including fire evacuation, and study human behaviour without exposing users to harmful elements. However, human perception of the surroundings is based on the integration of multiple sensory cues (visual, auditory, tactile, or/and olfactory) present in the environment. When some of the sensory stimuli are missing in the virtual experience, it can break the illusion of being there in the environment and could lead to actions that deviate from normal behaviour. In this work, we added an olfactory cue in a well-documented historic hotel fire scenario that was recreated in VR, and examined the effects of the olfactory cue on human behaviour. We conducted a between subject study on 40 naive participants. Our results show that the addition of the olfactory cue could increase behavioural realism. We found that 80% of the studied actions for the VR with olfactory cue condition matched the ones performed by the survivors. In comparison, only 40% of the participants’ actions for VR only condition were similar to the survivors.},
  keywords={Visualization;Ethics;Olfactory;Virtual environments;Safety;Augmented reality;Olfactory Augmentation;Human Behaviour;Virtual Reality;- Fire Evacuation;Emergency Response},
  doi={10.1109/ISMAR59233.2023.00097},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316521,
  author={Kim, Dooyoung and Woo, Woontack},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Edge-Centric Space Rescaling with Redirected Walking for Dissimilar Physical-Virtual Space Registration}, 
  year={2023},
  volume={},
  number={},
  pages={829-838},
  abstract={We propose a novel space-rescaling technique for registering dissimilar physical-virtual spaces by utilizing the effects of adjusting physical space with redirected walking. Achieving a seamless and immersive Virtual Reality (VR) experience requires overcoming the spatial heterogeneities between the physical and virtual spaces and accurately aligning the VR environment with the user’s tracked physical space. However, existing space-matching algorithms that rely on one-to-one scale mapping are inadequate when dealing with highly dissimilar physical and virtual spaces, and redirected walking controllers could not utilize basic geometric information from physical space in the virtual space due to coordinate distortion. To address these issues, we apply relative translation gains to partitioned space grids based on the main interactable object’s edge, which enables space-adaptive modification effects of physical space without coordinate distortion. Our evaluation results demonstrate the effectiveness of our algorithm in aligning the main object’s edge, surface, and wall, as well as securing the largest registered area compared to alternative methods under all conditions. These findings can be used to create an immersive play area for VR content where users can receive passive feedback from the plane and edge in their physical environment.},
  keywords={Legged locomotion;Aerospace electronics;Distortion;Partitioning algorithms;Augmented reality;Virtual Reality;Mixed Reality;spatial matching;registration;optimization;redirected walking;Human-centered computing;Human Computer Interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00098},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316354,
  author={Zhu, Xiuqi and Wang, Chenyi and Guo, Zichun and Zhao, Yifan and Jiao, Yang},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={“Can You Move It?”: The Design and Evaluation of Moving VR Shots in Sport Broadcast}, 
  year={2023},
  volume={},
  number={},
  pages={839-848},
  abstract={Virtual Reality (VR) broadcasting has seen widespread adoption in major sports events, attributed to its ability to generate a sense of presence, curiosity, and excitement among viewers. However, we have noticed that still shots reveal a limitation in the movement of VR cameras and hinder the VR viewing experience in current VR sports broadcasts. This paper aims to bridge this gap by engaging in a quantitative user analysis to explore the design and impact of dynamic VR shots on viewing experiences. We conducted two user studies in a digital hockey game twin environment and asked participants to evaluate their viewing experience through two questionnaires. Our findings suggested that the viewing experiences demonstrated no notable disparity between still and moving shots for single clips. However, when considering entire events, moving shots improved the viewer’s immersive experience, with no notable increase in sickness compared to still shots. We further discuss the benefits of integrating moving shots into VR sports broadcasts and present a set of design considerations and potential improvements for future VR sports broadcasting.},
  keywords={Bridges;Immersive experience;Production;Games;Broadcasting;Cameras;Ice;Human-centered computing;Human-Computer Interaction;Empirical studies in HCI},
  doi={10.1109/ISMAR59233.2023.00099},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316483,
  author={Jing, Allison and Frederick, Michael and Sewell, Monica and Karlson, Amy and Simpson, Brian and Smith, Missie},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={How Visualising Emotions Affects Interpersonal Trust and Task Collaboration in a Shared Virtual Space}, 
  year={2023},
  volume={},
  number={},
  pages={849-858},
  abstract={Emotion is dynamic. Changes in emotion can be hard to process during face-to-face interaction, yet transferring them into a shared virtual space becomes more challenging. This research first explores nine visual representations to amplify emotions in a virtual space, leading to a bi-directional emotion-sharing system (FeelMoji i/o). The second study investigates the effect of explicit emotion-sharing in interpersonal trust and task collaboration through three conditions - verbal only, verbal+positive visual, and verbal+honest visual using FeelMoji through the proposal of a framework of four factors (usability, integrity, behaviour, and collaboration). The results indicate that FeelMoji yields frequent emotion consensus as task milestones and positive interdependent behaviours between collaborators, which help develop conversations, affirm decision-making, and build familiarity and trust between strangers. Moreover, we discuss how our study can inspire future investigation in human-AI agent behaviours and large-scale multi-user virtual environments.},
  keywords={Visualization;Emotion recognition;Decision making;Collaboration;Virtual environments;Bidirectional control;Proposals;VR;Emotion;Empathic Computing;visualization},
  doi={10.1109/ISMAR59233.2023.00100},
  ISSN={2473-0726},
  month={Oct},}
@INPROCEEDINGS{10316487,
  author={Liu, Ruofan and Wu, Erwin and Liao, Chen-Chieh and Nishioka, Hayato and Furuya, Shinichi and Koike, Hideki},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PianoSyncAR: Enhancing Piano Learning through Visualizing Synchronized Hand Pose Discrepancies in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={859-868},
  abstract={Motor skill acquisition involves learning from spatiotemporal discrepancies between target and self-generated motions. However, in dexterous skills with numerous degrees of freedom, understanding and correcting these motor errors are challenging. This issue becomes crucial for experienced individuals who seek for mastering and sophisticating their skills, where even subtle errors need to be minimized. To enable efficient optimization of body posture in piano learning, we present PianoSyncAR, an augmented reality system that superimposes the time-varying complex hand postures of a teacher over the hand of a learner. Through a user study with 12 pianists, we demonstrate several advantages of the proposed system over conventional tablet-screen, which implicate the potential of AR training as a complementary tool for video-based skill learning in piano playing.},
  keywords={Training;Visualization;Three-dimensional displays;Motion capture;Spatiotemporal phenomena;Synchronization;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Mixed / augmented reality},
  doi={10.1109/ISMAR59233.2023.00101},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316496,
  author={Han, Dongyun and Kim, Donghoon and Kim, Kangsoo and Cho, Isaac},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating Psychological Ownership in a Shared AR Space: Effects of Human and Object Reality and Object Controllability}, 
  year={2023},
  volume={},
  number={},
  pages={869-874},
  abstract={Augmented reality (AR) provides users with a unique social space where virtual objects are natural parts of the real world. The users can interact with 3D virtual objects and virtual humans projected onto the physical environment. This work examines perceived ownership based on the reality of objects and partners, as well as object controllability in a shared AR setting. Our formal user study with 28 participants shows a sense of possession, control, separation, and partner presence affect their perceived ownership of a shared object. Finally, we discuss the findings and present a conclusion.},
  keywords={Three-dimensional displays;Design methodology;Psychology;Aerospace electronics;Controllability;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00102},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316433,
  author={Han, Dongyun and Kim, Donghoon and Kim, Kangsoo and Cho, Isaac},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Effects of VR Activities on Stress Relief: A Comparison of Sitting-in-Silence, VR Meditation, and VR Smash Room}, 
  year={2023},
  volume={},
  number={},
  pages={875-884},
  abstract={In our lives, we encounter various stressors that may cause negative mental and bodily reactions to make us feel frustrated, angry, or irritated. Effective methods to manage or reduce stress and anxiety are essential for a healthy life, and several stress-management approaches are found to be useful for stress relief, such as meditation, taking a rest, walking around nature, or even breaking things in a smash room. Previous research has revealed that certain experiences in virtual reality (VR) are effective for reducing stress as traditional real-world methods. However, it is still unclear how the stress relief effects are associated with other factors like individual user profile in terms of different treatment activities. In this paper, we report our findings from a formal user study that investigates the effects of two virtual activities: (1) VR Meditation and (2) VR Smash Room experience, compared with a traditional Sitting-in-Silence method. Our results show that VR Meditation has a better stress relief effect compared to VR Smash Room and Sitting-in-Silence, and the effects of the treatments are correlated with the participants’ personalities. We discuss the findings and implications addressing potential benefits/impacts of different stress-relief activities in VR.},
  keywords={Legged locomotion;Anxiety disorders;Human factors;Stress;Augmented reality;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00103},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316476,
  author={Kang, Jiho and Yang, Dongseok and Kim, Taehei and Lee, Yewon and Lee, Sung-Hee},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Real-time Retargeting of Deictic Motion to Virtual Avatars for Augmented Reality Telepresence}, 
  year={2023},
  volume={},
  number={},
  pages={885-893},
  abstract={Avatar-mediated augmented reality telepresence aims to enable distant users to collaborate remotely through avatars. When two spaces involved in telepresence are dissimilar, with different object sizes and arrangements, the avatar movement must be adjusted to convey the user’s intention rather than directly following their motion, which poses a significant challenge. In this paper, we propose a novel neural network-based framework for real-time retargeting of users’ deictic motions (pointing at and touching objects) to virtual avatars in dissimilar environments. Our framework translates the user’s deictic motion, acquired from a sparse set of tracking signals, to the virtual avatar’s deictic motion for a corresponding remote object in real-time. One of the main features of our framework is that a single trained network can generate natural deictic motions for various sizes of users. To this end, our network includes two sub-networks: AngleNet and MotionNet. AngleNet maps the angular state of the user’s motion into a latent representation, which is subsequently converted by MotionNet into the avatar’s pose, considering the user’s scale. We validate the effectiveness of our method in terms of deictic intention preservation and movement naturalness through quantitative comparison with alternative approaches. Additionally, we demonstrate the utility of our approach through several AR telepresence scenarios.},
  keywords={Telepresence;Tracking;Avatars;Real-time systems;Augmented reality;Computing methodologies;Computer graphics;Animation;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR59233.2023.00104},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316511,
  author={Brübach, Larissa and Westermeier, Franziska and Wienrich, Carolin and Latoschik, Marc Erich},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Systematic Evaluation of Incongruencies and Their Influence on Plausibility in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={894-901},
  abstract={Currently, there is an ongoing debate about the influencing factors of one’s extended reality (XR) experience. Plausibility, congruence, and their role have recently gained more and more attention. One of the latest models to describe XR experiences, the Congruence and Plausibility model (CaP), puts plausibility and congruence right in the center. However, it is unclear what influence they have on the overall XR experience and what influences our perceived plausibility rating. In this paper, we implemented four different incongruencies within a virtual reality scene using breaks in plausibility as an analogy to breaks in presence. These manipulations were either located on the cognitive or perceptual layer of the CaP model. They were also either connected to the task at hand or not. We tested these manipulations in a virtual bowling environment to see which influence they had. Our results show that manipulations connected to the task caused a lower perceived plausibility. Additionally, cognitive manipulations seem to have a larger influence than perceptual manipulations. We were able to cause a break in plausibility with one of our incongruencies. These results show a first direction on how the influence of plausibility in XR can be systematically investigated in the future.},
  keywords={Solid modeling;Systematics;Extended reality;Computational modeling;X reality;Task analysis;plausibility;congruence;XR;evaluation;experience;Human-centered computing;Empirical studies in HCI;Virtual reality;HCI theory;concepts and models},
  doi={10.1109/ISMAR59233.2023.00105},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316352,
  author={Nakano, Kizashi and Perusquia-Hernandez, Monica and Isoyama, Naoya and Uchiyama, Hideaki and Kiyokawa, Kiyoshi},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of Visual Presentation Near the Mouth on Cross-Modal Effects of Multisensory Flavor Perception and Ease of Eating}, 
  year={2023},
  volume={},
  number={},
  pages={902-911},
  abstract={Various studies have suggested that altering the appearance of food can impact multisensory flavor perception. The cross-modal effect of such visual changes on gustation may allow for the presentation of food tastes that are difficult to express with simple combinations of taste stimuli. This cross-modal effect of visual changes on gustation holds potential for applications in gustatory displays. However, the current limitation of existing Head-Mounted Displays (HMDs) is their restricted vertical Field of View (FoV), which prohibits the display of images near the mouth while eating. This limitation may impede the cross-modal effect of visual changes on multisensory flavor perception. Additionally, the lack of visibility around the mouth area challenges the ease of eating. To address these issues, we design a Video See-Through (VST)-HMD with an expanded vertical FoV (approx. 100 [deg]). Using the HMD, we investigated how presenting visual information near the mouth affects the cross-modal effects of flavor perception and ease of eating. In our experiment, machine learning techniques were utilized to alter the appearance of food. However, the result showed no significant differences in the amount of cross-modal effects or the ease of eating between the groups with and without visual information near the mouth. As a discussion of this result, the participants may not direct their visual attention to the food when they put the food in their mouths. The experiment also examined whether visual changes alter the taste as well as the smell and texture of the food. The findings demonstrated that visual changes could present the smell and texture of the food following the modifications. This result was confirmed irrespective of the visibility near the mouth.},
  keywords={Visualization;Head-mounted displays;Mouth;Modulation;Resists;Machine learning;Gaze tracking;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR59233.2023.00106},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316408,
  author={Vargas, Mauricio Flores and Fribourg, Rebecca and Bates, Enda and McDonnell, Rachel},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Now I Wanna Be a Dog: Exploring the Impact of Audio and Tactile Feedback on Animal Embodiment}, 
  year={2023},
  volume={},
  number={},
  pages={912-921},
  abstract={Embodying a virtual creature or animal in Virtual Reality (VR) is becoming common, and can have numerous beneficial impacts. For instance, it can help actors improve their performance of a computer-generated creature, or it can endow the user with empathy towards threatened animal species. However, users must feel a sense of embodiment towards their virtual representation, commonly achieved by providing congruent sensory feedback. Providing effective visuo-motor feedback in dysmorphic bodies can be challenging due to human-animal morphology differences. Thus, the purpose of this study was to experiment with the inclusion of audio and audio-tactile feedback to begin unveiling their influence towards animal avatar embodiment. Two experiments were conducted to examine the effects of different sensory feedback on participants’ embodiment in a dog avatar in an Immersive Virtual Environment (IVE). The first experiment (n= 24) included audio, tactile, audio-tactile, and baseline conditions. The second experiment (n= 34) involved audio and baseline conditions only.},
  keywords={Avatars;Virtual environments;Tactile sensors;Morphology;Dogs;Skin;Augmented reality;Virtual Reality;Embodiment;Audio Feedback;Tactile Feedback;Perception;Human-centered computing;Interaction design;Epirical studies in interaction design;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00107},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316438,
  author={Rupp, Daniel and Kuhlen, Torsten and Weissker, Tim},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={TENETvr: Comprehensible Temporal Teleportation in Time-Varying Virtual Environments}, 
  year={2023},
  volume={},
  number={},
  pages={922-929},
  abstract={The iterative design process of virtual environments commonly generates a history of revisions that each represent the state of the scene at a different point in time. Browsing through these discrete time points by common temporal navigation interfaces like time sliders, however, can be inaccurate and lead to an uncomfortably high number of visual changes in a short time. In this paper, we therefore present a novel technique called TENETvr (Temporal Exploration and Navigation in virtual Environments via Teleportation) that allows for efficient teleportation-based travel to time points in which a particular object of interest changed. Unlike previous systems, we suggest that changes affecting other objects in the same time span should also be mediated before the teleport to improve predictability. We therefore propose visualizations for nine different types of additions, property changes, and deletions. In a formal user study with 20 participants, we confirmed that this addition leads to significantly more efficient change detection, lower task loads, and higher usability ratings, therefore reducing temporal disorientation.},
  keywords={Visualization;Navigation;Design methodology;Virtual environments;Teleportation;Iterative methods;History;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/ISMAR59233.2023.00108},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316452,
  author={Nelson, Michael G. and Koilias, Alexandros and Kao, Dominic and Mousas, Christos},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of Speed of a Collocated Virtual Walker and Proximity Toward a Static Virtual Character on Avoidance Movement Behavior}, 
  year={2023},
  volume={},
  number={},
  pages={930-939},
  abstract={We explored the avoidance movement behaviors of study participants immersed in a virtual reality environment. We placed a static virtual character at the midpoint between the start and target spot for the avoidance task, and a virtual walker character in front of the starting spot and scripted it to reach the target spot. Participants were placed behind the virtual walker in order to measure its influence on participants’ behavior. We developed nine experimental conditions assigned to the virtual walker character by following a 3 (speed: slow vs. normal vs. fast walking speed) $\times 3$ (proximity: close vs. middle vs. far proximity to the static virtual character) study design. For this within-group study, we collected data from 22 study participants to explore how speed and proximity walking patterns assigned to a virtual walker character could impact participants’ avoidance movement behaviors and decisions. Our data revealed that 1) the speed factor impacted the participants’ avoidance movement behavior; 2) the proximity factor did not significantly impact the participants’ avoidance movement behavior; 3) the virtual walker character did not significantly impact participants’ avoidance decisions regarding the static virtual character; 4) in all examined conditions, the side-by-side distances between the participants and the static virtual character were inside the social space according to the proxemics model; and 5) in conditions in which a slow virtual walker character was present or in the condition of normal speed and far proximity, we observed an increased number of participants pass the virtual walker character.},
  keywords={Legged locomotion;Solid modeling;Atmospheric measurements;Particle measurements;Data models;Behavioral sciences;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00109},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316418,
  author={Zhang, Jingyi and Brandstätter, Klara and Steed, Anthony},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Supporting Co-Presence in Populated Virtual Environments by Actor Takeover of Animated Characters}, 
  year={2023},
  volume={},
  number={},
  pages={940-949},
  abstract={Online social virtual worlds are now becoming widely available on consumer devices including virtual reality headsets. One goal of a virtual world could be to give a user an experience of a crowded environment with many virtual humans. However, gathering enough personnel to control the necessary number of avatars for creating a realistic scene is usually difficult. Additionally, current technology is not capable of fully simulating avatars with behaviours, especially when interaction with users is required. In this paper, we develop a system that enables an actor to take over control of one of a set of avatars. We built an immersive interface that allows an actor to select an avatar to take over and then segue into the currently playing animation. By allowing one person to take control of multiple avatars, we can enhance the plausibility of environments inhabited by simulated characters. In an experiment, we show that in a cafe scenario, one actor can take over the roles of a barista and two customers. Experiment participants reported experiencing the scene as if it were populated by more than one actor. This system and experiment demonstrate the feasibility of one actor controlling multiple avatars sequentially, thus enhancing users’ feelings of being in a populated environment.},
  keywords={Headphones;Avatars;Virtual environments;Animation;Personnel;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00110},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316451,
  author={Song, Xiuqiang and Xie, Weijian and Li, Jiachen and Wang, Nan and Zhong, Fan and Zhang, Guofeng and Qin, Xueying},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Minilag Filter for Jitter Elimination of Pose Trajectory in AR Environment}, 
  year={2023},
  volume={},
  number={},
  pages={950-959},
  abstract={In AR applications, the jitter of virtual objects can weaken the sense of integration with the real environment. This jitter is often caused by noise in the pose obtained by 3D tracking or localization methods, especially in monocular vision systems without IMU support. Filtering the pose is an effective method to eliminate jitter, however, it can also cause significant lag in the filtered pose, seriously degrading the AR experience. Existing filters struggle to simultaneously reduce jitter while maintaining low lag. In this paper, we propose a novel Minilag filter, which achieves excellent pose smoothing while significantly reducing the lag through backtracking update and compensation strategies, and has excellent real-time performance. We represent the rotation in the pose in the Lie algebra and filter it in locally Euclidean space, ensuring that the filtering of rotation is consistent with that of vectors. We also analyze the noise distribution and characteristics in the tracked pose, providing a theoretical basis for setting filter parameters. We evaluated the proposed filter using both objective mathematical metrics and a user study, and the experimental results demonstrate that our method achieves state-of-the-art performance.},
  keywords={Measurement;Backtracking;Three-dimensional displays;Smoothing methods;Filtering;Jitter;Filtering algorithms;Human-centered computing;Visualization;Mathematics of Computing;Mathematical analysis},
  doi={10.1109/ISMAR59233.2023.00111},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316353,
  author={Phadnis, Vrushank and Moore, Kristin and Gonzalez-Franco, Mar},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Work Avatar Face-Off: Knowledge Worker Preferences for Realism in Meetings}, 
  year={2023},
  volume={},
  number={},
  pages={960-969},
  abstract={While avatars have grown in popularity in social settings, their use in the workplace is still debatable. We conducted a large-scale survey to evaluate knowledge worker sentiment towards avatars, particularly the effects of realism on their acceptability for work meetings. Our survey of 2509 knowledge workers from multiple countries rated five avatar styles for use by managers, known colleagues and unknown colleagues. In all scenarios, participants favored higher realism, but fully realistic avatars were sometimes perceived as uncanny. Less realistic avatars were rated worse when interacting with an unknown colleague or manager, as compared to a known colleague. Avatar acceptability varied by country, with participants from the United States and South Korea rating avatars more favorably. We supplemented our quantitative findings with a thematic analysis of open-ended responses to provide a comprehensive understanding of factors influencing work avatar choices. In conclusion, our results show that realism had a significant positive correlation with acceptability. Non-realistic avatars were seen as fun and playful, but only suitable for occasional use.},
  keywords={Surveys;Correlation;Avatars;Conferences;Employment;Organizations;Market research;Avatars;Realism;Virtual environment;Remote meeting;Survey;Knowledge work},
  doi={10.1109/ISMAR59233.2023.00112},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316425,
  author={Wimmer, Michael and Weidinger, Nicole and ElSayed, Neven and Müller-Putz, Gernot R. and Veas, Eduardo},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={EEG-Based Error Detection Can Challenge Human Reaction Time in a VR Navigation Task}, 
  year={2023},
  volume={},
  number={},
  pages={970-979},
  abstract={Error perception is known to elicit distinct brain patterns, which can be used to improve the usability of systems facilitating human-computer interactions, such as brain-computer interfaces. This requires a high-accuracy detection of erroneous events, e.g., misinterpretations of the user’s intention by the interface, to allow for suitable reactions of the system. In this work, we concentrate on steering-based navigation tasks. We present a combined electroencephalography-virtual reality (VR) study investigating different approaches for error detection and simultaneously exploring the corrective human behavior to erroneous events in a VR flight simulation. We could classify different errors allowing us to analyze neural signatures of unexpected changes in the VR. Moreover, the presented models could detect errors faster than participants naturally responded to them. This work could contribute to developing adaptive VR applications that exclusively rely on the user’s physiological information.},
  keywords={Human computer interaction;Visualization;Navigation;Brain modeling;User experience;Real-time systems;Physiology;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;HCI design and evaluation methods;User models},
  doi={10.1109/ISMAR59233.2023.00113},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316488,
  author={Allgaier, Mareen and Huettl, Florentine and Hanke, Laura Isabel and Lang, Hauke and Huber, Tobias and Preim, Bernhard and Saalfeld, Sylvia and Hansen, Christian},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={LiVRSono - Virtual Reality Training with Haptics for Intraoperative Ultrasound}, 
  year={2023},
  volume={},
  number={},
  pages={980-989},
  abstract={One of the biggest challenges in using ultrasound (US) is learning to create a spatial mental model of the interior of the scanned object based on the US image and the probe position. As intraoperative ultrasound (IOUS) cannot be easily trained on patients, we present LiVRSono, an immersive VR application to train this skill. The immersive environment, including an US simulation with patientspecific data as well as haptics to support hand-eye coordination, provides a realistic setting. Four clinically relevant training scenarios were identified based on the described learning goal and the workflow of IOUS for liver. The realism of the setting and the training scenarios were evaluated with eleven physicians, of which six participants are experts in IOUS for liver and five participants are potential users of the training system. The setting, handling of the US probe, and US image were considered realistic enough for the learning goal. Regarding the haptic feedback, a limitation is the restricted workspace of the input device. Three of the four training scenarios were rated as meaningful and effective. A pilot study regarding learning outcome shows positive results, especially with respect to confidence and perceived competence. Besides the drawbacks of the input device, our training system provides a realistic learning environment with meaningful scenarios to train the creation of a mental 3D model when performing IOUS. We also identified important improvements to the training scenarios to further enhance the training experience.},
  keywords={Training;Solid modeling;Ultrasonic imaging;Three-dimensional displays;Input devices;Liver;Surgery;Applied computing;Life and medical science;Health informatics;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Humancentered computing;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR59233.2023.00114},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316464,
  author={Kim, Donghoon and Han, Dongyun and Cho, Isaac},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparative Analysis of Change Blindness in Virtual Reality and Augmented Reality Environments}, 
  year={2023},
  volume={},
  number={},
  pages={990-998},
  abstract={Change blindness is a phenomenon where an individual fails to notice alterations in a visual scene when a change occurs during a brief interruption or distraction. Understanding this phenomenon is specifically important for the technique that uses a visual stimulus, such as Virtual Reality (VR) or Augmented Reality (AR). Previous research had primarily focused on 2D environments or conducted limited controlled experiments in 3D immersive environments. In this paper, we design and conduct two formal user experiments to investigate the effects of different visual attention-disrupting conditions (Flickering and Head-Turning) and object alternative conditions (Removal, Color Alteration, and Size Alteration) on change blindness detection in VR and AR environments. Our results reveal that participants detected changes more quickly and had a higher detection rate with Flickering compared to Head-Turning. Furthermore, they spent less time detecting changes when an object disappeared compared to changes in color or size. Additionally, we provide a comparison of the results between VR and AR environments.},
  keywords={Visualization;Three-dimensional displays;Design methodology;Blindness;Color;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00115},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316444,
  author={Monteiro, João and Mendes, Daniel and Rodrigues, Rui},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={TouchRay: Towards Low-effort Object Selection at Any Distance in DeskVR}, 
  year={2023},
  volume={},
  number={},
  pages={999-1005},
  abstract={DeskVR allows users to experience Virtual Reality (VR) while sitting at a desk without requiring extensive movements. This makes it better suited for professional work environments where productivity over extended periods is essential. However, tasks that typically resort to mid-air gestures might not be suitable for DeskVR. In this paper, we focus on the fundamental task of object selection. We present TouchRay, an object selection technique conceived specifically for DeskVR that enables users to select objects at any distance while resting their hands on the desk. It also allows selecting objects’ sub-components by traversing their corresponding hierarchical trees. We conducted a user evaluation comparing TouchRay against state-of-the-art techniques targeted at traditional VR. Results revealed that participants could successfully select objects in different settings, with consistent times and on par with the baseline techniques in complex tasks, without requiring mid-air gestures.},
  keywords={Productivity;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00116},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316423,
  author={Lin, Tica and Lafreniere, Ben and Xu, Yan and Grossman, Tovi and Wigdor, Daniel and Glueck, Michael},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={XR Input Error Mediation for Hand-Based Input: Task and Context Influences a User’s Preference}, 
  year={2023},
  volume={},
  number={},
  pages={1006-1015},
  abstract={Many XR devices use bare-hand gestures to reduce the need for handheld controllers. Such gestures, however, lead to false positive and false negative recognition errors, which detract from the user experience. While mediation techniques enable users to overcome recognition errors by clarifying their intentions via UI elements, little research has explored how mediation techniques should be designed in XR and how a user’s task and context may impact their design preferences. This research presents empirical studies about the impact of user perceived error costs on users’ preferences for three mediation technique designs, under different simulated scenarios that were inspired by real-life tasks. Based on a large-scale crowd-sourced survey and an immersive VR-based user study, our results suggest that the varying contexts within each task type can impact users’ perceived error costs, leading to different preferred mediation techniques. We further discuss the study implications of these results on future XR interaction design.},
  keywords={Surveys;Human computer interaction;Costs;User experience;Mediation;X reality;Task analysis;Human-centered computing;Human computer interaction;Interaction paradigms;Mixed / augmented reality;Interaction techniques;Gestural input;Empirical studies in HCI},
  doi={10.1109/ISMAR59233.2023.00117},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316474,
  author={Nelson, Cassidy R. and Gabbard, Joseph L.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmented Reality Rehabilitative and Exercise Games (ARREGs): A Systematic Review and Future Considerations}, 
  year={2023},
  volume={},
  number={},
  pages={1016-1025},
  abstract={Augmented Reality (AR) and exergames have been trending areas of interest in healthcare spaces for rehabilitation and exercise. This work reviews 25 papers featuring AR rehabilitative/exercise games and paints a picture of the literature landscape. The included results span twelve years, with the oldest paper published in 2010 and the most recent work in 2022. More specifically, this work contributes a bank of representative ARREGs and a synthesis of measurement strategies for player perceptions of Augmented Reality Rehabilitative and Exercise Game (ARREG) experiences, the elements that comprise the exergame experience, the intended use cases of ARREGs, whether participants are actually representative users, the utilized devices and AR modalities, the measures used to capture rehabilitative success, and the measures used to capture participant perceptions. Informed by the literature body, our most significant contribution is nine considerations for future ARREG development.},
  keywords={Systematics;Atmospheric measurements;Sociology;Games;Resists;Gain measurement;Particle measurements;Augmented Reality;Exergame;Rehabilitation;Serious Game;Systematic Literature Review;• General and reference;Document types;Surveys and overviews;• Human-centered computing;Human computer interaction (HCI);• Social and professional topics;User characteristics},
  doi={10.1109/ISMAR59233.2023.00118},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316482,
  author={Ogawa, Maki and Matsumoto, Keigo and Aoyama, Kazuma and Narumi, Takuji},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Expansion of Detection Thresholds for Hand Redirection using Noisy Tendon Electrical Stimulation}, 
  year={2023},
  volume={},
  number={},
  pages={1026-1035},
  abstract={To increase the flexibility of haptic feedback in virtual reality (VR), hand redirection (HR) has been proposed to shift the hand’s virtual position from its actual position. To expand the range of HR applications, a method to broaden the detection threshold (DT), which is the maximum amount of shift that can be applied without the user noticing, is required. Multisensory integration studies have revealed that the reliability of senses affects the weight of integration. To expand the DTs of HR, we propose a method to increase visual dominance in the integration of vision and proprioception by introducing noise to the latter, thereby decreasing its reliability through weak Gaussian white noise electrical stimulation ($\sigma$=0.5mA). The results of a user study comprising 22 participants (11 women and 11 men) confirm that noisy electrical stimulation significantly expands the DTs of HR with the mean range of DTs ($R_{DT}$) was 20.48° (SD =7.90) with electrical stimulation and 19.15° (SD =7.11) without electrical stimulation. Interestingly, this effect was only observed in women. The average $R_{DT}$ for men was 15.36° (SD =6.13) and 15.18°(SD=5.58), whereas that for women was 25. 61°(SD=5.89) and 23.12°(SD=6.21), with and without electrical stimulation, respectively. Electrical stimulation was mostly tolerable for the participants and did not affect embodiment or presence ratings. These results suggest that expansion of the DT without disturbing the user’s VR experience is feasible.},
  keywords={Visualization;Electrical stimulation;White noise;User experience;Multisensory integration;Haptic interfaces;Noise measurement;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR59233.2023.00119},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316361,
  author={Park, Jaejun and Han, Sangyoon and Choi, Seungmoon},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Merging Camera and Object Haptic Motion Effects for Improved 4D Experiences}, 
  year={2023},
  volume={},
  number={},
  pages={1036-1044},
  abstract={(Haptic) motion effects refer to the vestibular stimuli generated by a motion platform and delivered to the whole body of a user sitting on the platform. Motion effects are an essential tool for creating vivid sensory experiences in various extended reality (XR) applications, ranging from training simulators to recent 4D rides, films, and games for entertainment. For the latter purpose, motion effects emphasize audiovisual events occurring in the scene, such as camera motion, the movement of an object of interest, and special sounds. Recent research developed several algorithms to produce motion effects from the audiovisual stream automatically. However, these algorithms are designed for a single class of motion effects, and extension to multiple motion effect classes remains unexplored. In this paper, we propose an algorithmic framework that merges camera and object motion effects into one motion effect while preserving the perceptual consequences of the two effects. We validate the framework’s perceptual performance through a user study. To our knowledge, this work is one of the first successful reports of merging different kinds of motion effects for improved XR experiences.},
  keywords={Training;Extended reality;Films;Merging;Entertainment industry;Games;Cameras;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR59233.2023.00120},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316455,
  author={Wu, Renjie and Chen, Hsiang-Ting},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effect of Visual and Auditory Modality Mismatching between Distraction and Warning on Pedestrian Street Crossing Behavior}, 
  year={2023},
  volume={},
  number={},
  pages={1045-1054},
  abstract={Augmented reality (AR) headsets could provide useful information to users, but they may also be a source of distraction. Previous works have explored using AR to enhance pedestrian safety by providing real-time warnings, but there has been little research on the impact of modality matching between distractions and warnings on pedestrian street crossing behaviour. We conducted a VR experiment using a within-subjects 2-by-2 design (N=24) with four conditions: (auditory distraction, visual distraction) $\times$ (auditory warning, visual warning). When experienced conditions with mismatched modalities, participants exhibited more cautious street crossing behaviours, such as reduced walking speed, and increased scan range after receiving the warning, and significantly faster reaction times to the incoming vehicle. The participants also expressed a preference for warnings to be presented in a modality different from the distraction. Our findings suggest that in the context of utilizing AR for pedestrian road safety, future AR interfaces should incorporate a warning modality that differs from the one causing distraction.},
  keywords={Legged locomotion;Headphones;Visualization;Pedestrians;Design methodology;Road safety;Real-time systems;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00121},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316492,
  author={Ceyssens, Jeroen and van Deurzen, Bram and Ruiz, Gustavo Rovelo and Luyten, Kris and Di Fiore, Fabian},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AR Guidance Design for Line Tracing Speed Control}, 
  year={2023},
  volume={},
  number={},
  pages={1055-1063},
  abstract={In many jobs, workers execute precise line tracing tasks; welding, spray painting, or chiseling, for example. Training and support for such tasks can be done using VR and AR. However, to enable workers to achieve the required precision in movement and timing, the effect of visual guidance on continuous movement needs to be explored. In VR environments, we want to ensure people are trained so that the obtained skill is transferable to a real-world context, whereas, in AR, we want to ensure an ongoing task can be completed successfully when adding visual guidance. To simulate these various contexts, we employ a VR environment to investigate the effectiveness of different visualizations for motion-based guidance in a line tracing task. We tested five different visualizations, including faster and slower arrows on the pen, the same arrows on the line, a dynamic graph on the pen or line, and a ghost object to follow. Each visualization was tested with the same set of five lines of different target speeds (2cm/s to 10 cm/s in steps of 2 cm/s) with a training line of 5 cm/s. Our results show that the example ghost on the line turns out to be the most efficient visualization for allowing users to achieve a specific speed. Users also perceived this visualization as the most engaging and easy to use. These findings have significant implications for the development of AR-based guidance systems, specifically in the realm of speed control, across diverse domains such as industrial applications, training, and entertainment.},
  keywords={Training;Visualization;Welding;Design methodology;Velocity control;Dynamics;Entertainment industry;Human-centered computing;Visualization;Empirical studies in visualization;Visualization design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00122},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316506,
  author={Lunding, Rasmus S. and Lystbæk, Mathias N. and Feuchtner, Tiare and Grønbæk, Kaj},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AR-supported Human-Robot Collaboration: Facilitating Workspace Awareness and Parallelized Assembly Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1064-1073},
  abstract={While technologies for human-robot collaboration are rapidly advancing, plenty of aspects still need further investigation, such as ensuring workspace awareness, enabling the operator to reschedule tasks on the fly, and how users prefer to coordinate and collaborate with robots. To address these, we propose an Augmented Reality interface that supports human-robot collaboration in an assembly task by (1) enabling the inspection of planned and ongoing robot processes through dynamic task lists and a path visualization, (2) allowing the operator to also delegate tasks to the robot, and (3) presenting step-by-step assembly instructions. We evaluate our AR interface in comparison to a state-of-the-art tablet interface in a user study, where participants collaborated with a robot arm in a shared workspace to complete an assembly task. Our findings confirm the feasibility and potential of AR-assisted human-robot collaboration, while pointing to some central challenges that require further work.},
  keywords={Performance evaluation;Visualization;Robot kinematics;Buildings;Collaboration;Touch sensitive screens;Resource management;Human-centered computing;Human computer interaction (HCI);Mixed / augmented reality;User studies;Computer systems organization;External interfaces for robotics},
  doi={10.1109/ISMAR59233.2023.00123},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316531,
  author={Tolchinsky, Mark and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Empirical Evaluation of the Effects of Visuo-Auditory Perceptual Information on Head Oriented Tracking of Dynamic Objects in VR}, 
  year={2023},
  volume={},
  number={},
  pages={1074-1083},
  abstract={As virtual reality (VR) technology sees more use in various fields, there is a greater need to understand how to effectively design dynamic virtual environments. As of now, there is still uncertainty in how well users of a VR system are capable of tracking moving targets in a virtual space. In this work, we examined the influence of sensory modality and visual feedback on the accuracy of head-gaze moving target tracking. To this end, a between subjects study was conducted wherein participants would receive targets that were visual, auditory, or audiovisual. Each participant performed two blocks of experimental trials, with a calibration block in between. Results indicate that audiovisual targets promoted greater improvement in tracking performance over single-modality targets, and that audio-only targets are more difficult to track than those of other modalities.},
  keywords={Visualization;Target tracking;Uncertainty;Virtual environments;Calibration;Augmented reality;perception-action;user studies;head related tracking;perceptuo-motor calibration},
  doi={10.1109/ISMAR59233.2023.00124},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316497,
  author={Lisle, Lee and Davidson, Kylie and Pavanatto, Leonardo and Tahmid, Ibrahim A. and North, Chris and Bowman, Doug A.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Spaces to Think: A Comparison of Small, Large, and Immersive Displays for the Sensemaking Process}, 
  year={2023},
  volume={},
  number={},
  pages={1084-1093},
  abstract={Analysts need to process large amounts of data in order to extract concepts, themes, and plans of action based upon their findings. Different display technologies offer varying levels of space and interaction methods that change the way users can process data using them. In a comparative study, we investigated how the use of single traditional monitor, a large, high-resolution two-dimensional monitor, and immersive three-dimensional space using the Immersive Space to Think approach impact the sensemaking process. We found that user satisfaction grows and frustration decreases as available space increases. We observed specific strategies users employ in the various conditions to assist with the processing of datasets. We also found an increased usage of spatial memory as space increased, which increases performance in artifact position recall tasks. In future systems supporting sensemaking, we recommend using display technologies that provide users with large amounts of space to organize information and analysis artifacts.},
  keywords={Design methodology;Data mining;Task analysis;Monitoring;Augmented reality;Human-centered computing;Visualization;Visualization-techniques;Visualization design and evaluation methods Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR59233.2023.00125},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316471,
  author={Davidson, Kylie and Lisle, Lee and Tahmid, Ibrahim A. and Whitley, Kirsten and North, Chris and Bowman, Doug A.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Uncovering Best Practices in Immersive Space to Think}, 
  year={2023},
  volume={},
  number={},
  pages={1094-1103},
  abstract={As immersive analytics research becomes more popular, user studies have been aimed at evaluating the strategies and layouts of users’ sensemaking during a single focused analysis task. However, approaches to sensemaking strategies and layouts are likely to change as users become more familiar/proficient with the immersive analytics tool. In our work, we build upon an existing immersive analytics approach-Immersive Space to Think-to understand how schemas and strategies for sensemaking change across multiple analysis tasks. We conducted a user study with 14 participants who completed three different sensemaking tasks during three separate sessions. We found significant differences in the use of space and strategies for sensemaking across these sessions and correlations between participants’ strategies and the quality of their sensemaking. Using these findings, we propose guidelines for effective analysis approaches within immersive analytics systems for document-based sensemaking.},
  keywords={Visualization;Correlation;Layout;Task analysis;Best practices;Augmented reality;Guidelines;Human-centered computing;Visualization;Human computer interaction (HCI);Virtual reality Human-centered computing;Information visualization Human-centered computing;Sensemaking},
  doi={10.1109/ISMAR59233.2023.00126},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316459,
  author={Lisboa, Thállys and Macêdo, Horácio and Porcino, Thiago and Oliveira, Eder and Trevisan, Daniela and Clua, Esteban},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Is Foveated Rendering Perception Affected by Users’ Motion?}, 
  year={2023},
  volume={},
  number={},
  pages={1104-1112},
  abstract={Virtual reality (VR) is gaining increasing popularity across various domains, but the current state of technology imposes limitations on the level of realism and complexity achievable in computer graphics when displayed through VR head-mounted devices (HMDs). To improve the user experience in HMDs, optimization techniques are needed to enhance performance without sacrificing quality. One such technique is Foveated Rendering (FR), which leverages the human visual system to optimize resource usage. FR degrades the image quality at the periphery of the human vision, where visual acuity is lower, to save resources. This paper aims to investigate if the perception of the peripheral area is affected whenever users are in movement in a VR environment. Our findings show a significant correlation between speed movement and Foveated rendering parameters in both scenarios. The least amount of degradation was observed in the idle state and the most in the high-speed state, indicating that users perceive less degradation at higher speeds. These results are particularly relevant for path-tracing-based algorithms, due to the possibility of reducing the number of rays required for the rendering whenever there is movement.},
  keywords={Degradation;Performance evaluation;Visualization;Correlation;Visual systems;Rendering (computer graphics);Retina;Human-centered computing;Virtual reality;Games;Computing methodologies;Rendering;Foveated rendering;Perception-based rendering},
  doi={10.1109/ISMAR59233.2023.00127},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316490,
  author={Zhang, Yuchong and Nowak, Adam and Xuan, Yueming and Romanowski, Andrzej and Fjeld, Morten},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={See or Hear? Exploring the Effect of Visual/Audio Hints and Gaze-assisted Instant Post-task Feedback for Visual Search Tasks in AR}, 
  year={2023},
  volume={},
  number={},
  pages={1113-1122},
  abstract={Augmented reality (AR) is emerging in visual search tasks for increasingly immersive interactions with virtual objects. We propose an AR approach providing visual and audio hints along with gaze-assisted instant post-task feedback for search tasks based on mobile head-mounted display (HMD). The target case was a book-searching task, in which we aimed to explore the effect of the hints together with the task feedback with two hypotheses. H1: Since visual and audio hints can positively affect AR search tasks, the combination outperforms the individuals. H2: The gaze-assisted instant post-task feedback can positively affect AR search tasks. The proof-of-concept was demonstrated by an AR app in HMD and a comprehensive user study (n=96) consisting of two sub-studies, Study I (n=48) without task feedback and Study II (n=48) with task feedback. Following quantitative and qualitative analysis, our results partially verified H1 and completely verified H2, enabling us to conclude that the synthesis of visual and audio hints conditionally improves the AR visual search task efficiency when coupled with task feedback.},
  keywords={Visualization;Head-mounted displays;Resists;Search problems;Task analysis;Augmented reality;Augmented reality;Search task;Visual hint;Audio hint;Gaze assistance;Instant post-task feedback;User study},
  doi={10.1109/ISMAR59233.2023.00128},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316463,
  author={Reiske, Gunnar and In, Sungwon and Yang, Yalong},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Multi-Focus Querying of the Human Genome Information on Desktop and in Virtual Reality: an Evaluation}, 
  year={2023},
  volume={},
  number={},
  pages={1123-1131},
  abstract={The human genome is incredibly information-rich, consisting of approximately 25,000 protein-coding genes spread out over 3.2 billion nucleotide base pairs contained within 24 unique chromosomes. The genome is important in maintaining spatial context, which assists in understanding gene interactions and relationships. However, existing methods of genome visualization that utilize spatial awareness are inefficient and prone to limitations in presenting gene information and spatial context. This study proposed an innovative approach to genome visualization and exploration utilizing virtual reality. To determine the optimal placement of gene information and evaluate its essentiality in a VR environment, we implemented and conducted a user study with three different interaction methods. Two interaction methods were developed in virtual reality to determine if gene information is better suited to be embedded within the chromosome ideogram or separate from the ideogram. The final ideogram interaction method was performed on a desktop and served as a benchmark to evaluate the potential benefits associated with the use of VR. Our study findings reveal a preference for VR, despite longer task completion times. In addition, the placement of gene information within the visualization had a notable impact on the ability of a user to complete tasks. Specifically, gene information embedded within the chromosome ideogram was better suited for single target identification and summarization tasks, while separating gene information from the ideogram better supported region comparison tasks.},
  keywords={Proteins;Visualization;Genomics;Benchmark testing;Bioinformatics;Task analysis;Biological cells;Human–centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00129},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316407,
  author={Li, Ruichen and Wang, Yuyang and Yin, Handi and Chardonnet, Jean-Rémy and Hui, Pan},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Deep Cybersickness Predictor through Kinematic Data with Encoded Physiological Representation}, 
  year={2023},
  volume={},
  number={},
  pages={1132-1141},
  abstract={Users would experience individually different sickness symptoms during or after navigating through an immersive virtual environment, generally known as cybersickness. Previous studies have predicted the severity of cybersickness based on physiological and/or kinematic data. However, compared with kinematic data, physiological data rely heavily on biosensors during the collection, which is inconvenient and limited to a few affordable VR devices. In this work, we proposed a deep neural network to predict cybersickness through kinematic data. We introduced the encoded physiological representation to characterize the individual susceptibility; therefore, the predictor could predict cybersickness only based on a user’s kinematic data without counting on biosensors. Fifty-three participants were recruited to attend the user study to collect multimodal data, including kinematic data (navigation speed, head tracking), physiological signals (e.g., electrodermal activity, heart rate), and Simulator Sickness Questionnaire (SSQ). The predictor achieved an accuracy of 97.8% for cybersickness prediction by involving the pre-computed physiological representation to characterize individual differences, providing much convenience for the current cybersickness measurement.},
  keywords={Adaptation models;Cybersickness;Navigation;Virtual environments;Kinematics;Predictive models;Physiology;Cybersickness Prediction;VR;Kinematic data;Physiological Representation;Deep Neural Classifiers},
  doi={10.1109/ISMAR59233.2023.00130},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316532,
  author={Gu, Kai and Maugey, Thomas and Knorr, Sebastian and Guillemot, Christine},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Vanishing Point Aided Hash-Frequency Encoding for Neural Radiance Fields (NeRF) from Sparse 360°Input}, 
  year={2023},
  volume={},
  number={},
  pages={1142-1151},
  abstract={Neural Radiance Fields (NeRF) enable novel view synthesis of 3D scenes when trained with a set of 2D images. One of the key components of NeRF is the input encoding, i.e. mapping the coordinates to higher dimensions to learn high-frequency details, which has been proven to increase the quality. Among various input mappings, hash encoding is gaining increasing attention for its efficiency. However, its performance on sparse inputs is limited. To address this limitation, we propose a new input encoding scheme that improves hash-based NeRF for sparse inputs, i.e. few and distant cameras, specifically for 360° view synthesis. In this paper, we combine frequency encoding and hash encoding and show that this combination can increase dramatically the quality of hash-based NeRF for sparse inputs. Additionally, we explore scene geometry by estimating vanishing points in omnidirectional images (ODI) of indoor and city scenes in order to align frequency encoding with scene structures. We demonstrate that our vanishing point-aided scene alignment further improves deterministic and non-deterministic encodings on image regression and NeRF tasks where sharper textures and more accurate geometry of scene structures can be reconstructed.},
  keywords={Geometry;Image coding;Three-dimensional displays;Urban areas;Image capture;Feature extraction;Encoding;Computing methodologies;Artificial intelligence;Computer vision;3D imaging},
  doi={10.1109/ISMAR59233.2023.00131},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316510,
  author={Farzinnejad, Forouzan and Rasti, Javad and Khezrian, Navid and Grubert, Jens},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effect of an Exergame on the Shadow Play Skill Based on Muscle Memory for Young Female Participants: The Case of Forehand Drive in Table Tennis}, 
  year={2023},
  volume={},
  number={},
  pages={1152-1160},
  abstract={Learning and practicing table tennis with traditional methods is a long, tedious process and may even lead to the internalization of incorrect techniques if not supervised by a coach. To overcome these issues, the presented study proposes an exergame with the aim of enhancing young female novice players’ performance by boosting muscle memory, making practice more interesting, and decreasing the probability of faulty training. Specifically, we propose an exergame based on skeleton tracking and a virtual avatar to support correct shadow practice to learn forehand drive technique without the presence of a coach. We recruited 44 schoolgirls aged between 8 and 12 years without a background in playing table tennis and divided them into control and experimental groups. We examined their stroke skills (via the Mott-Lockhart test) and the error coefficient of their forehand drives (using a ball machine) in the pre-test, post-test, and follow-up tests (10 days after the post-test). Our results showed that the experimental group had progress in the short and long term, while the control group had an improvement only in the short term. Further, the scale of improvement in the experimental group was significantly higher than in the control group. Given that the early stages of learning, particularly in girls children, are important in the internalization of individual skills in would-be athletes, this method could support promoting correct training for young females.},
  keywords={Training;Avatars;Muscles;Aging;Boosting;Skeleton;Augmented reality;Table Tennis;Forehand drive;Shadow play;Body tracking;Exergame;Avatar;Muscle memory;Virtual environment},
  doi={10.1109/ISMAR59233.2023.00132},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316481,
  author={Eichhorn, Christian and Plecher, David A. and Mesmer, Tobias and Leder, Lucas and Simecek, Tim and Boukadida, Nassim and Klinker, Gudrun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Shopping in between Realities-Using an Augmented Virtuality Smartphone in a Virtual Supermarket}, 
  year={2023},
  volume={},
  number={},
  pages={1161-1170},
  abstract={In this project, the full spectrum provided by Milgram’s RealityVirtuality Continuum is utilized to enhance presence, usability, and interactions in an authentic Virtual Reality (VR) supermarket simulation used as a standardized evaluation platform for mHealth apps. We introduce a unique Unity-based modeling platform for supermarket environments and the option to design high-quality customized products. To achieve that, solutions are demonstrated by focusing on a recognizable replica of a discounter and utilizing Augmented Virtuality (AV) to include a physical smartphone in the virtual simulation. The user is able to manipulate the simulation from within the smartphone app through this versatile, highly usability-centered controller. To achieve reliable tracking of the smartphone screen, we propose a hybrid approach, which combines fiducial marker tracking with data acquired through a WiFi connection between the VR system and the smartphone. Furthermore, the AV concept is utilized to build scenarios for Mixed Reality (MR) use cases such as simulated AR to navigate to a chosen product in the market. After an initial pre-study with important insights to strengthen the platform, a broad user study involving the physical smartphone with a simulated AR scenario has been conducted. The goals with 30 participants were to evaluate spatial presence, involvement, experienced realism (IPQ), and usability of the system (SUS). Results showed ”Good” (SUS) usability and recorded data as well as the participants’ feedback brought important insights. We plan to release this unique VR supermarket platform to contribute to the science community and mHealth industry.},
  keywords={Industries;Solid modeling;Augmented virtuality;Navigation;Focusing;Mixed reality;Fiducial markers;Mixed Reality;Virtual Reality;Augmented Virtuality;HCI;Behavioral Change;Virtual Supermarket},
  doi={10.1109/ISMAR59233.2023.00133},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316533,
  author={Schott, Danny and Heinrich, Florian and Stallmeister, Lara and Moritz, Julia and Hensen, Bennet and Hansen, Christian},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Is this the vReal Life? Manipulating Visual Fidelity of Immersive Environments for Medical Task Simulation}, 
  year={2023},
  volume={},
  number={},
  pages={1171-1180},
  abstract={Recent developments and research advances contribute to an ever-increasing trend towards quality levels close to what we experience in reality. In this work, we investigate how different degrees of these quality characteristics affect user performance, qualia of user experience (UX), and sense of presence in an example medical task. To this end, a two-way within-subjects design user study was conducted, in which three different levels of visual fidelity were compared. In addition, two different interaction modalities were considered: (1) the use of conventional VR controllers and (2) natural hand interaction using 3D-printed, spatially-registered replicas of medical devices, to interact with their virtual representations. Consistent results indicate that higher degrees of visual fidelity evoke a higher sense of presence and UX. However, user performance was less affected. Moreover, no differences were detected between both interaction modalities for the examined task. Future work should investigate the discovered interaction effects between quality levels and interaction modalities in more detail and examine whether these results can be reproduced in tasks that require more precision. This work provides insights into the implications to consider when studying interactions in VR and paves the way for investigations into early phases of medical product development and workflow analysis.},
  keywords={Performance evaluation;Visualization;Solid modeling;Three-dimensional displays;Medical devices;Market research;User experience;Human-centered computing;Virtual reality;User studies},
  doi={10.1109/ISMAR59233.2023.00134},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316524,
  author={Hwang, Seokhyun and Kim, YoungIn and Seo, Youngseok and Kim, SeungJun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing Seamless Walking in Virtual Reality: Application of Bone-Conduction Vibration in Redirected Walking}, 
  year={2023},
  volume={},
  number={},
  pages={1181-1190},
  abstract={This study explored bone-conduction vibration (BCV) in redirected walking (RDW), a technology for seamless walking in large virtual spaces within confined physical areas, enhancing obstacle avoidance performance using nonelectrical vestibular stimulation without the side effects caused by electrical stimulation. We proposed four different BCV stimulation methods and evaluated their detection threshold (DT) extension performance and user experience in virtual reality (VR) conditions. The DT was successfully expanded from at least 23% to 45% under all BCV conditions while preserving the immersion and presence. Notably, user comfort increased when content sound was used for vestibular stimulation. Under the extended DT condition, a simulation study demonstrated that all BCV stimulation methods facilitated uninterrupted walking over extended distances when applying RDW to users with random movements. Thus, this research established the viability of using BCV in RDW applications and the potential for incorporating content sound into BCV stimulation techniques.},
  keywords={Legged locomotion;Vibrations;Electrical stimulation;User experience;Collision avoidance;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR59233.2023.00135},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316416,
  author={Haltner, Peter and Goddy-Worlu, Rowland and Forren, James and Nicholas, Claire and Reilly, Derek},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Comparative Evaluation of AR Embodiments vs. Videos and Figures for Learning Bead Weaving}, 
  year={2023},
  volume={},
  number={},
  pages={1191-1200},
  abstract={The most common learning materials for handcraft today are videos and figures, which are limited in their ability to express embodied knowledge as an in-person tutor could. We developed WeavAR, an application for headworn augmented reality (AR) displays designed to teach basic bead weaving patterns. WeavAR combines virtual 3D hands showing weaving sequences recorded from an experienced bead weaver and a dynamic 3D bead model showing how the work progresses. Using a mixed within/between-subjects user study (n=30), we compared learning materials (AR to videos and figures) and learning material placement (in the area of work or to the side). Results show that the AR learning materials had comparable effectiveness to video and figures. Hand visualizations were found to lack crucial context, however, making them less useful than the 3D bead model. Extra measures to prevent obstruction are required when placing learning materials at the area of work.},
  keywords={Solid modeling;Three-dimensional displays;Atmospheric measurements;Area measurement;Particle measurements;Weaving;Standards;Human-centered computing;Mixed / augmented reality;User studies},
  doi={10.1109/ISMAR59233.2023.00136},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316513,
  author={Kim, You-Jin and Wilson, Andrew D. and Jacobs, Jennifer and Höllerer, Tobias},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Reality Distortion Room: A Study of User Locomotion Responses to Spatial Augmented Reality Effects}, 
  year={2023},
  volume={},
  number={},
  pages={1201-1210},
  abstract={Reality Distortion Room (RDR) is a proof-of-concept augmented reality system using projection mapping and unencumbered interaction with the Microsoft RoomAlive system to study a user’s locomotive response to visual effects that seemingly transform the physical room the user is in. This study presents five effects that augment the appearance of a physical room to subtly encourage user motion. Our experiment demonstrates users’ reactions to the different distortion and augmentation effects in a standard living room, with the distortion effects projected as wall grids, furniture holograms, and small particles in the air. The augmented living room can give the impression of becoming elongated, wrapped, shifted, elevated, and enlarged. The study results support the implementation of AR experiences in limited physical spaces by providing an initial understanding of how users can be subtly encouraged to move throughout a room.},
  keywords={Navigation;Deformation;Spatial augmented reality;Transforms;Distortion;Visual effects;Standards;Human-centered computing;Empirical studies in RCI;Computing methodologies;Mixed / augmented reality Computing methodologies;Virtual reality Computing methodologies;Perception},
  doi={10.1109/ISMAR59233.2023.00137},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316448,
  author={Indyk, Ilia and Makarov, Ilya},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MonoVAN: Visual Attention for Self-Supervised Monocular Depth Estimation}, 
  year={2023},
  volume={},
  number={},
  pages={1211-1220},
  abstract={Depth estimation is crucial in various computer vision applications, including autonomous driving, robotics, and virtual and augmented reality. An accurate scene depth map is beneficial for localization, spatial registration, and tracking. It converts 2D images into precise 3D coordinates for accurate positioning, seamlessly aligns virtual and real objects in applications like AR, and enhances object tracking by distinguishing distances. The self-supervised monocular approach is particularly promising as it eliminates the need for complex and expensive data acquisition setups relying solely on a standard RGB camera. Recently, transformer-based architectures have become popular to solve this problem, but at high quality, they suffer from high computational cost and poor perception of small details as they focus more on global information. In this paper, we propose a novel fully convolutional network for monocular depth estimation, called MonoVAN, which incorporates the visual attention mechanism and applies super-resolution techniques in decoder to better capture fine-grained details in depth maps. To the best of our knowledge, this work pioneers the use of a convolutional visual attention in the context of depth estimation. Our experiments on outdoor KITTI benchmark and the indoor NYUv2 dataset show that our approach outperforms the most advanced self-supervised methods, including such state-of-the-art models as transformer-based VTDepth from ISMAR’22 and hybrid convolutional-transformer MonoFormer from AAAI’23, while having a comparable or even fewer number of parameters in our model than competitors. We also validate the impact of each proposed improvement in isolation, providing evidence of its significant contribution. Code and weights are available at https://github.com/IlyaInd/MonoVAN.},
  keywords={Convolutional codes;Visualization;Computational modeling;Superresolution;Estimation;Training data;Transformers;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Artificial intelligence;Computer vision;Localization;spatial registration and tracking;3D reconstruction},
  doi={10.1109/ISMAR59233.2023.00138},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316437,
  author={Zhu, Fengyuan and Sidenmark, Ludwig and Sousa, Mauricio and Grossman, Tovi},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PinchLens: Applying Spatial Magnification and Adaptive Control-Display Gain for Precise Selection in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={1221-1230},
  abstract={We present PinchLens, a new free-hand target selection technique for acquiring small and dense targets in Virtual Reality. Traditional pinch-based selection does not allow people to precisely manipulate small and dense objects effectively due to tracking and perceptual inaccuracies. Our approach combines spatial magnification, an adaptive control-display gain, and visual feedback to improve selection accuracy. When a user starts the pinching selection process, a magnifying bubble expands the scale of nearby targets, an adaptive control-to-display ratio is applied to the user’s hand for precision, and a cursor is displayed at the estimated pinch point for enhanced visual feedback. We performed a user study to compare our technique to traditional pinch selection and several variations to isolate the impact of each of the technique’s features. The results showed that PinchLens significantly outperformed traditional pinch selection, reducing error rates from 18.9% to 1.9%. Furthermore, we found that magnification was the dominant feature to produce this improvement, while the adaptive control-display gain and visual cursor of pinch were also helpful in several conditions.},
  keywords={Visualization;Target tracking;Error analysis;Process control;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/ISMAR59233.2023.00139},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316456,
  author={Yu, Rui and Wang, Jian and Ma, Sizhuo and Huang, Sharon X. and Krishnan, Gurunandan and Wu, Yicheng},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Be Real in Scale: Swing for True Scale in Dual Camera Mode}, 
  year={2023},
  volume={},
  number={},
  pages={1231-1239},
  abstract={Many mobile AR apps that use the front-facing camera can benefit significantly from knowing the metric scale of the user’s face. However, the true scale of the face is hard to measure because monocular vision suffers from a fundamental ambiguity in scale. The methods based on prior knowledge about the scene either have a large error or are not easily accessible. In this paper, we propose a new method to measure the face scale by a simple user interaction: the user only needs to swing the phone to capture two selfies while using the recently popular Dual Camera mode. This mode allows simultaneous streaming of the front camera and the rear cameras and has become a key feature in many social apps. A computer vision method is applied to first estimate the absolute motion of the phone from the images captured by two rear cameras, and then calculate the point cloud of the face by triangulation. We develop a prototype mobile app to validate the proposed method. Our user study shows that the proposed method is favored compared to existing methods because of its high accuracy and ease of use. Our method can be built into Dual Camera mode and can enable a wide range of applications (e.g., virtual try-on for online shopping, true-scale 3D face modeling, gaze tracking, and face anti-spoofing) by introducing true scale to smartphone-based XR. The code is available at https://github.com/ruiyu0/Swing-for-True-Scale.},
  keywords={Point cloud compression;Solid modeling;Three-dimensional displays;Estimation;Prototypes;Streaming media;Cameras;Computing methodologies;Computer vision;Human-centered computing;Interaction design},
  doi={10.1109/ISMAR59233.2023.00140},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10316458,
  author={Dong, Xin and Ling, Haibin and Huang, Bingyao},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Adaptive Color Structured Light for Calibration and Shape Reconstruction}, 
  year={2023},
  volume={},
  number={},
  pages={1240-1249},
  abstract={Color structured light (SL) plays an important role in spatial augmented reality and shape reconstruction. Compared to traditional non-color multi-shot SL, it has the advantage of fewer projections, and can even achieve single-shot. However, distortions caused by ambient light and imaging devices limit color SL’s applicability and accuracy. A common solution is to apply color adaptation techniques to cancel the disturbances. Previous studies focus on either robust fixed color patterns or adaptation approaches that may require preliminary geometric calibrations. In this paper, we propose an approach that can efficiently adapt color SL to arbitrary ambient light and imaging devices’ color responses, without device response function calibration or geometric calibration. First, we design a novel algorithm to quickly find the most distinct colors that are easily separable under a new environment and device setup. Then, we design a maximum a posteriori (MAP)-based color detection algorithm that can utilize ambient light and device priors to robustly detect the SL colors. In experiments, our adaptive color SL outperforms previous methods in both calibration and shape reconstruction tasks across a variety of setups.},
  keywords={Image color analysis;Shape;Spatial augmented reality;Imaging;Distortion;Calibration;Task analysis;Computing methodologies;Computer vision;Image and video acquisition;Camera calibration;Reconstruction},
  doi={10.1109/ISMAR59233.2023.00141},
  ISSN={2473-0726},
  month={Oct},}
