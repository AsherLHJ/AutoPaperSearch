@INPROCEEDINGS{8943728,
  author={Liu, Ruyu and Zhang, Jianhua and Chen, Shengyong and Arth, Clemens},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards SLAM-Based Outdoor Localization using Poor GPS and 2.5D Building Models}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  abstract={In this paper, we address the topic of outdoor localization and tracking using monocular camera setups with poor GPS priors. We leverage 2.5D building maps, which are freely available from open-source databases such as OpenStreetMap. The main contributions of our work are a fast initialization method and a non-linear optimization scheme. The initialization upgrades a visual SLAM reconstruction with an absolute scale. The non-linear optimization uses the 2.5D building model footprint, which further improves the tracking accuracy and the scale estimation. A pose optimization step relates the vision-based camera pose estimation from SLAM to the position information received through GPS, in order to fix the common problem of drift. We evaluate our approach on a set of challenging scenarios. The experimental results show that our approach achieves improved accuracy and robustness with an advantage in run-time over previous setups.},
  keywords={Simultaneous localization and mapping;Cameras;Optimization;Global Positioning System;Buildings;Image reconstruction;Solid modeling;Outdoor Localization and tracking;hybrid SLAM system;fast initialization;outdoor augmented reality},
  doi={10.1109/ISMAR.2019.00016},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943719,
  author={Gaudillière, Vincent and Simon, Gilles and Berger, Marie-Odile},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Camera Relocalization with Ellipsoidal Abstraction of Objects}, 
  year={2019},
  volume={},
  number={},
  pages={8-18},
  abstract={We are interested in AR applications which take place in man-made GPS-denied environments, as industrial or indoor scenes. In such environments, relocalization may fail due to repeated patterns and large changes in appearance which occur even for small changes in viewpoint. We investigate in this paper a new method for relocalization which operates at the level of objects and takes advantage of the impressive progress realized in object detection. Recent works have opened the way towards object oriented reconstruction from elliptic approximation of objects detected in images. We go one step further and propose a new method for pose computation based on ellipse/ellipsoid correspondences. We consider in this paper the practical common case where an initial guess of the rotation matrix of the pose is known, for instance with an inertial sensor or from the estimation of orthogonal vanishing points. Our contributions are twofold: we prove that a closed form estimate of the translation can be computed from one ellipse-ellipsoid correspondence. The accuracy of the method is assessed on the LINEMOD database using only one correspondence. Second, we prove the effectiveness of the method on real scenes from a set of object detections generated by YOLO. A robust framework that is able to choose the best set of hypotheses is proposed and is based on an appropriate estimation of the reprojection error of ellipsoids. Globally, considering pose at the level of object allows us to avoid common failures due to repeated structures. In addition, due to the small combinatory induced by object correspondences, our method is well suited to fast rough localization even in large environments.},
  keywords={Cameras;Ellipsoids;Three-dimensional displays;Object detection;Estimation;Computational modeling;Solid modeling;Camera relocalization;Ellipsoids;Ellipses;Augmented reality},
  doi={10.1109/ISMAR.2019.00017},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943776,
  author={Stotko, Patrick and Krumpen, Stefan and Weinmann, Michael and Klein, Reinhard},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client Live Telepresence}, 
  year={2019},
  volume={},
  number={},
  pages={19-25},
  abstract={Sharing live telepresence experiences for teleconferencing or remote collaboration receives increasing interest with the recent progress in capturing and AR/VR technology. Whereas impressive telepresence systems have been proposed on top of on-the-fly scene capture, data transmission and visualization, these systems are restricted to the immersion of single or up to a low number of users into the respective scenarios. In this paper, we direct our attention on immersing significantly larger groups of people into live-captured scenes as required in education, entertainment or collaboration scenarios. For this purpose, rather than abandoning previous approaches, we present a range of optimizations of the involved reconstruction and streaming components that allow the immersion of a group of more than 24 users within the same scene - which is about a factor of 6 higher than in previous work - without introducing further latency or changing the involved consumer hardware setup. We demonstrate that our optimized system is capable of generating high-quality scene reconstructions as well as providing an immersive viewing experience to a large group of people within these live-captured scenes.},
  keywords={Three-dimensional displays;Telepresence;Image reconstruction;Data models;Servers;Scalability;Cameras;Human centered computing Human computer interaction (HCI) Interaction paradigms Virtual reality, Human centered computing Human computer interaction (HCI) Interaction paradigms Collaborative interaction, Computing methodologies Computer graphics Graphics systems and interfaces Virtual reality, Computing methodologies Computer vision Computer vision problems Reconstruction},
  doi={10.1109/ISMAR.2019.00018},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943645,
  author={Wu, Yi-Chin and Chan, Liwei and Lin, Wen-Chieh},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Tangible and Visible 3D Object Reconstruction in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={26-36},
  abstract={Many crucial applications in the fields of filmmaking, game design, education, and cultural preservation - among others - involve the modeling, authoring, or editing of 3D objects and scenes. The two major methods of creating 3D models are 1) modeling, using computer software, and 2) reconstruction, generally using high-quality 3D scanners. Scanners of sufficient quality to support the latter method remain unaffordable to the general public. Since the emergence of consumer-grade RGBD cameras, there has been a growing interest in 3D reconstruction systems using depth cameras. However, most such systems are not user-friendly, and require intense efforts and practice if good reconstruction results are to be obtained. In this paper, we propose to increase the accessibility of depth-camera-based 3D reconstruction by assisting its users with augmented reality (AR) technology. Specifically, the proposed approach will allow users to rotate/move a target object freely with their hands and see the object being overlapped with its reconstructing model during the reconstruction process. As well as being more instinctual than conventional reconstruction systems, our proposed system will provide useful hints on complete 3D reconstruction of an object, including the best capturing range; reminder of moving and rotating the object at a steady speed; and which model regions are complex enough to require zooming-in. We evaluated our system via a user study that compared its performance against those of three other stateof- the-art approaches, and found our system outperforms the other approaches. Specifically, the participants rated it highest in usability, understandability, and model satisfaction.},
  keywords={Three-dimensional displays;Image reconstruction;Solid modeling;Cameras;Skin;Glass;Geometry},
  doi={10.1109/ISMAR.2019.00-30},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943678,
  author={Xu, Yi and Yang, Shanglin and Sun, Wei and Tan, Li and Li, Kefeng and Zhou, Hui},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={3D Virtual Garment Modeling from RGB Images}, 
  year={2019},
  volume={},
  number={},
  pages={37-45},
  abstract={We present a novel and efficient approach that automatically constructs 3D models for garments using only RGB images. Most of the previous methods deal with input photos in which the garment is on a human body or mannequin. Our approach can work with various types of input garment photos: photos in which the garment is worn by a model, or photos in which the garment is laid on a flat surface. To construct a complete 3D model, our approach requires minimum two images as input: one front view and one rear view. We propose a multi-task learning network called JFNet that jointly identifies the landmarks of the garment as well as parses the garment into semantic part segments. The predicted landmarks are used to estimate the garment size thus a template mesh can be deformed accordingly to construct the 3D mesh model. Color and textures of the model are extracted by exploiting the parsed semantic parts from input images. Our approach can be applied in various Virtual Reality and Mixed Reality applications involving garment modeling.},
  keywords={Clothing;Three-dimensional displays;Solid modeling;Semantics;Shape;Image segmentation;Computational modeling;landmark prediction, semantic part segmentation, 3D garment modeling},
  doi={10.1109/ISMAR.2019.00-28},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943701,
  author={Ladefoged, Kasper Skou and Madsen, Claus Brøndgaard},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Spatially-Varying Diffuse Reflectance Capture Using Irradiance Map Rendering for Image-Based Modeling Applications}, 
  year={2019},
  volume={},
  number={},
  pages={46-54},
  abstract={Image-based 3D modelling using Structure-form-Motion (SfM) has matured significantly over the last decade. Standard SfM methods create the object's texture from the appearance of the physical object at the time of acquisition. We propose a method for acquiring the diffuse per-point reflectance of the modelled object, as part of the image acquisition work flow, only adding one extra captured image and an irradiance rendering step, making it easy for anyone to digitize physical objects to create 3D content for AR/VR using only consumer grade hardware. Current state of the art of spatially varying reflectance capture requires either large, expensive, and purpose built setups or are optimization based approaches, whereas the proposed approach is model based. This paper proposes adding a render of irradiance with modelled camera and light source, using off the shelf hardware for image capture. The key element is taking two images at each imaging location: one with just the ambient illumination conditions, and one where the light from an on-camera flash is included. It is demonstrated how to get the ambient illumination to cancel out, and by assuming Lambertian materials, render the irradiance corresponding to the flash-only image, enabling computation of spatially varying diffuse reflectance rather than appearance. Qualitative results demonstrate the added realism of the modelled objects when used as assets in renders under varying illumination conditions, including limited outdoor scenarios. Quantitative tests demonstrate that the reflectance can be estimated correctly to within a few percent even in cases with severe un-even ambient illumination.},
  keywords={Lighting;Cameras;Three-dimensional displays;Light sources;Image reconstruction;Rendering (computer graphics);Solid modeling;Reflectance;Spatially Varying;Diffuse Reflectance;Image Based Modeling;Irradiance map Rendering;SfM;image based 3D modelling;albedo generation;diffuse albedo},
  doi={10.1109/ISMAR.2019.00-27},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943682,
  author={Kaminokado, Takumi and Iwai, Daisuke and Sato, Kosuke},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmented Environment Mapping for Appearance Editing of Glossy Surfaces}, 
  year={2019},
  volume={},
  number={},
  pages={55-65},
  abstract={We propose a novel spatial augmented reality (SAR) framework to edit the appearance of physical glossy surfaces. The key idea is utilizing the specular reflection, which was a major distractor in conventional SAR systems. Namely, we spatially manipulate the appearance of an environmental surface, which is observed through the specular reflection. We use a stereoscopic display to present two appearances with disparity on the environmental surface, by which the depth of the specularly reflected visual information corresponds to the glossy surface. We refer to this method as augmented environment mapping (AEM). The paper describes its principle, followed by three different implementation approaches inspired by typical virtual and augmented reality approaches. We confirmed the feasibility of AEM through both quantitative and qualitative experiments using prototype systems.},
  keywords={Observers;Surface treatment;Mirrors;Three-dimensional displays;Prototypes;Optical imaging;Cameras;Augmented reality;Projection mapping;Environment mapping},
  doi={10.1109/ISMAR.2019.00-26},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943754,
  author={Vasiliu, Valentin and Sörös, Gábor},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Coherent Rendering of Virtual Smile Previews with Fast Neural Style Transfer}, 
  year={2019},
  volume={},
  number={},
  pages={66-73},
  abstract={Coherent rendering in augmented reality deals with synthesizing virtual content that seamlessly blends in with the real content. Unfortunately, capturing or modeling every real aspect in the virtual rendering process is often unfeasible or too expensive. We present a post-processing method that improves the look of rendered overlays in a dental virtual try-on application. We combine the original frame and the default rendered frame in an autoencoder neural network in order to obtain a more natural output, inspired by artistic style transfer research. Specifically, we apply the original frame as style on the rendered frame as content, repeating the process with each new pair of frames. Our method requires only a single forward pass, our shallow architecture ensures fast execution, and our internal feedback loop inherently enforces temporal consistency.},
  keywords={Rendering (computer graphics);Image color analysis;Dentistry;Training;Augmented reality;Cameras;Task analysis;augmented reality;coherent rendering;style transfer;convolutional neural network;autoencoder},
  doi={10.1109/ISMAR.2019.00-25},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943651,
  author={Thompson, Stephen and Chalmers, Andrew and Rhee, Taehyun},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Real-Time Mixed Reality Rendering for Underwater 360° Videos}, 
  year={2019},
  volume={},
  number={},
  pages={74-82},
  abstract={We present a novel mixed reality (MR) rendering and composition solution that illuminates and blends virtual objects into underwater 360° videos (360-video) in real-time. Real-time underwater lighting (caustics, god rays, fog, and particulates) were developed to improve the overall lighting and blending quality. We also provide a MR toolkit, an interface to tune the parameters of the underwater lighting so the user can match the lighting observed in the 360-video. Our image based lighting provides automatic ambient and high frequency underwater lighting. This ensures that the virtual objects are lit and blend similarly to each frame of the video semi-automatically and in real-time. We conducted a user study by having participants rate our method based on the visual quality and presence using a five point Likert Scale. The results show that our underwater lighting is preferred over no underwater effects or using naive ambient lighting. We also have a few takeaways on what elements of our underwater lighting and interaction have a significant impact on visual quality and presence in underwater MR.},
  keywords={Lighting;Videos;Real-time systems;Rendering (computer graphics);Sea surface;Virtual reality;Visualization;Mixed reality;Virtual reality;360° video;underwater lighting;caustics;fog;god rays;particles;composition;lighting;image-based lighting},
  doi={10.1109/ISMAR.2019.00-24},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943647,
  author={Taylor, Catherine and Mullany, Chris and McNicholas, Robin and Cosker, Darren},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VR Props: An End-to-End Pipeline for Transporting Real Objects Into Virtual and Augmented Environments}, 
  year={2019},
  volume={},
  number={},
  pages={83-92},
  abstract={Improvements in both software and hardware, as well as an increase in consumer suitable equipment, have resulted in great advances in the fields of virtual and augmented reality. Typically, systems use controllers or hand gestures to interact with virtual objects. However, these motions are often unnatural and diminish the immersion of the experience. Moreover, these approaches offer limited tactile feedback. There does not currently exist a platform to bring an arbitrary physical object into the virtual world without additional peripherals or the use of expensive motion capture systems. Such a system could be used for immersive experiences within the entertainment industry as well as being applied to VR or AR training experiences, in the fields of health and engineering. We propose an end-to-end pipeline for creating an interactive virtual prop from rigid and non-rigid physical objects. This includes a novel method for tracking the deformations of rigid and non-rigid objects at interactive rates using a single RGBD camera. We scan our physical object and process the point cloud to produce a triangular mesh. A range of possible deformations can be obtained by using a finite element method simulation and these are reduced to a low dimensional basis using principal component analysis. Machine learning approaches, in particular neural networks, have become key tools in computer vision and have been used on a range of tasks. Moreover, there has been an increased trend in training networks on synthetic data. To this end, we use a convolutional neural network, trained on synthetic data, to track the movement and potential deformations of an object in unlabelled RGB images from a single RGBD camera. We demonstrate our results for several objects with different sizes and appearances.},
  keywords={Strain;Tracking;Pipelines;Training;Deformable models;Computational modeling;Solid modeling;Virtual Reality;Real Time Tracking;VR Props},
  doi={10.1109/ISMAR.2019.00-22},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943734,
  author={Sadri, Shirin and Kohen, Shalva A. and Elvezio, Carmine and Sun, Shawn H. and Grinshpoon, Alon and Loeb, Gabrielle J. and Basu, Naomi and Feiner, Steven K.},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Manipulating 3D Anatomic Models in Augmented Reality: Comparing a Hands-Free Approach and a Manual Approach}, 
  year={2019},
  volume={},
  number={},
  pages={93-102},
  abstract={Many AR and VR task domains involve manipulating virtual objects; for example, to perform 3D geometric transformations. These operations are typically accomplished with tracked hands or hand-held controllers. However, there are some activities in which the user's hands are already busy with another task, requiring the user to temporarily stop what they are doing to perform the second task, while also taking time to disengage and reengage with the original task (e.g., putting down and picking up tools). To avoid the need to overload the user's hands this way in an AR system for guiding a physician performing a surgical procedure, we developed a hands-free approach to performing 3D transformations on patient-specific virtual organ models. Our approach uses small head motions to accomplish first-order and zero-order control, in conjunction with voice commands to establish the type of transformation. To show the effectiveness of this approach for translating, scaling, and rotating 3D virtual models, we conducted a within-subject study comparing the hands-free approach with one based on conventional manual techniques, both running on a Microsoft HoloLens and using the same voice commands to specify transformation type. Independent of any additional time to transition between tasks, users were significantly faster overall using the hands-free approach, significantly faster for hands-free translation and scaling, and faster (although not significantly) for hands-free rotation.},
  keywords={Augmented Reality;Vascular Interventions;Hands Free Interaction;User Study;Multimodal Interaction;Head gaze Control},
  doi={10.1109/ISMAR.2019.00-21},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943683,
  author={Yu, Difeng and Liang, Hai-Ning and Lu, Xueshi and Zhang, Tianyu and Xu, Wenge},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DepthMove: Leveraging Head Motions in the Depth Dimension to Interact with Virtual Reality Head-Worn Displays}, 
  year={2019},
  volume={},
  number={},
  pages={103-114},
  abstract={Head-based interactions are very handy for virtual reality (VR) head-worn display (HWD) systems. A useful head-based interaction technique could help users to interact with VR environments in a hands-free manner (i.e., without the need of a hand-held de-vice). Moreover, it can sometimes be seamlessly integrated with other input modalities to provide richer interaction possibilities. This paper explores the potential of a new approach that we call DepthMove to allow interactions that are based on head motions along the depth dimension. With DepthMove, a user can interact with a VR system proactively by moving the head perpendicular to the VR HWD forward or backward. We use two user studies to investigate, model, and optimize DepthMove by taking into con-sideration user performance, subjective response, and social ac-ceptability. The results allow us to determine the optimal and comfortable DepthMove range. We also distill recommendations that can be used to guide the design interfaces that use DepthMove for efficient and accurate interaction in VR HWD systems. A third study is conducted to demonstrate the usefulness of DepthMove relative to other techniques in four application scenarios.},
  keywords={C++ languages;Augmented reality;Virtual reality;target selection;head-based interaction;hands-free interaction;head-worn displays;3D position tracking},
  doi={10.1109/ISMAR.2019.00-20},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943756,
  author={Peillard, Etienne and Argelaguet, Ferran and Normand, Jean-Marie and Lécuyer, Anatole and Moreau, Guillaume},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying Exocentric Distance Perception in Optical See-Through Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={115-122},
  abstract={While perceptual biases have been widely investigated in Virtual Reality (VR), very few studies have considered the challenging environment of Optical See-through Augmented Reality (OST-AR). Moreover, regarding distance perception, existing works mainly focus on the assessment of egocentric distance perception, i.e. distance between the observer and a real or a virtual object. In this paper, we study exocentric distance perception in AR, hereby considered as the distance between two objects, none of them being directly linked to the user. We report a user study (n=29) aiming at estimating distances between two objects lying in a frontoparallel plane at 2.1m from the observer (i.e. in the medium-field perceptual space). Four conditions were tested in our study: real objects on the left and on the right of the participant (called real-real), virtual objects on both sides (virtual-virtual), a real object on the left and a virtual one on the right (real-virtual) and finally a virtual object on the left and a real object on the right (virtual-real). Participants had to reproduce the distance between the objects by spreading two real identical objects presented in front of them. The main findings of this study are the overestimation (20%) of exocentric distances for all tested conditions. Surprisingly, the real-real condition was significantly more overestimated (by about 4%, p=.0166) compared to the virtual-virtual condition, i.e. participants obtained better estimates of the exocentric distance for the virtual-virtual condition. Finally, for the virtual-real/real-virtual conditions, the analysis showed a non-symmetrical behavior, which suggests that the relationship between real and virtual objects with respect to the user might be affected by other external factors. Considered together, these unexpected results illustrate the need for additional experiments to better understand the perceptual phenomena involved in exocentric distance perception with real and virtual objects.},
  keywords={Augmented reality;Perception;Distance;Augmented Reality;User Experiment;Psychophysical Study},
  doi={10.1109/ISMAR.2019.00-13},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943738,
  author={Dewez, Diane and Fribourg, Rebecca and Argelaguet, Ferran and Hoyet, Ludovic and Mestre, Daniel and Slater, Mel and Lécuyer, Anatole},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Influence of Personality Traits and Body Awareness on the Sense of Embodiment in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={123-134},
  abstract={With the increasing use of avatars (i.e. the virtual representation of the user in a virtual environment) in virtual reality, it is important to identify the factors eliciting the sense of embodiment or the factors that can disrupt this feeling. This paper reports an exploratory study aiming at identifying internal factors (personality traits and body awareness) that might cause either a resistance or a predisposition to feel a sense of embodiment towards a virtual avatar. To this purpose, we conducted an experiment (n=123) in which participants were immersed in a virtual environment and embodied in a gender-matched generic virtual avatar through a head-mounted display. After an exposure phase in which they had to perform a number of visuomotor tasks (during 2 minutes) a virtual character entered the virtual scene and stabbed the participants' virtual hand with a knife. The participants' sense of embodiment was measured, as well as several personality traits (Big Five traits and locus of control) and body awareness, to evaluate the influence of participants' personality on the acceptance of the virtual body. The major finding of the experiment is that the locus of control is linked to several components of embodiment: the sense of agency is positively correlated with an internal locus of control and the sense of body ownership is positively correlated with an external locus of control. Interestingly, both components are not influenced by the same traits, which confirms that they can appear independently. Taken together our results suggest that the locus of control could be a good predictor of the sense of embodiment when the user embodies an avatar with a similar physical appearance. Yet, further studies are required to confirm these results.},
  keywords={Avatars;Virtual environments;Task analysis;Visualization;Atmospheric measurements;Particle measurements;Avatar;Virtual Reality;Embodiment;Personality;Body Awareness},
  doi={10.1109/ISMAR.2019.00-12},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943577,
  author={Shin, Jae-eun and Kim, Hayun and Parker, Callum and Kim, Hyung-il and Oh, Seoyoung and Woo, Woontack},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Is Any Room Really OK? The Effect of Room Size and Furniture on Presence, Narrative Engagement, and Usability During a Space-Adaptive Augmented Reality Game}, 
  year={2019},
  volume={},
  number={},
  pages={135-144},
  abstract={One of the main challenges in creating narrative-driven Augmented Reality (AR) content for Head Mounted Displays (HMDs) is to make them equally accessible and enjoyable in different types of indoor environments. However, little has been studied in regards to whether such content can indeed provide similar, if not the same, levels of experience across different spaces. To gain more understanding towards this issue, we examine the effect of room size and furniture on the player experience of Fragments, a space-adaptive, indoor AR crime-solving game created for the Microsoft HoloLens. The study compares factors of player experience in four types of spatial conditions: (1) Large Room - Fully Furnished; (2) Large Room - Scarcely Furnished; (3) Small Room - Fully Furnished; and (4) Small Room - Scarcely Furnished. Our results show that while large spaces facilitate a higher sense of presence and narrative engagement, fully-furnished rooms raise perceived workload. Based on our findings, we propose design suggestions that can support narrative-driven, space-adaptive indoor HMD-based AR content in delivering optimal experiences for various types of rooms.},
  keywords={Usability;Games;Augmented reality;Indoor environment;Media;Mobile handsets;Tracking;Augmented Reality;Augmented Reality Game;Augmented Narrative},
  doi={10.1109/ISMAR.2019.00-11},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943689,
  author={Merenda, Coleman and Suga, Chihiro and Gabbard, Joseph L. and Misu, Teruhisa},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of "Real-World" Visual Fidelity on AR Interface Assessment: A Case Study Using AR Head-up Display Graphics in Driving}, 
  year={2019},
  volume={},
  number={},
  pages={145-156},
  abstract={Recent AR research efforts have explored the use of virtual environments to test augmented reality (AR) user interfaces. However, it is yet to be seen what effects the visual fidelity of such virtual environments may have on AR interface assessment, and specifically to what degree assessment results observed in a virtual world would apply to the real world. Automotive AR head-up (HUD) interfaces provide a meaningful application area to examine this problem, especially given that immersive, 3D-graphics-based driving simulators are established tools to examine in-vehicle interfaces safely before testing in real vehicles. In this work, we present an argument that adequately assessing AR interfaces requires a suite of different measures, and that such measures should be considered when debating the appropriateness of virtual environments for AR interface assessment. We present a case study that examines how an AR interface presented via HUD effects driver performance and behavior in different virtual and real environments. Twelve participants completed the study measuring driver task performance, eye gaze behavior and situational awareness during AR guided navigation in low-and high-fidelity virtual simulation, and an on-road environment. Our results suggest that the visual fidelity of the environmental in which an AR interface is assessed, could impact some measures of effectiveness. Discussion is guided by a proposed initial assessment classification for AR user interfaces that may serve to guide future discussions on AR interface evaluation, as well as the suitability of virtual environments for AR assessment.},
  keywords={Augmented reality;Augmented Reality;Virtual Reality;Head-up Displays;User Interface Assessment},
  doi={10.1109/ISMAR.2019.00-10},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943625,
  author={Norouzi, Nahal and Kim, Kangsoo and Lee, Myungho and Schubert, Ryan and Erickson, Austin and Bailenson, Jeremy and Bruder, Gerd and Welch, Greg},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Walking Your Virtual Dog: Analysis of Awareness and Proxemics with Simulated Support Animals in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={157-168},
  abstract={Domestic animals have a long history of enriching human lives physically and mentally by filling a variety of different roles, such as service animals, emotional support animals, companions, and pets. Despite this, technological realizations of such animals in augmented reality (AR) are largely underexplored in terms of their behavior and interactions as well as effects they might have on human users' perception or behavior. In this paper, we describe a simulated virtual companion animal, in the form of a dog, in a shared AR space. We investigated its effects on participants' perception and behavior, including locomotion related to proxemics, with respect to their AR dog and other real people in the environment. We conducted a 2 by 2 mixed factorial human-subject study, in which we varied (i) the AR dog's awareness and behavior with respect to other people in the physical environment and (ii) the awareness and behavior of those people with respect to the AR dog. Our results show that having an AR companion dog changes participants' locomotion behavior, proxemics, and social interaction with other people who can or can not see the AR dog. We also show that the AR dog's simulated awareness and behaviors have an impact on participants' perception, including co-presence, animalism, perceived physicality, and dog's perceived awareness of the participant and environment. We discuss our findings and present insights and implications for the realization of effective AR animal companions.},
  keywords={Dogs;Games;Augmented reality;Legged locomotion;Visualization;Augmented Reality;Augmented Reality Animals;Virtual Animals;Companion Animals;Virtual Reality},
  doi={10.1109/ISMAR.2019.000-8},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943774,
  author={Balasubramanian, Suprith and Soundararajan, Rajiv},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Prediction of Discomfort due to Egomotion in Immersive Videos for Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={169-177},
  abstract={We consider the problem of automatic assessment of visually induced motion sickness in virtual reality applications. In particular, we study the impact on visual discomfort due to camera motion or egomotion present in the video displayed through a head mounted display. We develop a database of 100 short duration videos with different camera trajectories, speeds and shake levels and conduct a large scale subjective study by collecting more than 4000 human ratings of discomfort levels. The videos are generated synthetically by applying different camera trajectories. We then use the subjective study to learn to predict discomfort by designing features describing the camera motion. The features are based on the ground truth camera trajectory and estimate the camera velocity and shake and depth of the visual scene. We show that these features can be effectively used to predict discomfort by obtaining a high correlation with the subjective discomfort scores provided by humans.},
  keywords={Videos;Cameras;Trajectory;Databases;Visualization;Virtual reality;Resists;Virtual reality, discomfort assessment, egomotion, monocular immersive videos},
  doi={10.1109/ISMAR.2019.000-7},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943618,
  author={Shi, Xinyu and Pan, Junjun and Hu, Zeyong and Lin, Juncong and Guo, Shihui and Liao, Minghong and Pan, Ye and Liu, Ligang},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Accurate and Fast Classification of Foot Gestures for Virtual Locomotion}, 
  year={2019},
  volume={},
  number={},
  pages={178-189},
  abstract={This work explores the use of foot gestures for locomotion in virtual environments. Foot gestures are represented as the distribution of plantar pressure and detected by three sparsely-located sensors on each insole. The Long Short-Term Memory model is chosen as the classifier to recognize the performer's foot gesture based on the captured signals of pressure information. The trained classifier directly takes the noisy and sparse input of sensor data, and handles seven categories of foot gestures (stand, walk forward/backward, run, jump, slide left and right) without manual definition of signal features for classifying these gestures. This classifier is capable of recognizing the foot gestures, even with the existence of large sensor-specific, inter-person and intra-person variations. Results show that an accuracy of ~80% can be achieved across different users with different shoe sizes and ~85% for users with the same shoe size. A novel method, Dual-Check Till Consensus, is proposed to reduce the latency of gesture recognition from 2 seconds to 0.5 seconds and increase the accuracy to over 97%. This method offers a promising solution to achieve lower latency and higher accuracy at a minor cost of computation workload. The characteristics of high accuracy and fast classification of our method could lead to wider applications of using foot patterns for human-computer interaction.},
  keywords={Foot;Legged locomotion;Footwear;Virtual reality;Pressure sensors;Three-dimensional displays;Human centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input;Interactive systems and tools;User interface programming},
  doi={10.1109/ISMAR.2019.000-6},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943620,
  author={Marquardt, Alexander and Trepkowski, Christina and Eibich, Tom David and Maiero, Jens and Kruijff, Ernst},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Non-Visual Cues for View Management in Narrow Field of View Augmented Reality Displays}, 
  year={2019},
  volume={},
  number={},
  pages={190-201},
  abstract={Head-worn devices with a narrow field of view are common commodity for Augmented Reality. However, their limited screen space makes view management difficult. Especially in dense information spaces this potentially leads to visual conflicts such as overlapping labels (occlusion) and visual clutter. In this paper, we look into the potential of using audio and vibrotactile feedback to guide search and information localization. Our results indicate users can be guided with high accuracy using audio-tactile feedback with maximum median deviations of only 2° on longitude, 3.6° on latitude and 0.07 meter in depth. Regarding the encoding of latitude we found a superior performance when using audio, resulting in an improvement of 61% and fastest search times. When interpreting localization cues the maximum median deviation was 9.9° on longitude and 18% of a selected distance to be encoded which could be reduced to 14% when using audio.},
  keywords={Visualization;Vibrations;Task analysis;Three-dimensional displays;Augmented reality;Navigation;Forehead;augmented reality, audio-tactile feedback, guidance, depth perception},
  doi={10.1109/ISMAR.2019.000-3},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943693,
  author={Erickson, Austin and Kim, Kangsoo and Schubert, Ryan and Bruder, Gerd and Welch, Greg},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Is It Cold in Here or Is It Just Me? Analysis of Augmented Reality Temperature Visualization for Computer-Mediated Thermoception}, 
  year={2019},
  volume={},
  number={},
  pages={202-211},
  abstract={Modern augmented reality (AR) head-mounted displays comprise a multitude of sensors that allow them to sense the environment around them. We have extended these capabilities by mounting two heat-wavelength infrared cameras to a Microsoft HoloLens, facilitating the acquisition of thermal data and enabling stereoscopic thermal overlays in the user's augmented view. The ability to visualize live thermal information opens several avenues of investigation on how that thermal awareness may affect a user's thermoception. We present a human-subject study, in which we simulated different temperature shifts using either heat vision overlays or 3D AR virtual effects associated with thermal cause-effect relationships (e.g., flames burn and ice cools). We further investigated differences in estimated temperatures when the stimuli were applied to either the user's body or their environment. Our analysis showed significant effects and first trends for the AR virtual effects and heat vision, respectively, on participants' temperature estimates for their body and the environment though with different strengths and characteristics, which we discuss in this paper.},
  keywords={Temperature sensors;Visualization;Cameras;Temperature measurement;Image color analysis;Heating systems;Mediation},
  doi={10.1109/ISMAR.2019.000-2},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943779,
  author={Nakano, Kizashi and Horita, Daichi and Sakata, Nobuchika and Kiyokawa, Kiyoshi and Yanai, Keiji and Narumi, Takuji},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DeepTaste: Augmented Reality Gustatory Manipulation with GAN-Based Real-Time Food-to-Food Translation}, 
  year={2019},
  volume={},
  number={},
  pages={212-223},
  abstract={We have been studying augmented reality (AR)-based gustatory manipulation interfaces and previously proposed a gustatory manipulation interface using generative adversarial network (GAN)-based real time image-to-image translation. Unlike three-dimensional (3D) food model-based systems that only change the color or texture pattern of a particular type of food in an inflexible manner, our GAN-based system changes the appearance of food into multiple types of food in real time flexibly, dynamically, and interactively. In the present paper, we first describe in detail a user study on a vision-induced gustatory manipulation system using a 3D food model and report its successful experimental results. We then summarize identified problems of the 3D model-based system and describe implementation details of the GAN-based system. We finally report in detail the main user study in which we investigated the impact of the GAN-based system on gustatory sensations and food recognition when somen noodles were turned into ramen noodles or fried noodles, and steamed rice into curry and rice or fried rice. The experimental results revealed that our system successfully manipulates gustatory sensations to some extent and that the effectiveness seems to depend on the original and target types of food as well as the experience of each individual with the food.},
  keywords={Three-dimensional displays;Solid modeling;Real-time systems;Visualization;Resists;Image color analysis;Modulation;Human centered computing Human computer interaction (HCI) Interaction paradigms Mixed / augmented reality Computing methodologies Computer graphics Graphics systems and interfaces Perception Computing methodologies Machine learning Machine learning approaches Neural networks},
  doi={10.1109/ISMAR.2019.000-1},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943608,
  author={Guo, Jie and Weng, Dongdong and Zhang, Zhenliang and Jiang, Haiyan and Liu, Yue and Wang, Yongtian and Duh, Henry Been-Lirn},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mixed Reality Office System Based on Maslow’s Hierarchy of Needs: Towards the Long-Term Immersion in Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={224-235},
  abstract={In a mixed reality (MR) environment that combines the physical objects with the virtual environments, users' feelings are immersed in the virtual world, while their bodies remain in the physical world. Compared to the purely physical environments, such characteristic has led to some special needs for users' long-term immersion. However, the deficiency needs that we have to face for long-term immersion still need further research. In this paper, we apply the theory of Maslow's Hierarchy of Needs (MHN) to guide the design of MR systems for long-term immersion. Taking the normal biological rhythm of human beings as the basic unit (24 hours), we propose the fundamental needs for long-term immersion in VEs through combining the theory of MHN with the special needs of virtual reality (VR). In order to verify whether those needs can satisfy users' long-term immersion, we design an MR office system for basic operations based on the theory of MHN. A long-term exposure experiment (duration of 8 hours) is designed to evaluate those needs by comparing the results with a physical work environment after a short-term preliminary study. The physiological and psychological effects are tested in both two environments and the deficiency needs for short-term immersion and long-term immersion are also compared. The results showed that the design based on the theory of MHN can support users' long-term immersion, which means that it can be a guideline for long-term use of MR systems.},
  keywords={Virtual reality;Psychology;Physiology;Health and safety;Visualization;Systematics;Human-centered-computing;Human-computer-interaction-(HCI);Interaction-paradigms;Mixed-/-augmented-reality;HCI-design-and-evaluation-methods;User-studies},
  doi={10.1109/ISMAR.2019.00019},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943760,
  author={Mahmood, Tahir and Fulmer, Willis and Mungoli, Neelesh and Huang, Jian and Lu, Aidong},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Improving Information Sharing and Collaborative Analysis for Remote GeoSpatial Visualization Using Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={236-247},
  abstract={Remote collaboration systems allow users at different sites to perform joint tasks, which are required by many real-life applications. For example, environmental pollution is a complex problem requiring many kinds of expertise to fully understand, as pollutants disperse not only locally but also regionally or even globally. This paper presents a remote collaborative visualization system through providing co-presence, information sharing, and collaborative analysis functions based on mixed reality techniques. We start with developing an immersive visualization approach for analyzing multi-attribute and geo-spatial data with intuitive multi-model interactions, simulating co-located collaboration effects. We then go beyond by designing a set of information sharing and collaborative analysis functions to support different users to share and analyze their sensemaking processes collaboratively. We provide example results and usage scenario to demonstrate that our system enables users to perform a variety of immersive and collaborative analytics tasks effectively. Through two small user studies focusing on evaluating our design of information sharing and system usability, the evaluation results confirm the effectiveness of comprehensive sharing among user, data, physical, and interaction spaces for improving remote collaborative analysis experience.},
  keywords={Collaboration;Data visualization;Task analysis;Visualization;Information management;Three-dimensional displays;Avatars;Remote collaboration;immersive analytics;geospatial visualization;mixed reality},
  doi={10.1109/ISMAR.2019.00021},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943737,
  author={Dey, Arindam and Chen, Hao and Hayati, Ashkan and Billinghurst, Mark and Lindeman, Robert W.},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Sharing Manipulated Heart Rate Feedback in Collaborative Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={248-257},
  abstract={We have explored the effects of sharing manipulated heart rate feedback in collaborative virtual environments. In our study, we created two types of different virtual environments (active and passive) with different levels of interactions and provided three levels of manipulated heart rate feedback (decreased, unchanged, and increased). We measured the effects of manipulated feedback on Social Presence, affect, physical heart rate, and overall experience. We noticed a significant effect of the manipulated heart rate feedback in affecting scariness and nervousness. The perception of the collaborator's valance and arousal was also affected where increased heart rate feedback perceived as a higher valance and lower arousal. Increased heart rate feedback decreased the real heart rate. The type of virtual environments had a significant effect on social presence, heart rate, and affect where the active environment had better performances across these measurements. We discuss the implications of this and directions for future research.},
  keywords={Collaboration;Real-time systems;Physiology;Heart beat;Games;Skin;Virtual Reality;Physiological Feedback;Collaboration;Heartrate;Empathic Cues},
  doi={10.1109/ISMAR.2019.00022},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943686,
  author={Thanyadit, Santawat and Punpongsanon, Parinya and Pong, Ting-Chuen},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ObserVAR: Visualization System for Observing Virtual Reality Users using Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={258-268},
  abstract={While virtual reality (VR) tools provide an immersive learning experience for students, it is difficult for an instructor to observe the students' learning activities in a virtual environment (VE). Thus, it hinders interactions that could occur between the instructor and students, which are usually required in a classroom environment to understand how each student learns. Previous work has added virtual awareness cues that can help a small group of students to collaborate in a VE. However, when the number of students increases, such virtual awareness cues can cause visual clutter and confuse the instructor. We propose ObserVAR, a visualization system that allows the instructor to observe students in a VE at scale. ObserVAR uses augmented reality techniques to visualize each student's gaze in a VE and improves the instructor's awareness of the entire class. The visualizations are then optimized to reduce visual clutter in the scene using a force-directed graph drawing algorithm. In designing ObserVAR, we first investigated visualizations that can provide the instructor with an overall awareness of the VE that can be scaled up as the number of users increases. Second, we optimized the visualization of students by leveraging a graph drawing algorithm to reduce the visual clutter in the class scene. We compared the performance of our prototype with some commercially available user interfaces for VE classrooms. In our study, ObserVAR has demonstrated improvement and flexibility in several application scenarios.},
  keywords={Visualization;Avatars;Task analysis;Two dimensional displays;Collaboration;Augmented reality;Monitoring;Visualization;Remote Collaboration;VR for Education;Asymmetric Interaction},
  doi={10.1109/ISMAR.2019.00023},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943602,
  author={Popovici, Irina and Vatavu, Radu-Daniel},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Understanding Users' Preferences for Augmented Reality Television}, 
  year={2019},
  volume={},
  number={},
  pages={269-278},
  abstract={We examine users' preferences for Augmented Reality for television and report findings from an exploratory study with 172 participants we conducted to understand the perceived value of twenty distinct AR-TV scenarios. We connect our findings to participants' overall perceptions of and experience with AR technology as well as to their self-reported television watching behavior. Our results reveal high perceived value for AR-TV scenarios involving interactive content, wall-sized and room-sized video projections, multiple perspectives of the same movie scene, and for virtual objects coming out of the TV screen into the room. Other scenarios, such as live video of remote friends watching the same broadcast or multiple virtual channels displayed around the TV screen were rated less valuable.},
  keywords={Augmented reality;Smart TV;Real-time systems;Entertainment industry;Prototypes;Motion pictures;augmented reality;television;study;iTV;questionnaire},
  doi={10.1109/ISMAR.2019.00024},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943748,
  author={Xu, Wenge and Liang, Hai-Ning and He, Anqi and Wang, Zifan},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Pointing and Selection Methods for Text Entry in Augmented Reality Head Mounted Displays}, 
  year={2019},
  volume={},
  number={},
  pages={279-288},
  abstract={Augmented reality (AR) is on the rise with consumer-level head-mounted displays (HMDs) becoming available in recent years. Text entry is an essential activity for AR systems, but it is still relatively underexplored. Although it is possible to use a physical keyboard to enter text in AR systems, it is not the most optimal and ideal way because it confines the uses to a stationary position and within indoor environments. Instead, a virtual keyboard seems more suitable. Text entry via virtual keyboards requires a pointing method and a selection mechanism. Although there exist various combinations of pointing+selection mechanisms, it is not well understood how well suited each combination is to support fast text entry speed with low error rates and positive usability (regarding workload, user experience, motion sickness, and immersion). In this research, we perform an empirical study to investigate user preference and text entry performance of four pointing methods (Controller, Head, Hand, and Hybrid) in combination with two input mechanisms (Swype and Tap). Our research represents a first systematic investigation of these eight possible combinations. Our results show that Controller outperforms all the other device-free methods in both text entry performance and user experience. However, device-free pointing methods can be usable depending on task requirements and users' preferences and physical condition.},
  keywords={Keyboards;Resists;Performance evaluation;Augmented reality;User experience;Task analysis;Handheld computers;Augmented Reality;Text Entry;User Performance;User Preference;Pointing Methods;Selection Mechanisms},
  doi={10.1109/ISMAR.2019.00026},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943750,
  author={Dudley, John and Benko, Hrvoje and Wigdor, Daniel and Kristensson, Per Ola},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Performance Envelopes of Virtual Keyboard Text Input Strategies in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={289-300},
  abstract={Virtual and Augmented Reality deliver engaging interaction experiences that can transport and extend the capabilities of the user. To ensure these paradigms are more broadly usable and effective, however, it is necessary to also deliver many of the conventional functions of a smartphone or personal computer. It remains unclear how conventional input tasks, such as text entry, can best be translated into virtual and augmented reality. In this paper we examine the performance potential of four alternative text entry strategies in virtual reality (VR). These four strategies are selected to provide full coverage of two fundamental design dimensions: i) physical surface association; and ii) number of engaged fingers. Specifically, we examine typing with index fingers on a surface and in mid-air and typing using all ten fingers on a surface and in mid-air. The central objective is to evaluate the human performance potential of these four typing strategies without being constrained by current tracking and statistical text decoding limitations. To this end we introduce an auto-correction simulator that uses knowledge of the stimulus to emulate statistical text decoding within constrained experimental parameters and use high-precision motion tracking hardware to visualise and detect fingertip interactions. We find that alignment of the virtual keyboard with a physical surface delivers significantly faster entry rates over a mid-air keyboard. Also, users overwhelmingly fail to effectively engage all ten fingers in mid-air typing, resulting in slower entry rates and higher error rates compared to just using two index fingers. In addition to identifying the envelopes of human performance for the four strategies investigated, we also provide a detailed analysis of the underlying features that distinguish each strategy in terms of its performance and behaviour.},
  keywords={Keyboards;Tracking;Layout;Decoding;Augmented reality;Performance evaluation;Indexes;virtual reality;text entry;head mounted display},
  doi={10.1109/ISMAR.2019.00027},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943627,
  author={Lages, Wallace and Li, Yuan and Lisle, Lee and Höllerer, Tobias and Bowman, Doug},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhanced Geometric Techniques for Point Marking in Model-Free Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={301-309},
  abstract={Specifying points in three-dimensional (3D) space is an essential function in many augmented reality (AR) applications. When an environment model is not available, a straightforward solution is to perform geometric triangulation using two rays. However, naïve implementations suffer from low accuracy caused by technical limitations of AR devices and human motor constraints. To overcome these issues, we designed and evaluated two enhanced geometric techniques for 3D point marking. VectorCloud uses multiple rays to reduce the effects of pointing jitter, and ImageRefinement improves the accuracy by allowing users to refine the 3D direction of the two rays. We conducted studies to understand the characteristics of these techniques in both ecologically valid outdoor settings using a mobile AR display and in more controlled setting using virtual reality simulation. Our experiments demonstrate that both techniques improve the precision of 3D point marking, and that ImageRefinement is superior to VectorCloud overall. These results are particularly relevant in the design of mobile AR systems intended for use in large outdoor areas.},
  keywords={Three-dimensional displays;Solid modeling;Biological system modeling;Augmented reality;Head;Estimation;Computational modeling;Point marking;Augmented Reality;user interface design},
  doi={10.1109/ISMAR.2019.00028},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943611,
  author={Moore, Alec and Kodeih, Marwan and Singhania, Anoushka and Wu, Angelina and Bashir, Tassneen and McMahan, Ryan},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Importance of Intersection Disambiguation for Virtual Hand Techniques}, 
  year={2019},
  volume={},
  number={},
  pages={310-317},
  abstract={Some of the most widely used selection techniques for extended reality (XR) are based on virtual hand interactions. Many existing XR frameworks provide this functionality by default; however, their implementation can differ in slight, but important ways. When preparing to make a selection with a virtual hand technique, a user's desired selection can potentially be ambiguous due to multiple intersections. Systems with varying underlying virtual hand implementations may yield contrasting selections due to resolving multiple intersections differently. This is particularly an issue when objects are smaller in size than the virtual hand representation and in dense environments. To demonstrate the importance of these differences, we present a virtual hand selection study comparing three methods that are currently used in popular XR frameworks for disambiguating selections: Closest Intersected, First Intersected, and Last Intersected. The results of our study show that the Closest Intersected method affords significantly faster selections, significantly fewer incorrect and missed selections, and yields significantly better effective throughput than the other two methods. These results show that using a framework's built-in selection technique can significantly affect an XR application's usability.},
  keywords={Task analysis;Three-dimensional displays;Grasping;X reality;Visualization;Taxonomy;Manuals;Intersection disambiguation;virtual hand;selection},
  doi={10.1109/ISMAR.2019.00029},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943763,
  author={Lee, Hyeopwoo and Kim, Hyejin and Monteiro, Diego Vilela and Goh, Youngnoh and Han, Daseong and Liang, Hai-Ning and Yang, Hyun Seung and Jung, Jinki},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Annotation vs. Virtual Tutor: Comparative Analysis on the Effectiveness of Visual Instructions in Immersive Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={318-327},
  abstract={In this paper we present a comparative study of visual instructions in Immersive Virtual Reality (IVR), i.e., annotation (ANN) that employs 3D texts and objects for instructions and virtual tutor (TUT) that demonstrates a task with a 3D character. The comparison is based on three tasks, maze escape (ME), stretching exercise (SE), and crane manipulation (CM), defined by the types of a unit instruction. We conducted an automated evaluation of user's memory recall performances (recall time, accuracy, and error) by mapping a sequence of user's behaviors and events as a string. Results revealed that ANN group showed significantly more accurate performance (1.3 times) in ME and time performance (1.64 times) in SE than TUT group, while no statistical main difference was found in CM. Interestingly, although ANN showed statistically shorter execution time, the recalling time pattern of TUT group showed a steep convergence after initial trial. The results can be used in the field in terms of informing designers of IVR on what types of visual instruction are best for different task purpose.},
  keywords={Task analysis;Visualization;Three-dimensional displays;Training;Annotations;Virtual reality;Cranes;Virtual reality;Evaluation;Visual guidance;Computer aided instruction;Human-computer interaction},
  doi={10.1109/ISMAR.2019.00030},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943767,
  author={Souchet, Alexis and Philippe, Stéphanie and Ober, Floriane and Lévêque, Aurélien and Leroy, Laure},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating Cyclical Stereoscopy Effects Over Visual Discomfort and Fatigue in Virtual Reality While Learning}, 
  year={2019},
  volume={},
  number={},
  pages={328-338},
  abstract={Purpose: It is hypothesized that cyclical stereoscopy (displaying stereoscopy or 2D cyclically) has effect over visual fatigue, learning curves and quality of experience, and that those effects are different from regular stereoscopy. Materials and Methods: 59 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD). Participants were randomly assigned to 3 groups: HMD with regular stereoscopy (S3D) and HMD with cyclical stereoscopy (cycles of 1 or 3 minutes). Participants played the game thrice (third try on a PC one month later). Visual discomfort, Flow, Presence, were measured with questionnaires. Visual Fatigue was assessed pre-and post-exposure with optometric measures. Learning traces were obtained in-game. Results: Visual discomfort and flow are lower with cyclical-S3D than S3D but not Presence. Cyclical stereoscopy every 1 minute is more tiring than stereoscopy. Cyclical stereoscopy every 3 minutes tends to be more tiring than stereoscopy. Cyclical stereoscopy groups improved during Short-Term Learning. None of the statistical tests showed a difference between groups in either Short-Term Learning or Long-Term Learning curves. Conclusion: cyclical stereoscopy displayed cyclically had a positive impact on Visual Comfort and Flow, but not Presence. It affects oculomotor functions in a HMD while learning with a serious game with low disparities and easy visual tasks. Other visual tasks should be tested, and eye-tracking should be considered to assess visual fatigue during exposure. Results in ecological conditions seem to support models suggesting that activating cyclically stereopsis in a HMD is more tiring than maintaining it.},
  keywords={Visualization;Fatigue;Task analysis;Resists;Games;Interviews;Imaging;virtual reality;serious game;visual fatigue;head mounted display;cyclical stereoscopy},
  doi={10.1109/ISMAR.2019.00031},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943730,
  author={Vargas González, Andrés and Koh, Senglee and Kapalo, Katelynn and Sottilare, Robert and Garrity, Patrick and Billinghurst, Mark and LaViola, Joseph},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Comparison of Desktop and Augmented Reality Scenario Based Training Authoring Tools}, 
  year={2019},
  volume={},
  number={},
  pages={339-350},
  abstract={This work presents a study that explores the differences between authoring Scenario-Based Training (SBT) simulation content using Augmented Reality (AR) and a Desktop interface. Through an iterative design process two interface conditions were developed and then evaluated qualitatively and quantitatively. Our conceptual model is a graph based visualization that is presented to help designers understand the scenario learning artifacts and relationships. Our major contribution relies on the comparison made between the two authoring tools (AR, Desktop) with the same capabilities. Results show that no significant difference was found in time taken to complete tasks nor on the perceived usability of the systems. However, as expected the Desktop interface was perceived as more efficient. Based on these findings, insights on future directions for building AR immersive authoring tools are provided.},
  keywords={Tools;Training;Visualization;Authoring systems;Augmented reality;Task analysis;Three-dimensional displays;Augmented Reality;Desktop;Scenario Based Training;Web;Usability Study;Programming/Development Support},
  doi={10.1109/ISMAR.2019.00032},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943688,
  author={Collins, Jonny and Regenbrecht, Holger and Langlotz, Tobias and Said Can, Yekta and Ersoy, Cem and Butson, Russell},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Measuring Cognitive Load and Insight: A Methodology Exemplified in a Virtual Reality Learning Context}, 
  year={2019},
  volume={},
  number={},
  pages={351-362},
  abstract={Recent improvements of Virtual Reality (VR) technology have enabled researchers to investigate the benefits VR may provide for various domains such as health, entertainment, training, and education. A significant proportion of VR system evaluations rely on perception-based measures such as user pre-and post-questionnaires and interviews. While these self-reports provide valuable insights into users' perceptions of VR environments, recent developments in digital sensors and data collection techniques afford researchers access to measures of physiological response. This work explores the merits of physiological measures in the evaluation of emotional responses in virtual environments (ERVE). We include and place at the center of our ERVE methodology emotional response data by way of electrodermal activity and heart-rate detection which are analyzed in conjunction with event-driven data to derive further measures. In this paper, we present our ERVE methodology together with a case study within the context of VR-based learning in which we derive measures of cognitive load and moments of insight. We discuss our methodology, and its potential for use in many other application and research domains to provide more in-depth and objective analyses of experiences within VR.},
  keywords={Time measurement;Physiology;Emotional responses;Task analysis;Usability;Biomedical monitoring;Virtual environments;Virtual Reality;Interactive Learning Environments;Methodology},
  doi={10.1109/ISMAR.2019.00033},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943733,
  author={Palmas, Fabrizio and Cichor, Jakub and Plecher, David A. and Klinker, Gudrun},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Acceptance and Effectiveness of a Virtual Reality Public Speaking Training}, 
  year={2019},
  volume={},
  number={},
  pages={363-371},
  abstract={Virtual Reality (VR) has been gaining importance due to its numerous advantages as an immersive technology for learning applications. Next to being used in fields like manufacturing, transportation, communication, retail and real estate, it has also increased in significance for human resources development. Virtual human training programs offer exposure to difficult situations without supervision in a safe and controlled environment, as they simulate nuanced interpersonal situations and therefore allow users to gain new skills and apply them to real-life situations. Furthermore, VR training engages employees in a training session by being presented with a realistic situation designed to challenge and engage them. Practising in a controlled virtual environment also allows for easy and objective measurements of user development and the identification of strengths and weaknesses. In this paper, we evaluate if there is acceptance for practising a presentation in a VR-Speech Training (VR-ST) session using six Degrees of Freedom (6DoF) and if the 44 participants of this study improved, from a subjective point of view, valuable soft skills needed to give a convincing presentation in real life. We observed a positive tendency in the acceptance and effectiveness of the VR-ST.},
  keywords={Training;Public speaking;Games;Engines;Virtual environments;Atmospheric measurements;Virtual Reality;Virtual Training;Leadership;Social Presence;Uncanny Valley;Soft Skills;Experimental Study;Public Speaking},
  doi={10.1109/ISMAR.2019.00034},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{8943582,
  author={},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Author index}, 
  year={2019},
  volume={},
  number={},
  pages={373-376},
  abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
  keywords={},
  doi={10.1109/ISMAR.2019.00065},
  ISSN={1554-7868},
  month={Oct},}
