@INPROCEEDINGS{10765453,
  author={Plümer, Jan Hendrik and Yu, Kevin and Eck, Ulrich and Kalkofen, Denis and Steininger, Philipp and Navab, Nassir and Tatzgern, Markus},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={XR Prototyping of Mixed Reality Visualizations: Compensating Interaction Latency for a Medical Imaging Robot}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Researching novel user experiences in medicine is challenging due to limited access to equipment and strict ethical protocols. Extended Reality (XR) simulation technologies offer a cost-and time-efficient solution for developing interactive systems. Recent work has shown Extended Reality Prototyping (XRP)’s potential, but its applicability to specific domains like controlling complex machinery needs further exploration. This paper explores the benefits and limitations of XRP in controlling a mobile medical imaging robot. We compare two XR visualization techniques to reduce perceived latency between user input and robot activation. Our XRP validation study demonstrates its potential for comparative studies, but identifies a gap in modeling human behavior in the analytic XRP validation framework.},
  keywords={Visualization;Ethics;Protocols;Extended reality;Interactive systems;Mixed reality;Behavioral sciences;Machinery;Robots;Biomedical imaging;Extended reality;prototyping;product development;human-robot interaction;mixed reality;latency visualization},
  doi={10.1109/ISMAR62088.2024.00014},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765495,
  author={Kundu, Ripan Kumar and Hoque, Khaza Anuarul},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Preserving Personal Space: Differentially Private Cybersickness Detection in Immersive Virtual Reality Environments}, 
  year={2024},
  volume={},
  number={},
  pages={11-20},
  abstract={Cybersickness is a common problem that users often encounter during virtual reality (VR) experiences. Several automated methods exist based on machine learning (ML)/deep learning (DL) to detect cybersickness. However, the sensitive nature of data used by these ML/DL models (e.g., eye-tracking, head-tracking, etc.) introduces significant privacy risks since adversaries could exploit this data to infer and leak sensitive personal information, track individuals, or manipulate user experiences. Our research seeks to address this gap, underscoring the necessity for a private approach to cybersickness detection to protect user privacy and ensure a better VR experience. Thus, this paper proposes a privacy-preserving mechanism for DL-enabled cybersickness detection modeled. Specifically, we employ differential privacy (DP) to develop four private DL cybersickness detection models: long short-term memory (LSTM), grated recurrent unit (GRU), convolutional neural network (CNN), and multilayer perceptron (MLP) using Simulations 2021 and Gameplay, two open-source datasets. Our proposed models show high cybersickness detection accuracy for the proposed private cybersickness models. For instance, the private LSTM model shows the cybersickness detection accuracy of up to 92% and 91% for the Simulations 2021 and Gameplay datasets, respectively. Our experimental results also exhibit the privacy-preserving nature of private cybersickness detection. For instance, the private LSTM model reduces the membership inference attack’s success rate by up to 32% and 45% for the Simulations 2021 and Gameplay datasets compared to the baseline/non-private LSTM model for the same datasets.},
  keywords={Headphones;Solid modeling;Privacy;Accuracy;Cybersickness;Machine learning;Gaze tracking;Multilayer perceptrons;Convolutional neural networks;Long short term memory;Cybersickness;Private Cybersickness Detection;Deep Learning;Differential Privacy;Memebership Inference Attack},
  doi={10.1109/ISMAR62088.2024.00015},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765409,
  author={Bremer, Gianni and Lappe, Markus},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Predicting Locomotion Intention using Eye Movements and EEG with LSTM and Transformers}, 
  year={2024},
  volume={},
  number={},
  pages={21-30},
  abstract={Predicting future locomotion based on intrinsic data serves many purposes, including optimizing the utilization of physical space in virtual reality environments and enhancing the control of electronic aids for patients with motor impairments. However, predicting human locomotion intentions proves challenging due to the inherent difficulty arising from the highly complex and nonlinear interactions among the relevant parameters. Deep neural networks offer a significant advantage over conventional approaches in addressing this challenge. We treat this task as a time series prediction problem and compare LSTM networks to transformer models. A distinctive aspect of our work is our approach’s emphasis on eye movements as a central feature, contributing to its novel predictive capabilities. Besides gaze data, we evaluate the addition of EEG as a data source for this prediction task to be used in brain-computer interfaces. To achieve this, we conducted two data collection experiments in custom virtual environments that feature different tasks utilizing joystick control. We present these novel datasets in conjunction with this work. The results demonstrate that gaze data proves to be a valuable tool for locomotion prediction in different contexts, even when there is not a strong and direct connection between gaze and future waypoints. Transformer models were able to achieve better performance than LSTM networks, and we conclude that successful prediction across diverse situations requires datasets containing a wide range of movement scenarios.},
  keywords={Solid modeling;Target tracking;Soft sensors;Time series analysis;Virtual environments;Predictive models;Transformers;Brain modeling;Electroencephalography;Long short term memory;Virtual Reality;Eye Tracking;Eye-Tracking;Locomotion;LSTM;Transformer;Path Prediction;Machine Learning;Deep Learning;Gaze},
  doi={10.1109/ISMAR62088.2024.00016},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765405,
  author={Ceyssens, Jeroen and van Deurzen, Bram and Ruiz, Gustavo Rovelo and Luyten, Kris and Fiore, Fabian Di},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Art of Timing: Effects of AR Guidance Timing on Speed Control}, 
  year={2024},
  volume={},
  number={},
  pages={31-40},
  abstract={Augmented Reality (AR) holds significant potential to facilitate users in executing manual tasks. For effective support, however, we need to understand how showing movement instructions in AR affects how well people can follow those movements in real life. In this paper, we examine the degree to which users can synchronize the speed of their movements with speed cues presented through an AR environment. Specifically, we investigate the effects of timing in AR visual guidance. We assess performance using a highly realistic Mixed Reality (MR) welding simulation. Welding is a task that requires very precise timing and control over hand and arm motion. Our results show that upfront visual guidance (before manual task execution) alone often fails to transfer the knowledge of intended speeds, especially at higher target speeds. Live guidance (during manual task execution) during the activity provides more accurate speed results but typically requires a higher overshoot at the start. Optimal outcomes occur when visual guidance appears upfront and continues during the activity for users to follow through.},
  keywords={Training;Visualization;Welding;Velocity control;User centered design;Mixed reality;Manuals;Motors;Synchronization;Augmented reality;Mixed reality;Augmented reality;Interaction design;Motion control},
  doi={10.1109/ISMAR62088.2024.00017},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765383,
  author={Nelson, Michael G. and Yang, Fu-Chia and Koilias, Alexandros and Anagnostopoulos, Christos-Nikolaos and Mousas, Christos},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Avoiding Virtual Characters: The Effects of Proximity and Gesture}, 
  year={2024},
  volume={},
  number={},
  pages={41-50},
  abstract={We explored how study participants interacted with virtual characters in a virtual reality study. Specifically, we developed a 3 (proximity: close vs. middle vs. far) $\times 2$ (gesture: passive vs. active) experimental design (N = 26) to understand how combinations of proximity between two virtual characters and gestures assigned to them influence study participants’ self-reported ratings (co-presence, attentional allocation, behavioral interdependence, emotional reactivity, and perceived politeness). We also examined their avoidance movements (duration, trajectory length, and speed) and their avoidance decisions (passing through/around and minimum distance side). We collected both survey responses and our participants’ trajectories. Our study revealed that 1) the proximity factor impacted how our participants rated their co-presence and behavioral interdependence, as well as whether they decided to pass through or around the virtual characters, and 2) the gesture factor impacted how participants rated their behavioral interdependence, emotional reactivity, perceived politeness, and also affected their duration, trajectory length, and speed. Our research contributes to understanding personal space and social norms in virtual environments, offering valuable insights for virtual reality developers on the importance of social dynamics in designing interactions with virtual characters.},
  keywords={Surveys;Virtual environments;Trajectory;Resource management;Augmented reality;Virtual Reality;Virtual Characters;Avoidance Behavior;Proximity;Gesture},
  doi={10.1109/ISMAR62088.2024.00018},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765446,
  author={Han, Sangyoon and Ahn, Jaehyeok and Choi, Seungmoon},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Generating Haptic Motion Effects for General Scenes to Improve 4D Experiences}, 
  year={2024},
  volume={},
  number={},
  pages={51-60},
  abstract={Motion effects are vital for enhancing 4D experiences in various applications, e.g., rides, films, and games, by physically moving the user. Recent research introduced algorithms for generating effective motion effects from the audiovisual stream, but these approaches either require object motion trajectories or miss important scene components, such as other objects, particles, and camera motion. This paper proposes a fully automatic algorithm synthesizing motion effects from all scene components. Our method leverages computer vision technologies to capture salient movements among multiple components, enabling the creation of holistic motion effects without the need for human intervention. Then, our method computes a motion proxy representing the movements of the scene components by compressing the captured movements into a singlepoint motion. The motion proxy is converted into a motion command through a motion cueing algorithm and delivered to the user. The results of a user study show that our algorithm can generate compelling motion effects that enhance the 4D experience better than semi-manually authored effects. Our approach can facilitate the production of captivating motion effects for many applications.},
  keywords={Computer vision;Films;Production;Games;Cameras;Trajectory;Haptic interfaces;Computational efficiency;Data mining;Object tracking;4D;mulsemedia;haptics;motion effects;motion cueing;vestibular;automatic generation},
  doi={10.1109/ISMAR62088.2024.00019},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765444,
  author={Hiroi, Yuichi and Hiraki, Takefumi and Itoh, Yuta},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={FactoredSweeper: Optical See-Through Display Integrating Light Attenuation and Addition with Single Spatial Light Modulator}, 
  year={2024},
  volume={},
  number={},
  pages={61-70},
  abstract={Light Attenuation Displays (LADs), a subset of Optical See-Through Head-Mounted Displays (OST-HMDs), enable image display in bright environments by filtering incident light at the pixel level. Although recent methods have proposed single-DMD light attenuation, they do not consider additive color display and background compensation, limiting their applicability in real-world scenarios. We present FactoredSweeper, a single digital micromirror device (DMD) system that incorporates both light attenuation and addition. By synchronizing the DMD, color filter, and light source, our system generates an additive virtual image, light attenuation, and occlusion through time multiplexing. To produce the target image while compensating for the background, we optimize time-multiplexed binary DMD patterns and LED/color filter schedules using perceptually-driven non-negative matrix factorization. Simulations and prototypes demonstrate that our integrated attenuation-addition single-SLM system achieves superior dynamic range and perceptual image quality compared to conventional occlusion-capable OST-HMDs using grayscale occlusion masks.},
  keywords={Optical filters;Optical polarization;Image color analysis;Optical switches;Lighting;Optical attenuators;Prototypes;Attenuation;Optical imaging;Optimization;Light attenuation display;see-through display;augmented reality;time-multiplexing;perceptual-driven matrix factorization},
  doi={10.1109/ISMAR62088.2024.00020},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765468,
  author={Kim, Hyunjeong and Lee, In-Kwon},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Is 3DGS Useful?: Comparing the Effectiveness of Recent Reconstruction Methods in VR}, 
  year={2024},
  volume={},
  number={},
  pages={71-80},
  abstract={Recent advances in 3D object reconstruction have been remarkable, and 3D object reconstruction methods capable of real-time rendering are crucial for the creation of real-world content. Most reconstruction research has focused on improving algorithmic performance (e.g., rendering time, and visual quality). However, we need to know which reconstruction method can improve the user experience when used in real-world content, and whether the experience differs across platforms. In this study, we investigate the effects of three different visualization methods, including two real-time reconstruction methods (3DGS and Image-to-3D) and video playback, on user recognition memory and experience on two different platforms (VR and PC). The results show that different visualization methods improve recognition memory and user experience differently and that there are differences in the effects across platforms. In addition, we investigate designers’ views on 3D object visualization techniques and discuss how they can be used in actual content creation and their scalability. The results of this study suggest that it is possible to improve user experience and recognition memory by recommending different methods depending on the visualization perspective and platform used.},
  keywords={Visualization;Three-dimensional displays;Reconstruction algorithms;Streaming media;Rendering (computer graphics);User experience;Real-time systems;Usability;Image reconstruction;Guidelines;3D Object Visualization;Gaussian Splatting;Image-to-3D;Neural Rendering;3D Reconstruction;Object Recognition;User Experience;Virtual Reality},
  doi={10.1109/ISMAR62088.2024.00021},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765161,
  author={Cheng, Yi Fei and Heller, Laurie M. and Cho, Stacey and Lindlbauer, David},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={First or Third-Person Hearing? A Controlled Evaluation of Auditory Perspective on Embodiment and Sound Localization Performance}, 
  year={2024},
  volume={},
  number={},
  pages={81-90},
  abstract={Virtual Reality (VR) allows users to flexibly choose the perspective through which they interact with a synthetic environment. Users can either adopt a first-person perspective, in which they see through the eyes of their virtual avatar, or a third-person perspective, in which their viewpoint is detached from the virtual avatar. Prior research has shown that the visual perspective affects different interactions and influences core experiential factors, such as the user’s sense of embodiment. However, there is limited understanding of how auditory perspective mediates user experience in immersive virtual environments. In this paper, we conducted a controlled experiment $(N=24)$ on the effect of the user’s auditory perspective on their performance in a sound localization task and their sense of embodiment. Our results showed that when viewing a virtual avatar from a third-person visual perspective, adopting the auditory perspective of the avatar may increase agency and self-avatar merging, even when controlling for variations in task difficulty caused by shifts in auditory perspective. Additionally, our findings suggest that differences in auditory perspective generally have a smaller effect than differences in visual perspective. We discuss the implications of our empirical investigation of audio perspective for designing embodied auditory experiences in VR.},
  keywords={Location awareness;Visualization;Avatars;Merging;Virtual environments;Auditory system;User experience;Augmented reality;Human-centered computing;Human computer interaction (HCI);Empirical Studies in HCI},
  doi={10.1109/ISMAR62088.2024.00022},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765452,
  author={Cheymol, Antonin and Fribourg, Rebecca and Lécuyer, Anatole and Normand, Jean-Marie and Argelaguet, Ferran},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Avatar-Centered Feedback: Dynamic Avatar Alterations Can Induce Avoidance Behaviors to Virtual Dangers}, 
  year={2024},
  volume={},
  number={},
  pages={91-100},
  abstract={Abstract One singularity of VR is its capacity to generate a strong sensation of being present in a dangerous environment, without risking the physical consequences. However, this absence of consequences when interacting with virtual dangers might also limit the induction of realistic responses, such as avoidance behaviors, which are a key factor of various VR applications (e.g., training, journalism, or exposure therapies). To address this limitation, we propose avatar-centered feedback, a novel, device-free approach, consisting of dynamically altering the avatar’s appearance and movements to offer coherent feedback from interactions with the virtual environment, including dangers. To begin with, we present a design space clarifying the range of potential implementations for this approach. Then, we tested this approach in a metallurgy scenario, where participants’ virtual hands would redden and display burns as they got closer to a virtual fire (appearance alteration), and simulate short withdrawal reflexes movements when a spark burst next to them (movement alteration). Our results show that in comparison to a control group, participants receiving avatar-centered feedback demonstrated significantly more avoidance behaviors to the virtual fire. Interestingly, we also found that experiencing avatar-centered feedback of fire significantly increased avoidance behaviors toward a following danger of different nature (a circular saw). These results suggest that avatar-centered feedback can also impact the general perception of the avatar vulnerability to the virtual environment.},
  keywords={Training;Avatars;Virtual environments;Medical treatment;Journalism;Fatigue;Metallurgy;Sparks;Augmented reality;: Virtual Reality;Embodiment;Perception},
  doi={10.1109/ISMAR62088.2024.00023},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765459,
  author={Kang, Seoyoung and Nguyen, Anh and Yoon, Boram and Kim, Kangsoo and Woo, Woontack},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Gender Differences in Perceiving Avatar Face and Interpersonal Distance: Exploring Realism and Social Presence in Mixed Reality}, 
  year={2024},
  volume={},
  number={},
  pages={101-110},
  abstract={Understanding gender differences in facial and spatial recognition is crucial for enhancing avatar-mediated communication. However, there remains a gap in understanding how participant gender influences perceptions of avatar facial expressions and spatial dynamics in Mixed Reality communication. Therefore, our study investigates how avatar non-verbal cues interact with gender differences to affect user experience and understanding in MR environments. To examine these complex relationships, we conducted a user study comparing the effects of various avatar facial expressions (Full, Mouth-Only, and Emotion-based) and interpersonal distances (Closer vs. Farther) on facial animation realism and social presence, with a focus on gender-balanced participant groups. Our findings revealed that female participants were particularly sensitive to the avatar’s proximity and facial expressions, reporting significantly higher perceptions of facial animation realism, copresence, message understanding, and affective understanding at farther distances compared to male participants. They also perceived higher copresence and message understanding when exposed to emotion-based facial expressions, as opposed to a mouth-only condition-a distinction not observed among male participants. Based on our findings, we advocate for avatar design strategies that accommodate gender differences in perception and preference, potentially through customizable levels of expressiveness to cater to diverse user needs and contexts.},
  keywords={Sensitivity;Avatars;Face recognition;Mixed reality;User experience;Facial animation;Augmented reality;Avatars;Facial Expression;Interpersonal Distance;Mixed Reality;Gender Effect;Realism;Social Presence},
  doi={10.1109/ISMAR62088.2024.00024},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765492,
  author={Zhao, Guanghan and Orlosky, Jason and Kiyokawa, Kiyoshi and Uranishi, Yuki},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={GlanXR: A Hands-Free Fast Switching System for Virtual Screens}, 
  year={2024},
  volume={},
  number={},
  pages={111-119},
  abstract={To date, virtual and augmented reality technologies enable users to view multiple, large virtual screens in their workspaces. However, users must frequently rotate their heads to shift focus among these screens. This paper presents GlanXR, a fast and robust handsfree approach for screen switching in virtual reality. GlanXR incorporates a peripheral interface that remains fixed within the user’s view, in which screens can be dynamically selected based on the user’s eye-head position beyond an adaptive range. Additionally, the user triggers the switch to the screen chosen by making an opposing head rotation in the direction of the eye-head position to minimize false triggers. We conducted an experiment including a fast-switching scenario and a working simulation scenario with 24 participants to assess the effectiveness of GlanXR as compared to a baseline (taskbar), an expansive multi-screen setup, and a gazebased screen selection method. The results indicate that GlanXR facilitates precise screen-switching, minimizes the necessity for head rotation, and allows users to maintain a neutral head position.},
  keywords={Switching systems;Switches;Augmented reality;Gaze;Eye-Head Coordination;Hands-Free Interaction;Screen Switching},
  doi={10.1109/ISMAR62088.2024.00025},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765410,
  author={Gao, Hong and Huai, Haochuan and Yildiz-Degirmenci, Sena and Bannert, Maria and Kasneci, Enkelejda},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DataliVR: Transformation of Data Literacy Education through Virtual Reality with ChatGPT-Powered Enhancements}, 
  year={2024},
  volume={},
  number={},
  pages={120-129},
  abstract={Data literacy is essential in today’s data-driven world, emphasizing individuals’ abilities to effectively manage data and extract meaningful insights. However, traditional classroom-based educational approaches often struggle to fully address the multifaceted nature of data literacy. As education undergoes digital transformation, innovative technologies such as Virtual Reality (VR) offer promising avenues for immersive and engaging learning experiences. This paper introduces DataliVR, a pioneering VR application aimed at enhancing the data literacy skills of university students within a contextual and gamified virtual learning environment. By integrating Large Language Models (LLMs) like ChatGPT as a conversational artificial intelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides personalized learning assistance, enriching user learning experiences. Our study employed an experimental approach, with chatbot availability as the independent variable, analyzing learning experiences and outcomes as dependent variables with a sample of thirty participants. Our approach underscores the effectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering data literacy skills. Moreover, our study examines the impact of the ChatGPT-based AI chatbot on users’ learning, revealing significant effects on both learning experiences and outcomes. Our study presents a robust tool for fostering data literacy skills, contributing significantly to the digital advancement of data literacy education through cutting-edge VR and AI technologies. Moreover, our research provides valuable insights and implications for future research endeavors aiming to integrate LLMs (e.g., ChatGPT) into educational VR platforms.},
  keywords={Electronic learning;Digital transformation;Large language models;Education;Data visualization;Learning (artificial intelligence);Data collection;Chatbots;User experience;Data mining;Virtual reality;data literacy;LLMs;ChatGPT;digital transformation;immersive learning},
  doi={10.1109/ISMAR62088.2024.00026},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765382,
  author={Lin, Jinghuai and Rack, Christian and Wienrich, Carolin and Latoschik, Marc Erich},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Usability, Acceptance, and Trust of Privacy Protection Mechanisms and Identity Management in Social Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={130-139},
  abstract={In social virtual reality (social VR), users are threatened by potential cybercrimes, such as identity theft, sensitive data breaches, and embodied harassment. These concerns are heightened by the increasing interest in the metaverse, the advancements in photorealistic 3D user reconstructions, and the rising incidents of online privacy violations. Designing secure social VR applications that protect users while enhancing their experience, acceptance and trust remains a challenge. This article investigates potential identity management solutions in social VR, and their impacts on usability and user acceptance. We developed a social VR prototype with novel and established countermeasures, including motion biometric verification, and conducted a study with 52 participants. Our findings reveal diverse preferences for identity management and underscore the importance of authenticity, autonomy, and reciprocity. Key findings include: passive verification is favored for pragmatic user experience, while active verification is preferred for its hedonic quality; continuous or periodic verification strengthens users’ confidence in their privacy; and while user awareness promotes authentic engagement, it may also diminish the willingness to disclose personal information. This research not only offers foundational insights into the evaluated scenarios and countermeasures, but also sheds light on the designs of more trustworthy and inclusive social VR applications.},
  keywords={Privacy;Three-dimensional displays;User centered design;Prototypes;User experience;Reliability;Protection;Usability;Research and development;Pragmatics;social virtual reality;metaverse;verification;authentication;privacy;identity;trust;usability;user study},
  doi={10.1109/ISMAR62088.2024.00027},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765369,
  author={Zhou, Yuqi and Popescu, Voicu},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Detectability of ETHD Position and Speed Redirection for VR Haptics}, 
  year={2024},
  volume={},
  number={},
  pages={140-149},
  abstract={Abstract An encountered-type haptic device (ETHD) moves a physical object to align it with the virtual object with which the user of a virtual reality application makes contact, providing haptic feedback. One limitation of ETHDs is their limited reachability due to mechanical constraints. One approach to extending the reachability of an ETHD involves redirection, achieved by altering the pose of the user’s body, hand, or handheld prop in the virtual world. While previous studies have quantified the detection thresholds of redirection in the context of stationary objects, ETHD’s present the opportunity and the need to study redirection in the context of dynamic objects. This paper presents a user study $(\mathrm{N}=25)$ with two experiments aimed at investigating whether dynamic object properties, such as direction and speed, significantly affect redirection detection thresholds. The first experiment finds that the mere presence of dynamic objects does not decrease the detection threshold. Consequently, previously measured detection thresholds remain applicable for ETHD reachability extension. However, the second experiment unveils a crucial relationship between the speed of dynamic objects and the original reachability of the ETHD. Although the virtual object can move $15 \mathrm{~cm} / \mathrm{s}$ faster than the ETHD, this increase in speed is insufficient to compensate for the extended reachability enabled by the detection threshold.},
  keywords={Haptic interfaces;Augmented reality;Virtual Reality;Haptics;Redirection;ETHD},
  doi={10.1109/ISMAR62088.2024.00028},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765163,
  author={Chiossi, Francesco and Weiss, Yannick and Steinbrecher, Thomas and Mai, Christian and Kosch, Thomas},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mind the Visual Discomfort: Assessing Event-Related Potentials as Indicators for Visual Strain in Head-Mounted Displays}, 
  year={2024},
  volume={},
  number={},
  pages={150-159},
  abstract={When using Head-Mounted Displays (HMDs), users may not always notice or report visual discomfort by blurred vision through unadjusted lenses, motion sickness, and increased eye strain. Current measures for visual discomfort rely on users’ self-reports those susceptible to subjective differences and lack of real-time insights. In this work, we investigate if Electroencephalography (EEG) can objectively measure visual discomfort by sensing Event-Related Potentials (ERPs). In a user study ($\mathrm{N}=20$), we compare four different levels of Gaussian blur in a user study while measuring ERPs at occipito-parietal EEG electrodes. The findings reveal that specific ERP components (i.e., P1, N2, and P3) discriminated discomfort-related visual stimuli and indexed increased load on visual processing and fatigue. We conclude that time-locked brain activity can be used to evaluate visual discomfort and propose EEG-based automatic discomfort detection and prevention tools.},
  keywords={Visualization;Head-mounted displays;Current measurement;Prevention and mitigation;Real-time systems;Electroencephalography;User experience;Sensors;Strain;Lenses;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/ISMAR62088.2024.00029},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765484,
  author={Jing, Allison and Teo, Theophilus and McDade, Jeremy and Zhang, Chenkai and Wang, Yi and Mitrofan, Andrei and Thareja, Rushil and Shin, Heesook and Lee, Yongho and Gil, Youn-Hee and Billinghurst, Mark and Lee, Gun A.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Superpowering Emotion Through Multimodal Cues in Collaborative VR}, 
  year={2024},
  volume={},
  number={},
  pages={160-169},
  abstract={Representing emotion in collaborative Virtual Reality (VR) environments is an emerging topic, as VR can enable humans to express augmented emotions beyond their normal abilities. This research explores how emotion can be represented in collaborative VR beyond facial expressions. We developed a virtual system that communicates emotion using three sensory modalities (textual, auditory and visual) through two spatiotemporal representations (human-form avatar and superpower). We show real-time emotion through a natural avatar and objectify emotion states into superpower phenomena in in-situ environments for time periods. We incorporated subjective, physiological and behavioural measures to evaluate emotion in an asynchronous VR collaboration scenario. The results suggested that showing emotions through the avatar and superpower augmentations (audio-visual) provided the best immersive VR experience, where users were more aroused, and the positive emotion felt more dominating. We also found that understanding emotion requires easy and relatable visuals that people commonly acknowledge, whereas arousing emotion requires a change of environmental contexts to indicate different states. We provide design insights for using multisensory modalities in empathic VR systems to address the lack of a standardised representation of emotion in collaborative VR.},
  keywords={Visualization;Avatars;Collaboration;Real-time systems;Physiology;Spatiotemporal phenomena;Augmented reality;empathic computing;Virtual Reality;Emotion},
  doi={10.1109/ISMAR62088.2024.00030},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765494,
  author={Acevedo, Pedro and Choi, Minsoo and Magana, Alejandra J. and Benes, Bedrich and Mousas, Christos},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Immersion and Dimensionality in Virtual Reality Science Simulations: The Case of Charged Particles}, 
  year={2024},
  volume={},
  number={},
  pages={170-179},
  abstract={Researchers have provided insights into using virtual reality (VR) for visualization and interaction with 3D models and simulations. The interaction allows users to manipulate the 3D elements and visualize changes based on their inputs from movement with controllers or spatial actions. However, some users may find this interaction overwhelming, especially when immersed in a virtual environment. Additionally, the choice of dimensionality for visualizations influences user interaction, with potential implications for immersive experiences. Thus, we conducted a 2 (Immersion: Desktop vs. HMDVR) $\times 2$ (Dimensionality: 2 D vs. 3 D) within-group study $(N=32)$ to explore the impact of the utilized immersive degree and the dimensionality representation of the content on participants’ experience in terms of engagement, task load, usability, skill, and emotions when interacting with a science simulation. We designed and developed an application to simulate charged particles and electric field lines. We asked participants to complete a task of changing particles by matching them to a given simulation output. Our results indicated higher workload rates for HMDVR conditions, particularly with 3D representation, compared to Desktop. However, HMDVR conditions also showed greater engagement, emotional response, and presence. Based on our findings, we argue that participants prefer HMDVR over Desktop environments regardless of dimensionality.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Computational modeling;Virtual environments;Emotional responses;Usability;Electric fields;Augmented reality;Load modeling;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Modeling and simulation-Simulation types and techniques-Interactive simulation;Human-centered computing-Visualization-Visualization application domainsScientific visualization},
  doi={10.1109/ISMAR62088.2024.00031},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765462,
  author={Wang, Qidi J. and Moore, Alec G. and Chawla, Nayan N. and McMahan, Ryan P.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Cross-Domain Gender Identification Using VR Tracking Data}, 
  year={2024},
  volume={},
  number={},
  pages={180-189},
  abstract={Recently, much work has been done to research personal identifiability of extended reality (XR) users. Many of these prior studies are task-specific and involve identifying users completing a specific XR task. On the other hand, some studies have been domainspecific and focus on identifying users completing different XR tasks from the same domain, such as watching 360° videos or assembling structures. In this paper, we present one of the few studies to investigate cross-domain identification (i.e., identifying users completing XR tasks from different domains). To facilitate our investigation, we used open-source datasets from two different virtual reality (VR) studies-one from an assembly domain and one from a gaming domain-to investigate the feasibility of cross-domain gender identification, as personal identification is not possible between these datasets. The results of our machine learning experiments clearly demonstrate that cross-domain gender identification is more difficult than domain-specific gender identification. Furthermore, our results indicate that head position is important for gender identification and demonstrate that the k-nearest neighbors (kNN) algorithm is not suitable for cross-domain gender identification, which future researchers should be aware of.},
  keywords={Training;Machine learning algorithms;Extended reality;Machine learning;Nearest neighbor methods;Magnetic heads;Augmented reality;Assembly;Videos;Testing;VR;machine learning;gender identification},
  doi={10.1109/ISMAR62088.2024.00032},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765472,
  author={Schieber, Hannah and Li, Shiyu and Corell, Niklas and Beckerle, Philipp and Kreimeier, Julian and Roth, Daniel},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation}, 
  year={2024},
  volume={},
  number={},
  pages={190-199},
  abstract={In medical and industrial domains, providing guidance for assembly processes can be critical to ensure efficiency and safety. Errors in assembly can lead to significant consequences such as extended surgery times and prolonged manufacturing or maintenance times in industry. Assembly scenarios can benefit from in-situ augmented reality visualization, i.e., augmentations in close proximity to the target object, to provide guidance, reduce assembly times, and minimize errors. In order to enable in-situ visualization, 6D pose estimation can be leveraged to identify the correct location for an augmentation. Existing 6D pose estimation techniques primarily focus on individual objects and static captures. However, assembly scenarios have various dynamics, including occlusion during assembly and dynamics in the appearance of assembly objects. Existing work focus either on object detection combined with state detection, or focus purely on the pose estimation. To address the challenges of 6D pose estimation in combination with assembly state detection, our approach ASDF builds upon the strengths of YOLOv8, a real-time capable object detection framework. We extend this framework, refine the object pose, and fuse pose knowledge with network-detected pose information. Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection. By combining both pose and state information, our Pose2State module predicts the final assembly state with precision. The evaluation of our ASDF dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation. Moreover, on the GBOT dataset, we outperform the pure deep learning-based network and even outperform the hybrid and pure tracking-based approaches.},
  keywords={Knowledge engineering;Visualization;Pose estimation;Surgery;Object detection;Real-time systems;Safety;Maintenance;Assembly;Augmented reality;6D pose estimation;assembly state detection;synthetic data},
  doi={10.1109/ISMAR62088.2024.00033},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765439,
  author={Kou, Simin and Zhang, Fang-Lue and Lai, Yu-Kun and Dodgson, Neil A.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Neural Panoramic Representation for Spatially and Temporally Consistent 360° Video Editing}, 
  year={2024},
  volume={},
  number={},
  pages={200-209},
  abstract={Content-based 360° video editing allows users to manipulate panoramic content for interaction in a dynamic visual world. However, the current related methods (2D neural representation and optical flow) show limitations in producing high-quality panoramic content from 360° videos due to their lack of capacity to model the inherent spatiotemporal relationships among pixels in the true panoramic space. To address this issue, we propose a Neural Panoramic Representation (NPR) method to model the global inter-pixel relationships, facilitating immersive video editing. Specifically, our method utilizes MLP-based networks to learn spherical implicit content layers, by encoding the spherical spatiotemporal positions and appearance details within the panoramic video, and bi-directional mapping between the original video frames and the learned content layers, to capture the interpretable and global omnidirectional visual characteristics of individual dynamic scenes. Additionally, we introduce innovative loss functions (spherical neighborhood consistency and unit spherical regularization) to ensure the creation of appropriate implicit spherical content layers. We further provide an interactive layer neural panoramic editing approach based on the proposed NPR, in the head-mounted display device. We evaluate this framework on diverse real-world 360° videos, showing superior performance on both reconstruction and consistent editing compared to existing state-of-the-art (SOTA) neural representation techniques.},
  keywords={Performance evaluation;Visualization;Head-mounted displays;Neural networks;Bidirectional control;Predictive models;Encoding;Spatiotemporal phenomena;Optical flow;Manipulator dynamics;Virtual reality;immersive video editing;360° video manipulation;spherical content modeling},
  doi={10.1109/ISMAR62088.2024.00034},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765407,
  author={Ai, Letian and Liu, Yihao and Armand, Mehran and Martin-Gomez, Alejandro},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Calibration of Augmented Reality Headset with External Tracking System Using AX=YB}, 
  year={2024},
  volume={},
  number={},
  pages={210-218},
  abstract={In Augmented Reality, a robust virtual-to-real calibration that aligns the virtual and real spaces is crucial to ensure accurate overlays. Inspired by popular methods in robotics, we propose establishing the virtual-to-real calibration as a hand-eye/robot-world calibration problem using an $A X=Y B$ formulation. This formulation uses both the self-localization of a head-mounted display and the tracking functionality of an external tracking system. Additional techniques are also provided to address the data synchronization issue between the two measurement systems. To further improve the results, we integrate a post-acquisition outlier filter based on the $A X-Y B$ Frobenius norm. Improvements resulting from the filter were first validated by simulation. For the assessment of the complete pipeline, both subjective evaluation based on human perception and objective evaluation based on computer vision methods were used. The results show that the proposed method is accurate and noise-resistant. With only 100 measurement samples collected, the overlay of a tracked object at the farthest distance (1300 mm) in front of the tracking system has an average rotation error of $1.77 \pm 0.54^{\circ}$ and an average translation error of $4.82 \pm 1.71 \mathrm{~mm}$.},
  keywords={Computer vision;Accuracy;Robot kinematics;Noise;Measurement uncertainty;Resists;Calibration;Synchronization;Rotation measurement;Augmented reality;Augmented Reality;Tracking Systems;Calibration},
  doi={10.1109/ISMAR62088.2024.00035},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765152,
  author={Hatira, Nour and Aliza, Aliza and Batmaz, Anil Ufuk and Sarac, Mine},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing Eye-Hand Coordination in Volleyball Players: A Comparative Analysis of VR, AR, and 2D Display Technologies and Task Instructions}, 
  year={2024},
  volume={},
  number={},
  pages={219-228},
  abstract={Previous studies analyzed user motor performance with Virtual Reality (VR) and Augmented Reality (AR) Eye-Hand Coordination Training Systems (EHCTSs) while asking participants to follow specific task instructions. Although these studies suggested VR & AR EHCTSs as potential training systems for sports players, they recruited participants for their user studies among general population. In this paper, we examined the training performance of 16 professional volleyball players over 8 days using EHCTSs with three display technologies (VR, AR, and 2D touchscreen) and with four distinct task instructions (prioritizing speed, error rate, accuracy, or none). Our results indicate that volleyball players performed best with 2D touchscreen in terms of time, error rate, accuracy, precision, and throughput. Moreover, their performance was superior when using VR over AR. They also successfully followed the task instructions given to them and consistently improved their throughput performance. These findings underscore the potential of EHCTS in volleyball training and highlight the need for further research to optimize VR & AR user experience and performance.},
  keywords={Training;Accuracy;Error analysis;Two-dimensional displays;Touch sensitive screens;Throughput;Motors;User experience;Augmented reality;Sports;Virtual Reality;Augmented Reality;Eye-Hand Coordination;Sports training;Display technologies;Task instructions},
  doi={10.1109/ISMAR62088.2024.00036},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765408,
  author={Guo, Zixuan and Deng, Hanxiao and Wang, Hongyu and Tan, Angel J. Y. and Xu, Wenge and Liang, Hai-Ning},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Impact of Passthrough on VR Exergaming in Public Environments: A Field Study}, 
  year={2024},
  volume={},
  number={},
  pages={229-238},
  abstract={Sedentary behavior is becoming increasingly prevalent in daily work and study environments. VR exergaming has emerged as a promising solution in these places of work and study. However, private spaces in these environments are not easy, and engaging in VR exergaming in public settings presents its own set of challenges (e.g., safety, social acceptance, isolation, and privacy protection). The recent development of Passthrough functionality in VR headsets allows users to maintain awareness of their surroundings, enhancing safety and convenience. Despite its potential benefits, little is known about how Passthrough could affect user performance and experience and solve the challenges of playing VR exergames in real-world public environments. To our knowledge, this work is the first to conduct a field study in an underground passageway on a university campus to explore the use of Passthrough in a realworld public environment, with a disturbance-free closed room as a baseline. Results indicate that enabling Passthrough in a public environment improves performance without compromising presence. Moreover, Passthrough can increase social acceptance, especially among individuals with higher levels of self-consciousness. These findings highlight Passthrough’s potential to encourage VR exergaming adoption in public environments, with promising implications for overall health and well-being.},
  keywords={Headphones;Privacy;Protection;Augmented reality;Virtual reality;exergaming;passthrough functionality;public environments;gameplay mechanics},
  doi={10.1109/ISMAR62088.2024.00037},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765400,
  author={Guo, Zixuan and Xu, Wenge and Wang, Hongyu and Wan, Tingjie and Baghaei, Nilufar and Lo, Cheng-Hung and Liang, Hai-Ning},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancement of Co-located Shared VR Experiences: Representing Non-HMD Observers on Both HMD and 2D Screens}, 
  year={2024},
  volume={},
  number={},
  pages={239-248},
  abstract={Virtual reality (VR) not only allows head-mounted display (HMD) users to immerse themselves in virtual worlds but also to share them with others. When designed correctly, this shared experience can be enjoyable. However, in typical scenarios, HMD users are isolated by their devices, and non-HMD observers lack connection with the virtual world. To address this, our research investigates visually representing observers on both HMD and 2D screens to enhance shared experiences. The study, including five representation conditions, reveals that incorporating observer representation positively impacts both HMD users and observers. For how to design and represent them, our work shows that HMD users prefer methods displaying real-world visuals, while observers exhibit diverse preferences regarding being represented with real or virtual images. We provide design guidelines tailored to both displays, offering valuable insights to enhance co-located shared VR experiences for HMD users and non-HMD observers.},
  keywords={Visualization;Head-mounted displays;Design methodology;Resists;Interference;Observers;Augmented reality;Virtual reality;mixed reality;co-located shared VR;shared spaces},
  doi={10.1109/ISMAR62088.2024.00038},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765443,
  author={McCracken, Maggie K. and Finney, Hunter C. and Yang, Serena and Bodenheimer, Bobby and Creem-Regehr, Sarah H. and Stefanucci, Jeanine K.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Big Feet for Little People: Scaling Gap Affordance Judgments of Children and Adults with Virtual Feet}, 
  year={2024},
  volume={},
  number={},
  pages={249-258},
  abstract={Virtual reality (VR) has become widely accessible through the development of more commercially available head-mounted displays (HMDs). This accessibility has particularly increased the use of VR in children. However, much of the previous research on understanding perception and action in virtual reality has only been conducted on adults, leaving many open questions about how children perceive and interact with virtual environments. In this paper, we examine whether there are age-related differences in judging the ability to step over a gap using immersive VR. Affordances are a useful measure for understanding objective perceptions of the actions that can be performed in an immersive virtual environment. Such measures are particularly well suited for children given they can easily respond yes or no as to whether they perceive that they can step over a gap. Further, manipulations of the size of virtual body parts could allow us to ascertain how much children rely on the perceived size of their bodies to make decisions about actions. In Experiment 1, adults and children saw motion-tracked virtual feet that were either larger or smaller than their actual foot size. They had to respond as to whether they could step over gaps that varied in width. They also gave perceptual estimates of the width of the gap in feet or meters. The results showed that adults who experience the smaller virtual feet underestimated their stepping ability more than adults with the larger feet. However, children had the opposite effect, such that seeing smaller virtual feet led to an overestimation of their stepping ability. To test whether this age-related difference in body scaling was due to misperception of foot size, adults and children matched virtual shoes to their actual feet size in Experiment 2. This matching task showed no perceptual differences between the age groups. Across our two experiments, we showed that children scale gap affordance judgments differently than adults and this difference cannot be explained by difference in perceptions of the size of virtual feet. The results suggest that children can effectively make perception and action judgments in virtual reality, but do not always do so in the same manner as adults. Such a finding has implications for the design of virtual reality games, educational tools, and training systems that are becoming increasingly common for children.},
  keywords={Training;Meters;Head-mounted displays;Affordances;Virtual environments;Footwear;Games;Augmented reality;Foot;Affordances;children;virtual environments},
  doi={10.1109/ISMAR62088.2024.00039},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765406,
  author={Hajahmadi, Shirin and Stacchio, Lorenzo and Giacché, Alessandro and Cascarano, Pasquale and Marfia, Gustavo},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating eXtended Reality-powered Digital Twins for Sequential Instruction Learning: the Case of the Rubik’s Cube}, 
  year={2024},
  volume={},
  number={},
  pages={259-268},
  abstract={Educational practices are increasingly experimenting with eXtended Reality (XR) paradigms to offer novel opportunities for boundaryless learning experiences with real-time interactions in immersive environments. Digital Twins (DT) are also gaining traction in this field to facilitate personalized learning experiences. However, a still unexplored space in learning frameworks amounts to the one where XR intersects with DTs. This work wants to move a step in such a direction with the design, implementation, and test of a DT-driven XR framework to learn procedural tasks. The framework offers three distinct learning modalities where virtual and physical interactions enhance learning retention by engaging users actively in digital and real-world environments. We contextualize such a framework for procedural task learning through one of its pivotal use cases: learning Rubik’s Cube notations. To evaluate and compare the effectiveness of these modalities, we perform an experimental user campaign evaluating short-term skill retention, performance accuracy, usability, and cognitive load of each of them. We then provide an extensive statistical analysis to compare each kind of guidance while analyzing correlations between the examined variables, offering insights into optimizing instructional methodologies within XR-based educational frameworks.},
  keywords={Visualization;Accuracy;Correlation;Statistical analysis;Extended reality;Cognitive load;Real-time systems;Digital twins;Sensors;Usability;eXtended Reality;Digital Twin;Training Tools;Sequential Learning},
  doi={10.1109/ISMAR62088.2024.00040},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765456,
  author={Monty, Samantha and Kern, Florian and Latoschik, Marc Erich},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Analysis of Immersive Mid-Air Sketching Behavior, Sketch Quality, and User Experience in Design Ideation Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={269-278},
  abstract={Immersive 3D sketching systems empower users with tools to create sketches directly in the air around themselves, in all three dimensions, using only simple hand gestures. These sketching systems have the potential to greatly extend the interactive capabilities of immersive learning environments. The perceptual challenges of Virtual Reality (VR), however, combined with the ergonomic and cognitive challenges of creating mid-air 3D sketches reduce the effectiveness of immersive sketching used for problem-solving, reflection, and to capture fleeting ideas. We contribute to the understanding of the potential challenges of mid-air sketching systems in educational settings, where expression is valued higher than accuracy, and sketches are used to support problem-solving and to explain abstract concepts. We conducted an empirical study with 36 participants with different spatial abilities to investigate if the way that people sketch in mid-air is dependent on the goal of the sketch. We compare the technique, quality, efficiency, and experience of participants as they create 3D mid-air sketches in three different tasks. We examine how users approach mid-air sketching when the sketches they create serve to convey meaning and when sketches are merely reproductions of geometric models created by someone else. We found that in tasks aimed at expressing personal design ideas, between starting and ending strokes, participants moved their heads more and their controllers at higher velocities and created strokes in faster times than in tasks aimed at recreating 3D geometric figures. They reported feeling less time pressure to complete sketches but redacted a larger percentage of strokes. These findings serve to inform the design of creative virtual environments that support reasoning and reflection through mid-air sketching. With this work, we aim to strengthen the power of immersive systems that support midair 3D sketching by exploiting natural user behavior to assist users to more quickly and faithfully convey their meaning in sketches.},
  keywords={Human computer interaction;Three-dimensional displays;Accuracy;Geometric modeling;Virtual environments;Reflection;User experience;Planning;Problem-solving;Immersive learning;: Human computer interaction (HCI);Interaction design;Visualization;Real-Time Systems;Software Creation and Management;Virtual Reality;3D Immersive Sketching;Mid-Air Sketching;Evaluating 3D Sketching},
  doi={10.1109/ISMAR62088.2024.00041},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765460,
  author={Lopez Garcia, Irene and Schott, Ephraim and Gohsen, Marcel and Bernhard, Volker and Stein, Benno and Froehlich, Bernd},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Speaking with Objects: Conversational Agents’ Embodiment in Virtual Museums}, 
  year={2024},
  volume={},
  number={},
  pages={279-288},
  abstract={Conversational agents in virtual environments are an established approach for immersively conveying the information and narratives of museums and cultural heritage while expanding their accessibility to a wider and remote audience. The rapid development of large language models and text-to-speech technologies has raised the agents’ conversational level significantly, which allows their use for proactive guidance of visitors. This raises the vital question of how such agents should be visually represented to promote Knowledge transfer in immersive virtual environments. In this paper, we compared two representation concepts for agent embodiments in the context of a virtual museum by examining a stylized humanoid guide and a novel animism-based approach that enables users to talk to exhibited objects. Our work addresses the challenge of naturally introducing a virtual educational environment to users and encouraging their interest and engagement with the content. A user study $(N=29)$ revealed high usability and similar presence scores for the experience with each of the embodiments. A majority of participants showed a preference for the animated objects. In terms of user experience, they evoked significant stimulation and high levels of engagement. Our results suggest that agents that show emotions through appropriate word choice influence engagement levels. Based on our findings, we recommend humanoid guides for delivering general background information, while animated objects promote detailed questions about their own stories and a more stimulating exchange.},
  keywords={Visualization;Humanoid robots;Virtual environments;Mouth;Switches;Oral communication;Virtual museums;User experience;Text to speech;Usability;Virtual Reality;VR;Agent Embodiment;Conversational Agent;Virtual Museum;Virtual Tour},
  doi={10.1109/ISMAR62088.2024.00042},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765423,
  author={Liu, Jiazhou and Satriadi, Kadek Ananta and Ens, Barrett and Dwyer, Tim},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating the Effects of Physical Landmarks on Spatial Memory for Information Visualisation in Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={289-298},
  abstract={Augmented Reality (AR) is touted to be beneficial in supporting situated information display, allowing virtual information panels to be overlaid on real-world scenes. People must then use their spatial memory to navigate among these virtual panels effectively. While spatial memory has been studied in physical environments (wall displays) and virtual reality environments, there has been little research on how physical surroundings might affect memorisation of virtual content in a mixed environment like AR. Therefore, we provide the first AR study of spatial memory, comparing two different room settings with two different situated layouts of virtual targets on an abstract spatial memory task. We find that participants recall spatial patterns with greater accuracy and higher subjective ratings in a room with furniture compared to an empty room. Our findings lead to important design implications for mixed-reality user interfaces, particularly in information-rich applications like situated analytics and small-multiples information visualisation.},
  keywords={Visualization;Accuracy;Navigation;Layout;Memory management;User interfaces;Augmented reality;spatial memory;immersive analytics;view management;physical landmark;augmented reality;mixed reality},
  doi={10.1109/ISMAR62088.2024.00043},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765392,
  author={Brandstätter, Klara and Congdon, Ben J. and Steed, Anthony},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Do you read me? (E)motion Legibility of Virtual Reality Character Representations}, 
  year={2024},
  volume={},
  number={},
  pages={299-308},
  abstract={We compared the body movements of five virtual reality (VR) avatar representations in a user study $(\mathrm{N}=53)$ to ascertain how well these representations could convey body motions associated with different emotions: one head-and-hands representation using only tracking data, one upper-body representation using inverse kinematics (IK), and three full-body representations using IK, motioncapture, and the state-of-the-art deep-learning model AGRoL. Participants’ emotion detection accuracies were similar for the IK and AGRoL representations, highest for the full-body motion-capture representation and lowest for the head-and-hands representation. Our findings suggest that from the perspective of emotion expressivity, connected upper-body parts that provide visual continuity improve clarity, and that current techniques for algorithmically animating the lower-body are ineffective. In particular, the deep-learning technique studied did not produce more expressive results, suggesting the need for training data specifically made for social VR applications.},
  keywords={Legged locomotion;Solid modeling;Visualization;Emotion recognition;Accuracy;Tracking;Avatars;Training data;Motion capture;Data models;Avatars;Character Representations;Virtual Reality;Emotion Recognition},
  doi={10.1109/ISMAR62088.2024.00044},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765397,
  author={Kelesbekov, Ulan and Marini, Gabriele and Bai, Zhongyi and Johal, Wafa and Velloso, Eduardo and Knibbe, Jarrod},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Stuet: Dual Stewart Platforms for Pinch Grasping Objects in VR}, 
  year={2024},
  volume={},
  number={},
  pages={309-318},
  abstract={Complex 3D shapes’ surfaces can be characterised using three shape descriptors: zeroth-order for rendering width; first-order to convey slope; and second-order for curvature. These shapes can be symmetric or asymmetric. To date, controllers in VR have been unable to render these properties in 3D. We present Stuet - a handheld VR controller that can render complex asymmetrical 3D objects for two-finger grasping and shape exploration. Stuet leverages dual 3 degrees of freedom (3-DOF) Stewart Platforms. This enables the contact plates for the fingers to be controlled individually, rendering object widths up to 75 mm and individual plate angles up to 30° in any tilt direction with respect to the vertical plane. We present the design and implementation of Stuet. We explain and benchmark its mechanical capabilities, present the inverse kinematics model required for its use, and report on a feasibility demonstration. Our results reveal that dual Stewart platforms offer new capabilities for asymmetric, advanced haptic interactions in VR.},
  keywords={Three-dimensional displays;Shape;3-DOF;Grasping;Kinematics;Benchmark testing;Rendering (computer graphics);Servomotors;Motion control;Augmented reality;Haptics;Controllers},
  doi={10.1109/ISMAR62088.2024.00045},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765156,
  author={Lee, Juyoung and Oh, Seo Young and Baeck, Minju and Yeo, Hui Shyong and Kim, Hyung-Il and Starner, Thad and Woo, Woontack},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Whirling Interface: Hand-based Motion Matching Selection for Small Target on XR Displays}, 
  year={2024},
  volume={},
  number={},
  pages={319-328},
  abstract={We introduce “Whirling Interface,” a selection method for XR displays using bare-hand motion matching gestures as an input technique. We extend the motion matching input method, by introducing different input states to provide visual feedback and guidance to the users. Using the wrist joint as the primary input modality, our technique reduces user fatigue and improves performance while selecting small and distant targets. In a study with 16 participants, we compared the whirling interface with a standard ray casting method using hand gestures. The results demonstrate that the Whirling Interface consistently achieves high success rates, especially for distant targets, averaging 95.58% with a completion time of 5.58 seconds. Notably, it requires a smaller camera sensing field of view of only 21.45° horizontally and 24.7° vertically. Participants reported lower workloads on distant conditions and expressed a higher preference for the Whirling Interface in general. These findings suggest that the Whirling Interface could be a useful alternative input method for XR displays with a small camera sensing FOV or when interacting with small targets.},
  keywords={Wrist;Casting;Visualization;Accuracy;Impedance matching;Cameras;Fatigue;Sensors;Augmented reality;Standards;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR62088.2024.00046},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765377,
  author={Kim, DongHoon and Han, Dongyun and Bak, Siyeon and Cho, Isaac},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Crossing Rays: Evaluation of Bimanual Mid-air Selection Techniques in an Immersive Environment}, 
  year={2024},
  volume={},
  number={},
  pages={329-338},
  abstract={Mid-air navigation offers a method of aerial travel that mitigates the constraints associated with continuous navigation. A mid-air selection technique is essential to enable such navigation. In this paper, we consider four variations of intersection-based bimanual midair selection techniques with visual aids and supporting features: Simple-Ray, Simple-Stripe, Precision-Stripe, and Cursor-Sync. We evaluate their performance and user experience compared to an unimanual mid-air selection technique using two tasks that require selecting a mid-air position with or without a reference object. Our findings indicate that the bimanual techniques generally demonstrate faster selection times compared to the unimanual technique. With a supporting feature, the bimanual techniques can provide a more accurate selection than the unimanual technique. Based on our results, we discuss the effect of selection technique’s visual aids and supporting features on performance and user experience for mid-air selection.},
  keywords={Visualization;Accuracy;Navigation;Focusing;User experience;Usability;Augmented reality;Virtual Reality;Mid-Air Selection;Bi-manual Interaction},
  doi={10.1109/ISMAR62088.2024.00047},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765370,
  author={Dong, Tianyang and Kong, Hubin and Zhang, Huanbo and Lv, Shuqian and Zhou, Xin},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Intelligent Pre-Reset for Redirected Walking through Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={339-348},
  abstract={Redirected walking (RDW) algorithms subtly manipulates users’ movements in virtual environments through three gains, distorting their trajectories in the physical space, thus allowing them to explore infinite virtual environments within limited physical spaces. However, collisions with physical boundaries or obstacles in physical spaces are inevitable. At this point, explicit redirected techniques, such as reset, are needed to pause users’ roaming experience and redirect them to open areas. Currently, most reset strategies wait until a collision occurs before resetting the user, while these reset algorithms can achieve certain effectiveness in simple physical environments. However, in complex physical environments, users may walk into narrow areas before collisions occur. Therefore, no matter which direction they turn, further collisions are likely to happen over short distances. To enhance obstacle avoidance capability while adapting to various complex physical layouts, this paper adopts the concept of pre-reset and presents a novel method of intelligent pre-reset IPR for redirected walking through reinforcement learning, which determines whether an early reset should be performed and the optimal reset direction based on the current physical environment, as well as the user’s position and walking trajectory. Through simulation and human experiments compared with other reset algorithms in several environments, we demonstrate the superiority of our approach, achieving higher average walking distances between resets while adapting to various nuances of reset techniques. At the same time, IPR is applicable to various RDW algorithms and environments of different complexities.},
  keywords={Legged locomotion;Layout;Virtual environments;Reinforcement learning;Intellectual property;Trajectory;Complexity theory;Collision avoidance;Augmented reality;Virtual reality;redirected walking;pre-reset;reinforcement learning},
  doi={10.1109/ISMAR62088.2024.00048},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765421,
  author={Wan, Tingjie and Zhang, Liangyuting and Xu, Yunxin and Atkinson, Katie and Yu, Lingyun and Liang, Hai-Ning},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Design and Evaluation of Controller-based Raycasting Methods for Secure and Efficient Text Entry in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={349-358},
  abstract={With the exponential growth of digital information, ensuring text security, a fundamental component of information security, becomes increasingly paramount. While authentication remains a primary focus for data access control and protection, the rich sensor ecosystem and immersive experiences of virtual reality (VR) environments introduce new privacy risks, particularly with inconspicuous sensors like motion and location sensors. In this context, protecting the security of text entered by users poses a unique challenge. This paper explores the feasibility of enhancing text security by introducing variability in virtual input tools during typing processes. Specifically, we investigate the impact of introducing successive and random intermittent variations to the virtual ray (start point and direction) with controller-based raycasting techniques on text security and typing experience. The results demonstrate that introducing variability in virtual ray effectively protects regular text and passwords. Random intermittent introducing variability balances security and user experience for regular text. These findings provide insights into enhancing text security beyond authentication and defending against the potential risks in VR environments.},
  keywords={Data privacy;Ecosystems;Authentication;Information security;Passwords;User experience;Security;Protection;Usability;Augmented reality;Virtual reality;text entry;text security;keyboard layout;user study},
  doi={10.1109/ISMAR62088.2024.00049},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765450,
  author={Ma, Qixiang and Wu, Jian and Fan, Runze and Sun, Guodong and Shi, Xuehuai},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ViP-Fluid: Visual Perception Driven Method for VR Fluid Rendering}, 
  year={2024},
  volume={},
  number={},
  pages={359-367},
  abstract={The demand for fluid simulation and rendering in virtual reality (VR) is increasing. However, achieving high visual quality while maintaining real-time efficiency remains a challenge. Traditional foveated rendering methods balance the simulation quality in the foveated region but neglect the physical realism in the peripheral areas, and fail to account for the perceptual degradation caused by frame rate fluctuations during adaptive updates. To address these challenges, we propose a novel visual perception driven fluid rendering method ViP-Fluid, which further enhances rendering quality while balancing efficiency. Our approach employs a spatiotemporal saliency model for multi-granularity simulation and rendering of Lagrangian fluid systems, and introduces a Perception Threshold for Physical Process Elapsing (PTPE) metric, which guides our temporal acceleration strategy. Through a series of objective experiments, we demonstrate the advantages of our method in rendering quality and performance efficiency. ViP-Fluid demonstrates superior metrics not only in the foveated region but also in the salient and overall regions, achieving up to 2.15 times speed-up compared to the high-resolution Position Based Fluids (PBF) benchmark. Subsequent user experiments further validate the visual perception advantages of ViP-Fluid over both traditional and state-of-the-art methods, confirming the spatiotemporal fidelity of our acceleration strategy as well as a user preference for our approach.},
  keywords={Measurement;Degradation;Visualization;Solid modeling;Fluids;Fluctuations;Rendering (computer graphics);Real-time systems;Spatiotemporal phenomena;Visual perception;Virtual reality;fluid simulation;visual perception;foveated rendering;temporal acceleration},
  doi={10.1109/ISMAR62088.2024.00050},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765422,
  author={Valcamonica, Giulia and Vona, Francesco and Patti, Alberto and Garzotto, Franca},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Communication among people with Neurodevelopmental Disorder during Cooperative Play in Augmented and Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={368-375},
  abstract={The article investigates cooperative interaction in Virtual Reality (VR) and Augmented Reality (AR) for individuals with neurodevelopmental disorders (NDDs) and compares the potential of these interaction paradigms to enhance the communication skills of this population. Two versions of a cooperative game called SMUP (Social Match UP) were created in AR and VR, respectively, involving NDD specialists in the co-design process. During a within-subjects empirical study involving 30 subjects with NDD, we measured both the usability and verbal interaction of the two versions, known as SMUP-AR and SMUP-VR. The findings suggest that both SMUP-AR and SMUP-VR exhibited a good degree of usability for individuals with NDD, with no statistically significant differences between the two versions. However, the evaluation of communication skills, based on the analysis of the verbal production of users under the two experimental conditions, indicates that SMUP-AR had a stronger effect in promoting dialogic interaction compared to SMUP-VR. In addition, the paper discusses insights from the study on the UX design of cooperative AR and VR technology for people with NDD.},
  keywords={Games;Production;Oral communication;Market research;Usability;Augmented reality;Testing;Virtual Reality;Augmented Reality;Neurodevelopmental Disorder;Usability;Verbal Production},
  doi={10.1109/ISMAR62088.2024.00051},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765437,
  author={Javaheri, Hamraz and Ghamarnejad, Omid and Lukowicz, Paul and Stavrou, Gregor Alexander and Karolus, Jakob},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Design and Clinical Evaluation of ARAS: An Augmented Reality Assistance System for Open Pancreatic Surgery}, 
  year={2024},
  volume={},
  number={},
  pages={376-385},
  abstract={The integration of Augmented Reality (AR) technology into surgical procedures offers significant potential to enhance clinical outcomes. While there are plenty of lab-proven prototypes, systems employed in actual clinical settings require specialized design and rigorous clinical evaluation of these AR-based solutions to meet the high demands of complex medical fields. Our research exposes these complex requirements emerging from clinical environments, such as operation theaters. To address the challenges, we introduce ARAS, an operational AR assistance system for live open pancreatic surgery. Employing a user-centric design methodology, we designed and refined ARAS through several iterations, ensuring its practical applicability and effectiveness in a real-world surgical setting during clinical trials. ARAS enables in situ and precise visualization of the patient’s 3D reconstructed vascular system and tumor during the surgical procedure. We evaluated ARAS through clinical trials (N=7) involving patients diagnosed with pancreatic cancer. Our interviews with the surgeons underscored the utility of ARAS for open pancreatic surgery, especially in critical and highly time-pressured phases of the surgery, as it proved to be exceptionally beneficial in aiding surgeons during in their decision-making process. In a post-surgery evaluation, the surgeons also certified the precise visualization accuracy of ARAS during those critical phases. Our findings showcased the heightened requirements for AR-based solutions in operational clinical use and proved that ARAS met the challenges emerging during live surgeries. Consequently, surgical AR assistance systems do have a transformative potential to revolutionize traditional practices, but their applicability is subject to high design constraints during critical medical procedures.},
  keywords={Visualization;Accuracy;Three-dimensional displays;Decision making;Surgery;Prototypes;Clinical trials;Augmented reality;Medical diagnostic imaging;Tumors;Augmented Reality;Surgical Assistance System;Pancreatic Surgery},
  doi={10.1109/ISMAR62088.2024.00052},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765432,
  author={Wu, Muzhe and Cheng, Yi Fei and Lindlbauer, David},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={New Ears: An Exploratory Study of Audio Interaction Techniques for Performing Search in a Virtual Reality Environment}, 
  year={2024},
  volume={},
  number={},
  pages={386-395},
  abstract={Efficiently searching and navigating virtual scenes is essential for performing various downstream tasks and ensuring a positive user experience in VR. Prior VR interaction techniques for such scenarios predominantly rely on users’ visual perception, which contrasts with physical reality, where people typically rely on multimodal information, especially auditory cues, to guide their spatial awareness. In this work, we explore the potential of leveraging auditory interaction techniques to enhance spatial navigation in virtual environments. We drew inspiration from prior distant interaction techniques and developed four approaches to augmenting how users hear in the virtual environment: Audio Teleportation, Audio Cone, Ninja Ears, and Boom Mic. In a comparative user study (N = 25), we evaluated these approaches against a baseline Teleportation technique in a search task, where participants traversed a virtual environment to locate target items. Our results suggest that several of our audio interaction techniques may enable more efficient search behaviors while enhancing overall user experience. However, not all techniques were appreciated equally, suggesting that careful attention to their design is critical for ensuring their effectiveness. We conclude by discussing the potential implications of our results for future audio interaction technique designs.},
  keywords={Microwave integrated circuits;Visualization;Navigation;Virtual environments;Ear;Teleportation;User experience;Augmented reality;Visual perception;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/ISMAR62088.2024.00053},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765488,
  author={Ramirez-Saffy, G. Nikki and Chelladurai, Pratheep Kumar and Vargas, Alances and Bukhari, Ibrahim and Selinger, Evan and Foster, Shaun and Heller, Brittan and David-John, Brendan},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Visceral Interfaces for Privacy Awareness of Eye Tracking in VR}, 
  year={2024},
  volume={},
  number={},
  pages={396-405},
  abstract={Eye tracking is increasingly being integrated into virtual reality (VR) devices to support a wide range of applications. It is used as a method of interaction, to support performance optimizations, and to create adaptive trainingor narrative experiences. However, providing access to eye-tracking data also introduces the ability to monitor user activity, detect and classify a user’s biometric identity, or otherwise reveal sensitive information such as medical conditions. As this technology continues to evolve, users should be made aware of the amount of information they are sharing about themselves to developers and how it can be used. While traditional terms of service may relay this type of information, previous work indicates they are not accessibly conveying privacy-related information to users. Considering this problem, we suggest the application of visceral interfaces that are designed to inform users about eye-tracking data within the VR experience. To this end, we designed and conducted a user study on three visceral interfaces to educate users about their eye-tracking data. Our results suggest that while certain visualizations can be distracting, participants ultimately found them informative and supported the development and availability of such interfaces even if they are not enabled by default or always enabled. Our research contributes to developing informative interfaces specific to eye tracking that promote transparency and privacy awareness in data collection for VR.},
  keywords={Biometrics;Data privacy;Technological innovation;Medical conditions;Data visualization;Gaze tracking;Data collection;Relays;Optimization;Monitoring;privacy notice;virtual reality;eye tracking},
  doi={10.1109/ISMAR62088.2024.00054},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765390,
  author={Chang, Zhuang and Kim, Kangsoo and Gupta, Kunal and Abouelenin, Jamila and Xiao, Zirui and Gu, Boyang and Bai, Huidong and Billinghurst, Mark},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perceived Empathy in Mixed Reality: Assessing the Impact of Empathic Agents’ Awareness of User Physiological States}, 
  year={2024},
  volume={},
  number={},
  pages={406-415},
  abstract={In human-agent interaction, establishing trust and a social bond with the agent is crucial to improving communication quality and performance in collaborative tasks. This paper investigates how a Mixed Reality Agent’s (MiRA) ability to acknowledge a user’s physiological state affects perceptions such as empathy, social connectedness, presence, and trust. In a within-subject study with 24 subjects, we varied the companion agent’s awareness during a mixed-reality first-person shooting game. Three agents provided feedback based on the users’ physiological states: (1) No Awareness Agent (NAA), which did not acknowledge the user’s physiological state; (2) Random Awareness Agent (RAA), offering feedback with varying accuracy; and (3) Accurate Awareness Agent (AAA), which provided consistently accurate feedback. Subjects reported higher scores on perceived empathy, social connectedness, presence, and trust with AAA compared to RAA and NAA. Interestingly, despite exceeding NAA in perception scores, RAA was the least favored as a companion. The findings and implications for the design of MiRA interfaces are discussed, along with the limitations of the study and directions for future work.},
  keywords={Accuracy;Mixed reality;Collaboration;Games;Physiology;Augmented reality;Empathic computing;virtual agent;mixed reality;augmented reality;physiological state},
  doi={10.1109/ISMAR62088.2024.00055},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765428,
  author={Kim, Heeyeon and Choi, Seungmoon},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Expressing the Social Intent of Touch Initiator in Virtual Reality Using Multimodal Haptics}, 
  year={2024},
  volume={},
  number={},
  pages={416-425},
  abstract={Touch is crucial in communicating different social intents in our everyday lives. However, we lack methods to bring the capability of touch to express various social signals into VR. This paper aims to bridge this gap by exploring the effectiveness of haptic feedback in conveying the social intent of a touch initiator (a user touching a virtual agent). In User Study 1, we observe the touch gestures that users employ to express different social intents and collect the haptic feedback parameters appropriate for representing these touch gestures. User Study 2 analyzes how social intent-dependent multimodal (pressure, thermal, and texture) haptic feedback affects the touch initiator’s virtual social experience. The results indicate that haptic feedback representing social intent strengthens the social expression of touch and fosters emotional closeness with virtual agents. Combining it with a physical proxy makes the effects even more effective, enhancing copresence and avatar embodiment.},
  keywords={Human computer interaction;Shape;Avatars;Buildings;Solids;Market research;Physiology;Haptic interfaces;Frequency measurement;Thermal analysis;Human computer interaction (HCI);Social touch;Multimodal haptic feedback;Virtual reality (VR)},
  doi={10.1109/ISMAR62088.2024.00056},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765465,
  author={Wu, Dai-Rong and Duffrin, Tyler and Venkatakrishnan, Roshan and Venkatakrishnan, Rohith and Babu, Sabarish V. and Pagano, Christopher and Lin, Wen-Chieh},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Room Size Perception in Virtual Reality by Means of Sound and Vision: The Role of Perception-Action Calibration}, 
  year={2024},
  volume={},
  number={},
  pages={426-435},
  abstract={Spatial perception in virtual reality (VR) has been a hot research topic for years. Most of the studies on this topic have focused on visual perception and distance perception. Fewer have examined auditory perception and room size perception, although these aspects are important for improving VR experiences. Recently, a number of studies have shown that perception can be calibrated to information that is relevant to the successful completion of everyday tasks in VR (such as distance estimation and spatial perception). Also, some recent studies have examined calibration of auditory perception as a way to compensate for the classic distance compression problem in VR. In this paper, we present a calibration method for both visual and auditory room size perception. We conducted experiments to investigate how people perceive the size of a virtual room and how the accuracy of their size perception can be calibrated by manipulating perceptible auditory and visual information in VR. The results show that people were more accurate in perceiving room size by means of vision than in audition, but that they could still use audition to perceive room size. The results also show that during calibration, auditory room size perception exhibits learning effects and its accuracy was greatly improved after calibration.},
  keywords={Visualization;Accuracy;Virtual environments;Estimation;Size measurement;Market research;Calibration;Reliability;Floors;Visual perception;Auditory Room Size Perception;Calibration;VR},
  doi={10.1109/ISMAR62088.2024.00057},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765448,
  author={Wang, Yi and Ma, Jian and Shao, Ruizhi and Feng, Qiao and Lai, Yu-Kun and Li, Kun},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={HumanCoser: Layered 3D Human Generation via Semantic-Aware Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={436-445},
  abstract={This paper aims to generate physically-layered 3D humans from text prompts. Existing methods either generate 3D clothed humans as a whole or support only tight and simple clothing generation, which limits their applications to virtual try-on and partlevel editing. To achieve physically-layered 3D human generation with reusable and complex clothing, we propose a novel layer-wise dressed human representation based on a physically-decoupled diffusion model. Specifically, to achieve layer-wise clothing generation, we propose a dual-representation decoupling framework for generating clothing decoupled from the human body, in conjunction with an innovative multi-layer fusion volume rendering method. To match the clothing with different body shapes, we propose an SMPL-driven implicit field deformation network that enables the free transfer and reuse of clothing. Extensive experiments demonstrate that our approach not only achieves state-of-the-art layered 3D human generation with complex clothing but also supports virtual try-on and layered human animation. More results and the code can be found on our project page at https: //cic.tju.edu.cn/faculty/likun/projects/HumanCoser},
  keywords={Three-dimensional displays;Codes;Shape;Deformation;Clothing;Diffusion models;Animation;Rendering (computer graphics);Augmented reality;3D Human Generation;Layered Clothing;Physical Decoupling;Human Animation},
  doi={10.1109/ISMAR62088.2024.00058},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765496,
  author={Freeling, Benjamin and Neyret, Solène and Gervilla, Miguel and Lécuyer, Flavien and Capobianco, Antonio},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={“Not my fault!” “All thanks to me!”: Studying Agency, Satisfaction and Self-Serving Attributional Bias with Rigged Archery in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={446-454},
  abstract={User’s experience in virtual reality includes different cognitive, emotional, and behavioural factors. One key factor that characterises the interaction between humans and their environment is the sense of Agency. In virtual reality, the sense of Agency is defined as the sensation of having a virtual body which moves and acts accordingly to one’s intentions. However, it is usually measured with self-assessed questionnaires, focusing mainly on motor control within the virtual environment. We designed an experiment based on a game of rigged archery, manipulating avatar control and distant consequences of user’s actions. This allowed us to separately study both dimensions of Agency: the Feeling of Agency and the Judgement of Agency. Through this, we studied how they are linked with other psychological factors such as Locus of Control, Satisfaction and Self-Serving Attributional Bias. Our results suggest an influence of Internal Locus of Control on the Feeling of Agency. We observed that participant’s satisfaction levels was correlated with their assessment of Agency, and more prominently their Judgement of Agency. In addition, our results show that Feeling of Agency and Judgement of Agency can be understood as two distinct parameters of one’s subjective experience in virtual reality. Lastly, we observed that participants immersed in our virtual environment did show a Self-Serving Attributional Bias, as in real life. Indeed, they tended to claim authorship on successful actions while blaming failure on external factors. We believe that these results offer a better understanding of the different factors that could impact the sense of Agency in virtual reality.},
  keywords={Motor drives;Avatars;Virtual environments;Psychology;Focusing;Games;Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Perception;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI},
  doi={10.1109/ISMAR62088.2024.00059},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765154,
  author={Zhang, Fengze and Zhang, Yunxiang and Peng, Xi and Achitoff, Sky and Torrens, Paul M. and Sun, Qi},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={May the Force Be with You: Dexterous Finger Force-Aware VR Interface}, 
  year={2024},
  volume={},
  number={},
  pages={455-464},
  abstract={Advances in virtual reality (VR) have reduced experience differentials for users. However, gaps between reality and virtuality persist in tasks that require coupling users’ multimodal physical skills with virtual environments in delicate ways. User embodiment in VR easily breaks when physicality feels inauthentic, especially when users invoke their innate predilection to touch and manipulate things that they encounter. In this research, we examine the potential of forceaware VR interfaces for enabling natural connections to user physicality and evaluate them in high-finesse cases of touch. Combining surface electromyography (SEMG) with visual tracking, we develop an end-to-end learning-based system, ForceSense, to decode users’ dexterous finger forces from their forearm sEMG signals for direct usage in standard VR pipelines. This approach eliminates the need for hand-held tactile equipment, thereby promoting natural embodiment. A series of user studies on manipulation tasks in VR validate that ForceSense is more accurate, robust, and intuitive than alternative solutions. Two proofs-of-concept VR applications, calligraphy and piano playing, demonstrate that the good synergy between visual, auditory, and tactile modalities, as ForceSense affords, has the potential of enhancing users’ task learning performance in VR. Our source code and trained models are released at https://github.com/NYU-ICL/vr-force-aware-multimodal-interface.},
  keywords={Visualization;Solid modeling;Adaptation models;Accuracy;Source coding;Force;Pipelines;Virtual environments;Sensors;Standards;Force sensing;tactile perception;sEMG learning},
  doi={10.1109/ISMAR62088.2024.00060},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765476,
  author={Gemici, Mucahit and Stuerzlinger, Wolfgang and Batmaz, Anil Ufuk},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Object Speed Control with a Signed Distance Field for Distant Mid-Air Object Manipulation in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={465-474},
  abstract={In Virtual Reality (VR) applications, interacting with distant objects relies heavily on mid-air object manipulation. Yet, the inherent distance between the user and the object often restricts movement precision. This paper introduces the Signed Distance Field (SDF) method for mid-air object manipulation and combines it with the ray casting interaction technique to investigate its effect on user performance and user experience. To increase movement accuracy, we leverage the speed-accuracy trade-off to dynamically adjust object manipulation speed based on the SDF algorithm’s output. Our study with 18 participants examines the effects of SDF across three different tasks with different complexity. Our results showed that ray casting with SDF reduces the number of errors in complex tasks without slowing down the participants and improves the user experience. We hope that our proposed assistive system, designed for tasks and applications, can be used as an interaction technique to enable more accurate manipulation of distant objects in fields like surgical planning, architecture, and games.},
  keywords={Casting;Accuracy;Heuristic algorithms;Velocity control;Surgery;Games;User experience;Planning;Complexity theory;Usability;Signed Distance Field;Ray Casting;Mid-Air Manipulation;Accuracy;Error rate},
  doi={10.1109/ISMAR62088.2024.00061},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765466,
  author={Kim, You-Jin and Sra, Misha and Höllerer, Tobias},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Audience Amplified: Virtual Audiences in Asynchronously Performed AR Theater}, 
  year={2024},
  volume={},
  number={},
  pages={475-484},
  abstract={Audience reactions can considerably enhance live experiences; conversely, in anytime/anywhere augmented reality (AR) experiences, large crowds of people might not always be available to congregate. To get closer to simulating live events with large audiences, we created a mobile AR experience where users can wander around naturally and engage in AR theater with virtual audiences trained from real audiences using imitation learning. This allows us to carefully capture the essence of human imperfections and behavior in artificial intelligence (AI) audiences. The result is a novel mobile AR experience in which solitary AR users experience an augmented performance in a physical space with a virtual audience. Virtual dancers emerge from the surroundings, accompanied by a digitally simulated audience, to provide a community experience akin to immersive theater. In a pilot study, simulated human avatars were vastly preferred over just audience audio commentary. We subsequently engaged 20 participants as attendees of an AR dance performance, comparing a no-audience condition with a simulated audience of six onlookers. Through questionnaires and experience reports, we investigated user reactions and behavior. Our results demonstrate that the presence of virtual audience members caused attendees to perceive the performance as a social experience with increased interest and involvement in the event. On the other hand, for some attendees, the dance performances without the virtual audience evoked a stronger positive sentiment.},
  keywords={Humanities;Avatars;Imitation learning;Mixed reality;User experience;Behavioral sciences;Artificial intelligence;Augmented reality;Mobile augmented reality;human-centered computing;Empirical studies in HCI;Computing methodologies;Mixed / augmented reality. Artificial Intelligence-Mobile Agents},
  doi={10.1109/ISMAR62088.2024.00062},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765411,
  author={Xu, Chengyuan and Kumaran, Radha and Stier, Noah and Yu, Kangyou and Höllerer, Tobias},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI}, 
  year={2024},
  volume={},
  number={},
  pages={485-494},
  abstract={Seamless integration of virtual and physical worlds in augmented reality benefits from the system semantically “understanding” the physical environment. AR research has long focused on the potential of context awareness, demonstrating novel capabilities that leverage the semantics in the 3D environment for various object-level interactions. Meanwhile, the computer vision community has made leaps in neural vision-language understanding to enhance environment perception for autonomous tasks. In this work, we introduce a multimodal 3D object representation that unifies both semantic and linguistic knowledge with the geometric representation, enabling user-guided machine learning involving physical objects. We first present a fast multimodal 3D reconstruction pipeline that brings linguistic understanding to AR by fusing CLIP vision-language features into the environment and object models. We then propose “in-situ” machine learning, which, in conjunction with the multimodal representation, enables new tools and interfaces for users to interact with physical spaces and objects in a spatially and linguistically meaningful manner. We demonstrate the usefulness of the proposed system through two real-world AR applications on Magic Leap 2: a) spatial search in physical environments with natural language and b) an intelligent inventory system that tracks object changes over time. We also make our full implementation and demo data available at (https://github.com/cy-xu/spatially_aware_AI) to encourage further exploration and research in spatially aware AI.},
  keywords={Solid modeling;Computer vision;Three-dimensional displays;Semantics;Pipelines;Machine learning;Context awareness;Linguistics;Search problems;Augmented reality;Augmented reality;artificial intelligence;interactive machine learning;scene understanding},
  doi={10.1109/ISMAR62088.2024.00063},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765424,
  author={Lúcio, Inês M. and Raidou, Renata G. and Rodrigues, Pedro and Lopes, Daniel Simões},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Your Face, Your Anatomy: Flashcard Lenses Enriched with Knowledge Maps for Anatomy Education}, 
  year={2024},
  volume={},
  number={},
  pages={495-504},
  abstract={Traditional anatomy flashcards, with their recognizable static illustrations on the front side and comprehensive lists of concepts on the back, are a long-standing tool for memorizing and refreshing anatomical concepts. This study repurposes such established tool by introducing two key elements: (i) Augmented Reality (AR) lenses acting as magic mirrors enabling users to view anatomical illustrations mapped onto their own faces, and (ii) a knowledge map layer acting as the card’s backside to visually and explicitly illustrate conceptual connections between anatomical reference points. Using Snapchat’s Lens Studio, we crafted a deck of interactive facial anatomy flashcards to assess the potential of AR and knowledge maps for retaining and refreshing anatomical concepts. We conducted a user study involving 44 university-level students. Divided into two groups, participants utilized either flashcard lenses with knowledge maps or traditional flashcards to quickly grasp and refresh anatomical concepts. By employing an approach that integrates anatomical quizzes for objective assessment with surveys and interviews for subjective feedback, our results indicate that anatomy flashcard lenses with knowledge maps offer a more engaging educational experience, yielding higher user preferences and satisfaction levels compared to traditional flashcards. While both approaches showed similar effectiveness in quiz scores, anatomy flashcard lenses with knowledge maps were favored for their usability, significantly reducing temporal demand. These findings underscore the engaging and effective nature of anatomy flashcard lenses with knowledge maps, highlighting them as an alternative tool for the quick retention and review of anatomical concepts.},
  keywords={Surveys;Visualization;Social networking (online);Education;Anatomy;Web sites;Usability;Faces;Augmented reality;Lenses;Flashcards;anatomy education;mobile augmented reality;embodied learning;knowledge maps;Snapchat},
  doi={10.1109/ISMAR62088.2024.00064},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765436,
  author={Hu, Xuning and Yan, Xinan and Wei, Yushi and Xu, Wenxuan and Li, Yue and Liu, Yue and Liang, Hai-Ning},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Effects of Spatial Constraints and Curvature for 3D Piloting in Virtual Environments}, 
  year={2024},
  volume={},
  number={},
  pages={505-514},
  abstract={Piloting requires users to control and navigate the aircraft within a designated pathway, with a controller that utilizes two joysticks to control the aircraft. This task is representative of various daily and gaming scenarios, such as controlling the aircraft to capture the photo or navigating an object in a game from the start position to the end via a trajectory. In this work, we explore a model (based on the Steering Law) that predicts the piloting time required in spatial-constrained environments. Thus, two user studies are conducted to help us understand the relationship between path complexity (curvature) and spatial constraints (width and height). According to the results, we propose a model that can achieve $52.6 \%$ and $60.6 \%$ improvement in R-square and the Akaike Information Criterion (AIC), respectively. Next, an additional study was conducted to further verify the performance and efficiency of our proposed model with the change of movement direction and orientation. Our model and experimental results can benefit both game and interface designers of applications that require controlling moving objects along specific trajectories in virtual reality environments.},
  keywords={Solid modeling;Three-dimensional displays;Virtual environments;Games;Predictive models;Aircraft navigation;Trajectory;Complexity theory;Aircraft;Aerospace control;Virtual Reality;Game Controller;Steering Law;Graphical User Interfaces;Modeling;Navigation},
  doi={10.1109/ISMAR62088.2024.00065},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765489,
  author={Lin, Yilong and Xie, Tianze and Chang, Yingjie and Luo, Hu and Wang, Dangxiao and Je, Seungwoo},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VibroArm: Enhancing the Sensation of Forearm Deformation in Virtual Reality Using Vibrotactile Funneling Illusion}, 
  year={2024},
  volume={},
  number={},
  pages={515-524},
  abstract={When we enter Virtual Reality (VR) in the first person, the avatar replaces our real body. Within a certain range of mismatches, we tend to identify with our avatar and perceive it as our own body. Previous research has shown that we can even believe that a deformed forearm is our own through haptic and visual feedback. However, the haptic devices used in earlier research were only explored using skin-stretching and weight-shifting. Vibrotactile feedback is the most common technique used to create a haptic VR experience in academia and industry because of its light weight and ease of application. Taking these advantages, we introduce VibroArm, a lightweight wearable haptic system with the vibrotactile funneling illusion that enhances the virtual forearm’s ownership in the forearm deformation illusion. In a perceptual study, we determined that users can make correct direction judgments about vibration on the forearm in different directions. The result shows that all vibration patterns can be sufficiently recognized, with an average recognition rate of $87.17 \%$. Based on our results, we designed 18 vibration patterns and compared their impact on the forearm deformation illusion. Finally, our study shows that using VibroArm in VR applications can significantly improve user realism and enjoyment compared to relying on visual feedback alone.},
  keywords={Vibrations;Industries;Visualization;Deformation;Avatars;Haptic interfaces;Pattern recognition;Trajectory;Augmented reality;Body Ownership;Body Illusion;Vibrotactile Funneling Illusion;Virtual Reality},
  doi={10.1109/ISMAR62088.2024.00066},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765420,
  author={Yao, Yuexin and Li, Yue},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Textual Information Presentation in Virtual Museums: Exploring Environment-, Object-, and User-based Approaches}, 
  year={2024},
  volume={},
  number={},
  pages={525-533},
  abstract={In a physical museum, text descriptions are typically displayed on placards or signage next to exhibits. Within a virtual museum environment, these text descriptions can be presented in various ways, such as fixed in the virtual environment, attached to the exhibits, or held in users’ hands. By seamlessly integrating text descriptions into the virtual environment and allowing users to engage with the content, the information presentation can be highly interactive. However, the design space of artifact information presentation in virtual museums was under-explored. In this paper, we investigated appropriate ways to present text descriptions of artifacts in immersive virtual museums. Specifically, we studied (1) users’ perceived importance of various information dimensions (observable, non-observable, and interpretation), (2) users’ expected display of text panels (shown or hidden), and (3) the relationship between the artifact information dimensions and layout types (environment-, object-, and user-based). Our results showed that participants rated significantly higher importance for non-observable information than observable and interpretation information. In addition, we summarize a design space for artifact information presentation using different layout types with prioritized options. Our work provides insights for the interaction design of artifact information in virtual museums and the presentation of text in virtual reality.},
  keywords={Interactive systems;Design methodology;Layout;Virtual environments;Virtual museums;Excavation;Cognitive science;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR62088.2024.00067},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765373,
  author={Wang, Zhimin and Sun, Jingyi and Hu, Mingwei and Rao, Maohang and Song, Weitao and Lu, Feng},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={GazeRing: Enhancing Hand-Eye Coordination with Pressure Ring in Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={534-543},
  abstract={Hand-eye coordination techniques find widespread utility in augmented reality and virtual reality headsets, as they retain the speed and intuitiveness of eye gaze while leveraging the precision of hand gestures. However, in contrast to obvious interactive gestures, users prefer less noticeable interactions in public settings due to concerns about social acceptance. To address this, we propose GazeRing, a multimodal interaction technique that combines eye gaze with a smart ring, enabling private and subtle hand-eye coordination while allowing users’ hands complete freedom of movement. Specifically, we design a pressure-sensitive ring that supports sliding interactions in eight directions to facilitate efficient 3D object manipulation. Additionally, we introduce two control modes for the ring: finger-tap and finger-slide, to accommodate diverse usage scenarios. Through user studies involving object selection and translation tasks under two eye-tracking accuracy conditions, with two degrees of occlusion, GazeRing demonstrates significant advantages over existing techniques that do not require obvious hand gestures (e.g., gaze-only and gaze-speech interactions). Our GazeRing technique achieves private and subtle interactions, potentially improving the user experience in public settings. A demo video can be found at zhimin-wang.github.io/GazeRing.html.},
  keywords={Headphones;Privacy;Three-dimensional displays;Accuracy;Gaze tracking;User experience;Usability;Augmented reality;Augmented reality;object manipulation;hand-eye coordination;pressure ring},
  doi={10.1109/ISMAR62088.2024.00068},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765449,
  author={Kim, Seok Joon and Cao, Dinh Duc and Spinola, Federica and Lee, Se Jin and Cho, Kyu Sung},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={RoomRecon: High-Quality Textured Room Layout Reconstruction on Mobile Devices}, 
  year={2024},
  volume={},
  number={},
  pages={544-553},
  abstract={Widespread RGB-Depth (RGB-D) sensors and advanced 3D reconstruction technologies facilitate the capture of indoor spaces, improving the fields of augmented reality (AR), virtual reality (VR), and extended reality (XR). Nevertheless, current technologies still face limitations, such as the inability to reflect minor scene changes without a complete recapture, the lack of semantic scene understanding, and various texturing challenges that affect the 3D model’s visual quality. These issues affect the realism required for VR experiences and other applications such as in interior design and real estate. To address these challenges, we introduce RoomRecon, an interactive, real-time scanning and texturing pipeline for 3D room models. We propose a two-phase texturing pipeline that integrates AR-guided image capturing for texturing and generative AI models to improve texturing quality and provide better replicas of indoor spaces. Moreover, we suggest to focus only on permanent room elements such as walls, floors, and ceilings, to allow for easily customizable 3D models. We conduct experiments in a variety of indoor spaces to assess the texturing quality and speed of our method. The quantitative results and user study demonstrate that RoomRecon surpasses state-of-the-art methods in terms of texturing quality and on-device computation time.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Pipelines;Layout;Mobile handsets;Sensors;Floors;Image reconstruction;Augmented reality;: Room layout;Indoor 3D reconstruction;Texturing;AR-assisted image capturing;Mobile application},
  doi={10.1109/ISMAR62088.2024.00069},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765464,
  author={Song, Tianyu and Eck, Ulrich and Navab, Nassir},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Optimizing In-Contact Force Planning in Robotic Ultrasound with Augmented Reality Visualization Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={554-563},
  abstract={The utilization of augmented reality (AR) in medical robotics offers significant advancements in enhancing procedural accuracy and patient safety. This paper investigates novel AR visualization techniques designed to depict in-contact force applied by a robotic ultrasound probe, aiming to optimize the control practitioners have over probe force for ultrasound procedures, thereby enhancing both image quality and patient comfort. We developed and evaluated four distinct AR visualization techniques through a comprehensive user study conducted in a clinical setting. The study assessed the efficiency and user experience associated with each technique. The findings revealed notable differences in user performance and preferences, indicating that specific visualizations significantly improve the precision of force application and could lead to better procedural outcomes. The results underscore the potential of AR visualizations to transform robotic-assisted medical procedures by improving the interface between clinicians and robotic systems. Moreover, these advancements foster a deeper trust and acceptance of robotic technologies among healthcare professionals and patients. This study not only highlights the immediate benefits of AR in enhancing robotic ultrasound but also sets the stage for further research into AR’s expansive role in complex medical robotics scenarios.},
  keywords={Visualization;Ultrasonic imaging;Medical robotics;Accuracy;Force;Vectors;Planning;Probes;Robots;Augmented reality;Human-centered computing;Visualization;Visualization design and evaluation methods;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR62088.2024.00070},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765419,
  author={He, Zhiting and Fan, Min and Guo, Xinyi and Zhao, Yifan and Wang, Yuqiu},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={“I Feel Myself So Small!”: Designing and Evaluating VR Awe Experiences Based on Theories Related to Sublime}, 
  year={2024},
  volume={},
  number={},
  pages={564-573},
  abstract={Research suggests the potential of employing VR to elicit awe experiences, thereby promoting well-being. Building upon theories related to the sublime and embodiment, we designed three VR scenes to evaluate the effectiveness of sublime and embodied design elements in invoking awe experiences. We conducted a within-subject study involving 28 young adults who experienced the three VR designs. Results demonstrated that the VR design with sublime elements significantly elicited more intense awe experiences compared to the one without, while adding embodied elements did not enhance the intensity of awe. Qualitative interviews revealed critical design elements (e.g., the obscure event should be reasonable) and their underlying mechanisms (e.g., leading to feelings of enlightenment) in invoking awe experiences. We further discuss considerations and implications for the design of effective awe-inspiring VR applications.},
  keywords={Control design;Buildings;Moon;Emotional responses;Tides;Interviews;Augmented reality;Virtual Reality;sublime;awe;embodiment;design elements},
  doi={10.1109/ISMAR62088.2024.00071},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765162,
  author={Luan, Hongqiu and Wang, Lutong and Gai, Wei and Lv, Gaorong and Luan, Xiaona and Yang, Chenglei},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Effects of Viewpoint Height & Fluctuation on Walking Perception in Panoramic Tours}, 
  year={2024},
  volume={},
  number={},
  pages={574-583},
  abstract={Panoramic roaming with HMDs provides an immersive VR experience and the realism of user’s navigation in panoramic virtual environments (VEs) can be further enhanced by real-walking locomotion technology. However, the mismatched viewpoint height & fluctuation with the user’s visual senses may lead to perceptual conflicts during real-walking interaction. We conducted a series of user studies to investigate how viewpoint height & fluctuation affect the walking perception of users by real-walking interaction to navigate panoramic VEs and constructed a virtual camera motion model of viewpoint fluctuation and eyetracking compensation for walking behavior in panoramic virtual roaming. It was found that mismatched viewpoint fluctuation triggered generalized discomfort in participants and that participants preferred virtual viewpoint heights below eye level when roaming in panoramic VEs by real-walking. Considering these findings, we proposed viewpoint adaptive adjustment method based on visual manipulation and conducted a preliminary evaluation, which showed that the proposed method can effectively mitigate the conflict between the visual system and other sensory systems. It made that panoramic VEs constructed at a certain fixed height are rendered universal for users with different eyelevel heights and viewpoint fluctuation is matched to the undulation of their center of gravity during real-walking interaction.},
  keywords={Legged locomotion;Visualization;Fluctuations;Navigation;Virtual environments;Propioception;Resists;Cameras;Visual perception;Videos;Virtual reality;viewpoint height;viewpoint fluctuation;real-walking interaction},
  doi={10.1109/ISMAR62088.2024.00072},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765440,
  author={You, Jaehwan and Jung, Myeongul and Kim, Kwanguk Kenny},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Inter Brain Synchrony in Remote AR Education: Can Warming up Activities Positively Impact Educational Quality?}, 
  year={2024},
  volume={},
  number={},
  pages={584-593},
  abstract={Previous education studies suggested that “warming up” activities featuring interpersonal synchrony components help improve educational quality and social closeness between teachers and students. While educators are eager to incorporate augmented reality (AR) into remote educational settings, prior research has not investigated the beneficial impacts of interpersonal synchrony on remote AR education. This study investigates the effects of prior physical synchrony (PS) on learning outcomes, inter brain synchrony (IBS), and social closeness in remote AR education. 41 participants were recruited for learning English vocabulary from a teacher. They conducted a movement learning task using prior PS and physical asynchrony (PA). The results showed that the positive effects of warming up activity on quiz score, IBS, and social closeness were observed in the PS section, but not in the PA section. Moreover, we found a high correlation between quiz score and IBS but not with social closeness. These results suggest that prior PS is a beneficial component of remote AR education and that IBS could be used as an index for evaluating educational quality. The implications of these results for remote AR education are discussed.},
  keywords={Vocabulary;Correlation;Avatars;Education;Indexes;Augmented reality;Augmented Reality;Remote Education;Physical Synchrony;Inter Brain Synchrony},
  doi={10.1109/ISMAR62088.2024.00073},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765480,
  author={Ahmmed, Asif and Butts, Erica and Naeiji, Kimia and Thiamwong, Ladda and Daher, Salam},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={System Usability and Technology Acceptance of a Geriatric Embodied Virtual Human Simulation in Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={594-603},
  abstract={With the rise of the aging population, the demand of care increases yet the turnover rate of caregivers for geriatric patients has increased over the past few decades. While many factors contribute to the caregivers’ turnover rate, inadequate training and lack of communication skills and relationship-building skills are vital issues. Technology can help in providing training for soft skills. Embodied Virtual Human (VH) assistants and Augmented Reality (AR) can be a key method to train caregivers and to improve their experience in interacting with older adults, and their perception of the interaction. We designed and developed an immersive simulation in AR where caregivers interact with an embodied VH in two different conditions, i.e., first with an unaware VH and then with a VH having simulated awareness achieved through human-in-the-loop. We conducted a study with caregivers of older adults to evaluate the usability and technology acceptance of our system. The findings suggest that the majority of the participants found the system acceptable, and rated the system as user-friendly and easy to use.},
  keywords={Training;Medical services;Aging;User experience;Human in the loop;Usability;Older adults;Augmented reality;Geriatrics;System usability;technology acceptance;virtual human;augmented reality;AR;geriatric;older adults;simulation;aware virtual human;virtual assistant;geriatric patient},
  doi={10.1109/ISMAR62088.2024.00074},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765412,
  author={Hong, Jinseok and Ha, Taewook and Park, Hyerim and Kim, Hayun and Woo, Woontack},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Crowd Data-driven Artwork Placement in Virtual Exhibitions for Visitor Density Distribution Planning}, 
  year={2024},
  volume={},
  number={},
  pages={604-613},
  abstract={We propose a novel crowd data-driven optimization approach for artwork placement in virtual exhibitions. With the emerging concept of Metaverse, a multitude of users can engage with content contemporaneously in virtual exhibitions. Yet, few studies have suggested a method to resolve crowd density concentration in multiuser Mixed Reality (MR) and Virtual Reality (VR) environments. In this study, our approach leverages crowd data engaged with artworks to predict optimal placement of artworks to distribute crowd density in virtual exhibitions, prior to exhibition planning. To investigate the requirement and validity of our approach, we conducted focus group interviews and an artwork relocation experiment as preliminary studies. In the generation of solution scenes for optimal placement, our optimizer adaptively scrutinizes placeable areas with considerations of crowd density distribution, scene rationality, and artwork similarity. Through a performance comparison analysis between optimization results, we confirmed that the optimizer successfully fulfilled the intended objectives with respect to the design considerations, resolving practical scenarios in exhibition planning.},
  keywords={Metaverse;Mixed reality;Reinforcement learning;Data collection;Planning;Interviews;Optimization;Space heating;Augmented reality;Virtual reality;placement optimization;3D scene synthesis;deep reinforcement learning},
  doi={10.1109/ISMAR62088.2024.00075},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765498,
  author={Yi, Hyung Ii and Lee, Hojeong and Yoon, Sang Ho},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ThermicVib: Enabling Dynamic Thermal Sensation with Multimodal Haptic Glove for Thermal-Responsive Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={614-623},
  abstract={We propose ThermicVib, a wearable multimodal haptic glove that enhances the active perception of thermo-tactile interactions with virtual objects by integrating thermal referrals and vibrotactile phantom sensations. By fusing multimodal sensory illusions through flexible thermoelectric devices (FTED) and linear resonant actuators (LRAs), we aim to support dynamic thermal sensation adaptive to the user’s action in virtual reality (VR). Here, we developed an algorithm to render a whole-hand thermal sensation while accommodating contact and noncontact heat conditions. Based on the computed heat, we propose a simultaneous thermal and tactile rendering approach to enable dynamic thermal sensation. The user study validated the capability of our interface to support various whole-hand thermal sensations.},
  keywords={Heating systems;Heuristic algorithms;Active perception;Phantoms;Thermal sensors;Rendering (computer graphics);User experience;Thermoelectric devices;Haptic interfaces;Augmented reality;Haptics;Virtual Reality;Thermal Feedback;Wearables},
  doi={10.1109/ISMAR62088.2024.00076},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765387,
  author={Swamy, Shneka Muthu Kumara and Han, Qi},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={APPEAR: Adaptive Pose Estimation for Mobile Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={624-632},
  abstract={In Mobile Augmented Reality (MAR) applications, rendering virtual objects accurately on the user’s screen relies on knowing the poses of the user and the virtual objects. Typically, Simultaneous Localization and Mapping (SLAM) is used for pose estimation, but SLAM consumes lots of space and time. Many MAR applications such as gaming and navigation do not need continuous mapping, hence Visual Odometry (VO) is developed that provides localization without map generation. This paper evaluates VO’s accuracy compared to SLAM by specifically considering feature points at varying depths which distinguishes indoor and outdoor environments. Our findings suggest VO performs better with distant feature points (i.e., commonly outdoors), while SLAM excels with closer feature points (i.e., typically indoors). These findings inspire us to design APPEAR, an adaptive approach that switches between VO and SLAM as needed. We implemented and evaluated APPEAR on a computer rather than a smartphone because the chosen VO method was not available on a phone. APPEAR demonstrates memory savings of up to 100 MB and a 1.67 x speed boost on medium-sized datasets without losing accuracy. Additionally, while Absolute Trajectory Error (ATE) has been the primary metric for evaluating pose estimation, we argue for more comprehensive quality metrics for MAR applications: accuracy, precision, and recall. We also define selective ATE where ATE is only calculated in the region where the virtual objects are visible.},
  keywords={Measurement;Mars;Visualization;Simultaneous localization and mapping;Accuracy;Pose estimation;Rendering (computer graphics);Trajectory;Augmented reality;Visual odometry;Human-centered computing;Human computer interaction;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR62088.2024.00077},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765159,
  author={Liu, Qianru and Li, Yue and Chen, Bingqing and Wu, Huiyue and Liang, Hai-Ning},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={User-Defined Gesture Interactions for VR Museums: An Elicitation Study}, 
  year={2024},
  volume={},
  number={},
  pages={633-642},
  abstract={Recognizing the potential of freehand gestures for interacting with virtual objects in virtual environments, our research introduces a user-defined freehand gesture set of typical referents in virtual reality (VR), focusing on a specific scenario: VR museums. We conducted a comprehensive elicitation study with two experiments to define and refine the gesture set. Meanwhile, we demonstrated an enhanced real-time Wizard of Oz approach that facilitated users’ understanding of referents in VR and their gesture design. Our findings revealed significant improvements in gesture consistency and user agreement through two experiments, with an average agreement score of firstchoice and second-choice advancing from 0.211 and 0.160 to 0.412 and 0.284, respectively. By offering a consistent user-centered gesture set, this work contributes to guiding museum curators toward creating more immersive user experiences in VR museums. The gestures can also be extended to other VR applications that necessitate travel, selection and manipulation, and system control tasks.},
  keywords={Three-dimensional displays;Focusing;Virtual environments;Medical services;Aerospace electronics;User interfaces;Control systems;Museums;Real-time systems;Space exploration;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Humancentered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/ISMAR62088.2024.00078},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765482,
  author={Kim, Bowon and Hwang, Eugene and Lee, Jeongmi},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Self-Avatar Recognition in Virtual Reality: the Self Advantage Phenomenon and the Relative Importance of Motion and Visual Congruence}, 
  year={2024},
  volume={},
  number={},
  pages={643-651},
  abstract={In virtual reality, avatars represent ourselves and serve as a means to interact with others and the environment. Thus, understanding the self-recognition process in VR and designing self-avatars that have strong connections with the self is critical in enhancing immersion and efficiency of interaction in VR. In this paper, we investigated the characteristics of the self-recognition process in VR and the crucial factors that enhance the bond between the self and the avatar. In Study 1, we tested whether the advantage in cognitive processing of self-related information, often observed in reality, is also replicated for briefly embodied self-avatars in VR and how it is modulated by way of avatar selection and the degree of embodiment. The results showed that the self-avatar was processed more efficiently than other avatars, despite the constant changes in the appearance and a brief embodiment period. Moreover, the self-advantage effect was more pronounced for personally selected avatars, rather than those assigned by the experimenter. In Study 2, we compared the relative importance of motion and visual congruence in the process of identifying an entity as the self-avatar. The results indicated that motion synchrony between the user and the avatar is relatively more emphasized than the match in visual appearance when identifying oneself in VR. These findings highlight the underlying mechanisms and crucial factors for self-recognition in VR, and provide valuable insights for designing more immersive virtual experiences in various social VR applications.},
  keywords={Visualization;Avatars;Psychology;Synchronization;Character recognition;Augmented reality;virtual reality;avatar;self-recognition;embodiment;user-avatar bond},
  doi={10.1109/ISMAR62088.2024.00079},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765415,
  author={Sunday, Kissinger and Li, Yiwei and Sun, Junwei and Wehbe, Rina and Neyedli, Heather and Batmaz, Anil Ufuk and Machuca, Mayra Donaji Barrera},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Pinging Between Worlds: Training Table Tennis Novice Players in Real Environment for Virtual Reality Competitions}, 
  year={2024},
  volume={},
  number={},
  pages={652-661},
  abstract={Modern Virtual Reality (VR) technology has enabled users to experience Real Environment (RE) sports in their homes. For VR table tennis, one of the most popular VR sports, the players have rankings and tournaments and compete for RE awards. Based on this phenomenon, this paper aims to understand the benefits of RE training in improving VR table tennis skills. In a user study with 12 novice table tennis players, we measured their performance in 16 basic skills via a pre- and post-study design using a novel training protocol designed for both RE and VR players. We also asked participants for their insights into the training and to evaluate their experience. Our results show a significant improvement in all measured skills. However, participants identified issues with the technology that caused discomfort. Our findings provide valuable insights for software developers working on VR sports applications, enabling them to create better experiences for VR table tennis players. They can also help developers of VR training applications identify areas for improvement with the current technology.},
  keywords={Training;Protocols;Atmospheric measurements;Games;Particle measurements;Software;Physics;Faces;Augmented reality;Sports;Virtual Reality;Table Tennis;Training;Skill Transfer;Sports},
  doi={10.1109/ISMAR62088.2024.00080},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765402,
  author={Kalamkar, Snehanjali and Biener, Verena and Pauls, Daniel and Lindlein, Leon and Izadifar, Morteza and Kristensson, Per Ola and Grubert, Jens},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Accented Character Entry Using Physical Keyboards in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={662-670},
  abstract={Research on text entry in Virtual Reality (VR) has gained popularity but the efficient entry of accented characters, characters with diacritical marks, in VR remains underexplored. Entering accented characters is supported on most capacitive touch keyboards through a long press on a base character and a subsequent selection of the accented character. However, entering those characters on physical keyboards is still challenging, as they require a recall and an entry of respective numeric codes. To address this issue this paper investigates three techniques to support accented character entry on physical keyboards in VR. Specifically, we compare a contextaware numeric code technique that does not require users to recall a code, a key-press-only condition in which the accented characters are dynamically remapped to physical keys next to a base character, and a multimodal technique, in which eye gaze is used to select the accented version of a base character previously selected by keypress on the keyboard. The results from our user study $(n=18)$ reveal that both the key-press-only and the multimodal technique outperform the baseline technique in terms of text entry speed.},
  keywords={Presses;Codes;Keyboards;Augmented reality;: Text Entry;Virtual Reality;Eye-tracking;Accented Character Input},
  doi={10.1109/ISMAR62088.2024.00081},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765366,
  author={Wieland, Jonathan and Cho, Hyunsung and Hubenschmid, Sebastian and Kiuchi, Akihiro and Reiterer, Harald and Lindlbauer, David},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Push2AR: Enhancing Mobile List Interactions Using Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={671-680},
  abstract={Smartphones provide convenient access to vast data collections (e.g., online shops, social media) within a compact, portable form factor. While the prevalent infinite scroll lists address the inherently restricted screen space, they also introduce navigation and orientation challenges. Users often lose track of their position within these lists and find it difficult to efficiently access, compare, and filter items of interest. To address this challenge, we introduce Push2AR, a novel interaction concept that extends the phone’s high-resolution display and familiar touch interaction with the virtual display space offered by Augmented Reality (AR) headsets. Push2AR enables users to transfer individual list items from their phone to its surrounding AR space, facilitating bookmarking, filtering, and side-by-side comparisons while maintaining orientation through visual links to the original scroll position. Our evaluation shows that our approach enhances user experience and reduces subjective workload involved in locating and comparing list items in contrast to conventional phone-only lists.},
  keywords={Headphones;Visualization;Navigation;Social networking (online);Prototypes;User interfaces;User experience;Synchronization;Smart phones;Augmented reality;Augmented reality;mobile devices;cross-device interaction},
  doi={10.1109/ISMAR62088.2024.00082},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765470,
  author={Huang, Yuxuan and Zhang, Danhua and Rosenberg, Evan Suma},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Direction-Based Authentication: Combining Symbolic Input and Contextual Cues for Virtual Reality Password Entry}, 
  year={2024},
  volume={},
  number={},
  pages={681-689},
  abstract={This paper presents Direction-Based Authentication (DBA), a novel authentication method for virtual reality that combines symbolic input and contextual information to balance efficiency with memorability. In DBA, the password consists of four view directions selected by the user across four virtual environments. The user can either physically turn their head or use buttons to select directions, and remember the directions by either symbol or visual context. We conducted a within-subjects study (N=32) to evaluate the efficiency, memorability, and security compared to methods based on symbols or context only. The results demonstrated that password entry with DBA was more efficient than the contextual-only approach. While recall rate was not significantly higher than the symbolic-only method, participants’ subjective ratings indicated that DBA better supported memorability. All three methods were highly resistant to observational attack, although the user-defined passwords appeared homogeneous in certain cases. Overall, the study shows that combining symbolic and contextual information is promising to balance efficiency with memorability for VR authentication, but potential usability and security issues can arise without careful consideration. Based on these findings, we discussed future directions for optimizing the usability and security of DBA and insights regarding participants’ reliance on symbolic vs. visual information.},
  keywords={Threat modeling;Resistance;Visualization;Privacy;Authentication;Symbols;Virtual environments;Passwords;Security;Usability;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Security and privacy;Human and societal aspects of security and privacy;Usability in security and privacy},
  doi={10.1109/ISMAR62088.2024.00083},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765434,
  author={Stanescu, Ana and Mohr, Peter and Thaler, Franz and Kozinski, Mateusz and Skreinig, Lucchas Ribeiro and Schmalstieg, Dieter and Kalkofen, Denis},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Error Management for Augmented Reality Assembly Instructions}, 
  year={2024},
  volume={},
  number={},
  pages={690-699},
  abstract={Augmented reality (AR) lends itself to presenting visual instructions on how to assemble or disassemble an object. Splitting the assembly procedure into shorter steps and presenting the corresponding instructions in AR supports their comprehension. However, one can still misinterpret instructions and make errors while manipulating the object. While previous work supports detecting the occurrence of errors, we investigate handling such errors. This requires knowledge of the error at runtime of the application. Starting from a categorization of the errors, we investigate how to automatically derive common error states to generate training data. We introduce an extension to a state-of-the-art deep-learning-based object detector for supporting the detection of assembly states at real-time update rates, based on contrastive learning. We evaluated the proposed detector, showing that it outperforms the state-of-the-art, and we demonstrate our work with an AR application that alerts the user if errors occur and provides visual help to correct the error.},
  keywords={Visualization;Runtime;Training data;Detectors;Contrastive learning;Real-time systems;Augmented reality;Assembly;Mixed / augmented reality;assembly tutorial;object detection},
  doi={10.1109/ISMAR62088.2024.00084},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765430,
  author={Villa, Steeven and Ishihara, Kenji and Ziarko, Moritz and Günther, Sebastian and Müller, Florian},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Touch It Like It’s Hot: A Thermal Feedback Enabled Encountered-type Haptic Display for Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={700-709},
  abstract={In recent years, the community has presented various novel solutions to address the lack of haptic feedback in virtual reality experiences. Yet, it remains a major challenge for Virtual Reality applications. Encountered-type Haptic Displays (ETHDs) have emerged as a promising alternative to enable haptic feedback in VR without requiring the user to wear any device while allowing for sensorily rich experiences such as texture, kinaesthetic feedback, and even ultrasonic tactile feedback. Nevertheless, as important as thermal feedback is for daily life interactions, such as assessing the temperature of a mug or knowing if the microwave is on, thermal feedback in ETHD has remained largely unexplored. In this paper, we present a novel ETHD that provides thermal feedback and explore its potential in VR. We describe the design of our ETHD, and we report the results of a user study that compares different thermal feedback settings in VR. Our results show that thermal feedback can significantly enhance the user immersion and haptic experience in VR, and we discuss the implications of our findings for the design of ETHD and VR experiences.},
  keywords={Temperature sensors;Thermal factors;Atmospheric measurements;Tactile sensors;Microwave devices;Particle measurements;User experience;Acoustics;Haptic interfaces;Augmented reality;Haptics;Robotics;Thermal Feedback;Encountered-type Haptics;Virtual Reality},
  doi={10.1109/ISMAR62088.2024.00085},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765481,
  author={Sarvesh, Muskan and Kang, Ryan and Nam, Hyeongil and Park, Simon and Hugo, Ron and Maurer, Frank and Kim, Kangsoo},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Extended Reality and Digital Twin in the Oil and Gas Pipeline Industry: A Systematic Review on Applications, Trends, and Future Directions}, 
  year={2024},
  volume={},
  number={},
  pages={710-719},
  abstract={Immersive and simulation technologies, such as Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), and Digital Twins (DTs), have proven effective in enhancing decision-making, streamlining operations, and improving safety through intuitive visualizations. Unsurprisingly, the oil and gas (O&G) pipeline industry is increasingly turning to these technologies to tackle complex construction and resource management challenges. This paper reviews the applications, benefits, and future trends of these technologies, focusing on integration and emerging trends in monitoring, training, maintenance, and testing. It identifies potential gaps, offers guidance for future research, and emphasizes adapting to technological evolution, providing insights to improve system reliability and sustainability in O&G pipeline operations using immersion and simulation methods.},
  keywords={Training;Oils;Pipelines;Focusing;Market research;Safety;Maintenance;Digital twins;Sustainable development;Augmented reality;Systematic literature review;augmented reality;virtual reality;mixed reality;digital twin;pipeline;gas and oil industry},
  doi={10.1109/ISMAR62088.2024.00086},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765396,
  author={Ruocco, Martina and Saeghe, Pejman and Kerber, Frederic and Gugenheimer, Jan and McGill, Mark and Khamis, Mohamed},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={From Redirected Navigation to Forced Attention: Uncovering Manipulative and Deceptive Designs in Augmented Reality through Retail Shopping}, 
  year={2024},
  volume={},
  number={},
  pages={720-729},
  abstract={In the near future ubiquitous Augmented Reality (AR) will see virtual spatial content seamlessly integrated into our everyday lives through fashionable, wearable devices such as AR glasses. In doing so, we will unlock the capacity for multiple stakeholders to augment and personalize our view of reality - not always to the benefit of users. Using speculative design, we explore the risks and repercussions of third party-driven AR-enacted manipulation and deception through the lens of supermarket grocery shopping - an activity where consumers are routinely tracked and exposed to manipulation of attention and purchasing decisions. Through a scenario generation activity, 20 participants (mixing both existing XR users and frequent shoppers) co-created 58 scenarios reflecting on how AR-driven manipulation or deception of shoppers could be enacted. Our results show that (1) manipulation, rather than deception, is the primary means of affecting consumer behaviour, necessitating we consider AR Manipulative and Deceptive Designs (MDDs) more broadly. Moreover (2), we highlight a) how known MDDs can manifest in AR, and b) four novel MDDs that are specific to AR: Redirected Navigation, Directed & Forced Attention Shifts, Reality Interference and Delayed and Detained. We reflect on the stakeholders that would manipulate consumer perception of reality, the different manipulations enacted across the lifecycle of consumer shopping, and the AR elements exploited to deliver these ARMDDs, deriving insights into future harms, ethics and safeguarding around ARMDDs to minimize their impact.},
  keywords={Visualization;Privacy;Navigation;Interference;Metadata;Stakeholders;Scenario generation;Wearable devices;Augmented reality;Lenses;ubiquitous augmented reality;deceptive design;grocery shopping;retail shopping;manipulation;attention},
  doi={10.1109/ISMAR62088.2024.00087},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765485,
  author={Zaman, Ammaar and Kruijff, Ernst and Veas, Eduardo and Krajnc, Aleksandra and ElSayed, Neven},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Subtle Cueing For Improving Depth Perception in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={730-739},
  abstract={Understanding an environment relies on human sensory systems, with visual perception as the primary source for defining spatial relationships and estimating distances. The visual system uses natural cues, each offering partial information that can lead to bias and conflicts, especially when ambiguous. Virtual Reality (VR) environments challenge these natural depth cues with discrepancies in perspective and variations in light and shadow depiction, leading to potential confusion in depth perception. However, VR also allows for the isolation and study of these cues. This paper introduces artificial subtle cues (texture blur) to enhance natural depth information in VR. Our results show that augmenting natural depth cues with artificial ones improves depth prediction accuracy and spatial relationship awareness. Subtle blur cues enhance depth estimation without participants’ subjective awareness of the augmentation, suggesting that such subtle cueing can effectively enhance depth perception.},
  keywords={Accuracy;Virtual environments;Lighting;Color;Distortion;Augmented reality;Visual perception;Testing;Virtual Reality;Perception;Depth Perception;Augmented cues;Subtle Cues;Implicit Cues;Explicit Cues},
  doi={10.1109/ISMAR62088.2024.00088},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765157,
  author={Kaeder, Janne and Vergari, Maurizio and Biener, Verena and Kojić, Tanja and Grubert, Jens and Möller, Sebastian and Antons, Jan-Niklas Voigt},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Working with Mixed Reality in Public: Effects of Virtual Display Layouts on Productivity, Feeling of Safety, and Social Acceptability}, 
  year={2024},
  volume={},
  number={},
  pages={740-748},
  abstract={Nowadays, Mixed Reality (MR) headsets are a game-changer for knowledge work. Unlike stationary monitors, MR headsets allow users to work with large virtual displays anywhere they wear the headset, whether in a professional office, a public setting like a cafe, or a quiet space like a library. This study compares four different layouts (eye level-close, eye level-far, below eye level-close, below eye level-far) of virtual displays regarding feelings of safety, perceived productivity, and social acceptability when working with MR in public. We test which layout is most preferred by users and seek to understand which factors affect users’ layout preferences. The aim is to derive useful insights for designing better MR layouts. A field study in a public library was conducted using a within-subject design. While the participants interact with a layout, they are asked to work on a planning task. The results from a repeated measure ANOVA show a statistically significant effect on productivity but not on safety and social acceptability. Additionally, we report preferences expressed by the users regarding the layouts and using MR in public.},
  keywords={Productivity;Headphones;Layout;Mixed reality;Particle measurements;Libraries;Safety;Planning;Space stations;Monitoring;Mixed Reality;User Experience;Social Acceptability;Safety;Productivity},
  doi={10.1109/ISMAR62088.2024.00089},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765487,
  author={Abraham, Melvin and Khamis, Mohamed and McGill, Mark},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Don’t Record My Private pARts: Understanding The Role of Sensitive Contexts and Privacy Perceptions in Influencing Attitudes Towards Everyday Augmented Reality Sensor Usage}, 
  year={2024},
  volume={},
  number={},
  pages={749-758},
  abstract={Everyday Augmented Reality (AR) headsets come with an array of sensing capabilities. Users wearing these headsets for extended periods may prefer specific sensors to remain inactive in some contexts for privacy and sensitivity reasons. Currently, the contexts in which users wish to limit sensor data collection are unclear. To explore this, we conducted a survey ($\mathrm{N}=100$), collecting 552 scenarios to understand which situations users wish to restrict or completely block data collection by specific sensors or combinations on their AR headset. Our results show the sensitive contexts can be classified into seven categories: 1) presence of confidential information; 2) risk of data quantification; 3) expectation of solitude; 4) rules prohibiting data collection; 5) modesty and nudity; 6) home environments; and 7) outdoor public locations. Our results provide insights into privacy-invasive contexts when people want to limit and restrict their AR sensors, building towards automating permission configurations during the prolonged use of everyday AR headsets.},
  keywords={Headphones;Surveys;Privacy;Sensitivity;Attitude control;Buildings;Data collection;Sensors;Arrays;Augmented reality;Augmented reality;data access;permissions;context;location;privacy;access control;sensitive contexts},
  doi={10.1109/ISMAR62088.2024.00090},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765376,
  author={Srinidhi, Sruti and Lu, Edward and Rowe, Anthony},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={XaiR: An XR Platform that Integrates Large Language Models with the Physical World}, 
  year={2024},
  volume={},
  number={},
  pages={759-767},
  abstract={This paper discusses the integration of Multimodal Large Language Models (MLLMs) with Extended Reality (XR) headsets, focusing on enhancing machine understanding of physical spaces. By combining the contextual capabilities of MLLMs with the sensory inputs from XR, there is potential for more intuitive spatial interactions. However, the integration faces challenges due to the inherent limitations of MLLMs in processing 3D inputs and their significant resource demands for XR headsets. We introduce XaiR, a platform that facilitates integrating MLLMs with XR applications. XaiR uses a split architecture that offloads complex MLLM operations to a server while handling 3D world processing on the headset. This setup manages multiple input modalities, parallel models, and links them with real-time pose data, improving AR content placement in physical scenes. We tested XaiR’s effectiveness with a “cognitive assistant” application that guides users through tasks like making coffee or assembling furniture. Results from a 15-participant study shows over 90% accuracy in task guidance and 85% accuracy in AR content anchoring. Additionally, we evaluate MLLMs against human operators for cognitive assistant tasks which provides insights into the quality of the captured data as well as the current gap in performance for cognitive assistant tasks.},
  keywords={Headphones;Three-dimensional displays;Accuracy;Extended reality;Large language models;Focusing;Real-time systems;Data models;Servers;Faces;Large Language Models;eXtended Reality;Multimodal},
  doi={10.1109/ISMAR62088.2024.00091},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765399,
  author={Lavenant, Suliac and Goguey, Alix and Malacria, Sylvain and Nigay, Laurence and Pietrzak, Thomas},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying the Perception of Vibrotactile Haptic Cues on the Finger, Hand and Forearm for Representing Microgestures}, 
  year={2024},
  volume={},
  number={},
  pages={768-776},
  abstract={We explore the use of vibrotactile haptic cues for representing microgestures. We built a four-axes haptic device for providing vibrotactile cues mapped to all four fingers. We also designed six patterns, inspired by six most commonly studied microgestures. The patterns can be played independently on each axis of the device. We ran an experiment with 36 participants testing three different device locations (fingers, back of the hand, and forearm) for pattern and axis recognition. For all three device locations, participants interpreted the patterns with similar accuracy. We also found that they were better at distinguishing the axes when the device is placed on the fingers. Hand and Forearm device locations remain suitable alternatives but involve a greater trade-off between recognition rate and expressiveness. We report the recognition rates obtained for the different patterns, axes and their combinations per device location. These results per device location are important, as constraints of various kinds, such as hardware, context of use and user activities, influence device location. We discuss this choice of device location by improving literature microgesture-based scenarios with haptic feedback or feedforward.},
  keywords={Vocabulary;Accuracy;Hardware;Haptic interfaces;Pattern recognition;Sensors;Augmented reality;Testing;Vibrotactile cues;microgestures;haptic vocabulary},
  doi={10.1109/ISMAR62088.2024.00092},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765491,
  author={Qu, Tangjun and Wang, Junjie and Lin, Yongjiu and Liu, Juan and Zhou, Chao and Zhang, Baiqiao and Jiang, Kaiyuan and Bian, Yulong},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Becoming An Animal? Exploring Proteus Effect Based on Human-avatar Hand Gesture Consistency}, 
  year={2024},
  volume={},
  number={},
  pages={777-786},
  abstract={Human cognition and behavior can be unconsciously affected by personal avatars in the virtual world, a phenomenon known as the Proteus Effect. When using first-person non-human avatars, the characteristics of virtual hands may also induce relevant cognitive and even behavioral patterns in real human hands. Therefore, evaluating human-avatar gesture consistency may be a potentially effective method for objectively assessing the Proteus Effect when using non-human avatars. To explore this question, we first created human and non-human avatars, including three animals. Then, we constructed a dataset of hand gestures and trained a model to dynamically recognize real-hand gestures that were consistent with corresponding avatar hands. Next, we designed a virtual reality experimental task involving grasping objects with intuitive gestures and performed a 2 (avatar type: human/non-human) $* 2$ (virtual hand: presence/absence of spontaneous movement) within-subject experiment to examine the effects of avatar characteristics on self-illusion and human-avatar gesture consistency. The results showed that participants performed a significantly larger percentage of gestures that were consistent with their currently used avatars. Additionally, participants did experience self-illusion when using non-human avatars, although the levels were significantly lower than those when using human avatars. Therefore, self-illusion may serve as a perceptual antecedent of the Proteus Effect, even with non-human avatars, inadvertently altering the behavioral gestures of their real hands. In conclusion, detecting human-avatar gesture consistency can help evaluate the Proteus Effect.},
  keywords={Solid modeling;Animals;Avatars;Grasping;Real-time systems;Cognition;Augmented reality;Proteus Effect;self-illusion;virtual hands;non-human avatar},
  doi={10.1109/ISMAR62088.2024.00093},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765160,
  author={Wernikowski, Marek and March, Joseph G. and Mantiuk, Radosław and Yöntem, Ali Özgür and Mantiuk, Rafał K.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Tracking Eye Position and Gaze Direction in Near-Eye Volumetric Displays}, 
  year={2024},
  volume={},
  number={},
  pages={787-796},
  abstract={Near-eye volumetric displays, showing multiple focal planes, require knowledge of the accurate position of the nodal point of the eye to correctly render a 3D scene. This is because pixels seen through multiple planes must be accurately aligned with the eye’s visual axis to ensure consistency across focal planes. While most eye-tracking methods focus on determining a gaze position within a designated target space, this work aims to track both the eye position and the corresponding gaze direction expressed in coordinates relative to the physical location of the volumetric display planes. To achieve this, we rely on a near-infra-red (NIR) camera image of the pupil and corneal reflections (glints). The existing eye model is used to establish the relationship between the pupil and glint positions in a NIR image and the eye position and rotation in a 3D space. We address the key challenge of robust tracking of the glints in a system that introduces multiple reflections. We also demonstrate that the system reduces the need for recalibration on subsequent uses. Our experiments on a multiple-focal plane display demonstrate that the method can maintain an accurate projection point for volumetric displays.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Accuracy;Target tracking;Computational modeling;Gaze tracking;Reflection;Calibration;Pupils;Eye tracking;gaze tracking;eye model;volumetric displays;multi-focal plane displays},
  doi={10.1109/ISMAR62088.2024.00094},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765474,
  author={Schwandt, Tobias and Kumari, Gunjan and Stolz, Georg and Werner, Stephan and Broll, Wolfgang},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing Human Task Performance through Audiovisual Augmentation}, 
  year={2024},
  volume={},
  number={},
  pages={797-805},
  abstract={In unusual environments and situations like extreme sports, underground environments, underwater, conflict zones or outer space, human sensory perception is often limited, which can adversely affect task performance, situational awareness, and the feeling of presence. This paper explores the efficacy of augmenting human perception with audiovisual information to support individuals in such contexts. Specifically, we investigate the impact of audiovisual augmentation on presence, situational awareness, and task performance. To conduct this research, we designed a virtual reality (VR) simulation of a space mission aboard the International Space Station (ISS). Within this simulation, participants were tasked with performing maintenance activities while receiving artificial augmentations. We conducted a user study involving 43 participants who performed the same maintenance task under four augmentation conditions: audio cues, visual cues, audiovisual cues, and no cues. Our findings reveal that adding audiovisual information significantly enhances performance. Participants with audiovisual augmentations had a 60% improvement in task completion speed compared to those without augmentations. Moreover, the workload was substantially reduced, and the sense of presence was increased when audiovisual support was used. The results highlight the potential of audiovisual augmentation as a valuable tool for individuals engaging in situations with reduced audiovisual perception. The insights demonstrating the versatility and promise of audiovisual augmentation in enhancing task performance.},
  keywords={Visualization;Space missions;International Space Station;Maintenance;Augmented reality;Sports;Audiovisual augmentation;Task performance;Workload;Maintenance;Virtual reality;User study},
  doi={10.1109/ISMAR62088.2024.00095},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765153,
  author={Chiu, Chao-Kuo and Chuang, Jung-Hong and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating the Carryover Effects of Calibration of Size Perception in Augmented Reality to the Real World}, 
  year={2024},
  volume={},
  number={},
  pages={806-815},
  abstract={Many AR applications require users to perceive, estimate and calibrate to the size of objects presented in the scene. Distortions in size perception in AR could potentially influence the effectiveness of skills transferred from the AR to the real world. We investigated the after-effects or carry-over effects of calibration of size perception in AR to the real world (RW), by providing feedback and an opportunity for participants to correct their judgments in AR. In an empirical evaluation, we employed a three-phase experiment design. In the pretest phase, participants made size estimations to target objects concurrently using both verbal reports and physical judgment in RW as a baseline. Then, they estimated the size of targets, and then were provided with feedback and subsequently corrected their judgments in a calibration phase. Followed by which, participants made size estimates to target objects in the real world. Our findings revealed that the carryover effects of calibration successfully transferred from AR to RW in both verbal reports and physical judgment methods.},
  keywords={Estimation;Distortion;Calibration;Augmented reality;Augmented Reality;Size Perception;Perceptuomotor Calibration;Perception-Action;Empirical Evaluation},
  doi={10.1109/ISMAR62088.2024.00096},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765483,
  author={Nam, Hyeongil and Lee, Kisub and Sarvesh, Muskan and Cho, Sangwoo and Park, Jong-Il and Kim, Kangsoo},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Watch Buddy: Evaluating the Impact of an Expressive Virtual Agent on Video Consumption Experience in Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={816-825},
  abstract={The proliferation of personalized video content consumption, amplified by advances in virtual reality (VR) and augmented reality (AR), has introduced new paradigms in media engagement.This paper presents the development and evaluation of “Watch Buddy,” an expressive virtual agent designed to enhance the video-watching experience within an AR environment. The research assesses the impact of the agent’s presence and expressiveness on user satisfaction, technology acceptance, social presence, and emotional intimacy. A comparative user study with 30 participants reveals that the expressive agent significantly improves the viewing experience, as well as the perceived social connection between users and the agent, compared with an inexpressive agent and no agents at all. This paper contributes to VR/AR and human-agent interaction by demonstrating how virtual companions can influence media consumption and foster social connections, particularly for isolated demographics like the elderly or those living alone.},
  keywords={Measurement;Biometrics;Media;User experience;Real-time systems;Older adults;Artificial intelligence;Augmented reality;Embodied virtual agent;video co-watching;augmented reality;user experience},
  doi={10.1109/ISMAR62088.2024.00097},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765467,
  author={Quinn, Kelsey and Gabbard, Joseph L.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmented Reality Visualization Techniques for Attention Guidance to Out-of-View Objects: A Systematic Review}, 
  year={2024},
  volume={},
  number={},
  pages={826-835},
  abstract={Recent advancements in augmented reality (AR) hardware, software and application capabilities have introduced exciting benefits and advantages, especially in industrial and occupational fields. However, many new challenges have arisen with the increasing use of AR in real work settings, such as increased visual capture and attention demanded by graphics presented within a relatively small field of view (FOV) (as compared to users’ natural FOV). This systematic review paper addresses how to effectively use and design AR visualization techniques to cue objects located outside a user’s FOV. We posit that new visualization techniques are needed to cue out-of-view objects while maintaining user attention and minimizing distraction from the user’s primary task. A significant amount of research has been done to examine effective visualizations for guiding attention in AR, specifically how to encode direction and distance of out-of-view objects. Our review compares the performance associated with existing techniques, as well as what characteristics have been implemented and studied. In this work, we also present design guidelines derived from our analysis and synthesis to understand what visualization technique characteristics may be effective. Our final recommendations describe the value of reference lines, feedback to users, location indicators, best practices to encode direction and distance, and the use of subtle cues.},
  keywords={Visualization;Systematics;Visual systems;Software;Hardware;Clutter;Augmented reality;Best practices;Testing;Guidelines;Human-centered computing;Mixed / augmented reality;Visualization;Visualization techniques;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/ISMAR62088.2024.00098},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765381,
  author={Zou, Qianyuan and Bai, Huidong and Chang, Zhuang and Xiao, Zirui and Tian, Suizi and Duh, Henry Been-Lirn and Fowler, Allan and Billinghurst, Mark},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effect of Interface Types and Immersive Environments on Drawing Accuracy and User Comfort}, 
  year={2024},
  volume={},
  number={},
  pages={836-845},
  abstract={In this research, we investigate the effectiveness of asymmetric interactions (HandStylus, HandController, and TwoHands) in Augmented Reality (AR), Virtual Reality (VR), and Extended Reality (XR) for 3D digital drawing overlaying on physical and virtual objects. We evaluate the input accuracy and fatigue of these object-based 3D drawing experiences using quantitative measurements and further explore the correlation between these outcomes with subjective questionnaires. We found significant independence between environments and interface types, which considerably influence the performance and usability of 3D immersive drawing. We noted discrepancies between users’ subjective experiences and objective performance. Specifically, although AR drawing on physical objects provides superior accuracy and minimal muscle fatigue due to tangible feedback, and the TwoHands interaction offers the highest precision, the subjective results show the reverse outcome. Based on these findings, we propose design recommendations and discuss directions for future research in immersive drawing environments.},
  keywords={Three-dimensional displays;Accuracy;Atmospheric measurements;Muscles;Fatigue;Particle measurements;Cognitive load;Sensors;Usability;Augmented reality;Mixed reality;virtual reality;immersive drawing;content creation;fatigue;surface electromyography},
  doi={10.1109/ISMAR62088.2024.00099},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765394,
  author={Singhal, Yatharth and Honrales, Daniel and Ho, Hsin-Ni and Kim, Jin Ryong},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Wetness Illusion in Mid-Air}, 
  year={2024},
  volume={},
  number={},
  pages={846-854},
  abstract={This study explores the impact of combining multiple senses on how people perceive wetness in mid-air. We examine how factors like temperature, pressure, and visual stimuli influence the illusion of wetness on users’ palms. The first user study examines these effects and the complex relationships between these variables. The findings suggest that increased temperature and pressure increase the likelihood of perceiving wetness. Interestingly, we note the influence of visual scene content on wetness perception. This leads to our second user study, which investigates how visual context impacts wetness perception. The results demonstrate a strong link between visual content and wetness perception, offering valuable insights for designing immersive virtual reality experiences that enhance sensory perception.},
  keywords={Temperature sensors;Visualization;Temperature;Design methodology;User experience;Haptic interfaces;Augmented reality;: Wetness Perception;Multisensory Integration;Thermal Feedback;Mid-Air Haptics},
  doi={10.1109/ISMAR62088.2024.00100},
  ISSN={2473-0726},
  month={Oct},}
@INPROCEEDINGS{10765374,
  author={Qu, Zhehan and Byrne, Ryleigh and Gorlatova, Maria},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={“Looking” into Attention Patterns in Extended Reality: An Eye Tracking-Based Study}, 
  year={2024},
  volume={},
  number={},
  pages={855-864},
  abstract={Virtual reality (VR) simulations have been adopted to provide controllable environments for running augmented reality (AR) experiments in diverse scenarios. However, insufficient research has explored the impact of AR applications on users, especially their attention patterns, and whether VR simulations accurately replicate these effects. In this work, we propose to analyze user attention patterns via eye tracking during XR usage. To represent applications that provide both helpful guidance and irrelevant information, we built a Sudoku Helper app that includes visual hints and potential distractions during the puzzle-solving period. We conducted two user studies with 19 different users each in AR and VR, in which we collected eye tracking data, conducted gaze-based analysis, and trained machine learning (ML) models to predict user attentional states and attention control ability. Our results show that the AR app had a statistically significant impact on enhancing attention by increasing the fixated proportion of time, while the VR app reduced fixated time and made the users less focused. Results indicate that there is a discrepancy between VR simulations and the AR experience. Our ML models achieve 99.3% and 96.3% accuracy in predicting user attention control ability in AR and VR, respectively. A noticeable performance drop when transferring models trained on one medium to the other further highlights the gap between the AR experience and the VR simulation of it.},
  keywords={Measurement;Solid modeling;Visualization;Computational modeling;Gaze tracking;Machine learning;Predictive models;Media;Augmented reality;Load modeling;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Virtual reality},
  doi={10.1109/ISMAR62088.2024.00101},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765429,
  author={Gower, Jamie and Baumeister, James and Thomas, Bruce H. and Zucco, Joanne},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmented Reality Annotations for Assisting with Decision-Making and Time-Critical Tasking}, 
  year={2024},
  volume={},
  number={},
  pages={865-873},
  abstract={This research explores various Augmented Reality (AR) annotations for supporting performance and decision-making in a time-critical task. The task was designed in collaboration with defence contacts to mimic a command and control scenario, requiring rapid decisions under time pressure. A set of annotations was derived from previous literature, and modifications were made to suit the tasking requirements. The annotations chosen were based on their level of abstraction from least to most abstract: Animations, StickyNotes, Icons, and Highlights. The study compared five conditions (Animations, StickyNotes, Icons, Highlights, and no annotations) for their effectiveness in supporting performance and decision-making in a time-critical task. StickyNotes was found to be the most effective for supporting performance. The study’s results also identify that annotations for assisting with time-critical tasking should be easy and quick to understand, contain sufficient complexity, and not be too abstract in detail.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Annotations;Decision making;Animation;Complexity theory;Time factors;Augmented reality;Guidelines;Computing methodologies;Computer graphicsGraphic systems and interfaces;Mixed / augmented reality Humancentered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR62088.2024.00102},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765413,
  author={Hu, Yong-Hao and Hatada, Yuji and Narumi, Takuji},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Impact of Role Assignment through Complementary Design of Self and Other Avatars on Self-Image and Behavior Change}, 
  year={2024},
  volume={},
  number={},
  pages={874-883},
  abstract={This study investigates the impact of both self-and other’s avatars (virtual bodies) with complementary traits on self-image and behavior change, drawing on role theory: individuals adopt and internalize their social roles as expected by others. In our experiment, participants and a non-player character played a cooperative virtual reality action game together, embodying a “warrior” avatar and a “witch” avatar with complementary appearances and in-game abilities. Results revealed that role assignments based on the use of complementary avatars has significant interaction effects with participants’ personal characteristics on influencing individuals’ behavior and self-image. Furthermore, a risk of unpredicted change resulting from failed role assignments was discovered, suggesting the importance of achieving role assignments that match avatar complementarity. By providing a new perspective on the impact of interactions between multiple users with diverse avatars, these findings contribute to our understanding of the mechanisms of cognitive augmentation with avatars and have implications for the design of avatar-related experiences in the metaverse.},
  keywords={Metaverse;Avatars;Games;Augmented reality;avatar;behavioral changes;self-image;role theory;complementary of expectations;proteus effect},
  doi={10.1109/ISMAR62088.2024.00103},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765451,
  author={Woodworth, Jason W. and Borst, Christoph W.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Study of Interfaces for Time-Continuous Emotion Reporting and the Relationship Between Interface and Reported Emotion}, 
  year={2024},
  volume={},
  number={},
  pages={884-892},
  abstract={This paper presents interfaces for reporting emotion in real-time during VR stimuli. Self-reported emotional responses are critical for developing emotion recognition systems. Such responses can vary throughout a stimulus such as 360° video, but most interfaces for reporting emotion are designed to be used after the experience. This reduces the entire experience to a single data point and raises concerns about validity when multiple emotions can be elicited across the stimulus. We introduce and compare user interfaces that allow for real-time emotion reporting throughout the length of the stimulus. Each interface varies on how emotion is physically input by the user and displayed back to them for confirmation. A preliminary study compared five such interfaces, gathering initial impressions, comparing control schemes, and rating intuitiveness. A primary study considered four refined interface designs and compared reporting precision and subjective opinions. Results suggest that a single interface face icon responding to arousal and valence reports and a gradiating color wheel are intuitive, precise, and unobtrusive. More broadly, results indicate the type of rating interface has a significant effect on the given ratings.},
  keywords={Training;Emotion recognition;Three-dimensional displays;Minimally invasive surgery;Limiting;Wheels;Streaming media;Real-time systems;Usability;Testing;Affective Computing;Emotion Rating Interfaces},
  doi={10.1109/ISMAR62088.2024.00104},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765445,
  author={Dudley, John J. and Karlson, Amy and Todi, Kashyap and Benko, Hrvoje and Longest, Matt and Wang, Robert and Kristensson, Per Ola},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Efficient Mid-Air Text Input Correction in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={893-902},
  abstract={The task of inputting text within virtual reality has attracted significant research attention over the last five years. Less well explored is the related task of correcting inputted text when errors are made. This is despite the fact that considerable time and frustration stems from efforts to correct text. In this paper, we bridge this gap in prior research and explore efficient methods for supporting text input correction in virtual reality. We present a characterization of the types and frequencies of errors encountered when inputting text in virtual reality and an analysis of effective editing strategies. We also present the results of a user study evaluating the performance and usability trade-offs for several interaction methods leveraging the unique capabilities of modern head-mounted displays.},
  keywords={Bridges;Head-mounted displays;Terminology;Error correction;Usability;Standards;Augmented reality;text entry;error correction;virtual reality},
  doi={10.1109/ISMAR62088.2024.00105},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765155,
  author={Dai, Ruyun and Kim, Jimoon and Han, Sangsun and Kim, Jisun and Kim, Sungkean and Kim, Binna and Kim, Kibum},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Virtual Reality Multimodal Stabilization Intervention for Covid-19 Trauma Treatment}, 
  year={2024},
  volume={},
  number={},
  pages={903-912},
  abstract={THE COVID-19 pandemic had a serious impact on the public’s mental health and may give rise to post-traumatic symptoms. As the first treatment phase, stabilization focuses on reducing psychological discomfort after experiencing trauma. Previously, stabilization was performed by imaginative guided imagery (GI) and digital audio files. However, this kind of intervention limits the patient’s imagination and ability to recall from memory. Virtual reality (VR) has been considered to have great potential in psychotherapy because of its characteristics of immersion, vividness, and interactivity. Our study proposes multimodal VR stabilization intervention that includes three stabilization techniques: light stream, breathing relaxation, and containment. We combined multisensory (thermal) stimulation and eye-tracking interaction in the VR environment. We conducted a preliminary verification of the treatment effect on groups that were at high risk of experiencing psychological trauma during COVID-19, including healthcare workers and COVID-19 survivors, and conducted a controlled experiment with a previously released audio-based trauma stabilization mobile App. The results show that VR stabilization intervention is feasible, and compared with the App group, the subjective unit of disturbance (SUD) and the objective physiological indicator, the heart rate variability (HRV) of participants in the VR group improved more significantly. Our research experience helps to enrich the research scope of VR technology in the field of trauma treatment through the evidence provided by physiological and psychological measurements.},
  keywords={COVID-19;Atmospheric measurements;Medical services;Gaze tracking;Streaming media;Particle measurements;Physiology;Mobile applications;Heart rate variability;Research and development;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR62088.2024.00106},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765385,
  author={Tian, Boyuan and Pang, Yihan and Huzaifa, Muhammad and Wang, Shenlong and Adve, Sarita},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Energy-Efficiency by Navigating the Trilemma of Energy, Latency, and Accuracy}, 
  year={2024},
  volume={},
  number={},
  pages={913-922},
  abstract={Extended Reality (XR) enables immersive experiences through untethered headsets but suffers from stringent battery and resource constraints. Energy-efficient design is crucial to ensure both longevity and high performance in XR devices. However, latency and accuracy are often prioritized over energy, leading to a gap in achieving energy efficiency. This paper examines scene reconstruction, a key building block for immersive XR experiences, and demonstrates how energy efficiency can be achieved by navigating the trilemma of energy, latency, and accuracy. We explore three classes of energy-oriented optimizations, covering the algorithm, execution, and data, that reveal a broad de-sign space through configurable parameters. Our resulting 72 designs expose a wide range of latency and energy trade-offs, with a smaller range of accuracy loss. We identify a Pareto-optimal curve and show that the designs on the curve are achievable only through synergistic co-optimization of all three optimization classes and by considering the latency and accuracy needs of downstream scene reconstruction consumers. Our analysis covering various use cases and measurements on an embedded class system shows that, relative to the baseline, our designs offer energy benefits of up to $60 \times$ with potential latency range of $4 \times$ slowdown to $2 \times$ speedup. Detailed exploration of a use case across representative data sequences from ScanNet showed about $25 \times$ energy savings with $1.5 \times$ latency reduction and negligible reconstruction quality loss.},
  keywords={Performance evaluation;Headphones;Accuracy;Navigation;Extended reality;Energy measurement;Extraterrestrial measurements;Energy efficiency;Space exploration;Optimization;Energy efficiency;mobile computing;design space exploration;extended reality;scene reconstruction;TSDF fusion},
  doi={10.1109/ISMAR62088.2024.00107},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765371,
  author={Shen, Junxiao and Dudley, John J. and Kristensson, Per Ola},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception}, 
  year={2024},
  volume={},
  number={},
  pages={923-931},
  abstract={We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as lifelogging. However, a significant challenge arises from the sheer volume of video data generated through lifelogging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation agent that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Additionally, we propose using large language models to facilitate natural language querying. Our agent underwent extensive evaluation using the QA-Ego4D dataset and achieved state-of-the-art results with a BLEU score of 8.3, outperforming conventional machine learning models that scored between 3.4 and 5.8. Additionally, we conducted a user study in which participants interacted with the human memory augmentation agent through episodic memory and open-ended questions. The results of this study show that the agent results in significantly better recall performance on episodic memory tasks compared to human participants. The results also highlight the agent’s practical applicability and user acceptance.},
  keywords={Privacy;Large language models;Natural languages;Memory management;Machine learning;Encoding;Vectors;Real-time systems;Augmented reality;Videos;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Information systems;Information retrieval;Retrieval tasks and goals;Question answering;Computing methodologies;Artificial intelligence;Computer vision;Computer vision tasksVisual content-based indexing and retrieval},
  doi={10.1109/ISMAR62088.2024.00108},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765404,
  author={Ferreira, João P. and Ferreira-Brito, Filipa and Guerreiro, João and Guerreiro, Tiago},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Crafting Virtual Realities: Designing a VR End-User Authoring Platform for Personalised Exposure Therapy}, 
  year={2024},
  volume={},
  number={},
  pages={932-940},
  abstract={Abstract Exposure therapy (ET) gradually introduces people to the objects, animals, or situations they fear to help them overcome the angst with that source of anxiety. VR(ET) enables exposure to various triggers in the safety of the clinical or home environments. While prior work has explored how specific VR environments support therapy in contexts such as social anxiety or arachnophobia, therapy’s individualised nature is often overlooked. We used an iterative participatory design approach to develop an authoring platform for therapists, enabling them to tailor VR environments during exposure by changing and parameterising elements and reapplying past scenes. We used this platform as a design probe in a study with ten therapists to elicit discussions about the design of VRET experiences and therapists’ authoring needs. Findings highlight the value of controlling the stimuli presented to patients and deviating from stereotypical scenarios, and the importance of further investigating the therapist’s virtual representation.},
  keywords={Animals;Anxiety disorders;Medical treatment;Safety;Iterative methods;Probes;Augmented reality;Participatory design;virtual reality;exposure therapy;anxiety disorders;VRET;authoring},
  doi={10.1109/ISMAR62088.2024.00109},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765158,
  author={Krogmeier, Claudia and Tison, Emma and Dillmann, Justin and Prouzeau, Arnaud and Prouteau, Antoinette and Hachet, Martin},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Leveraging Augmented Reality for Understanding Schizophrenia - Design and Evaluation of a Dedicated Educational Tool}, 
  year={2024},
  volume={},
  number={},
  pages={941-950},
  abstract={Schizophrenia is a serious mental health disorder which may include symptoms such as hallucinations, delusions, and disorganized behavior. In addition to experiencing a diverse symptomatology, individuals with schizophrenia suffer from significant stigmatization which can interfere with effective treatment of the disorder among other issues. As a primary source of stigmatization comes from healthcare professionals, we were motivated to explore new ways in which to educate healthcare students about the symptoms of schizophrenia. Despite its potential to immerse users in new experiences within a real environment, little research concerning the use of augmented reality (AR) to simulate schizophrenia exists. In this paper, we present Live-It, a tool designed using recommendations from prior work as well as inspiration from lived experiences to educate mental health students about schizophrenia. The simulation uses the video passthrough capabilities of the Meta Quest 3 headset to simulate delusions, auditory hallucinations and additional symptomatology. Using thematic analysis, we evaluated our simulation with nineteen students and eighteen experts in healthcare to understand its ability to engage users and reliably represent symptoms of the disorder, as well as to determine how best to improve upon the design before the tool is widely deployed in mental health curricula.Our findings suggest that participants better understood symptoms of schizophrenia after experiencing the simulation, highlighting the potential of Live-It to be used as an educational tool. We present our design, provide a detailed analysis of our findings, and underline next steps in the development of our tool.},
  keywords={Headphones;Measurement;Visualization;TV;Image resolution;Surveillance;Medical services;Mental health;Reliability engineering;Augmented reality;Human-centered computing;Augmented reality;Simulation;Schizophrenia;Education;Design},
  doi={10.1109/ISMAR62088.2024.00110},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765490,
  author={Tanaka, Yudai and Weiss, Neil and Bolger-Cruz, Robert Cole and Hartcher-O’Brien, Jess and Flynn, Brendan and Boldu, Roger and Colonnese, Nicholas},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ReaWristic: Remote Touch Sensation to Fingers from a Wristband via Visually Augmented Electro-Tactile Feedback}, 
  year={2024},
  volume={},
  number={},
  pages={951-960},
  abstract={We present a technique for providing remote tactile feedback to the thumb and index finger via a wristband device. This enables haptics for touch and pinch interactions in mixed reality (MR) while keeping the user’s hand entirely free. We achieve this through a novel cross-modal stimulation, which we term visually augmented electro-tactile feedback. This consists of (1) electrically stimulating the nerves that innervate the targeted fingers using our wristband device; and (2) concurrently, visually augmenting the targeted finger in MR to steer the perceived sensation to the desired location. In our psychophysics study, we found that our approach provides tactile perception akin to tapping and, even from the wrist, it is capable of delivering the sensation to the targeted fingers with $\sim$50% of sensation occurring in the thumb and $\sim$40% of sensation occurring in the index finger. These results on localizability are unprecedented compared to electro-tactile feedback alone or any prior work for creating sensations in the hand with devices worn on the wrist/arm. Moreover, unlike conventional electro-tactile techniques, our wristband dispenses with gel electrodes. Instead, it incorporates custommade elastomer-based dry electrodes and a stimulation waveform designed for the electrodes, ensuring the practicality of the device beyond laboratory settings. Lastly, we evaluated the haptic realism of our approach in mixed reality and elicited qualitative feedback from users. Participants preferred our approach to a baseline vibrotactile wrist-worn device.},
  keywords={Electrodes;Wrist;Visualization;Thumb;Tactile sensors;Mixed reality;Haptic interfaces;Augmented reality;Electro-tactile;haptics;mixed reality;wristband},
  doi={10.1109/ISMAR62088.2024.00111},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765368,
  author={O’Keeffe, Spencer and Thomas, Bruce H and O’Hehir, Jim and Rombouts, Jan and Balasso, Michelle and Cunningham, Andrew},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Immersive Focus+Context Techniques to Assist in Interpretation of High Density Forest Point Clouds}, 
  year={2024},
  volume={},
  number={},
  pages={961-970},
  abstract={Virtual Reality (VR) is changing how we interact with complex spatial datasets. VR has the potential within forestry to help analyse remotely sensed spatial data for inventory management. However, challenges of scale and legibility of dense forest point clouds must be addressed. To do so, we present three VR Focus+Context (F+C) techniques: Wedgelight, Remote Lantern, and Ray Lantern. These techniques present point cloud data using a constrained highdetail focus within a broader abstracted context representation, manipulated using egocentric and exocentric interactions. We conducted two user studies. The first informed the design of the F+C techniques. The results from the first study indicate that VR significantly outperforms desktop displays in basic analysis of forestry point clouds, with improvements to both time and accuracy. The second study evaluated the performance of the resulting F+C techniques in a task analogous to real-world forestry operations. Our F+C techniques significantly outperformed current approaches, confirming their viability for facilitating detailed and effective forest assessments. These findings indicate that VR has significant advantages for spatial data analysis tasks in forestry, making a strong argument for its adoption in future analytical frameworks. The FF+CC visualisation techniques we introduced show considerable promise for analysis of high density point clouds, opening new avenues for development in immersive analytics. Supplemental materials (code, scans, results) are available at https://osf.io/hqc9u.},
  keywords={Point cloud compression;Data analysis;Accuracy;Forestry;Vegetation;Switches;Inventory management;Spatial databases;Remote sensing;Synthetic data;Virtual Reality;Immersive Analytics;Visualisation},
  doi={10.1109/ISMAR62088.2024.00112},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765414,
  author={Treffer, Anna and Clark, Adrian and Lukosch, Stephan},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Teaching Dance with Mixed Reality Mirrors}, 
  year={2024},
  volume={},
  number={},
  pages={971-980},
  abstract={Regardless of the style, most western dance is taught in a similar manner. A student comes to a studio, the instructor demonstrates a series of movements that the student attempts to replicate - often in front of a mirror - and the instructor provides corrective feedback. Unfortunately, this approach means that beginners are only able to practice and receive corrective feedback when the instructor is available. This paper reports on a study that assessed whether a Mixed Reality (MR) mirror displaying a virtual instructor overlaid with visual feedback can be used to teach a beginner a simple dance routine, replacing the traditional instructor and mirror method, and making beginners dance training more accessible. Three visual feedback modes to indicate how to achieve a correct pose in a dance sequence were designed, based on findings from a literature review, input from expert interviews and an online survey. These feedback modes, titled Spheres, Rubber Bands, and Arrows, were implemented and used as randomized conditions in a user study where participants learned three simple dance sequences. The user study showed that participants performed best with the Arrows feedback mode, though the preference rank for this feedback mode was the lowest. In contrast, the participants’ most preferred feedback mode was Spheres, though participants performed poorest with this mode. These findings suggest that user preference and performance in MR mirror dance training systems need to be balanced to create a system that is effective and enjoyable.},
  keywords={Training;Surveys;Humanities;Visualization;Bibliographies;Mixed reality;Particle measurements;Mirrors;Rubber;Interviews;Mixed Reality;MR Mirror;Magic Mirror;virtual instructor;visual feedback;dance;teaching;learning},
  doi={10.1109/ISMAR62088.2024.00113},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765427,
  author={Hu, Jinghui and Dudley, John J. and Kristensson, Per Ola},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={LookUP: Command Search Using Dwell-free Eye Typing in Mixed Reality}, 
  year={2024},
  volume={},
  number={},
  pages={981-989},
  abstract={We introduce LookUP, a novel general purpose command search system for mixed reality headsets, offering a hands-free experience through dwell-free eye typing. With LookUP, users can trigger the display of a virtual keyboard with a simple upward head motion. The keyboard then uses a statistical decoder to interpret users’ intended text based on their eye movements. This approach diverges from traditional dwell-time methods, significantly enhancing typing speed and efficiency. Our research involved deploying LookUP on a HoloLens 2, and benchmarking it against a dwell-based command search baseline and the native HoloLens system menu. Our user study indicated that participants spent a significantly shorter time using LookUP with dwell-free eye typing in command search and entry, demonstrating LookUP’s potential to be a complementary command input for mixed reality headsets.},
  keywords={Headphones;Mixed reality;Keyboards;Benchmark testing;Decoding;Augmented reality;Mixed Reality;Command search;gaze tracking},
  doi={10.1109/ISMAR62088.2024.00114},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765403,
  author={Maslych, Mykola and Yu, Difeng and Ghasemaghaei, Amirpouya and Hmaiti, Yahya and Martinez, Esteban Segarra and Simon, Dominic and Taranta, Eugene M. and Bergström, Joanna and LaViola, Joseph J.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={From Research to Practice: Survey and Taxonomy of Object Selection in Consumer VR Applications}, 
  year={2024},
  volume={},
  number={},
  pages={990-999},
  abstract={Object selection has been explored extensively in the VR research literature. However, the research is typically conducted in constrained experimental setups. It remains unclear whether the designed selection techniques fit the prevalent practical uses and whether the experimental tasks represent important challenges in real applications. To identify and help bridge these gaps, we surveyed current consumer VR applications, containing 206 popular VR game and 3D modeling applications. We extracted 1300+ selection scenarios based on video analyses of these applications and derived a taxonomy to understand common patterns on where and how selections occur. Our findings reveal significant gaps in selection tasks and techniques between research and consumer applications. We also present an interactive visualization tool to help researchers explore the VR object selection scenarios. Finally, we discuss how our work can help researchers and developers evaluate techniques in meaningful tasks and drive the design of techniques.},
  keywords={Surveys;Bridges;Visualization;Three-dimensional displays;Databases;Taxonomy;Games;Drives;Augmented reality;3D user interfaces;consumer applications;database;object selection;target selection;video games;virtual reality},
  doi={10.1109/ISMAR62088.2024.00115},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765375,
  author={Gottsacker, Matt and Furuya, Hiroshi and Battistel, Laura and Jimenez, Carlos Pinto and LaMontagna, Nicholas and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Spatial Cognitive Residue and Methods to Clear Users’ Minds When Transitioning Between Virtual Environments}, 
  year={2024},
  volume={},
  number={},
  pages={1000-1009},
  abstract={In most cases, retaining memories of things we have experienced in the past is desirable, but in some cases, we want to clear our minds so that we may focus completely on subsequent activities. When someone switches from one task to another, they commonly incur some “cognitive residue” where some of their cognitive resources such as working memory and attention remain devoted to their previous task even after they try to switch their focus to their new task. This residue could have a negative impact on their performance in the next task, and in such circumstances, it is important to reduce that residue. In this paper, we explore the concept of cognitive residue in the context of switching between virtual reality (VR) environments. We conducted a human-subject experiment (N=24) with a spatial recall task to investigate how different visual transitions might reduce participants’ spatial cognitive residue. In this instance, more errors on the recall task corresponds to less spatial cognitive residue. We found that transitions that lasted one minute successfully reduced spatial cognitive residue: they significantly reduced participants’ abilities to recall the positions of objects in their previous VE compared to an instantaneous cut transition. Additionally, for transitions that showed a nature scene, greater head movement significantly correlated with more spatial memory errors (i.e., less spatial cognitive residue). We discuss how these findings can be applied to support users transitioning between virtual tasks and environments in VR task switching scenarios.},
  keywords={Fading channels;Visualization;Memory management;Virtual environments;Switches;Augmented reality;Extended reality;transitions;task switching;spatial memory;cognitive residue},
  doi={10.1109/ISMAR62088.2024.00116},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765380,
  author={Ablett, Daniel and Cunningham, Andrew and Lee, Gun A. and Thomas, Bruce H.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Adaptive Portals: Enhancing Virtual Reality Interaction Spaces With Real-Time Self-Adjusting Portals}, 
  year={2024},
  volume={},
  number={},
  pages={1010-1018},
  abstract={This paper introduces foundational techniques for Adaptive Portals (AP); portals that adjust to users’ needs in Virtual Reality (VR). At the forefront of our contributions is the AP-Reach technique, with Entry and Exit strategies, designed to overcome the spatial limitations of portals and VR, enabling users to comfortably extend their reach to objects beyond their immediate vicinity. In parallel, we present PH-Reach as an alternative to the AP concept, also aimed at extending user reach inside portals. The effectiveness of these reach techniques is evaluated through a comparative user study, which compared Entry AP-Reach, Exit AP-Reach, and PHReach techniquesagainst traditional natural reach as a baseline. Results show Exit AP-Reach surpassed both its counterparts and the baseline to various degrees in terms of reducing task completion time, physical movement/demand, effort, and frustration. Additionally, this paper introduces AP-Bounds and AP-Restriction.},
  keywords={Accuracy;Ergonomics;Real-time systems;Portals;Augmented reality;Virtual reality;portal;reach},
  doi={10.1109/ISMAR62088.2024.00117},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765364,
  author={Li, Gang and Guha, Tanaya and Onuoha, Ogechi and Qiu, Zhanyan and Grant, Alana and Feng, Zejian and Zhang, Zirui and Pohlmann, Kathariana and McGill, Mark and Brewster, Stephen and Pollick, Frank},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Detecting in-car VR Motion Sickness from Lower Face Action Units}, 
  year={2024},
  volume={},
  number={},
  pages={1019-1028},
  abstract={This paper presents the first in-car VR motion sickness (VRMS) detection model based on lower face action units (LF-AUs). Initially developed in a simulated in-car environment with 78 participants, the model’s generalizability was later tested in realworld driving conditions. Motion sickness was induced using visual linear motion in the VR headset and physical horizontal rotation via a rotating chair. We used a convolutional neural network (MobileNetV3) to automatically extract LF-AUs from images of the users’ mouth region, captured by the VR headset’s built-in camera. These LF-AUs were then used to train a Support Vector Regression (SVR) model to estimate motion sickness scores. We compared the SVR model’s performance using LF-AUs, pupil diameters, and physiological features (individually and in combination) from the same VR headset. Results showed that both individual LF-AU (right dimple) and combined LF-AUs had significant Pearson correlations with self-reported motion sickness scores and achieved lower root mean squared error compared to pupil diameters. The best detection results were obtained by combining LF-AUs and pupil diameters, while physiological features alone did not yield significant results. The LF-AUs-based model demonstrated encouraging generalizability across different settings in the independent studies.},
  keywords={Headphones;Visualization;Correlation;Machine learning;Motion sickness;Cameras;Feature extraction;Physiology;Pupils;Faces;VR;Motion Sickness;Lower Face Action Units},
  doi={10.1109/ISMAR62088.2024.00118},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765486,
  author={Ibrahim, Fatma E. and Zayed, Hala H. and Koura, Manal H. and ElSayed, Neven},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Efficacy of Virtual Reality Distraction for Reducing Chronic Pain}, 
  year={2024},
  volume={},
  number={},
  pages={1029-1037},
  abstract={Chronic pain is a major health problem that requires the development of novel treatment strategies to address this growing issue. This paper introduces the application of virtual reality technology for managing chronic pain in outpatient settings. Virtual reality is unique in its ability to engage users through a multisensory experience merging visual, auditory, and sometimes tangible stimuli. We conducted a study involving patients experiencing chronic lumbar and cervical pain to assess its effectiveness in reducing chronic pain. Participants were asked to report their pain scores before, during, and immediately after the virtual reality session. Additionally, the study automatically recorded each participant’s completion time and error rate. These metrics were then analyzed to investigate the potential relationship between pain intensity and task performance. The results demonstrated a significant decrease in pain ratings both during and after the virtual reality session. The findings also revealed an interaction between pain intensity levels and the time spent playing the virtual reality game. Specifically, higher pain levels were associated with shorter completion times for tasks. Additionally, as pain increased, patients’ accuracy in performing tasks decreased. The study we conducted showed a high potential for enhancing distraction levels for patients with chronic pain using virtual reality.},
  keywords={Measurement;Visualization;Accuracy;Pain;Error analysis;Merging;Virtual reality;Games;Time factors;Augmented reality;Virtual Reality;Chronic Pain;Distraction;Outpatient Clinic},
  doi={10.1109/ISMAR62088.2024.00119},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765388,
  author={Bauer, David and Zheng, Chengbo and Kwon, Oh-Hyun and Ma, Kwan-Liu},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Multi-Layout Design For Immersive Visualization of Hierarchical Network Data}, 
  year={2024},
  volume={},
  number={},
  pages={1038-1047},
  abstract={Visualization plays a vital role in making sense of complex network data. Recent studies have shown the potential of using extended reality (XR) for the immersive exploration of networks. The additional depth cues offered by XR help users perform better in certain tasks when compared to using traditional desktop setups. However, prior works on immersive network visualization rely mostly on singular, static graph layouts to present the data to the user. This poses a problem since there is no optimal layout for all possible tasks. The choice of layout heavily depends on the type of network and the task at hand. We introduce a multi-layout design that promotes more efficient use of the available space in VR environments and allows users to explore hierarchical network data in immersive space effectively. We implement our design with a choice of four distinct views on the network. The resulting system leverages various existing layout techniques to efficiently use the available space in VR and provide an optimal view of the data depending on the task and the level of detail required to solve it. To evaluate our approach, we conducted a user study comparing it against the state of the art for immersive network visualization. Participants performed tasks at varying spatial scopes. The results show that our approach outperforms the baseline in spatially focused scenarios as well as when the whole network needs to be considered.},
  keywords={Extended reality;Layout;Data visualization;Network analyzers;Complex networks;Augmented reality;Immersive environment;head-mounted display;graph layout;network community structure},
  doi={10.1109/ISMAR62088.2024.00120},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765425,
  author={Setu, Jyotirmay Nag and Le, Joshua M and Kundu, Ripan Kumar and Giesbrecht, Barry and Höllerer, Tobias and Hoque, Khaza Anuarul and Desai, Kevin and Quarles, John},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mazed and Confused: A Dataset of Cybersickness, Working Memory, Mental Load, Physical Load, and Attention During a Real Walking Task in VR}, 
  year={2024},
  volume={},
  number={},
  pages={1048-1057},
  abstract={Virtual Reality (VR) is quickly establishing itself in various industries, including training, education, medicine, and entertainment, in which users are frequently required to carry out multiple complex cognitive and physical activities. However, the relationship between cognitive activities, physical activities, and familiar feelings of cybersickness is not well understood and thus can be unpredictable for developers. Researchers have previously provided labeled datasets for predicting cybersickness while users are stationary, but there have been few labeled datasets on cybersickness while users are physically walking. Moreover, it is unclear how walking while cybersick will affect cognitive load, even though room-scale interaction is typical in many VR games. Thus, from 39 participants, we collected head orientation, head position, eye tracking, images, physiological readings from external sensors, and the self-reported cybersickness severity, physical load, and mental load in VR. Throughout the data collection, participants navigated mazes via real walking and performed tasks challenging their attention and working memory. To demonstrate the dataset’s utility, we conducted a case study of training classifiers in which we achieved 95% accuracy for cybersickness severity classification. The noteworthy performance of the straightforward classifiers makes this dataset ideal for future researchers to develop cybersickness detection and reduction models. To better understand the features that helped with classification, we performed SHAP(SHapley Additive exPlanations) analysis, highlighting the importance of eye tracking and physiological measures for cybersickness prediction while walking. This open dataset can allow future researchers to study the connection between cybersickness and cognitive loads and develop prediction models. This dataset will empower future VR developers to design efficient and effective Virtual Environments by improving cognitive load management and minimizing cybersickness.},
  keywords={Legged locomotion;Training;Solid modeling;Cybersickness;Virtual environments;Gaze tracking;Predictive models;Cognitive load;Sensors;Load modeling;virtual reality;real walking;cybersickness;cognitive load;attention;working memory;datasets;user studies;machine learning;deep learning;classification;explainable AI},
  doi={10.1109/ISMAR62088.2024.00121},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765431,
  author={Jung, Sungchul and Wile, Nicholas and Jung, Kyung Hun Jay},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Implicit and Explicit Stimuli of Virtual Agents with Increased Group Size in VR}, 
  year={2024},
  volume={},
  number={},
  pages={1058-1067},
  abstract={It remains unknown whether the virtual agents’ implicit behaviors (e.g., the agent’s eye-gaze shifting) would also cause such an observable effect on the users’ experiences even when the users are not consciously aware of the implicit behaviors of the agents. More importantly, it is unclear whether the effect of the implicit behaviors of agents can be amplified as the agents’ group size increases. To answer these questions, we asked two participant groups ($\mathrm{N}=12$ each) to deliver a public speech in front of the virtual agents showing either an explicit (head movement) or implicit (eye movement) behavior as we manipulated the agents’ group sizes (small, medium, and large). We measured the users’ perceptions, emotions, and behavior (head and eye movements) in the study. A posterior psychophysical experiment confirmed the implicit nature of the eye-movement behavior of the virtual agents (i.e., participants’ detection rate of the eye movements was less than the chance level). The results showed that the explicit behavior of the virtual agents caused the expected participant behavior changes along with negative emotions. In contrast, the implicit behavior of the agents caused largely positive emotions, yet, without significant behavior changes of the participants. Most importantly, as the group size of the virtual agents increased, the degree of their effect on users increased in both the explicit head-movement condition and the implicit eye-movement condition.},
  keywords={Atmospheric measurements;Public speaking;Particle measurements;User experience;Emotional responses;Augmented reality;Virtual Agents;Virtual Classroom;Public Speaking;Implicit-Stimuli;Group Size;Behavior;Perceptual Response;Emotion;Head and Eye Tracking;Detection Rates;Human-centered computing—Virtual reality;Computing methodologies—Perception;Human-centered computing—User studies;Human-centered computing—User interface design},
  doi={10.1109/ISMAR62088.2024.00122},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765477,
  author={Gunawardhana, Bhasura S. and Zhang, Yunxiang and Sun, Qi and Deng, Zhigang},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Toward User-Aware Interactive Virtual Agents: Generative Multi-Modal Agent Behaviors in VR}, 
  year={2024},
  volume={},
  number={},
  pages={1068-1077},
  abstract={Virtual agents serve as a vital interface within XR platforms. However, generating virtual agent behaviors typically rely on pre-coded actions or physics-based reactions. In this paper we present a learning-based multimodal agent behavior generation framework that adapts to users’ in-situ behaviors, similar to how humans interact with each other in the real world. By leveraging an in-house collected, dyadic conversational behavior dataset, we trained a conditional variational autoencoder (CVAE) model to achieve user-conditioned generation of virtual agents’ behaviors. Together with large language models (LLM), our approach can generate both the verbal and non-verbal reactive behaviors of virtual agents. Our comparative user study confirmed our method’s superiority over conventional animation graph-based baseline techniques, particularly regarding user-centric criteria. Thorough analyses of our results underscored the authentic nature of our virtual agents’ interactions and the heightened user engagement during VR interaction.},
  keywords={Bridges;Adaptation models;Large language models;Neural networks;Animation;Real-time systems;Augmented reality;Virtual agents;human-VR interaction;user-conditioned motion generation;user-aware interaction},
  doi={10.1109/ISMAR62088.2024.00123},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765435,
  author={Brübach, Larissa and Röhm, Mona and Westermeier, Franziska and Latoschik, Marc Erich and Wienrich, Carolin},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Manipulating Immersion: The Impact of Perceptual Incongruence on Perceived Plausibility in VR}, 
  year={2024},
  volume={},
  number={},
  pages={1078-1086},
  abstract={This work presents a study where we used incongruencies on the cognitive and the perceptual layer to investigate their effects on perceived plausibility and, thereby, presence and spatial presence. We used a $2 \times 3$ within-subject design with the factors familiar size (cognitive manipulation) and immersion (perceptual manipulation). For the different levels of immersion, we implemented three different tracking qualities: rotation-and-translation tracking, rotation-only tracking, and stereoscopic-view-only tracking. Participants scanned products in a virtual supermarket where the familiar size of these objects was manipulated. Simultaneously, they could either move their head normally or need to use the thumbsticks to navigate their view of the environment. Results show that both manipulations had a negative effect on perceived plausibility and, thereby, presence. In addition, the tracking manipulation also had a negative effect on spatial presence. These results are especially interesting in light of the ongoing discussion about the role of plausibility and congruence in evaluating XR environments. The results can hardly be explained by traditional presence models, where immersion should not be an influencing factor for perceived plausibility. However, they are in agreement with the recently introduced Congruence and Plausibility (CaP) model and provide empirical evidence for the model’s predicted pathways.},
  keywords={Headphones;Tracking;Navigation;Cause effect analysis;Predictive models;Augmented reality;plausibility;immersion;presence;VR;congruence},
  doi={10.1109/ISMAR62088.2024.00124},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765384,
  author={Macdonald, Shaun and Bretin, Robin and ElSayed, Salma},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating Transferable Emotion Expressions for Zoomorphic Social Robots using VR Prototyping}, 
  year={2024},
  volume={},
  number={},
  pages={1087-1096},
  abstract={Zoomorphic robots have the potential to offer companionship and well-being as accessible, low-maintenance alternatives to pet ownership. Many such robots, however, feature limited emotional expression, restricting their potential for rich affective relationships with everyday domestic users. Additionally, exploring this design space using hardware prototyping is obstructed by physical and logistical constraints. We leveraged virtual reality rapid prototyping with passive haptic interaction to conduct a broad mixed-methods evaluation of emotion expression modalities and participatory prototyping of multimodal expressions. We found differences in recognisability, effectiveness and user empathy between modalities while highlighting the importance of facial expressions and the benefits of combining animal-like and unambiguous modalities. We use our findings to inform promising directions for the affective zoomorphic robot design and potential implementations via hardware modification or augmented reality, then discuss how VR prototyping makes this field more accessible to designers and researchers.},
  keywords={Emotion recognition;Text recognition;Social robots;Tail;Rapid prototyping;Hardware;Space exploration;Text to speech;Robots;Augmented reality;Human-Robot Interaction;Zoomorphic Robots;Virtual Reality;Affective Computing},
  doi={10.1109/ISMAR62088.2024.00125},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765457,
  author={Tao, Yujie and Egelman, Jordan and Bailenson, Jeremy N.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={I Feel You: Impact of Shared Body Sensations on Social Interactions in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={1097-1106},
  abstract={While one’s facial expression and voice can be easily broadcasted from one to many via digital media, the sense of touch is limited to direct interactions. What happens if such body sensations can be shared across individuals, in which one feels a touch while watching someone else being touched? In this work, we investigated the impact of such shared body sensations on social interactions in virtual reality (VR). Building upon previous research that used psychophysics methods, our work explores the practical implications of shared body sensations in Social VR, which enables interactions beyond what’s physically possible. We conducted a withingroup user study ($\mathrm{n}=32$) in which participants observed conversations between two virtual agents and shared touch with one of the agents, as shown in Figure 1. Our results showed that even experiencing shared touch sensations several times during a conversation can affect social perception and behavior. Participants reported a stronger body illusion and empathy towards the virtual agent they shared touch with and stood closer to them. These results occurred both with and without a virtual mirror that made participants’ selfavatars more salient. The findings from this study introduce a new technique to enhance social connectedness in VR, and we discuss its applications in various contexts, such as asynchronous communication and collaboration.},
  keywords={Asynchronous communication;Buildings;Collaboration;Oral communication;Media;Mirrors;Indexes;Augmented reality;Index Terms: Social Touch;Haptics;Virtual Reality},
  doi={10.1109/ISMAR62088.2024.00126},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765473,
  author={Chen, Yi-An and Wong, Sai-Keung and Chao, Yu-Ting and Babu, Sabarish V.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of Organizational and Behavioral Reactions of Virtual Crowds on Users’ Affect and Behavior in a Simulated Stressful Situation}, 
  year={2024},
  volume={},
  number={},
  pages={1107-1116},
  abstract={Evidence from the behavioral psychology literature suggests that in unexpected stressful situations, crowd behavior can have a direct impact on people’s emotions and behaviors. This paper investigates whether this effect is elicited in immersive virtual reality by a crowd of virtual humans (VHs) in an external stressful situation induced by a fire. We conducted a mixed-design study in an experimental scenario where users were asked to collect 10 items from a virtual market. The between-subjects factors were reaction type and agent type, while the within-subjects factor was the simulation phase, which included the normal and stressful situations. There were two reaction types where the VHs showed either active (agitated) or passive (calm) behavior towards the external threat situation. For the agent type, the VHs exhibited either individualistic or group (3-4 member group) behaviors. In a mixed factorial 2(crowd type)x 2(reaction type) x 2(simulation phase) design, we examined the influences of these factors on participants’ emotional and behavioral responses. The results showed that the participants’ task performance and social interaction were affected by the simulated stressor. Negative emotions were elicited when they encountered active VHs and small groups of VHs. Furthermore, a greater change in their interpersonal closeness was experienced when interacting with small groups of VHs, than when interacting with individual VHs.},
  keywords={Multimedia systems;Psychology;Augmented reality;Virtual Humans and Crowds;Stressful Simulations;Affect and Behavior;Human-Computer Interaction;Virtual Reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Animations, Evaluation/ methodology; I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism;Virtual reality},
  doi={10.1109/ISMAR62088.2024.00127},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765393,
  author={Halbig, Andreas and Latoschik, Marc Erich},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Common Cues? Toward the Relationship of Spatial Presence and the Sense of Embodiment}, 
  year={2024},
  volume={},
  number={},
  pages={1117-1126},
  abstract={The sense of presence and the sense of embodiment are two fundamental qualia, pivotal to many virtual reality experiences. Empirical research indicates a notable interdependence between these two qualia, where manipulations designed to affect one often exhibit a concurrent influence on the other. Existing theories on the development of qualia in virtual reality make no or only insufficient statements on this deep interdependence. In this work, we present a novel theoretical perspective on this connection. Based on existing theories, we argue that all the fundamental cues influencing one quale have the potential to impact the other one too. We present three studies ($n=42, n=42, n=32$) that generally support this novel perspective. Among other things, they show that traditional spatial presence cues such as head-tracking and passive depth cues (stereoscopy, linear perspective, etc.) can potentially also serve as embodiment cues. Conversely, they show that typical embodiment cues such as the visuotactile and visuoproprioceptive synchrony of a virtual hand are also spatial presence cues. The cues only differ in terms of how strongly they influence the respective quale. This novel perspective not only enhances our understanding of fundamental mechanics of virtual reality but it can also guide the development of more effective measurement instruments.},
  keywords={Stereo image processing;Instruments;Mechanical variables measurement;Augmented reality;Virtual reality;virtual embodiment;body ownership;spatial presence;mixed reality},
  doi={10.1109/ISMAR62088.2024.00128},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765455,
  author={Merz, Christian and Wienrich, Carolin and Latoschik, Marc Erich},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Does Voice Matter? The Effect of Verbal Communication and Asymmetry on the Experience of Collaborative Social XR}, 
  year={2024},
  volume={},
  number={},
  pages={1127-1136},
  abstract={This work evaluates how the asymmetry of device configurations and verbal communication influence the user experience of social eXtended Reality (XR) for self-perception, other-perception, and task perception. We developed an application that enables social collaboration between two users with varying device configurations.We compare the conditions of one symmetric interaction, where both device configurations are Head-Mounted Displays (HMDs) with tracked controllers, with the conditions of one asymmetric interaction, where one device configuration is an HMD with tracked controllers and the other device configuration is a desktop screen with a mouse. In our study, 52 participants collaborated in a dyadic interaction on a sorting task while talking to each other. We compare our results to previous work that evaluated the same scenario without verbal communication. In line with prior research, self-perception is influenced by the immersion of the used device configuration and verbal communication. While co-presence was not affected by the device configuration or the inclusion of verbal communication, social presence was only higher for HMD configurations that allowed verbal communication. Task perception was hardly affected by the device configuration or verbal communication. We conclude that the device in social XR is important for self-perception with or without verbal communication. However, the results indicate that the device configuration only affects the qualities of social interaction in collaborative scenarios when verbal communication is enabled. To sum up, asymmetric collaboration maintains the high quality of self-perception and interaction for highly immersed users while still enabling the participation of less immersed users.},
  keywords={Head-mounted displays;Extended reality;Collaboration;Virtual environments;Resists;User experience;Augmented reality;Sorting;:VR;XR;Social VR;Verbal Communication;Immersion;Co-presence;Social Presence;Asymmetric Collaboration;Dyadic;Cross-Device},
  doi={10.1109/ISMAR62088.2024.00129},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765426,
  author={Moraes, Adrielle and Hynes, Eoghan and Flynn, Ronan and Hines, Andrew and Murray, Niall},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Impact of Auditory and Audiovisual Distractors on Task Performance in a VR-based Auditory Attention Task}, 
  year={2024},
  volume={},
  number={},
  pages={1137-1146},
  abstract={Auditory attention is a fundamental cognitive process essential for effective communication and interaction. Auditory stimuli are often accompanied by distractors that can significantly impact task performance by means of reducing attention. This study investigates the influence of auditory and audiovisual distractors on an auditory attention task within a Virtual Reality classroom. As part of the user evaluation, participants had to listen to two short stories. They performed two tasks: (i) “DISTRACTORS”, where participants had to identify an auditory or an audiovisual stimulus and (ii) “KEYWORDS,” where participants had to identify a specific keyword in the story by pressing a button on the controller. During the experiment, the participants’ physiological (e.g. skin conductance levels, gaze data, etc.) and subjective data (i.e. questionnaires) were collected. The results revealed that the interval between the presentation of the keyword and the presentation of the distractors impacted task performance by negatively affecting auditory attention. Also, it was observed that the time spent looking at the speaker telling the story positively correlated with task performance, whereas the time spent looking at distractors was found to be negatively correlated with task performance. Finally, this study gives insights into how physiological metrics can be used to infer auditory attention in VR experiences.},
  keywords={Measurement;Correlation;Virtual environments;Gaze tracking;Pressing;Physiology;Skin;Real-time systems;Synchronization;Standards;Spatial audio;audio-visual systems;immersive audio;virtual reality;quality of experience},
  doi={10.1109/ISMAR62088.2024.00130},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765493,
  author={Kang, Seoyoung and Song, Hail and Yoon, Boram and Kim, Kangsoo and Woo, Woontack},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Influence of Emotion-based Prioritized Facial Expressions on Social Presence in Avatar-mediated Remote Communication}, 
  year={2024},
  volume={},
  number={},
  pages={1147-1156},
  abstract={In avatar-mediated remote communication, avatars’ facial expressions can be dynamically adjusted according to each user’s computational and device constraints, highlighting the importance of varied expressions and their impact on user perception. However, there is a lack of research on how variations in avatar facial expressions, especially when simplified, influence user perception, particularly in terms of social presence. To address this, we examine the impact of various facial expression combinations on social presence in avatar-mediated communication scenarios, ranging from informative speeches to emotional conversations. Our approach involves prioritizing avatar facial blendshape combinations using two main approaches: (1) commonly activated expressions that reflect the active facial movements observed during casual conversations, and (2) emotion-based expressions derived from Facial Action Coding System (FACS). These combinations were compared against minimal baseline and full blendshape conditions through a comprehensive study involving 32 participants. Our findings reveal that emotion-based condition achieves comparable levels of social presence and communication quality to the full condition, in both informative speeches and emotional conversations. This highlights the effectiveness of prioritizing emotion-based expressions and adopting a streamlined approach to avatar facial control. By focusing on emotional expressions while optimizing resources, this approach shows potential for enhancing the avatar-mediated communication experience, accommodating the diverse users’ contexts.},
  keywords={Emotion recognition;Avatars;Focusing;Hardware;Encoding;Distance measurement;Computational efficiency;Augmented reality;Avatars;Facial Expressions;Emotion;Social Presence;Communication Quality;Avatar-mediated Communication},
  doi={10.1109/ISMAR62088.2024.00131},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765478,
  author={Raikwar, Aditya and Plabst, Lucas and Batmaz, Anil Ufuk and Niebling, Florian and Ortega, Francisco R.},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Ping! Your Food is Ready: Comparing Different Notification Techniques in 3D AR Cooking Environment}, 
  year={2024},
  volume={},
  number={},
  pages={1157-1166},
  abstract={Implementing visual and audio notifications on augmented reality devices is a crucial element of intuitive and easy-to-use interfaces. In this paper, we explored creating intuitive interfaces through visual and audio notifications. The study evaluated user performance and preference across three conditions: visual notifications in fixed positions, visual notifications above objects, and no visual notifications with monaural sounds. The users were tasked with cooking and serving customers in an open-source Augmented-Reality sandbox environment called ARtisan Bistro. The results indicated that visual notifications above objects combined with localized audio feedback were the most effective and preferred method by participants. The findings highlight the importance of strategic placement of visual and audio notifications in AR, providing insights for engineers and developers to design intuitive 3D user interfaces.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Design methodology;User interfaces;Augmented reality;Augmented Reality;Human-computer interaction (HCI);Visualization design and evaluation methods;Notification},
  doi={10.1109/ISMAR62088.2024.00132},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765471,
  author={Das, Satabdi and Nasser, Arshad and Hasan, Khalad},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Finger-Worn Solutions for Transitioning between the Reality-Virtuality Continuum}, 
  year={2024},
  volume={},
  number={},
  pages={1167-1176},
  abstract={Head-mounted displays (HMDs) enable users to navigate the Reality-Virtuality Continuum, facilitating transitions between the Real world, Augmented Reality, Augmented Virtuality, and the Virtual world. Traditional transition methods use double taps on HMDs or buttons on handheld controllers to transition between the worlds. However, this can often disrupt hands-free interaction and hinder the overall immersion. Although prior work explored transitioning within a reality, little is known about solutions facilitating transitioning across multiple worlds. In this paper, we investigate index finger-based solutions for transitioning between multiple realities. We designed and fabricated finger-worn button configurations of 2 × 2, 2 × 1, and 4 × 1, and compared them with finger-worn solutions such as Joystick, Rotary wheel, and Slider. The results showed that the 2 × 2 button configuration is the most effective technique, minimizing trial time and ensuring user comfort. Overall, this research enhances VR user experiences by improving interaction techniques for fluid switching between realities in the Reality-Virtuality Continuum.},
  keywords={Head-mounted displays;Fluids;Accuracy;Navigation;Augmented virtuality;Wheels;Switches;Resists;Indexes;Augmented reality;Reality-Virtuality Continuum;Multiple Reality Transitions;Finger-Based Solution;2×2 Button;Physical Button},
  doi={10.1109/ISMAR62088.2024.00133},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765367,
  author={Jannat, Marium-E- and Dhaka, Prateek and Katsuragawa, Keiko and Hasan, Khalad},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Augmented Reality User Interface Transitions Across Mid-Air, On-Body and Physical Surfaces}, 
  year={2024},
  volume={},
  number={},
  pages={1177-1186},
  abstract={Augmented Reality User Interfaces commonly stay floated in midair unless explicitly moved by users. This often results in suboptimal performance due to the absence of haptic feedback and arm fatigue. We explore the transition of AR Interfaces to leverage onbody and physical surfaces, e.g., the arm and desk, in addition to mid-air. We begin with a user study to assess the potential of these surfaces for transitioning virtual UIs. Study results indicate a strong user preference for transitioning interfaces from mid-air to physical surfaces when they are available. We further explore three UI transition mechanisms: manual, semi-automatic, and automatic, each with varying levels of automation and user control. Results from a user study reveal that semi-automatic transition is the most preferred method, as it offers a good balance between automation and user control. We conclude with design guidelines for transitioning AR UIs across mid-air, on-body, and physical surfaces.},
  keywords={Automation;Design methodology;Focusing;Manuals;User interfaces;Aerospace electronics;Fatigue;Haptic interfaces;Augmented reality;Guidelines;Transitional User Interface;Augmented Reality;Transition;Usability},
  doi={10.1109/ISMAR62088.2024.00134},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765469,
  author={Yang, Yuanwang and Feng, Qiao and Lai, Yu-Kun and Li, Kun},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={R2Human: Real-Time 3D Human Appearance Rendering from a Single Image}, 
  year={2024},
  volume={},
  number={},
  pages={1187-1196},
  abstract={Rendering 3D human appearance from a single image in real-time is crucial for achieving holographic communication and immersive VR/AR. Existing methods either rely on multi-camera setups or are constrained to offline operations. In this paper, we propose R2Human, the first approach for real-time inference and rendering of photorealistic 3D human appearance from a single image. The core of our approach is to combine the strengths of implicit texture fields and explicit neural rendering with our novel representation, namely Z-map. Based on this, we present an end-to-end network that performs high-fidelity color reconstruction of visible areas and provides reliable color inference for occluded regions. To further enhance the 3D perception ability of our network, we leverage the Fourier occupancy field as a prior for generating the texture field and providing a sampling surface in the rendering stage. We also propose a consistency loss and a spatial fusion strategy to ensure the multi-view coherence. Experimental results show that our method outperforms the state-of-the-art methods on both synthetic data and challenging real-world images, in real-time. The project page can be found at http://cic.tju. edu.cn/faculty/likun/projects/R2Human.},
  keywords={Surface reconstruction;Three-dimensional displays;Image color analysis;Coherence;Rendering (computer graphics);Real-time systems;Surface texture;Reliability;Image reconstruction;Synthetic data;3D human appearance;rendering;single image;real-time},
  doi={10.1109/ISMAR62088.2024.00135},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765441,
  author={Zou, Xinrui and Zhang, Zheyuan and Schwarz, Alexander and Armand, Mehran and Martin-Gomez, Alejandro},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ARthroNeRF: Field of View Enhancement of Arthroscopic Surgeries using Augmented Reality and Neural Radiance Fields}, 
  year={2024},
  volume={},
  number={},
  pages={1197-1205},
  abstract={Arthroscopy is a minimally invasive orthopedic procedure commonly used to treat joints such as the shoulder, hip, or knee. A major difficulty for surgeons in arthroscopic procedures is the simultaneous coordination of the surgical tools used for manipulation and the arthroscopic camera. To further complicate this task, the narrow space where arthroscopic procedures are performed limits the ability to move the arthroscope inside the patient’s body, restricting the field of view. In this work, to overcome these limitations, we introduce ARthroNeRF, a novel framework that combines Neural Radiance Fields (NeRF) and Augmented Reality (AR). This framework allows for the generation of synthetic viewpoints from the perspective of surgical tools without the need for an additional camera. To evaluate the feasibility of the proposed framework, we conducted a user study with 18 participants. In this study, participants were tasked to touch hidden targets assisted by a synthetic view generated from the perspective of a surgical tool. The results of this study demonstrate that ARthroNeRF provides accurate supplementary visual information and suggest that ARthroNeRF has the potential to streamline the learning process in arthroscopic surgery. In addition, we built a system capable of presenting the reconstructed scenes using the Microsoft HoloLens 2. The incorporation of AR, overlaying synthesized images alongside the original arthroscopic footage and within the surgeon’s visual field, represents a viable alternative to enhance perception in spatially constrained scenarios.},
  keywords={Visualization;Accuracy;Surgery;Orthopedic procedures;Streaming media;Neural radiance field;Cameras;Optical imaging;Robustness;Augmented reality;Neural Radiance Fields;Visualization;Augmented Reality;Computer-Assisted Surgery},
  doi={10.1109/ISMAR62088.2024.00136},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765418,
  author={Ren, Chenqu and Qiu, Haolei and Shao, Yeheng and Qiu, Zherui and Song, Kaiwen},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PaletteGaussian: 3D Photorealistic Color Editing with Gaussian Splatting}, 
  year={2024},
  volume={},
  number={},
  pages={1206-1215},
  abstract={3D editing, particularly involving realistic color editing, plays a crucial role in various multimedia domains, such as augmented reality and filmmaking. Traditional 3D reconstruction methods encounter challenges in achieving high-fidelity reconstruction for complex scenes. In recent years, methods based on implicit 3D representations, like Neural Radiance Fields (NeRF), have demonstrated effectiveness in rendering complex scenes. However, these methods face difficulties in interactively editing scene colors and often exhibit slow processing speeds. Addressing these challenges, we propose the PaletteGaussian framework for interactive color editing and real-time rendering based on a palette and 3D Gaussian Splatting (3DGS). First, we introduce a two-stage training strategy to ensure rendering quality and enhance the accuracy of object extraction in the scene. Next, we present an image-driven learning-based approach, I-learning, for convenient interactive color editing driven by both images and text. Finally, we perform parameter baking to achieve real-time rendering. In summary, PaletteGaussian supports two editing levels, scene-level and object-level, offering three interaction modes: manual, image-driven, and text-driven editing. It enables high-resolution real-time rendering. Our comprehensive experiments demonstrate that PaletteGaussian exhibits efficient performance, diverse interaction modes, and realistic color editing.},
  keywords={Training;Three-dimensional displays;Accuracy;Image color analysis;Semantics;Rendering (computer graphics);Neural radiance field;Real-time systems;Image reconstruction;Augmented reality;3D scene color editing;Novel view synthesis;3D reconstruction},
  doi={10.1109/ISMAR62088.2024.00137},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765433,
  author={Hecquard, Jeanne and Saint-Aubert, Justine and Argelaguet, Ferran and Pacchierotti, Claudio and Lécuyer, Anatole and Macé, Marc J-M},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Warm regards: Influence of thermal haptic feedback during social interactions in VR}, 
  year={2024},
  volume={},
  number={},
  pages={1216-1225},
  abstract={In this paper, we study how thermal haptic feedback can influence social interactions in virtual environments, with an emphasis on persuasion, focus, co-presence, and friendliness. Physical and social warmth have been repeatedly linked in psychological literature, which allows for speculations on the effect of thermal haptics on virtual social interactions. To that effect, we conducted a study on thermal feedback during simulated social interactions with a virtual agent. We tested three conditions: warm, cool, and neutral. Results showed that warm feedback positively influenced users’ perception of the agent and significantly enhanced persuasion and thermal comfort. Multiple users reported the agent feeling less ‘robotic’ and more ‘human’ during the warm condition. Moreover, multiple studies have previously shown the potential of vibrotactile feedback for social interactions. A second study thus evaluated the combination of warmth and vibrations for social interactions. The study included the same protocol and three similar conditions: warmth, vibrations, and warm vibrations. Warmth was perceived as more friendly, while warm vibrations heightened the agent’s virtual presence and persuasion. These results encourage the study of thermal haptics to support positive social interactions. Moreover, they suggest that some haptic feedback are more suited to certain types of social interactions and communication than others.},
  keywords={Vibrations;Protocols;Virtual environments;Psychology;Thermal conductivity;Haptic interfaces;Augmented reality;Affective haptics;Thermal feedback;Virtual reality;Human Machine Interaction;Social Touch},
  doi={10.1109/ISMAR62088.2024.00138},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765395,
  author={Salagean, Anca and Jicol, Crescent and Dasalla, Kenneth and Clarke, Christopher and Lutteroth, Christof},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Watch Out! XR Mobile Displays Improve the Experience of Co-Located VR Gaming Observers}, 
  year={2024},
  volume={},
  number={},
  pages={1226-1235},
  abstract={Co-located social gaming with players and observers talking to each other is commonplace. However, it becomes less attractive when using virtual reality (VR): while VR increases players’ presence by immersing them in a virtual world, it also shuts out their observing friends. We investigate whether extended reality (XR) can improve the observer experience by giving them agency over their viewpoint in the game and personalizing the representation they see of the player. In a user study, 24 pairs of players and observers experienced co-located VR gaming. Observers either watched the player’s gameplay on a stationary screen i) from the player’s viewpoint or ii) controlling their viewpoint with a gamepad, or on an XR-enabled mobile screen controlling their viewpoint by moving the screen iii) with the player represented as a generic game avatar or iv) a personalized avatar obtained through live player footage augmentation. Results show that giving observers agency over their viewpoint with an XR-enabled mobile screen provides substantial benefits to presence and observers’ overall experience.},
  keywords={Extended reality;Avatars;Games;Observers;Watches;Time measurement;Augmented reality;co-located gaming;VR;XR;presence;agency},
  doi={10.1109/ISMAR62088.2024.00139},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765458,
  author={Shen, Junxiao and Lange, Matthias De and Xu, Xuhai and Zhou, Enmin and Tan, Ran and Suda, Naveen and Lazarewicz, Maciej and Kristensson, Per Ola and Karlson, Amy and Strasnick, Evan},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Open-World Gesture Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1236-1245},
  abstract={Providing users with accurate gestural interfaces, such as gesture recognition based on wrist-worn devices, is a key challenge in mixed reality. However, static machine learning processes in gesture recognition assume that training and test data come from the same underlying distribution. Unfortunately, in real-world applications involving gesture recognition, such as gesture recognition based on wrist-worn devices, the data distribution may change over time. We formulate this problem of adapting recognition models to new tasks, where new data patterns emerge, as open-world gesture recognition (OWGR). We propose the use of continual learning to enable machine learning models to be adaptive to new tasks without degrading performance on previously learned tasks. However, the process of exploring parameters for questions around when, and how, to train and deploy recognition models requires resource-intensive user studies may be impractical. To address this challenge, we propose a design engineering approach that enables offline analysis on a collected large-scale dataset by systematically examining various parameters and comparing different continual learning methods. Finally, we provide design guidelines to enhance the development of an open-world wrist-worn gesture recognition process.},
  keywords={Continuing education;Training;Performance evaluation;Design engineering;Adaptation models;Design methodology;Mixed reality;Gesture recognition;Machine learning;Guidelines;Computing methodologies;Machine learning;Learning paradigms;Lifelong machine learning;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User models},
  doi={10.1109/ISMAR62088.2024.00140},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765391,
  author={Bernal-Berdun, Edurne and Pina, Jorge and Vallejo, Mateo and Serrano, Ana and Martin, Daniel and Masia, Belen},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={AViSal360: Audiovisual Saliency Prediction for 360° Video}, 
  year={2024},
  volume={},
  number={},
  pages={1246-1255},
  abstract={Saliency prediction in 360° video plays an important role in modeling visual attention, and can be leveraged for content creation, compression techniques, or quality assessment methods, among others. Visual attention in immersive environments depends not only on visual input, but also on inputs from other sensory modalities, primarily audio. Despite this, only a minority of saliency prediction models have incorporated auditory inputs, and much remains to be explored about what auditory information is relevant and how to integrate it in the prediction. In this work, we propose an audiovisual saliency model for 360° video content, AViSal360. Our model integrates both spatialized and semantic audio information, together with visual inputs. We perform exhaustive comparisons to demonstrate both the actual relevance of auditory information in saliency prediction, and the superior performance of our model when compared to previous approaches.},
  keywords={Visualization;Semantics;Predictive models;Quality assessment;Augmented reality;Audiovisual Saliency;Visual Behavior;Ambisonic Sound;360° Videos},
  doi={10.1109/ISMAR62088.2024.00141},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765463,
  author={Kim, Dooyoung and Kim, Seonji and Choi, Selin and Woo, Woontack},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Spatial Affordance-aware Interactable Subspace Allocation for Mixed Reality Telepresence}, 
  year={2024},
  volume={},
  number={},
  pages={1256-1265},
  abstract={To enable remote Virtual Reality (VR) and Augmented Reality (AR) clients to collaborate as if they were in the same space during Mixed Reality (MR) telepresence, it is essential to overcome spatial heterogeneity and generate a unified shared collaborative environment by integrating remote spaces into a target host space. Especially when multiple remote users connect, a large shared space is necessary for people to maintain their personal space while collaborating, but the existing simple intersection method leads to the creation of narrow shared spaces as the number of remote spaces increases. To robustly align to the host space even as the number of remote spaces increases, we propose a spatial affordance-aware interactable subspace allocation algorithm. The key concept of our approach is to consider the perceivable and interactable areas separately, where every user views the same mutual space, but each remote user has a different interactable subspace, considering their location and spatial affordance. We conducted an evaluation with 900 space combinations, varying the number of remote spaces as two, four, and six, and results show our method outperformed in securing wide interactable mutual space and instantiating users compared to the other spatial matching methods. Our work enables multiple clients from diverse remote locations to access the AR host’s space, allowing them to interact directly with the table, wall, or floor by aligning their physical subspaces within a connected mutual space.},
  keywords={Telepresence;Affordances;Mixed reality;Collaboration;Resource management;Augmented reality;Floors;Mixed Reality;mutual space;spatial affordance;optimization;subspace allocation;scene graph},
  doi={10.1109/ISMAR62088.2024.00142},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765454,
  author={Ito, Kenichi and Hosoi, Juro and Takanohashi, Kei and Ban, Yuki and Warisawa, Shin’Ichi},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Clothes Vibration Device for Producing Tactile Stimuli to Evoke the Perception of Strong Wind}, 
  year={2024},
  volume={},
  number={},
  pages={1266-1275},
  abstract={In virtual environments, wind displays can reproduce the sensation of wind to enhance user experience such as presence. Enabling users to perceive strong winds can benefit several applications, such as disaster education and entertainment content. However, the large and bulky equipment required to blow strong winds across the entire body of the user limit its application. Although cross-modal effects are proposed to alter wind perceptions through multisensory stimuli, but methods to induce the perception of strong winds without generating a strong wind need further exploration. In this study, we propose a method to create an illusion of strong wind by providing tactile stimuli through clothes. We developed a compact and lightweight wearable device that vibrates the fabric by a motor to simulate its fluttering motion under a strong wi nd. Two user studies were conducted to investigate the effect of vibrating the clothes on the perceived wind velocity, realness of the wind, and presence. The results indicated that tactile stimuli provided by vibrating the clothes increased the perceived wind velocity to a similar level as the strong wind exceeding $10 \mathrm{~m} / \mathrm{s}$ while the parameters of the physical wind were unchanged. It was also revealed that the vibration of the clothes decreases the realness of the actual wind and a visual scene of the clothes of the avatar flutter increases presence. Thus, the proposed wearable device is useful for creating the experience of strong winds while keeping the wind displays compact.},
  keywords={Vibrations;Wind energy generation;Visualization;Fans;Wind speed;Avatars;Virtual environments;Wind power generation;Skin;Wearable devices;Wind display;wearable device;haptics;crossmodal effect;virtual reality},
  doi={10.1109/ISMAR62088.2024.00143},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765378,
  author={Jin, Du and Zhang, Rui and Li, Yuan and Ban, Yuki and Warisawa, Shin’Ichi},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mitigating Latency Effects on Subjective Experience in Robot Teleoperation Using a VR-Enabled Virtual Spring}, 
  year={2024},
  volume={},
  number={},
  pages={1276-1282},
  abstract={Remote robot teleoperation is crucial for tasks that require human oversight, yet latency can significantly impair operator performance and result in discomfort, break inpresence and increased workload. In this paper, we propose a new robot teleoperation technique based on Virtual Reality that is expected to alleviate the negative impactson subjective experience caused by latency. The technique allows users to manipulate a virtual robot synchronized with a real one by ‘grabbing’ a virtual spring attached to it. Controller vibration is also used to simulate spring force feedback, together creating an illusion that the position discrepancy is caused by the dynamics of the spring instead of latency. We hypothesize that this approach can mitigate the negative effects of latency by making the robot’s movement appear less strange and more natural. A user study was conducted to evaluate the effectiveness of virtual spring and controller vibration separately using a $2 \times 2$ factorial design. The results suggest that the virtual spring enhanced user comfort level and the sense of presence, but the controller vibration showed no clear benefits.},
  keywords={Vibrations;Force feedback;Dynamics;Aerospace electronics;Robot sensing systems;Cognitive load;Synchronization;Springs;Augmented reality;Virtual Reality;Robot Teleoperation;Latency;Subjective Experience},
  doi={10.1109/ISMAR62088.2024.00144},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765401,
  author={Faleel, Shariff A M and Kwon, SoonUk and Ahlström, David and Irani, Pourang},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Validating Eyes-free Affordance of On-Finger Hand Proximate User Interfaces in In-situ Scenarios}, 
  year={2024},
  volume={},
  number={},
  pages={1283-1292},
  abstract={We explore the value of Hand Proximate User Interfaces (HPUIs) for in-situ interactive head-mounted systems, i.e. systems designed to support a user’s primary activity. HPUIs are unencumbered, single-handed, and have tactile and proprioceptive affordances. This allows novice and expert users to rely on the visual cues available in the UI, but to then gradually interact eyes-free such that they minimize interrupting the user’s core task. Prior work on HPUI falls short of validating this premise. We address this gap with comparative analysis in the context of a compound task, where one hand is involved in a primary task and the second interacts with a secondary but supporting task (e.g. selecting items from a menu). We compare HPUI with mid-air direct interactions, a common form of interacting with head-mounted displays. The results show that HPUI performs similarly to mid-air but with better eyes-free affordances and user preferences.},
  keywords={Visualization;Head-mounted displays;Affordances;Propioception;User interfaces;Compounds;Augmented reality;in-situ interactions;mid-air interactions;hand interactions},
  doi={10.1109/ISMAR62088.2024.00145},
  ISSN={2473-0726},
  month={Oct},}@INPROCEEDINGS{10765379,
  author={Skreinig, Lucchas Ribeiro and Mohr, Peter and Berger, Blanca and Tatzgern, Markus and Schmalstieg, Dieter and Kalkofen, Denis},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Immersive Authoring by Demonstration of Industrial Procedures}, 
  year={2024},
  volume={},
  number={},
  pages={1293-1302},
  abstract={This work presents an authoring tool for supporting the creation of immersive instructions for industrial processes. Our system simplifies the creation of instructional content by providing an immersive virtual reality environment that enables expert operators to interact directly with virtual replicas of industrial devices. Hand movements, tool usage, gaze, spoken comments, and machine part movement are recorded using a head-mounted display. Editing of instructions in virtual reality is aided by automatic segmentation of recorded data into individual steps and visualizations of regions with intensive activity. A qualitative evaluation of our system by industrial experts shows that it is a viable alternative to current practices in authoring instructions for assembly and maintenance.},
  keywords={Head-mounted displays;Authoring systems;Data visualization;Maintenance;Augmented reality;Assembly;Virtual reality;Computer-assisted instruction;User interface design},
  doi={10.1109/ISMAR62088.2024.00146},
  ISSN={2473-0726},
  month={Oct},}
