@INPROCEEDINGS{9994856,
  author={Cui, Dixuan and Mousas, Christos},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Estimating the Just Noticeable Difference of Tactile Feedback in Oculus Quest 2 Controllers}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={In virtual reality (VR) applications, humans experience the provided content not only through the visual and auditory systems but also through the somatosensory system. Thus, we decided to conduct a VR study to further explore the just noticeable difference (JND) of tactile feedback to understand humans’ perceptions of tactile stimuli. Our VR study examined the JND in terms of the intensity, duration, and frequency of tactile feedback provided through commercially available vibrotactile motion controllers, the Oculus Quest 2 controllers. We instructed participants to report whether they perceived a difference between a reference (variation) and a testing stimulus at each point in the experiment for a different property (intensity, duration, and frequency) of tactile feedback. We report both positive and negative JND values for the three properties of tactile feedback. We discuss our findings and limitations and provide directions for future studies regarding tactile perception for commercially available tactile feedback devices.},
  keywords={Somatosensory;Visualization;Tactile sensors;Auditory system;Frequency control;Augmented reality;Testing;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Hardware;Communication hardware;interfaces and storage;Tactile and hand-based interfaces;Haptic devices},
  doi={10.1109/ISMAR55827.2022.00013},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994946,
  author={Giovannelli, Alexander and Lisle, Lee and Bowman, Doug A.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Impact of Visual Information on Intermittent Typing in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={8-17},
  abstract={For touch typists, using a physical keyboard ensures optimal text entry task performance in immersive virtual environments. However, successful typing depends on the user’s ability to accurately position their hands on the keyboard after performing other, non-keyboard tasks. Finding the correct hand position depends on sensory feedback, including visual information. We designed and conducted a user study where we investigated the impact of visual representations of the keyboard and users’ hands on the time required to place hands on the homing bars of a keyboard after performing other tasks. We found that this keyboard homing time decreased as the fidelity of visual representations of the keyboard and hands increased, with a video pass-through condition providing the best performance. We discuss additional impacts of visual representations of a user’s hands and the keyboard on typing performance and user experience in virtual reality.},
  keywords={Performance evaluation;Visualization;Keyboards;Virtual environments;User experience;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction devices;Keyboards Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00014},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995546,
  author={Harris, Christene and Sun, Bo},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Assessing the Effect of Interactivity Design In VR Based Second Language Learning Tool}, 
  year={2022},
  volume={},
  number={},
  pages={18-25},
  abstract={Virtual Reality (VR), as a helpful tool in language education, is widely supported by the current literature. VR can provide a variety of stimulating scenarios that keep learner engagement high so its use in the language classroom has increased rapidly. This makes it necessary for further research to be conducted in the field to determine ways to maximize its potential. This research aims to determine if the level of interactivity presented in an Immersive VR Environment is a factor that will impact a user’s capability to successfully learn a second language, particularly when facing subjects with different age, gender, and previous VR experience. To satisfy these aims, 3 versions of a Virtual Reality Language Learning Application were created with varying levels of interactivity. Participants of this study were administered pre and post evaluations to analyze the efficiency of their second language learning experience. Our results show that the level of interactivity may not be a factor that impacts a user’s capability of learning and provide insight into the factors needed for successful language learning in Virtual Reality.},
  keywords={Design methodology;Education;Software;Augmented reality;VR Based Learning;Interactivity;Language Learning. Human Computer Interaction;Human Factor;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR55827.2022.00015},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995407,
  author={Hopkins, Torin and Weng, Suibi Che-Chuan and Vanukuru, Rishi and Wenzel, Emma and Banic, Amy and Gross, Mark D and Do, Ellen Yi-Luen},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying the Effects of Network Latency on Audio-Visual Perception During an AR Musical Task}, 
  year={2022},
  volume={},
  number={},
  pages={26-34},
  abstract={Augmented Reality (AR), with its ability to make people feel like they are in the same space as friends from across the world, is an ideal medium for the purpose of Networked Musical Collaboration. Most conventional systems that enable networked musical collaboration minimize network latency by focusing on the transfer of auditory information at the expense of visual feedback. Studies into human perception have shown that sensory integration of audio and visual stimuli can take place even when there is a slight delay between the two signals. We studied the way changes in network latency effect participants’ auditory and visual perception in latency detection, latency tolerance and attention focus; in this paper, we explore the trade-off between the presence of AR visuals and the minimization of latency. Twenty-four participants were asked to play a hand drum and collaborate with a prerecorded remote musician rendered as an avatar in AR. Multiple trials involving different levels of audio-visual latency were conducted. We then analyzed the subjective responses of the participants together with the recorded musical information from each session. Results indicate a minimum noticeable delay value–defined as the highest amount of delay that can be experienced before two stimulated senses are perceived as separate events–between 160 milliseconds (ms) and 320 ms. Players also reported that a delay between sound and an accompanying avatar animation became less tolerable at 320 ms of delay, but was never completely intolerable, even up to 1200 ms of delay. We conclude that players begin to notice delay at about 320 ms and most players can tolerate large delays between sound and animation.},
  keywords={Visualization;Avatars;Music;Collaboration;Streaming media;Animation;Real-time systems;Human-centered computing;Interaction paradigms;Mixed / augmented reality;Collaborative Interaction;Applied computing;Sound and music computing},
  doi={10.1109/ISMAR55827.2022.00016},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995584,
  author={Popescu, Voicu and Lee, Seung Heon and Choi, Andrew Shinyoung and Fahmy, Sonia},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Complex Virtual Environments on Thin VR Systems Through Continuous Near-Far Partitioning}, 
  year={2022},
  volume={},
  number={},
  pages={35-43},
  abstract={This paper describes a method for reducing rendering load such that complex virtual environments (VEs) can be deployed on “thin” VR systems with limited rendering power. The method partitions the VE into four regions: a near region, an intermediate region, a stationary far region, and a dynamic far region. The stationary far region is replaced with an environment map, which brings a substantial rendering load reduction. The other three regions are rendered from geometry: the near region is rendered from the user viewpoint, the dynamic far geometry is rendered from the center of the environment map, and the intermediate region is rendered with a morph that switches viewpoint gradually from the user viewpoint to the center of the environment map. The intermediate region connects the near and far regions seamlessly. Furthermore, the environment map is enhanced with per pixel range which allows depth compositing the dynamic and stationary far geometry. An IRB approved user study (N = 22) found significant advantages for our method over conventional near-far partitioning.},
  keywords={Geometry;Virtual environments;Rendering (computer graphics);Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00017},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995181,
  author={Li, Ke and Schmidt, Susanne and Bacher, Reinhard and Leemans, Wim and Steinicke, Frank},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mixed Reality Tunneling Effects for Stereoscopic Untethered Video-See-Through Head-Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={44-53},
  abstract={We present mixed reality (MR) tunneling, a novel method to balance the trade-off between limited render performance and high visual quality of video see-through head-mounted displays (VST-HMDs) through fusing images of two types of camera sensors with different resolutions and frame rates. By merging a color video stream from an external stereoscopic camera with the grayscale VST commonly integrated into today’s standalone virtual reality (VR) headsets, we create a perceptually high-resolution and wide field of view VSTHMD prototype. The external high-resolution VST displayed at the central foveal to the para-peripheral region of the human visual field complements the low-resolution, low-latency grayscale VST at the far peripheral region, producing a tunneling effect, which simulates the human foveal and peripheral vision, with the potential to reduce cybersickness as in the tunneling effect in immersive VR. We propose two extensions to the MR tunneling method. The first one accommodates the user’s head movement speed by fading out the external VST when fast head movements are detected, thus potentially compensating for video streaming latency. The second one is a foveated MR tunneling effect, which displays the center of the external VST based on the tracked user eye movements. We evaluated the three MR tunneling methods in a within-subject study with 24 participants. The user study demonstrates the potential of our prototype and techniques based on the example of an assembly task that requires hand-eye coordination, untethered locomotion, and fine motor skills. The results demonstrate that, although not significant, the MR tunneling effects lead to higher overall usability, less perceived motion sickness, and a better sense of presence.1},
  keywords={Headphones;Visualization;Stereo image processing;Prototypes;Virtual reality;Tunneling;Streaming media;Video See-through Head Mounted Display;Cybersickness;Sensor Fusion;Foveated Rendering},
  doi={10.1109/ISMAR55827.2022.00018},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995479,
  author={Satriadi, Kadek Ananta and Cunningham, Andrew and Thomas, Bruce H. and Drogemuller, Adam and Odi, Antoine and Patel, Niki and Aston, Cathlyn and Smith, Ross T.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={54-63},
  abstract={Recent research in immersive visualisations has explored the use of physical 3D models together with virtual data visualisations, with a simple data encoding. However, little is known about how this technique could be extended towards more complex multivariate data with multiple charts. We present augmented scale models, immersive visualisations that place charts of multivariate data via Augmented Reality (AR) registered to physical 3D models. We identified two main factors for presenting AR charts in the limited display space around the models: 1) how charts are laid out (Slides vs Dashboard), and 2) how the chart views are arranged in the 3D space (On Scale Model, On Table, On Virtual Board). In a within-subject user study, we evaluated these two design considerations. We found that chart layout and view arrangement do not affect task error but do vary in response time. Dashboard and Slides perform equally well on simple tasks that require comparison of a single chart across scale models. However, in more complex tasks such as comparing multiple charts within a scale model or across several scale models, Dashboard shows no decrease in time performance, while Slides’s time performance decreases significantly. We also found that On Scale Model has the fastest performance, has good chart-scale model integration, and supports charts comparison well. On Table and On Virtual Board on the other hand, show a trade-off between the ability to support charts comparison across models and the chart-scale model integration.},
  keywords={Solid modeling;Three-dimensional displays;Computational modeling;Layout;Data visualization;Data models;Encoding;500[Human-centered computing];Visualization techniques},
  doi={10.1109/ISMAR55827.2022.00019},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994997,
  author={Chen, Chen and Yarmand, Matin and Singh, Varun and Sherer, Michael V. and Murphy, James D. and Zhang, Yang and Weibel, Nadir},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VRContour: Bringing Contour Delineations of Medical Structures Into Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={64-73},
  abstract={Contouring is an indispensable step in Radiotherapy (RT) treatment planning. However, today’s contouring software is constrained to only work with a 2D display, which is less intuitive and requires high task loads. Virtual Reality (VR) has shown great potential in various specialties of healthcare and health sciences education due to the unique advantages of intuitive and natural interactions in immersive spaces. VR-based radiation oncology integration has also been advocated as a target healthcare application, allowing providers to directly interact with 3D medical structures. We present VRContour and investigate how to effectively bring contouring for radiation oncology into VR. Through an autobiographical iterative design, we defined three design spaces focused on contouring in VR with the support of a tracked tablet and VR stylus, and investigating dimensionality for information consumption and input (either 2D or 2D + 3D). Through a within-subject study (n = 8), we found that visualizations of 3D medical structures significantly increase precision, and reduce mental load, frustration, as well as overall contouring effort. Participants also agreed with the benefits of using such metaphors for learning purposes.},
  keywords={Three-dimensional displays;Target tracking;Two dimensional displays;Data visualization;Medical services;Radiation therapy;Planning;Human–Computer Interactions for Health;Radiotherapy (RT) Treatment Planning;VR in Health & Medical Applications;Applied computing—Healthcare information systems},
  doi={10.1109/ISMAR55827.2022.00020},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995155,
  author={Meng, Xuanru and Xu, Wenge and Liang, Hai-Ning},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Exploration of Hands-free Text Selection for Virtual Reality Head-Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={74-81},
  abstract={Hand-based interaction, such as using a handheld controller or making hand gestures, has been widely adopted as the primary method for interacting with both virtual reality (VR) and augmented reality (AR) head-mounted displays (HMDs). In contrast, hands-free interaction avoids the need for users’ hands and although it can afford additional benefits, there has been limited research in exploring and evaluating hands-free techniques for these HMDs. As VR HMDs become ubiquitous, people will need to do text editing, which requires selecting text segments. Similar to hands-free interaction, text selection is underexplored. This research focuses on both, text selection via hands-free interaction. Our exploration involves a user study with 24 participants to investigate the performance, user experience, and workload of three hands-free selection mechanisms (Dwell, Blink, Voice) to complement head-based pointing. Results indicate that Blink outperforms Dwell and Voice in completion time. Users’ subjective feedback also shows that Blink is the preferred technique for text selection. This work is the first to explore handsfree interaction for text selection in VR HMDs. Our results provide a solid platform for further research in this important area.},
  keywords={Head-mounted displays;Solids;User experience;Task analysis;Augmented reality;Text Selection;Virtual Reality;User Study;Hands-free Interaction;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;interaction techniques;Interaction design;Empirical studies in interaction design},
  doi={10.1109/ISMAR55827.2022.00021},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995442,
  author={Chen, Lingling and Li, Yingxi and Bai, Xiaowei and Wang, Xiaodong and Hu, Yongqiang and Song, Mingwu and Xie, Liang and Yan, Ye and Yin, Erwei},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Real-time Gaze Tracking with Head-eye Coordination for Head-mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={82-91},
  abstract={High-accuracy, low-latency gaze tracking is becoming one of the indispensable features in augmented reality (AR) head-mounted devices (HMDs). Researchers have proposed different approaches to predict gaze positions from eye images. However, since only the eye modality is focused, these appearance-based algorithms are still struggle to trade off the accuracy and running speed in HMDs. In this paper, we propose a lightweight multi-modal network (HE-Tracker) to regress gaze positions. By fusing head-movement features with eye features, HE-Tracker achieves comparable accuracy (3.655° in all subjects) and $27 \times$ speedup (48 fps in the specialized AR HMD) compared to the state-of-the-art gaze tracking algorithm. We further demonstrate that when applying our head-eye coordination strategy to other baseline models, all these models achieve at least 6.36% performance improvement without a pronounced effect on running speed. Moreover, we construct HE-Gaze, the first multi-modal dataset with eye images and head-movement data for near-eye gaze tracking. This dataset is currently made of 757,360 frames and 15 persons, providing an opportunity to foster research in multi-modal gaze tracking approaches. Our dataset is available at DOWNLOAD LINK 1.},
  keywords={Performance evaluation;Head;Gaze tracking;Resists;Predictive models;Rendering (computer graphics);Prediction algorithms;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Augmented reality;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR55827.2022.00022},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995144,
  author={Bonis, Michele De and Nguyen, Huyen and Bourdot, Patrick},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Literature Review of User Studies in Extended Reality Applications for Archaeology}, 
  year={2022},
  volume={},
  number={},
  pages={92-101},
  abstract={In the present study we conducted a systematic review on user studies for Archaeology in eXtended Reality of the last 10 years. After a screening and selection process, 52 articles were selected for an in-depth analysis. Their classification follows different axes: devices, location dependency, type of users, interaction and collaboration. We also organised the existing user studies according to tasks, evaluation measurements, number of participants, and how the study was conducted (pre-test and/or post-test, formative and summative evaluation, quantitative and qualitative data). We found an intertwined relation between Archaeology and Cultural Heritage, which is reflected in the vast presence of applications for museum exhibitions and tours on archaeological sites. Similarities between systems developed for archaeologists and for general public were also investigated. Our purpose was to find a common ground between different user studies that could help designers of the next systems have a base on which they can build their system. We also highlighted which would be the preferred and most suitable evaluation techniques, when they are needed, with the type of users to address. The results show a heterogeneity of measurable variables and possible choices, but some guidelines could be derived.},
  keywords={Archeology;Systematics;Extended reality;Bibliographies;Collaboration;Particle measurements;Museums;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality/ Mixed or augmented reality;Applied computing;Physical sciences and engineering;Archaeology},
  doi={10.1109/ISMAR55827.2022.00023},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995290,
  author={Cheymol, Antonin and Fribourg, Rebecca and Ogawa, Nami and Lécuyer, Anatole and Hirao, Yutaro and Narumi, Takuji and Argelaguet, Ferran and Normand, Jean-Marie},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying the Role of Self and External Touch in the Appropriation of Dysmorphic Hands}, 
  year={2022},
  volume={},
  number={},
  pages={102-111},
  abstract={In Virtual Reality, self-touch (ST) stimulation is a promising method of sense of body ownership (SoBO) induction that does not require an external effector. However, its applicability to dysmorphic bodies has not been explored yet and remains uncertain due to the requirement to provide incongruent visuomotor sensations. In this, paper, we studied the effect of ST stimulation on dysmorphic hands via haptic retargeting, as compared to a classical external-touch (ET) stimulation, on the SoBO. Our results indicate that ST can induce similar levels of dysmorphic SoBO than ET stimulation, but that some types of dysmorphism might decrease the ST stimulation accuracy due to the nature of the re-targeting that they induce.},
  keywords={Limiting;Haptic interfaces;Usability;Augmented reality;Virtual Reality;Embodiment;Human-centered computing;Human computer interaction (HCI);Interaction paradigms},
  doi={10.1109/ISMAR55827.2022.00024},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995082,
  author={Li, Ke and Choudhuri, Aradhana and Schmidt, Susanne and Lang, Tino and Bacher, Reinhard and Hartl, Ingmar and Leemans, Wim and Steinicke, Frank},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Stereoscopic Video See-Through Head-Mounted Displays for Laser Safety: An Empirical Evaluation at Advanced Optics Laboratories}, 
  year={2022},
  volume={},
  number={},
  pages={112-120},
  abstract={Nowadays, high-power and multi-spectral lasers are used in many scientific experiments and industrial processes. Those laser sources can rapidly cause permanent damage to human eyes. Research and development work with those laser sources requires typically wearing personal protective equipment (PPE), such as laser safety goggles as eye protectors. Currently, laser safety goggles are based on optical spectral filters, which block spectral bands where hazardous laser radiation is emitted. Such laser safety goggles can filter up to 99% of the visible spectrum, rendering researchers working in hazardous and complex laboratory environments visually impaired. Video see-through head-mounted displays (VST-HMD) could be used as eye protectors without reducing users’ visibility of the environment since they can be constructed such that all laser and ambient light is blocked from the human eye. To date, this application domain is still largely unexplored in the MR community. To our best knowledge, there has been no comprehensive work that investigates the human factors of such an eye protection method at an advanced optics laboratory. In this work, we present the results of an empirical study where we evaluate the usability, perceived safety, advantages, and limitations of using VST-HMDs as laser safety goggles. We use a stereoscopic VST-HMD developed through a human-centered design approach at one of the most advanced optics laboratories in the world. 18 participants including 14 laser experts evaluated the current prototype. Our user evaluation and field studies confirm that the complex and hazardous working conditions at high-energy laser laboratories could be significantly improved with MR technology.},
  keywords={Optical filters;Personal protective equipment;Head-mounted displays;Stereo image processing;Prototypes;Laser feedback;Optics;Laser Safety;Mixed Reality;Video See-through Head Mounted Display;Human-Centered Computing},
  doi={10.1109/ISMAR55827.2022.00025},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995267,
  author={Islam, Rifatul and Desai, Kevin and Quarles, John},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Forecasting the Onset of Cybersickness by Fusing Physiological, Head-tracking and Eye-tracking with Multimodal Deep Fusion Network}, 
  year={2022},
  volume={},
  number={},
  pages={121-130},
  abstract={A plethora of studies has been conducted to detect and reduce cybersickness in real-time. However, prior attempts to detect and minimize cybersickness after its onset may be ineffective as the onset tends to persist beyond its first occurrence. By forecasting the onset of cybersickness, it may be possible to mitigate the severity of cybersickness through earlier interventions. This research proposed a multimodal deep fusion approach to forecast cybersickness from the user’s physiological, head-tracking, and eye-tracking data. We proposed several hybrid multimodal deep fusion neural networks with Long short-term memory (LSTMs), Neural basis expansion analysis for interpretable time series forecasting(NBEATs) and Deep Temporal Convolutional Networks(DeepTCN) neural models to forecast cybersickness 30-60s in advance to its onset. To validate our proposed approach, we recruited 30 participants who were immersed in five virtual reality simulations. We collected eye-tracking, head-tracking, heart rate, and galvanic skin response data and used the fast-motion scale as ground truth. Our results suggest that the DeepTCN model with our proposed multimodal fusion network can forecast cybersickness onset 60 seconds in advance with a root-mean-square error of 0.49 (on a scale from 0-10). Furthermore, our results demonstrated that fusing eye tracking, heart rate, and galvanic skin response data outperformed other data fusion approaches. This research clarifies how early cybersickness can be forecast, paving the way for future research on early cybersickness mitigation approaches.},
  keywords={Heart rate;Solid modeling;Cybersickness;Neural networks;Time series analysis;Gaze tracking;Predictive models;Cybersickness;Forecasting;Eye-tracking;Physiological Data;Deep Fusion;Multimodal Deep Neural Network;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/ISMAR55827.2022.00026},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995581,
  author={Xu, Wenge and Meng, Xuanru and Yu, Kangyou and Sarcar, Sayan and Liang, Hai-Ning},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluation of Text Selection Techniques in Virtual Reality Head-Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={131-140},
  abstract={Text selection is an essential activity in interactive systems, including virtual reality (VR) head-mounted displays (HMDs). It is useful for: sharing information across apps or platforms, highlighting and making notes while reading articles, and text editing tasks. Despite its usefulness, the space of text selection interaction is underexplored in VR HMDs. In this research, we performed a user study with 24 participants to investigate the performance and user preference of six text selection techniques (Controller+Dwell, Controller+Click, Head+Dwell, Head+Click, Hand+Dwell, Hand+Pinch). Results reveal that Head+Click is ranked first since it has excellent speedaccuracy performance (2nd fastest task completion speed with 3rd lowest total error rate), provides the best user experience, and produces a very low workload—followed by Controller+Click, which has the fastest speed and comparable experience with Head+Click, but much higher total error rate. Other methods can also be useful depending on the goals of the system or the users. As a first systematic evaluation of pointing $\times$ selection techniques for text selection in VR, the results of this work provide a strong foundation for further research in this area of growing importance to the future of VR to help it become a more ubiquitous and pervasive platform.},
  keywords={Head-mounted displays;Systematics;Error analysis;Interactive systems;Aerospace electronics;User experience;Neck;Text Selection;Virtual Reality;Pointing Methods;Selection Mechanisms;User Study;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality; Human-centered computing;interaction techniques; Human-centered computing;Interaction design;Empirical studies in interaction design},
  doi={10.1109/ISMAR55827.2022.00027},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995684,
  author={Freeling, Benjamin and Lécuyer, Flavien and Capobianco, Antonio},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Petting a cat helps you incarnate the avatar: Influence of the emotions over embodiment in VR}, 
  year={2022},
  volume={},
  number={},
  pages={141-149},
  abstract={Thanks to its strong capacity to immerse users in virtual worlds, virtual reality can elicit various emotions with diverse environments. This aspect of virtual reality makes it an interesting and powerful tool in many fields, such as entertainment with scenarios based on a strong emotional implication, training in particular for social or communication skills, or even medical therapy with phobia or addiction treatment. However, in virtual reality the participant lives the experience through an avatar, and feels the emotion according to what happens to this avatar.This paper discusses the link between embodiment and emotional implication in virtual reality. In particular, we looked at how emotions and the sense of embodiment are correlated in virtual reality. Through an experiment, we demonstrate that the sense of embodiment is strongly correlated with the emotional experience of the virtual environment. The sense of embodiment is increased when the virtual scenarios make the participants feel strong emotions, whether those emotions are positive or negative. We also show that emotions mainly affect two sub-components of embodiment: the appearance and response sub-scales.},
  keywords={Training;Human computer interaction;Avatars;Addiction;Anxiety disorders;Virtual environments;Medical treatment;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Perception;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI},
  doi={10.1109/ISMAR55827.2022.00028},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995404,
  author={Cheng, Yi Fei and Luong, Tiffany and Fender, Andreas Rene and Streli, Paul and Holz, Christian},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ComforTable User Interfaces: Surfaces Reduce Input Error, Time, and Exertion for Tabletop and Mid-air User Interfaces}, 
  year={2022},
  volume={},
  number={},
  pages={150-159},
  abstract={Real-world work-spaces typically revolve around tables, which enable knowledge workers to comfortably perform tasks over an extended period of time during productivity tasks. Tables afford more ergonomic postures and provide opportunities for rest, which raises the question of whether they may also benefit prolonged interaction in Virtual Reality (VR). In this paper, we investigate the effects of tabletop surface presence in situated VR settings on task performance, behavior, and subjective experience. In an empirical study, 24 participants performed two tasks (selection, docking) on virtual interfaces placed at two distances and two orientations. Our results show that a physical tabletop inside VR improves comfort, agency, and task performance while decreasing physical exertion and strain of the neck, shoulder, elbow, and wrist, assessed through objective metrics and subjective reporting. Notably, we found that these benefits apply when the UI is placed on and aligned with the table itself as well as when it is positioned vertically in mid-air above it. Our experiment therefore provides empirical evidence for integrating physical table surfaces into VR scenarios to enable and support prolonged interaction. We conclude by discussing the effective usage of surfaces in situated VR experiences and provide initial guidelines.},
  keywords={Wrist;Productivity;Measurement;Human computer interaction;Ergonomics;Behavioral sciences;Neck;Human-centered computing;Human computer interaction (HCI);Empirical Studies in HCI},
  doi={10.1109/ISMAR55827.2022.00029},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995211,
  author={Ablett, Daniel and Cunningham, Andrew and Lee, Gun A. and Thomas, Bruce H.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Portal Rendering and Creation Interactions in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={160-168},
  abstract={Transformative portals are a novel technique for supporting visualizations and interactions between locations that are not co-located in virtual reality (VR). We are interested in improving upon current portal rendering for VR, and supporting VR users to create new portals quickly and accurately. In this paper, we introduce a new high-level algorithm for rendering transformative portals in VR and present the benefits of our approach. By leveraging single-pass stereo and stencil portal rendering, we developed our algorithm to support multiple portals with “infinite” recursion, while running performant on mobile VR devices. We then benchmarked our performance against other common portal rendering implementations. In addition, we focused our research on VR interactions for handheld portals, in which the initial placement of the portal’s destination is imperative to its effectiveness. In this paper, we present four new portal creation interactions for handheld portals: fishing reel, raycast with fishing reel, marker, and projectile curve. Based on existing research, we also established standard controls for fine-grained portal manipulation. In a user study, we compared and evaluated how well the creation interactions performed and argue that projectile curve is the most suitable general-purpose technique.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Navigation;Projectiles;Benchmark testing;Rendering (computer graphics);Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/ISMAR55827.2022.00030},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994970,
  author={Verhulst, Adrien and Namikawa, Yasuko and Kasahara, Shunlchl},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Parallel Adaptation: Switching between Two Virtual Bodies with Different Perspectives Enables Dual Motor Adaptation}, 
  year={2022},
  volume={},
  number={},
  pages={169-177},
  abstract={Virtual Reality (VR) lets us experiment embodiment. Here we investigate dual embodiment under the prism of dual motor adaptation. We asked participants (N=21) to perform reaching motions in VR with opposite gradual visuomotor perturbations. The participants sequentially switched between 2 VR bodies and had to adapt to the VR body’s perturbation (up to +15° for the VR bodyA, and-15° for the VR bodyB). We then designed a 2$\times$2 within-subject study: 1 factor being the perspective (1st person or 3rd person), and 1 factor being the head rotation (without head rotation before the reaching motion or with head rotation before the reaching motion). We found that by providing strong visual cues between bodies (alternating symmetric perspective and/or symmetric head rotation), participants had little awareness of the perturbations, good adaptation, and large aftereffects in both VR bodies. Those elements are consistent with implicit dual adaptation. In contrast, a naive 1st person perspective resulted in little to no adaptation with a high cognitive load and no aftereffects.},
  keywords={Adaptation models;Visualization;Solid modeling;Computational modeling;Perturbation methods;Switches;Cognitive load;multiple embodiment;motor adaptation;virtual bodies},
  doi={10.1109/ISMAR55827.2022.00031},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994985,
  author={Li, Tianyu and Liu, Yue and Ma, Shining and Hu, Mingwei and Liu, Tong and Song, Weitao},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={NailRing: An Intelligent Ring for Recognizing Micro-gestures in Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={178-186},
  abstract={Gesture interaction is currently a main interaction technology in the field of mixed reality. However, long-term and large-scale gesture in mid-air will lead to muscle fatigue and privacy problems, which cannot meet the comfort requirements of continuous interaction and inevitably hinder the development of mixed reality systems. To solve this problem, we propose NailRing, an intelligent ring to recognize fingertip micro-gestures using a micro-close-focus camera on a fingertip bracket. Such fingertip physiological characteristics as the changes in fingertip color distribution and muscle shape changes caused by fingertip pressure have been studied. According to the recognition principle, ten types of micro-gestures have been designed and used for contact interaction and one-hand interaction respectively. The accuracy of gesture recognition (cross-session $ F_{Macro}=98.3\%$; cross-person $ F_{Macro}=86.4\%$) in user studies verifies the performances of NailRing under different interaction conditions. Finally, the capability of NailRing in a series of potential application scenarios has also been discussed and analyzed.},
  keywords={Privacy;Shape;Mixed reality;Resists;Gesture recognition;Muscles;Fatigue;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Mixed / augmented reality;Interaction techniques;Gestural input},
  doi={10.1109/ISMAR55827.2022.00032},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994944,
  author={Dastan, Mine and Uva, Antonio Emmanuele and Fiorentino, Michele},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Gestalt Driven Augmented Collimator Widget for Precise 5 DOF Dental Drill Tool Positioning in 3D Space}, 
  year={2022},
  volume={},
  number={},
  pages={187-195},
  abstract={Drill tool positioning in dental implantology is a challenging task requiring 5DOF precision as the rotation around the tool axis is not influential. This work improves the quasi-static visual elements of the state-of-the-art with a novel Augmented Collimation Widget (ACW), an interactive tool of position and angle error visualization based on the gestalt reification, the human ability to group geometric elements. The user can seek in a quick, pre-attentive way the collimation of five (three positional and two rotational) error component widgets (ECWs), taking advantage of three key aspects: component separation and reification, error visual amplification, and dynamic hiding of the collimated components. We compared the ACW with the golden standard in a within-subjects (N=30) user test using 32 implant targets, measuring the time, error, and usability. ACW performed significantly better in positional (+19%) and angular (+47%) precision accuracy and with less mental demand (-6%) and frustration (-13%), but with an expected increase in task time (+59%) and physical demand (+64%). The interview indicated the ACW as the main preference and aesthetically more pleasant than GSW, candidating it as the new golden standard for implantology, but also for other applications where 5DOF positioning is key.},
  keywords={Visualization;Three-dimensional displays;Measurement uncertainty;Implants;Time measurement;Dentistry;Task analysis;Augmented Reality;dental implantology;spatial orientation;augmented tool;gestalt principles;visualization design},
  doi={10.1109/ISMAR55827.2022.00033},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995385,
  author={Krug, Katja and Büschel, Wolfgang and Klamka, Konstantin and Dachselt, Raimund},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={CleAR Sight: Exploring the Potential of Interacting with Transparent Tablets in Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={196-205},
  abstract={In this paper, we examine the potential of incorporating transparent, handheld devices into head-mounted Augmented Reality (AR). Additional mobile devices have long been successfully used in head-mounted AR, but they obscure the visual context and real world objects during interaction. Transparent tangible displays can address this problem, using either transparent OLED screens or rendering by the head-mounted display itself. However, so far, there is no systematic analysis of the use of such transparent tablets in combination with AR head-mounted displays (HMDs), with respect to their benefits and arising challenges. We address this gap by introducing a research platform based on a touch-enabled, transparent interaction panel, for which we present our custom hardware design and software stack in detail. Furthermore, we developed a series of interaction concepts for this platform and demonstrate them in the context of three use case scenarios: the exploration of 3D volumetric data, collaborative visual data analysis, and the control of smart home appliances. We validate the feasibility of our concepts with interactive prototypes that we used to elicit feedback from HCI experts. As a result, we contribute to a better understanding of how transparent tablets can be integrated into future AR environments.},
  keywords={Visualization;Home appliances;Three-dimensional displays;Head-mounted displays;Annotations;Collaboration;Full stack;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction design},
  doi={10.1109/ISMAR55827.2022.00034},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995072,
  author={Gattullo, Michele and Laviola, Enricoandrea and Romano, Sara and Evangelista, Alessandro and Manghisi, Vito Modesto and Fiorentino, Michele and Uva, Antonio Emmanuele},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Biophilic Enriched Virtual Environments for Industrial Training: a User Study}, 
  year={2022},
  volume={},
  number={},
  pages={206-214},
  abstract={Immersive Virtual Reality (IVR) training offers the capability to industrial workers to acquire skills and address complex tasks by immersing them in a safe and controlled virtual environment (VE). However, in the literature, IVR training is mainly based on principles of standardization and efficiency without considering the operators’ well-being. A novel design approach consists of the introduction in the VE of Positive Computing to improve workers’ well-being by applying the Biophilia hypothesis. In this work, we explored the possibility of introducing biophilic elements in a VE training scenario that would support psychological well-being and human potential. However, the introduction of virtual elements not related to the training task may distract operators, impairing their performance. We selected as a training scenario the assembly of a real truck engine. It is accomplished in a workstation, and operators do not interact with the surrounding VE. Therefore, we placed the training area into four different types of VEs: 3D Minimal (MIN), 3D Minimal Biophilic enriched (MIN+BIO), 3D Realistic (REAL), and 3D Realistic Biophilic enriched (REAL+BIO). We compared the MIN and REAL scenarios with the respective biophilic enriched scenarios. The performance of 40 participants was evaluated in terms of completion time, object fixation time, training task accuracy, knowledge accuracy, cognitive load, and user experience. The results revealed that introducing biophilic elements in a VR training environment attracts users’ attention in the idle phase of the training. In contrast, they keep concentrating on the task without worsening their performance during the task accomplishment.},
  keywords={Training;Productivity;Industries;Three-dimensional displays;Virtual environments;Psychology;Standardization;User experience;Workstations;Task analysis;Positive Computing;Biophilia Hypothesis;Virtual Reality;Industrial Training},
  doi={10.1109/ISMAR55827.2022.00035},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994865,
  author={Zhibin, Zhang and Hiroi, Yuichi and Itoh, Yuta},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Towards Spatial Airflow Interaction: Schlieren Imaging for Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={215-223},
  abstract={This work integrates Schlieren Imaging, a unique sensing modality, into Augmented Reality (AR) to explore ways to utilize invisible airflows for AR. Schlieren imaging is an imaging technique that visualizes the flow of fluids, which is normally invisible to the eyes. Theoretically, the technique can calculate the motion, pressure, temperature, and density of the airflow in our physical world. This unique, but less applied modality may expand interaction paradigms in AR and VR. We build a proof-of-concept AR system combined with Schlieren imaging that allows real airflow to affect virtual objects. The results of quantitative analyses show that our system can integrate different types of airflow with pressure values ranging from weak breathing actions to a heat gun up to 10m/s or 0.25m3/min airflow. We also showcase AR use cases including blowing out a virtual candle and a heat gun.},
  keywords={Heating systems;Temperature sensors;Photography;Visualization;Sensitivity;Statistical analysis;User interfaces;Schlieren imaging;augmented reality;spatial airflow interaction},
  doi={10.1109/ISMAR55827.2022.00036},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995299,
  author={Zhao, Lizhi and Lu, Xuequan and Bao, Qianyue and Wang, Meili},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={In-Place Gestures Classification via Long-term Memory Augmented Network}, 
  year={2022},
  volume={},
  number={},
  pages={224-233},
  abstract={In-place gesture-based virtual locomotion techniques enable users to control their viewpoint and intuitively move in the 3D virtual environment. A key research problem is to accurately and quickly recognize in-place gestures, since they can trigger specific movements of virtual viewpoints and enhance user experience. However, to achieve real-time experience, only short-term sensor sequence data (up to about 300ms, 6 to 10 frames) can be taken as input, which actually affects the classification performance due to limited spatiotemporal information. In this paper, we propose a novel long-term memory augmented network for in-place gestures classification. It takes as input both short-term gesture sequence samples and their corresponding long-term sequence samples that provide extra relevant spatio-temporal information in the training phase. We store long-term sequence features with an external memory queue. In addition, we design a memory augmented loss to help cluster features of the same class and push apart features from different classes, thus enabling our memory queue to memorize more relevant long-term sequence features. In the inference phase, we input only short-term sequence samples to recall the stored features accordingly, and fuse them together to predict the gesture class. We create a large-scale in-place gestures dataset from 25 participants with 11 gestures. Our method achieves a promising accuracy of 95.1% with a latency of 192ms, and an accuracy of 97.3% with a latency of 312ms, and is demonstrated to be superior to recent in-place gesture classification techniques. User study also validates our approach. Our source code and dataset will be made available to the community.},
  keywords={Training;Three-dimensional displays;Fuses;Source coding;Virtual environments;User experience;Real-time systems;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00037},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995693,
  author={Song, Wenshuang and Gong, Yanhe and Wang, Yongcai},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VTONShoes: Virtual Try-on of Shoes in Augmented Reality on a Mobile Device}, 
  year={2022},
  volume={},
  number={},
  pages={234-242},
  abstract={The virtual try-on (VTON) system in augmented reality (AR) has attracted significant research interest. This paper presents a novel real-time AR virtual shoe try-on system (VTONShoes). Users can see the virtual shoes with full degrees of freedom on the mobile device. In particular, we propose an efficient framework to detect, classify, and recover 6-DoF pose of shoes from the captured images and then accurately render the 3D shoe model on the screen in realtime and in full degrees of freedom. For accurate pose recovery, dense keypoints are designed on the 3D shoe model. An efficient joint 2D keypoint localization and leg silhouette segmentation module (KeyPointLoc) is designed to predict keypoint projections on 2D images and the shoe-leg occlusion relationship. In order to reduce jitter between frames, an optimization-based framework with the longest continuous invariant subarray constraints is proposed to minimize classification errors caused by model switching, and a smoothing module with Exponential Weights Decay is presented to post-process the rendered results. We also developed a large-scale dataset named Diverse-Shoes which contains images extracted from 80K videos, annotated with the shoe bounding box, transformation matrices, and silhouettes of legs. Our system has achieved a smooth and stable try-on effect on mainstream devices with a real-time speed of around 25 to 45 FPS on mainstream mobile phones, which significantly outperforms state-of-the-art methods for real-time performance and rendering accuracy.},
  keywords={Legged locomotion;Training;Solid modeling;Three-dimensional displays;Smoothing methods;Footwear;Real-time systems;Human-centered computing;Mixed/augmented reality;6-DoF pose estimation;Virtual try-on},
  doi={10.1109/ISMAR55827.2022.00038},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995521,
  author={Jang, Jaehyun and Frier, William and Park, Jinah},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Multimodal Volume Data Exploration through Mid-Air Haptics}, 
  year={2022},
  volume={},
  number={},
  pages={243-251},
  abstract={We present a mid-air haptic rendering method to explore volumetric data in augmented reality (AR) environments. Users directly interact with a volume-rendered hologram using their bare hands. Since the volume rendering method accumulates color along with its transparency, the depth perception of the user’s hand within the hologram is vague when used with only visual feedback. Therefore, to enhance the localization of internal structures within the volume data, we propose an AR system that provides a tactile presence for boundaries and associated information (e.g., texture). Leveraging Low-High (LH) histograms, our system provides boundary information to the user both visually and haptically. Specifically, when the user interacts with the volume data using their hands, the system computes a set of focal points or a set of tactile patterns using a GPGPU-based estimation. Additionally, we developed mid-air haptic rendering methods using amplitude modulation (AM) and spatiotemporal modulation (STM) techniques. Both methods were implemented on HoloLens 2 and could run in real-time. The effectiveness of each proposed method was evaluated with respect to various volume data sets, including synthetic and computed tomography (CT) scan data. Our results show that while both haptic rendering methods produce tangible experiences from the hologram interaction, our spatiotemporal modulation method generates better shape discrimination performance with respect to the amplitude modulation method in the case of multiple region of interests.},
  keywords={Visualization;Histograms;Shape;Computed tomography;Estimation;Rendering (computer graphics);Real-time systems;Human-centered computing;Interaction paradigms;Mixed / augmented reality;Interaction devices;Haptic devices;Visualization},
  doi={10.1109/ISMAR55827.2022.00039},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995302,
  author={Pan, Xingyu and Zheng, Mengya and Xu, Xuanhui and Xu, Zixiang and Campbell, Abraham G.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={How can the Additional Motion Parallax along the y and z-axis Affect Viewer’s 3D Perception?: A Generic Approach and Evaluation}, 
  year={2022},
  volume={},
  number={},
  pages={252-259},
  abstract={Light Field Displays (LFD) offer the potential for a true window into a virtual world without any form of headset. However, the vast majority of LFD only provide motion parallax along the x-axis (horizontal) due to its domination of human 3D perception. The additional motion parallax along y and the z-axis are rarely or never achieved and let alone evaluated. This paper proposed the first approach that provided the real full-motion parallax covering the z-axis. Moreover, this generic approach enables on-demand y and z-axis motion parallax on any off-the-shelf LFD. A prototype was created according to the proposed approach, and with the use of it, an experiment with two tasks was carried out among 24 participants. This pioneering study explored the effect of the motion parallax along the additional axes. Subjective and objective metrics were collected to measure participants’ 3D perception on four viewing conditions (LFD with motion parallax along the x-axis, x-and-y-axis, x-and-z-axis, and x-y-and-z-axis). The study manifested three main findings: 1) The additional y-axis and z-axis motion parallax increased viewers’ engagement with the LFD. 2) LFD with full-motion parallax provided the optimal user experience. 3) The additional motion parallax along with the y-axis increased viewers’ user experience more than the z-axis.},
  keywords={Industries;Human computer interaction;Headphones;Three-dimensional displays;Prototypes;Particle measurements;User experience;Human-centered computing;Virtual reality;Visualization techniques;Empirical studies in HCI},
  doi={10.1109/ISMAR55827.2022.00040},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994882,
  author={Bartl, Andrea and Merz, Christian and Roth, Daniel and Latoschik, Marc Erich},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Avatar and Environment Design on Embodiment, Presence, Activation, and Task Load in a Virtual Reality Exercise Application}, 
  year={2022},
  volume={},
  number={},
  pages={260-269},
  abstract={The development of embodied Virtual Reality (VR) systems involves multiple central design choices. These design choices affect the user perception and therefore require thorough consideration. This article reports on two user studies investigating the influence of common design choices on relevant intermediate factors (sense of embodiment, presence, motivation, activation, and task load) in a VR application for physical exercises. The first study manipulated the avatar fidelity (abstract, partial body vs. anthropomorphic, full-body) and the environment (with vs. without mirror). The second study manipulated the avatar type (healthy vs. injured) and the environment type (beach vs. hospital) and, hence, the avatar-environment congruence. The full-body avatar significantly increased the sense of embodiment and decreased mental demand. Interestingly, the mirror did not influence the dependent variables. The injured avatar significantly increased the temporal demand. The beach environment significantly reduced the tense activation. On the beach, participants felt more present in the incongruent condition embodying the injured avatar.},
  keywords={Hospitals;Avatars;Design methodology;Mirrors;Task analysis;Augmented reality;Human-centered computing;Empirical studies in HCI;User studies;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00041},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995060,
  author={Dorzhieva, Ekaterina and Baza, Ahmed and Gupta, Ayush and Fedoseev, Aleksey and Cabrera, Miguel Altamirano and Karmanova, Ekaterina and Tsetserukou, Dzmitry},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DroneARchery: Human-Drone Interaction through Augmented Reality with Haptic Feedback and Multi-UAV Collision Avoidance Driven by Deep Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={270-277},
  abstract={We propose a novel concept of augmented reality (AR) human-drone interaction driven by RL-based swarm behavior to achieve intuitive and immersive control of a swarm formation of unmanned aerial vehicles. The DroneARchery system developed by us allows the user to quickly deploy a swarm of drones, generating flight paths simulating archery. The haptic interface LinkGlide delivers a tactile stimulus of the bowstring tension to the forearm to increase the precision of aiming. The swarm of released drones dynamically avoids collisions between each other, the drone following the user, and external obstacles with behavior control based on deep reinforcement learning. The developed concept was tested in the scenario with a human, where the user shoots from a virtual bow with a real drone to hit the target. The human operator observes the ballistic trajectory of the drone in an AR and achieves a realistic and highly recognizable experience of the bowstring tension through the haptic display. The experimental results revealed that the system improves trajectory prediction accuracy by 63.3% through applying AR technology and conveying haptic feedback of pulling force. DroneARchery users highlighted the naturalness (4.3 out of 5 point Likert scale) and increased confidence (4.7 out of 5) when controlling the drone. We have designed the tactile patterns to present four sliding distances (tension) and three applied force levels (stiffness) of the haptic display. Users demonstrated the ability to distinguish tactile patterns produced by the haptic display representing varying bowstring tension(average recognition rate is of 72.8%) and stiffness (average recognition rate is of 94.2%). The novelty of the research is the development of an AR-based approach for drone control that does not require special skills and training from the operator. In the future, the proposed interaction can be applied in various fields, for example, for fast swarm deployment in search and rescue missions, crop monitoring, inspection and maintenance.},
  keywords={Deep learning;Training;Force;Reinforcement learning;Haptic interfaces;Trajectory;Pattern recognition;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR55827.2022.00042},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995276,
  author={Kodama, Daiki and Mizuho, Takato and Hatada, Yuji and Narumi, Takuji and Hirose, Michitaka},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing the Sense of Agency by Transitional Weight Control in Virtual Co-Embodiment}, 
  year={2022},
  volume={},
  number={},
  pages={278-286},
  abstract={Virtual reality helps us learn complex motor skills by providing a situation in which we observe or follow a teacher’s movements from a first-person perspective. However, it has been suggested that if the learners themselves do not behave actively, their body schemes will not be updated and motor skills will be acquired temporarily, but will not be retained in the long term. As a solution to this problem, “co-embodiment” in which two people embody an avatar that reflects the weighted average of their movements was proposed, and it is shown that the user can feel an excessive sense of agency (SoA) even when their control weight is small. From the perspective of motor skill learning, the learner must feel as strong a SoA as possible while performing the exercise as close to the teacher as possible. Therefore, in this study, we propose a method to transitionally change the weights in a situation where co-embodiment is used, such that a strong SoA is felt despite the high weights of control by others. Considering the two-step account of the agency model, which states that the SoA is influenced by context, we tested the hypothesis that an initially strong SoA can maintain the SoA despite a gradually decreased control weight. The experimental results support this hypothesis, and it is expected that the proposed method will enhance the effectiveness of motor skill learning using co-embodiment.},
  keywords={Solid modeling;Weight control;Design methodology;Avatars;Task analysis;Augmented reality;Context modeling;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Laboratory experiments},
  doi={10.1109/ISMAR55827.2022.00043},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995271,
  author={Xu, Xuanhui and Pan, Xingyu and Kilroy, David and Kumar, Arun and Mangina, Eleni and Campbell, Abraham G.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Using HMD-based Hand Tracking Virtual Reality in Canine Anatomy Summative Assessment: a User Study}, 
  year={2022},
  volume={},
  number={},
  pages={287-296},
  abstract={Due to the global pandemic, heavily practical based teaching in veterinary education has been hugely impacted. Even before the crisis, an innovative way to provide an assessment that could simulate the examination of a cadaver has been a long-standing dream for veterinary schools and faculty. We designed and implemented a summative assessment system using touchless interaction head-mounted display (HMD) Virtual Reality (VR), and presented a user study that was an initial exploration of the proposed system designed to replace traditional spatial multiple choices questions (MCQs) for veterinary students. The performance relating to the score and time spent of 24 veterinary students was measured in online MCQ and VR MCQ. Questionnaires were sent out to gather demographic data and opinions. Manipulation time in the VR environment was tracked. The results showed that the time spent was significantly shorter using VR as an assessment tool compared to online MCQ. Participants revealed that the VR assessment outperformed the traditional method in terms of satisfaction, distinctiveness, concentration, and nervousness levels. However, they underperformed in the VR examination significantly. The summative assessment in VR has the potential to further support veterinary education, and the usage of HMDs and touchless interaction will boost the process. This evaluation illustrates the gap between students’ perception of VR examination and their MCQ performance that will require future research to untangle if this technology is to be adopted in the anatomy classroom.},
  keywords={Solid modeling;Three-dimensional displays;Head-mounted displays;Pandemics;Education;Resists;Particle measurements;virtual reality;head mounted display;hand tracking;veterinary education;summative assessment;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques;Gestural input},
  doi={10.1109/ISMAR55827.2022.00044},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995481,
  author={Scargill, Tim and Chen, Ying and Marzen, Nathan and Gorlatova, Maria},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Integrated Design of Augmented Reality Spaces Using Virtual Environments}, 
  year={2022},
  volume={},
  number={},
  pages={297-306},
  abstract={Demand is growing for markerless augmented reality (AR) experiences, but designers of the real-world spaces that host them still have to rely on inexact, qualitative guidelines on the visual environment to try and facilitate accurate pose tracking. Furthermore, the need for visual texture to support markerless AR is often at odds with human aesthetic preferences, and understanding how to balance these competing requirements is challenging due to the siloed nature of the relevant research areas. To address this, we present an integrated design methodology for AR spaces, that incorporates both tracking and human factors into the design process. On the tracking side, we develop the first VI-SLAM evaluation technique that combines the flexibility and control of virtual environments with real inertial data. We use it to perform systematic, quantitative experiments on the effect of visual texture on pose estimation accuracy; through 2000 trials in 20 environments, we reveal the impact of both texture complexity and edge strength. On the human side, we show how virtual reality (VR) can be used to evaluate user satisfaction with environments, and highlight how this can be tailored to AR research and use cases. Finally, we demonstrate our integrated design methodology with a case study on AR museum design, in which we conduct both VI-SLAM evaluations and a VR-based user study of four different museum environments.},
  keywords={Visualization;Systematics;Pose estimation;Virtual environments;Human factors;Integrated design;Museums;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR55827.2022.00045},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995410,
  author={Luong, Tiffany and Pléchata, Adela and Möbus, Max and Atchapero, Michael and Böhm, Robert and Makransky, Guido and Holz, Christian},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Demographic and Behavioral Correlates of Cybersickness: A Large Lab-in-the-Field Study of 837 Participants}, 
  year={2022},
  volume={},
  number={},
  pages={307-316},
  abstract={ybersickness has been one of the main impediments to the widespread adoption of Virtual Reality for decades. It has been argued that several factors can influence the occurrence of cybersickness, such as technical factors, interaction design, but also users’ demographics and their perceived presence. Yet, previous studies had comparably small sample sizes and demographically homogeneous samples; comparisons across studies (e.g., regarding demographic factors) are challenging due to the large variation in the studied virtual environments. In this paper, we address these limitations and report the results of a lab-in-the-field experiment on cybersickness with a large and heterogeneous sample of $N =837$ participants who navigated and interacted inside a virtual environment (ages 18–80, $M = 29.34, SD = 9.50$, 431 males, 400 females, 6 non-binaries and other). We found that female participants and participants with lower VR experience were more susceptible to experiencing higher levels of cybersickness. Participants’ cybersickness levels increased with the time spent in VR and with the distance traversed in the virtual world up to a point, above which reported levels declined. We also found a link between higher levels of cybersickness and reduced head motion, as well as between lower levels of cybersickness and more head motion, which led them to explore more of the virtual environment. In contrast to past studies, we did not find any evidence suggesting an effect of age on cybersickness, nor a negative correlation between presence and cybersickness. Based on our results, we derived a model that achieves a mean classification accuracy of 67.1% for two levels of cybersickness using demographic, user experience, and behavioral data in VR.},
  keywords={Solid modeling;Correlation;Cybersickness;Navigation;Virtual environments;Particle measurements;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00046},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995582,
  author={Chowdhury, Sohan and Ullah, A K M Amanat and Pelmore, Nathan Bruce and Irani, Pourang and Hasan, Khalad},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={WriArm: Leveraging Wrist Movement to Design Wrist+Arm Based Teleportation in VR}, 
  year={2022},
  volume={},
  number={},
  pages={317-325},
  abstract={Teleportation, a widely used locomotion technique in Virtual Reality (VR), is used to move users through a virtual environment. Until recently, handheld controllers have been used for teleportation, where users use controllers to point to a location and perform an action (e.g., button press) to be instantly moved to the targeted location. Recent advancements in VR hand tracking enable users to move through and interact with the virtual world without controllers. This opens the opportunity for compelling alternatives to explore hand tracking-based teleportation techniques for more natural, intuitive and immersive interactions. Prior work mostly explores using arm movement for teleportation as an alternative to using the controller. In this paper, we design and evaluate WriArm, a VR locomotion technique that leverages both wrist and arm movement for VR teleportation. We first conduct a design study to find suitable hand gesture sets that can be mapped to teleportation activities such as activation, pointing, confirmation and cancellation for WriArm and arm-based techniques. Based on the results, we conduct a study comparing users’ performance while navigating tasks with the two techniques and three gesture sets. Results show that WriArm improves navigation efficiency by allowing users to navigate the environment quickly. We conclude with design guidelines for arm and wrist-based teleportation in VR.},
  keywords={Wrist;Headphones;Presses;Target tracking;Navigation;Design methodology;Virtual environments;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality;Interaction techniques;Gestural input},
  doi={10.1109/ISMAR55827.2022.00047},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994974,
  author={Chidambaram, Subramanian and Reddy, Sai Swarup and Rumple, Matthew and Ipsita, Ananya and Villanueva, Ana and Redick, Thomas and Stuerzlinger, Wolfgang and Ramani, Karthik},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={EditAR: A Digital Twin Authoring Environment for Creation of AR/VR and Video Instructions from a Single Demonstration}, 
  year={2022},
  volume={},
  number={},
  pages={326-335},
  abstract={Augmented/Virtual reality and video-based media play a vital role in the digital learning revolution to train novices in spatial tasks. However, creating content for these different media requires expertise in several fields. We present EditAR, a unified authoring, and editing environment to create content for AR, VR, and video based on a single demonstration. EditAR captures the user’s interaction within an environment and creates a digital twin, enabling users without programming backgrounds to develop content. We conducted formative interviews with both subject and media experts to design the system. The prototype was developed and reviewed by experts. We also performed a user study comparing traditional video creation with 2D video creation from 3D recordings, via a 3D editor, which uses freehand interaction for in-headset editing. Users took 5 times less time to record instructions and preferred EditAR, along with giving significantly higher usability scores.},
  keywords={Three-dimensional displays;Prototypes;Media;Digital twins;Recording;Usability;Task analysis;Augmented Reality;Virtual Reality;Authoring;Video},
  doi={10.1109/ISMAR55827.2022.00048},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994987,
  author={Nelson, Cassidy R. and Gabbard, Joseph L. and Moats, Jason B. and Mehta, Ranjana K.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={User-Centered Design and Evaluation of ARTTS: an Augmented Reality Triage Tool Suite for Mass Casualty Incidents}, 
  year={2022},
  volume={},
  number={},
  pages={336-345},
  abstract={In this work we present ARTTS: a head-worn Augmented Reality (AR) Triage Tool Suite containing an initial sorting tool, virtual assessment tool, and virtual triage tag to assist emergency responders in mass casualty incidents. The initial sorting tool can prompt novice responders through first-wave tasks to aid recalibration from shock to triage. The virtual assessment tool provides novice responders, potentially confused by the chaos, with a walkthrough of the SALT triage flowchart. Finally, current emergency medical triage processes leverage static paper tags susceptible to loss or illegible damage. ARTTS’ virtual triage tags are dynamic, can be updated through responder interaction, and employ user interface emergent features based on individual patient conditions. This paper describes ARTTS’ capabilities, as well as the applied user-centered design process including review of existing triage material, subject-matter expert interview transcripts, wireframing, application of usability and user-centered design principles, as well as iterative usability subject-matter expert assessments and design walkthroughs. The ARTTS user experience aims to enhance, not upend, existing triage processes. Finally, this paper provides a usability evaluation comparing ARTTS’ virtual triage tag to a physical paper triage tag. Our tag achieved requisite System Usability Scale (SUS) scores and showed negligible differences to the paper triage tag on usability and mental workload.},
  keywords={Training;Hospitals;User centered design;User interfaces;User experience;Recording;Usability;Augmented reality;triage;first responders;emergency response;mass casualty incident},
  doi={10.1109/ISMAR55827.2022.00049},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995239,
  author={Rebol, Manuel and Pietroszek, Krzysztof and Ranniger, Claudia and Hood, Colton and Rutenberg, Adam and Sikka, Neal and Li, David and Gütl, Christian},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mixed Reality Communication for Medical Procedures: Teaching the Placement of a Central Venous Catheter}, 
  year={2022},
  volume={},
  number={},
  pages={346-354},
  abstract={Medical procedures are an essential part of healthcare delivery, and the acquisition of procedural skills is a critical component of medical education. Unfortunately, procedural skill is not evenly distributed among medical providers. Skills may vary within departments or institutions, and across geographic regions, depending on the provider’s training and ongoing experience. We present a mixed reality real-time communication system to increase access to procedural skill training and to improve remote emergency assistance. Our system allows a remote expert to guide a local operator through a medical procedure. RGBD cameras capture a volumetric view of the local scene including the patient, the operator, and the medical equipment. The volumetric capture is augmented onto the remote expert’s view to allow the expert to spatially guide the local operator using visual and verbal instructions. We evaluated our mixed reality communication system in a study in which experts teach the ultrasound-guided placement of a central venous catheter (CVC) to students in a simulation setting. The study compares state-of-the-art video communication against our system. The results indicate that our system enhances and offers new possibilities for visual communication compared to video teleconference-based training.},
  keywords={Training;Visualization;Ultrasonic imaging;Visual communication;Mixed reality;Streaming media;Cameras;Rendering (computer graphics);Real-time systems;Catheters;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computer vision;Computer vision problems;Reconstruction Social and professional topics;Medical},
  doi={10.1109/ISMAR55827.2022.00050},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995485,
  author={Eom, Sangjun and Sykes, David and Rahimpour, Shervin and Gorlatova, Maria},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={NeuroLens: Augmented Reality-based Contextual Guidance through Surgical Tool Tracking in Neurosurgery}, 
  year={2022},
  volume={},
  number={},
  pages={355-364},
  abstract={External ventricular drain (EVD) is a common, yet challenging neurosurgical procedure of placing a catheter into the brain ventricular system that requires prolonged training for surgeons to improve the catheter placement accuracy. In this paper, we introduce NeuroLens, an Augmented Reality (AR) system that provides neurosurgeons with guidance that aides them in completing an EVD catheter placement. NeuroLens builds on prior work in AR-assisted EVD to present a registered hologram of a patient’s ventricles to the surgeons, and uniquely incorporates guidance on the EVD catheter’s trajectory, angle of insertion, and distance to the target. The guidance is enabled by tracking the EVD catheter. We evaluate NeuroLens via a study with 33 medical students, in which we analyzed students’ EVD catheter insertion accuracy and completion time, eye gaze patterns, and qualitative responses. Our study, in which NeuroLens was used to aid students in inserting an EVD catheter into a realistic phantom model of a human head, demonstrated the potential of NeuroLens as a tool that will aid and educate novice neurosurgeons. On average, the use of NeuroLens improved the EVD placement accuracy of year 1 students by 39.4% and of the year 2–4 students by 45.7%. Furthermore, students who focused more on NeuroLens-provided contextual guidance achieved better results.},
  keywords={Training;Visualization;Target tracking;Head;Phantoms;Neurosurgery;Trajectory;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction devices;Displays and imagers},
  doi={10.1109/ISMAR55827.2022.00051},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995550,
  author={Nam, Hyeongil and Kim, Chanhee and Kim, Kangsoo and Yeo, Ji-Young and Park, Jong-Il},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Emotionally Responsive Virtual Parent for Pediatric Nursing Education: A Framework for Multimodal Momentary and Accumulated Interventions}, 
  year={2022},
  volume={},
  number={},
  pages={365-374},
  abstract={Immersive virtual reality (VR) simulations become more and more popular for basic nursing skills training in a realistic VR environment. However, there is still a gap of research that specifically focuses on pediatric nursing interventions where nurse trainees should be able to recognize and deal with the patient parent’s emotional responses appropriately. In this paper, we propose a novel nursing intervention analysis framework that evaluates the user’s nursing performance, by analyzing not only their momentary multimodal (verbal and nonverbal) behaviors, but also accumulated intervention behaviors that capture the overall nursing context. Based on the proposed framework, we developed an immersive VR-based nursing education system with an emotionally responsive virtual parent, and designed a realistic pediatric nursing intervention scenario in collaboration with a subject-matter expert (a nursing faculty in our university). An expert-evaluation study with professional nurses was conducted to assess the potential of the developed system—in particular, the effects of the emotionally responsive virtual parent— as an effective education tool, by comparing with an emotionally static/neutral virtual parent. Several factors of learning experience, e.g., immersion, realism, learning efficacy, and usefulness, were examined through a subjective questionnaire, and the results support the effectiveness of our system in clinical practice or educational settings. We discuss the findings and implications of our research with the nurse participants’ qualitative feedback, while also addressing possible limitations and future research directions.},
  keywords={Training;Solid modeling;Emotion recognition;Design methodology;Collaboration;Medical services;Behavioral sciences;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;HCI design and evaluation methods;User studies;Applied computing;Education;Interactive learning environments},
  doi={10.1109/ISMAR55827.2022.00052},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995692,
  author={Lu, Conny and Chakravarthula, Praneeth and Liu, Kaihao and Liu, Xixiang and Li, Siyuan and Fuchs, Henry},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Neural 3D Gaze: 3D Pupil Localization and Gaze Tracking based on Anatomical Eye Model and Neural Refraction Correction}, 
  year={2022},
  volume={},
  number={},
  pages={375-383},
  abstract={Eye tracking has already made its way to current commercial wearable display devices, and is becoming increasingly important for virtual and augmented reality applications. However, the existing model-based eye tracking solutions are not capable of conducting very accurate gaze angle measurements, and may not be sufficient to solve challenging display problems such as pupil steering or eyebox expansion. In this paper, we argue that accurate detection and localization of pupil in 3D space is a necessary intermediate step in model-based eye tracking. Existing methods and datasets either ignore evaluating the accuracy of 3D pupil localization or evaluate it only on synthetic data. To this end, we capture the first 3D pupilgaze-measurement dataset using a high precision setup with head stabilization and release it as the first benchmark dataset to evaluate both 3D pupil localization and gaze tracking methods. Furthermore, we utilize an advanced eye model to replace the commonly used oversimplified eye model. Leveraging the eye model, we propose a novel 3D pupil localization method with a deep learning-based corneal refraction correction. We demonstrate that our method outperforms the state-of-the-art works by reducing the 3D pupil localization error by 47.5% and the gaze estimation error by 18.7%. Our dataset and codes can be found here: link.},
  keywords={Location awareness;Solid modeling;Analytical models;Three-dimensional displays;Head;Gaze tracking;Benchmark testing;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR55827.2022.00053},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995686,
  author={Chen, Chen and Yarmand, Matin and Xu, Zhuoqun and Singh, Varun and Zhang, Yang and Weibel, Nadir},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating Input Modality and Task Geometry on Precision-first 3D Drawing in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={384-393},
  abstract={Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is indispensable for many precise 3D tasks. However, due to lack of physical support, limited depth perception, and the non-planar nature of 3D curves, it is challenging to adjust mid-air strokes to achieve high precision. Instead of creating new interaction techniques, we investigated how task geometric shapes and input modalities affect precision-first drawing performance in a within-subject study (n=12) focusing on 3D target tracing in commercially available VR headsets. We found that compared to using bare hands, VR controllers and pens yield nearly 30% of precision gain, and that the tasks with large curvature, forward-backward or left-right orientations perform best. We finally discuss opportunities for designing novel interaction techniques for precise 3D drawing. We believe that our work will benefit future research aiming to create usable toolboxes for precise 3D drawing.},
  keywords={Headphones;Geometry;Three-dimensional displays;Shape;Focusing;User experience;Task analysis;VR;Precise-First 3D Drawing;Usability Studies},
  doi={10.1109/ISMAR55827.2022.00054},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995241,
  author={Shin, Jae-Eun and Yoon, Boram and Kim, Dooyoung and Kim, Hyung-Il and Woo, Woontack},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Device and Spatial Layout on Social Presence During a Dynamic Remote Collaboration Task in Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={394-403},
  abstract={This paper evaluates factors of social presence during a dynamic remote collaboration task in a technologically asymmetric Mixed Reality (MR) setting for two spatial layouts. While active movement during MR remote collaboration is afforded by how the shared 3D space is mediated and configured, studies investigating the impact of these conditions on user experience have been scarce. In a between-group study $(\mathrm{n}=48)$, a host user in Augmented Reality (AR) and a remote user in Virtual Reality (VR), both wearing Head Mounted Displays (HMDs), simultaneously moved around the shared space to find and assemble parts of a Mars exploration rover together, one group in a Peripheral layout and the other in a Scattered layout with disparate levels of spatial affordance. Results show that while VR facilitates higher co-presence and spatial presence than AR through HMDs, the Peripheral layout enables users to pay more attention to one another than the Scattered. We analyze the results and derive implications aimed at bridging the AR-VR gap in social presence for dynamic MR remote collaboration through the adaptive placement of virtual content in shared spaces.},
  keywords={Space vehicles;Three-dimensional displays;Affordances;Layout;Collaboration;Mixed reality;User experience;Human-centered computing;Human-Computer Interaction;Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR55827.2022.00055},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995510,
  author={Babu, Sabarish V. and Huang, Hsiao-Chuan and Teather, Robert J. and Chuang, Jung-Hong},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparing the Fidelity of Contemporary Pointing with Controller Interactions on Performance of Personal Space Target Selection}, 
  year={2022},
  volume={},
  number={},
  pages={404-413},
  abstract={The goal of this research is to provide much needed empirical data on how the fidelity of popular hand gesture tracked based pointing metaphors versus commodity controller based input affects the efficiency and speed-accuracy tradeoff in users’ spatial selection in personal space interactions in VR. We conduct two experiments in which participants select spherical targets arranged in a circle in personal space, or near-field within their maximum arms reach distance, in VR. Both experiments required participants to select the targets with either a VR controller or with their dominant hand’s index finger, which was tracked with one of two popular contemporary tracking methods. In the first experiment, the targets are arranged in a flat circle in accordance with the ISO 9241-9 Fitts’ law standard, and the simulation selected random combinations of 3 target amplitudes and 3 target widths. Targets were placed centered around the users’ eye level, and the arrangement was placed at either 60%, 75%, or 90% depth plane of the users’ maximum arm’s reach. In experiment 2, the targets varied in depth randomly from one depth plane to another within the same configuration of 13 targets within a trial set, which resembled button selection task in hierarchical menus in differing depth planes in the near-field. The study was conducted using the HTC Vive head-mounted display, and used either a VR controller (HTC Vive), low-fidelity virtual pointing (Leap Motion), or a high-fidelity virtual pointing (tracked VR glove) conditions. Our results revealed that low-fidelity pointing performed worse than both high-fidelity pointing and the VR controller. Overall, target selection performance was found to be worse in depth planes closer to the maximum arms reach, as compared to middle and nearer distances.},
  keywords={Target tracking;Head-mounted displays;ISO Standards;Fingers;Aerospace electronics;Task analysis;Standards;Human-centered computing;Virtual reality;Computing methodologies;Perception},
  doi={10.1109/ISMAR55827.2022.00056},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995125,
  author={Li, Yihan and Hu, Yong and Wang, Zidan and Shen, Xukun},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating the Object-Centered User Interface in Head-Worn Mixed Reality Environment}, 
  year={2022},
  volume={},
  number={},
  pages={414-421},
  abstract={User interface (UI) in head-worn mixed reality (MR) can be divided into two categories: user-centered and object-centered. Many studies have focused on user-centered UI, but few have explored object-centered UI. In this research, we explored the design of object-centered UI in head-worn MR environment. First, (b) we proposed three design considerations for object-centered UI in head-worn MR environment, and then verified them through two user studies with four UIs, where one is pure object-centered UI, one is pure user-centered UI, the others are hybrid UIs. In the first study, we conducted a user experience (UX) research on the access comfort, convenience and unobstructed sight degree of each UI, and demonstrated users’ preferences for the object-centered UI. In the second study, we designed a real application scenario to further evaluate the design considerations in a complex MR environment, and assessed its usability. We found positive user experience qualities of object-centered UI. Finally, we summarized four design recommendations, which could guide the UI design for the interaction with real-world objects in the future of everyday life with head-worn MR.},
  keywords={Visualization;Mixed reality;Virtual reality;Interference;User interfaces;User experience;Usability;Human-centered computing;Mixed / augmented reality;User interface design},
  doi={10.1109/ISMAR55827.2022.00057},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995436,
  author={Fukumoto, Naoya and Isoyama, Naoya and Uchiyama, Hideaki and Sakata, Nobuchika and Kiyokawa, Kiyoshi},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Object Synthesis Method to Enhance Visuo-Haptic Consistency}, 
  year={2022},
  volume={},
  number={},
  pages={422-430},
  abstract={The sense of reality is enhanced by presenting appropriate haptic feedback when the user interacts with a virtual object in virtual reality (VR). To present appropriate feedback, we often use a real object that resembles the virtual one to manipulate. However, such a real object is not always available. The user may feel a visuohaptic inconsistency between real and virtual objects when their shapes are different. To alleviate such an inconsistency, we propose a novel object synthesis method that combines the shape of the real object that the user manipulates in reality and the shape of the virtual object which was to be presented to the user in VR. In other words, this synthesized object, a chimera object, is created by transforming the part of the virtual object that the user would touch into a shape that is similar to the corresponding part of the real object while maintaining the other parts of the virtual object intact. The expected haptic sensation from the appearance of our chimera object is more consistent with the one produced by the real object. Therefore, the visuo-haptic inconsistency is expected to alleviate in VR, compared to the original virtual object. In addition, we propose an interactive system to support the design of a chimera object for users. To investigate the effectiveness of our method, we conducted two user studies. The first experiment confirmed that our proposed chimera object helps enhance visuo-haptic consistency. The second experiment confirmed that our system was effective for chimera object creation by users with acceptable system usability.},
  keywords={Visualization;Three-dimensional displays;Shape;Interactive systems;Nearest neighbor methods;Haptic interfaces;Usability;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;virtual reality;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR55827.2022.00058},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995593,
  author={Liu, Jen-Shuo and Wang, Portia and Tversky, Barbara and Feiner, Steven},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Adaptive Visual Cues for Guiding a Bimanual Unordered Task in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={431-440},
  abstract={Work on cueing performance in AR and VR has focused on sequential tasks in which each step must be completed in order before the user can proceed to the next. However, for unordered tasks such as putting books back on a library shelf, the user may be able to perform multiple steps concurrently without needing to follow a specific order. In such situations, giving the user multiple cues for potentially concurrent steps may improve performance time. To investigate this, we built a bimanual VR testbed in which the user needs to move objects to designated destinations, guided by different numbers of cues. The user can decide the order to perform the cued steps and, in some conditions, can affect which cues are shown.In a formal user study, we found that in most conditions, participants perform fastest with three cues. Dynamically updating the set of displayed cues based on hand proximity improves performance, and updating the set based on eye gaze improves performance even more. Finally, for both the hand-proximity and eye-gaze mechanisms, performance can be further improved by locking the cues for objects predicted to be moved next based on hand distance.},
  keywords={Visualization;Design methodology;Libraries;Task analysis;Augmented reality;Human-centered computing;Human-centered computing (HCI);Interaction paradigms;Virtual reality;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR55827.2022.00059},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994935,
  author={Wei, Xiaoheng and Shi, Xuehuai and Wang, Lili},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Label Guidance based Object Locating in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={441-449},
  abstract={Object locating in virtual reality (VR) has been widely used in many VR applications, such as virtual assembly, virtual repair, virtual remote coaching. However, when there are a large number of objects in the virtual environment(VE), the user cannot locate the target object efficiently and comfortably. In this paper, we propose a label guidance based object locating method for locating the target object efficiently in VR. Firstly, we introduce the label guidance based object locating pipeline to improve the efficiency of the object locating. It arranges the labels of all objects on the same screen, lets the user select the target labels first, and then uses the flying labels to guide the user to the target object. Then we summarize five principles for constructing the label layout for object locating and propose a two-level hierarchical sorted and orientated label layout based on the five principles for the user to select the candidate labels efficiently and comfortably. After that, we propose the view and gaze based label guidance method for guiding the user to locate the target object based on the selected candidate labels. It generates specific flying trajectories for candidate labels, updates the flying speed of candidate labels, keeps valid candidate labels, and removes the invalid candidate labels in real time during object locating with the guidance of the candidate labels. Compared with the traditional method, the user study results show that our method significantly improves efficiency and reduces task load for object locating.},
  keywords={Layout;Pipelines;Maintenance engineering;Real-time systems;Trajectory;Task analysis;Augmented reality;Virtual Reality;Object Locating;Label Guidance},
  doi={10.1109/ISMAR55827.2022.00060},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995452,
  author={Genay, Adélaïde and Lécuyer, Anatole and Hachet, Martin},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={What Can I Do There? Controlling AR Self-Avatars to Better Perceive Affordances of the Real World}, 
  year={2022},
  volume={},
  number={},
  pages={450-459},
  abstract={This work explores a new usage of Augmented Reality (AR) to extend perception and interaction within physical areas ahead of ourselves. To do so, we propose to detach ourselves from our physical position by creating a controllable “digital copy”; of our body that can be used to navigate in local space from a third-person perspective. With such a viewpoint, we aim to improve our mental representation of distant space and understanding of action possibilities (called affordances), without requiring us to physically enter this space. Our approach relies on AR to virtually integrate the user’s body in remote areas in the form of an avatar. We discuss concrete application scenarios and propose several techniques to manipulate avatars in the third person as a part of a larger conceptual framework. Finally, through a user study employing one of the proposed techniques (puppeteering), we evaluate the validity of using third-person embodiment to extend our perception of the real world to areas outside of our proximal zone. We found that this approach succeeded in enhancing the user’s accuracy and confidence when estimating their action capabilities at distant locations.},
  keywords={Navigation;Avatars;Affordances;Decision making;Aerospace electronics;Augmented reality;Guidelines;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR55827.2022.00061},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995594,
  author={Plasson, Carole and Blanch, Renaud and Nigay, Laurence},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Selection Techniques for 3D Extended Desktop Workstation with AR HMD}, 
  year={2022},
  volume={},
  number={},
  pages={460-469},
  abstract={Extending a standard desktop workstation (i.e. a screen, a mouse, a keyboard) with virtual scenes displayed on an Augmented Reality Head-Mounted Display (AR HMD) offers many identified advantages including limited physical space requirements, very large and flexible display spaces, and 3D stereoscopic views. While the technologies become more mainstream, the remaining open question is how to interact with such hybrid workstations that combine 2D views displayed on a physical monitor and 3D views displayed on a HoloLens. For a selection task, we compared mouse-based interaction (standard for 2D desktop workstations) and direct touch interaction in mid-air (standard for 3D AR) while considering different positions of the 3D scene according to a physical monitor. To extend mouse-based selection to 3D views, we experimentally explored different interaction metaphors where the mouse cursor moves either on a horizontal or a vertical plane in a 3D virtual scene. To check for ecological validity of our results, we conducted an additional study focusing on interaction with a 2D/3D Gapminder dataset visualization. The results show 1) that the mouse-based interaction, as compared to direct touch interaction in mid-air, is easy and efficient, 2) that using a vertical plane placed in front of the 3D virtual scene to mimic the double screen metaphor outperforms other interaction techniques and 3) that flexibility is required to allow users to choose the selection techniques and to position the 3D virtual scene relative to the physical monitor. Based on these results, we derive interaction design guidelines for hybrid workstations.},
  keywords={Visualization;Three-dimensional displays;Stereo image processing;Resists;Mice;Workstations;Task analysis;Human-centered computing;Visualization;Interaction techniques;Hybrid 2D/3D workstation;AR},
  doi={10.1109/ISMAR55827.2022.00062},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995228,
  author={Wilmott, James P. and Erkelens, Ian M. and Murdison, T. Scott and Rio, Kevin W.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perceptibility of Jitter in Augmented Reality Head-Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={470-478},
  abstract={When using a see-through augmented reality head-mounted display system (AR HMD), a user’s perception of virtual content may be degraded by a variety of perceptual artifacts resulting from the architecture of rendering and display pipelines. In particular, virtual content that is rendered to appear stationary in the real world (worldlocked) can be susceptible to spatial and temporal 3D position errors. A subset of these errors, termed jitter, result from mismatches between the spatial localization, rendering, and display pipelines, and can manifest as perceived motion of intended-to-be stationary content. Here, we employ psychophysical methods to quantify the perceptibility of jitter artifacts in an AR HMD. For some viewing conditions, participants perceived jitter that was smaller than the pixel pitch of the testbed (i.e., subpixel jitter). In general, we found that jitter perceptibility increased as viewing distance increased and decreased as background luminance increased. We did not find that the contrast ratio of virtual content, age, or experience with AR/VR modulatedjitter perceptibility. Taken together, this study quantifies the degree of jitter that a user can perceive in an AR HMD and demonstrates that it is critical to consider the capabilities and limits of the human visual system when designing the next generation of spatial computing platforms.},
  keywords={Location awareness;Head-mounted displays;Three-dimensional displays;Pipelines;Resists;Jitter;Visual systems;Human-centered computing;Human-computer interaction(HCI);HCI design and evaluation methods;Laboratory experiments;Human-centered computing;Human-computer interaction (HCI);Interaction paradigms-Mixed / augmented reality},
  doi={10.1109/ISMAR55827.2022.00063},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995165,
  author={Tahmid, Ibrahim A. and Lisle, Lee and Davidson, Kylie and North, Chris and Bowman, Doug A.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating the Benefits of Explicit and Semi-Automated Clusters for Immersive Sensemaking}, 
  year={2022},
  volume={},
  number={},
  pages={479-488},
  abstract={Immersive spaces have great potential to support analysts in complex sensemaking tasks, but the use of only manual interactions for organizing data elements can become tedious. We analyzed the user interactions to support cluster formation in an immersive sensemaking system, and we designed a semi-automated cluster creation technique that determines the user’s intent to create a cluster based on object proximity. We present the results of a user study comparing this proximity-based technique with a manual clustering technique and a baseline immersive workspace with no explicit clustering support. We found that semi-automated clustering was faster and preferred, while manual clustering gave greater control to users. These results provide support for the approach of adding intelligent semantic interactions to aid the users of immersive analytics systems.},
  keywords={Semantics;Manuals;Task analysis;Artificial intelligence;Augmented reality;Virtual Reality;Human AI Collaboration;Semantic Interaction;Immersive Analytics;Clustering},
  doi={10.1109/ISMAR55827.2022.00064},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995637,
  author={Wolf, Erik and Mal, David and Frohnapfel, Viktor and Döllinger, Nina and Wenninger, Stephan and Botsch, Mario and Latoschik, Marc Erich and Wienrich, Carolin},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Plausibility and Perception of Personalized Virtual Humans between Virtual and Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={489-498},
  abstract={This article investigates the effects of different XR displays on the perception and plausibility of personalized virtual humans. We compared immersive virtual reality (VR), video see-through augmented reality (VST AR), and optical see-through AR (OST AR). The personalized virtual alter egos were generated by state-of-the-art photogrammetry methods. 42 participants were repeatedly exposed to animated versions of their 3D-reconstructed virtual alter egos in each of the three XR display conditions. The reconstructed virtual alter egos were additionally modified in body weight for each repetition. We show that the display types lead to different degrees of incongruence between the renderings of the virtual humans and the presentation of the respective environmental backgrounds, leading to significant effects of perceived mismatches as part of a plausibility measurement. The device-related effects were further partly confirmed by subjective misestimations of the modified body weight and the measured spatial presence. Here, the exceedingly incongruent OST AR condition leads to the significantly highest weight misestimations as well as to the lowest perceived spatial presence. However, similar effects could not be confirmed for the affective appraisal (i.e., humanness, eeriness, or attractiveness) of the virtual humans, giving rise to the assumption that these factors might be unrelated to each other.},
  keywords={Weight measurement;Rendering (computer graphics);Optical imaging;Appraisal;X reality;Image reconstruction;Mixed reality;immersion;coherence;presence;body weight perception;body image;serious application;uncanny valley Human-centered computing;Empirical studies in HCI;Mixed / augmented reality;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00065},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995035,
  author={Yang, Xingrui and Li, Hai and Zhai, Hongjia and Ming, Yuhang and Liu, Yuqian and Zhang, Guofeng},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation}, 
  year={2022},
  volume={},
  number={},
  pages={499-507},
  abstract={In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods. Our approach is inspired by the recently developed implicit mapping and positioning system and further extends the idea so that it can be freely applied to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation to encode and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to divide the scene and support dynamic expansion, enabling our system to track and map arbitrary scenes without knowing the environment like in previous works. Moreover, we proposed a high-performance multi-process framework to speed up the method, thus supporting some applications that require real-time performance. The evaluation results show that our methods can achieve better accuracy and completeness than previous methods. We also show that our Vox-Fusion can be used in augmented reality and virtual reality applications. Our source code is publicly available at https://github.com/zju3dv/Vox-Fusion.},
  keywords={Fuses;Source coding;Rendering (computer graphics);Real-time systems;Systems support;Augmented reality;Dense SLAM;Implicit Networks;Voxelization;Surface Rendering},
  doi={10.1109/ISMAR55827.2022.00066},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995087,
  author={Zheng, Mengya and Thomas, Rosemary J and Pan, Xingyu and Xu, Zixiang and Liang, Yuan and Campbell, Abraham G.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Augmenting Feature Importance Analysis: How Color and Size Can Affect Context-Aware AR Explanation Visualizations?}, 
  year={2022},
  volume={},
  number={},
  pages={508-517},
  abstract={Augmented Reality (AR) has shown significant potential in supporting in-situ decision-making in various application areas. Many prior works have shown how AR can visualize the decision support data in various contexts. However, prior research about AR-based decision support systems rarely explored how the explanations were visualized. Providing context-aware explanations within AR-based recommendation systems may help users instantly understand the recommendations they have been given. Therefore, this paper presents the world-first user study exploring AR explanation visualization designs. Three feature importance analysis visualizations that apply different color-coding and size-scaling strategies were designed to explain the recommendations provided by a context-aware AR shopping assistant system. Twenty-four participants were recruited to evaluate these three explanations in a shopping scenario. The results revealed novel findings that could help guide the appropriate utilization of descriptive parameters when designing AR explanation artifacts. The results also show the potential of providing intuitive visualization to explain recommendations in AR.},
  keywords={Decision support systems;Human computer interaction;Image color analysis;Decision making;Data visualization;Recommender systems;Augmented reality;Human-centered computing;Visualization;Empirical studies in Visualization;Human computer interaction;Empirical studies in HCI},
  doi={10.1109/ISMAR55827.2022.00067},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995192,
  author={Satkowski, Marc and Rzayev, Rufat and Goebel, Eva and Dachselt, Raimund},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ABOVE & BELOW: Investigating Ceiling and Floor for Augmented Reality Content Placement}, 
  year={2022},
  volume={},
  number={},
  pages={518-527},
  abstract={Augmented Reality (AR) interfaces support users by providing access to digital content within real-world environments. However, displaying content at the users’ eye level might result in the occlusion of the real world. Therefore, it requires finding AR content placement areas that free the users’ field of vision. In this work, we systematically investigate two content placement areas beyond the users’ eye level: the ceiling and floor. To understand how potential users perceive virtual content on the ceiling and floor and how the content should be placed on these areas, we conducted two user studies. While the first exploratory study showed the general usefulness of either area, the second quantitative study allowed us to define optimal placement parameters regarding visibility and comfort. With insights from our studies, we provide design recommendations for future AR applications that support 2D content presentation on the ceiling and the floor.},
  keywords={User interfaces;Floors;Augmented reality;User Study;Augmented Reality;Mixed Reality;Ceiling;Floor;Content Placement Human-centered computing—Mixed/Augmented Reality; Human-centered computing—Visualization},
  doi={10.1109/ISMAR55827.2022.00068},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994912,
  author={Cools, Robbe and Esteves, Augusto and Simeone, Adalberto L.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Blending Spaces: Cross-Reality Interaction Techniques for Object Transitions Between Distinct Virtual and Augmented Realities}, 
  year={2022},
  volume={},
  number={},
  pages={528-537},
  abstract={Cross-Reality (CR) involves interaction between different modalities and levels of immersion such as Virtual and Augmented Reality, as we explore in this paper. Whereas previous work assumed similarity between their respective Virtual and Augmented Environment (VE and AE), we explore the case in which VE and AE are distinct. This gives rise to novel and critical problems, such as how to visualise and interact with the other environment. In this context we investigate the fundamental interaction of transitioning an object across environments, to which we contribute five interaction techniques. Two are inspired by literature: Virtual Magic Lens and Binary Transition; while the other three are entirely novel: Auto Blended Space, Manual Blended Space - Button Transition and Manual Blended Space - Touch Transition. In a study evaluating the first four techniques, we found that participants (N=20) performed a CR object manipulation and transition task significantly faster using our Auto Blended Space technique. We then modified Manual Blended Space - Button Transition into Manual Blended Space - Touch Transition in response to these results, and reassessed the four techniques in a more complex object manipulation task (N=16). We found that this type of task was better suited to manual transition methods rather than automatic methods. Taken together, our final contribution are five blended space design factors, and timely Cross-Reality transition design guidelines.},
  keywords={Visualization;Navigation;Design methodology;Virtual environments;Manuals;Task analysis;Augmented reality;Human-centered computing;Mixed / augmented reality;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00069},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995530,
  author={Huard, Andrew and Chen, Mengyu and Sra, Misha},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={CardsVR: A Two-Person VR Experience with Passive Haptic Feedback from a Deck of Playing Cards}, 
  year={2022},
  volume={},
  number={},
  pages={538-547},
  abstract={Presence in virtual reality (VR) is meaningful for remotely connecting with others and facilitating social interactions despite great distance while providing a sense of “being there.” This work presents CardsVR, a two-person VR experience that allows remote participants to play a game of cards together. An entire deck of tracked cards are used to recreate the sense of playing cards in-person. Prior work in VR commonly provides passive haptic feedback either through a single object or through static objects in the environment. CardsVR is novel in providing passive haptic feedback through multiple cards that are individually tracked and represented in the virtual environment. Participants interact with the physical cards by picking them up, holding them, playing them, or moving them on the physical table. Our participant study (N=23) shows that passive haptic feedback provides significant improvement in three standard measures of presence: Possibility to Act, Realism, and Haptics.},
  keywords={Atmospheric measurements;Virtual environments;Games;Particle measurements;Haptic interfaces;Standards;Augmented reality;Human-centered computing;Mixed / augmented reality},
  doi={10.1109/ISMAR55827.2022.00070},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995505,
  author={Muthukumarana, Sachith and Nassani, Alaeddin and Park, Noel and Steimle, Jürgen and Billinghurst, Mark and Nanayakkara, Suranga},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={XRtic: A Prototyping Toolkit for XR Applications using Cloth Deformation}, 
  year={2022},
  volume={},
  number={},
  pages={548-557},
  abstract={This paper presents XRtic, a prototyping toolkit enabling real-world cloth deformations to be used in novel ways in eXtended Reality (XR) applications. XRtic was developed based on the insights gathered from semi-structured interviews with XR developers. It consists of custom-made actuators that can be attached to regular clothing, a controller bus system, and a controller interface. Using our toolkit, users can design and integrate different cloth deformation types synchronised with virtual content in a plug-and-play manner. Along with a technical analysis of the actuation behaviour of the XRtic actuators, we present the findings gathered from a user study with eight XR developers, focusing on the usability of the system and creative support. Overall, participants found it an easy-to-use toolkit that supports iterative and rapid prototyping, and enables cloth to be deformed in unique ways in synchronisation with XR applications. Based on the findings, we also report limitations and future work relating to our system.},
  keywords={Actuators;Focusing;Control systems;Rapid prototyping;Synchronization;Iterative methods;X reality;Human-centered computing;Human computer interaction (HCI);Interactive systems and tools;User interface toolkits;Interaction paradigms;Virtual Reality},
  doi={10.1109/ISMAR55827.2022.00071},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995110,
  author={Lavric, Traian and Bricard, Emmanuel and Preda, Marius and Zaharia, Titus},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ATOFIS, an AR Training System for Manual Assembly: A Full Comparative Evaluation against Guides}, 
  year={2022},
  volume={},
  number={},
  pages={558-567},
  abstract={This paper reports on a user study to comparatively evaluate two AR training systems designed for step-by-step manual operations: ATOFIS - recently proposed in the literature, and Microsoft Dynamics 365 Guides (hereinafter Guides) - one of the most relevant state-of-the-art commercial solutions. The user study (N=16) was conducted in two stages- i.e., training and authoring, on a partial replica of a real-world assembly workstation. During training, the participant learns a sequence of manual operations by performing two assembly cycles, guided by each of the two AR training systems. During authoring, the participant creates the two sets of AR work instructions used in the next training session, one set with each of the two authoring systems. We bound the authoring and training procedures during the experiment to comparatively assess the AR systems overall, and address at the same time an evaluation gap observed in the literature. The experimental results demonstrated advantages of the authoring approach proposed by ATOFIS (i.e., low-cost, formalized, in-situ, immersive and on-the-fly), proved the usability and effectiveness of the AR instructions authored with ATOFIS and validated a set of hypotheses formulated by the authors of the system. ATOFIS authoring was $1.72\times$ faster and unanimously preferred by the participants; ATOFIS training reported zero assembly errors and was 13% faster than Guides. ATOFIS reported excellent system usability (i.e., SUS) and mental workload (i.e., NASA-TLX) scores for both authoring and training, outperforming Guides on all dimensions.},
  keywords={Training;Authoring systems;Design methodology;Manuals;Workstations;Usability;Augmented reality;augmented reality;content authoring;work instructions;training;manual assembly;user study},
  doi={10.1109/ISMAR55827.2022.00072},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994967,
  author={Xu, Leiqing and Zhang, Zhubai Mutsing},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Effects of User Construction Behavior on User Experience in a Virtual Indoor Environment}, 
  year={2022},
  volume={},
  number={},
  pages={568-575},
  abstract={The metaverse elicits an extensive range of user curiosity by attractive virtual indoor environments, in which users can solve different tasks and conduct enormous collaborations. However, limited interaction methods are slowing down this development process, and the understanding of how such methods impact user experience is limited. To address this problem, we conducted a lab experiment with 150 valid participants to analyze the effect of user construction behavior on user satisfaction, perceived performance, and recommendation intention. By using the PROCESS 3.3 model, the results show that a higher level of participation facilitates satisfaction, which is a complete mediator that has a significant positive effect on perceived performance. Additionally, recommendation intention is positively affected by both satisfaction and perceived performance. Demographic factors could also generate significant impacts on user experience. We discuss the theoretical explanations, as well as design implications. Taken together, this study represents the first inquiry into the relationship between user construction behavior and user experience, thereby laying the groundwork for future interaction methods.},
  keywords={Metaverse;Design methodology;Computational modeling;Collaboration;User experience;Behavioral sciences;Indoor environment;Virtual Reality;User Construction Behavior;User Experience;Recommendation Intention},
  doi={10.1109/ISMAR55827.2022.00073},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995629,
  author={Halkola, Joona and Suomalainen, Markku and Sakcak, Basak and Mimnaugh, Katherine J. and Kalliokoski, Juho and Chambers, Alexis P. and Ojala, Timo and LaValle, Steven M.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Leaning-Based Control of an Immersive-Telepresence Robot}, 
  year={2022},
  volume={},
  number={},
  pages={576-583},
  abstract={In this paper, we present an implementation of a leaning-based control of a differential drive telepresence robot and a user study in simulation, with the goal of bringing the same functionality to a real telepresence robot. The participants used a balance board to control the robot and viewed the virtual environment through a head-mounted display. The main motivation for using a balance board as the control device stems from Virtual Reality (VR) sickness; even small movements of your own body matching the motions seen on the screen decrease the sensory conflict between vision and vestibular organs, which lies at the heart of most theories regarding the onset of VR sickness. To test the hypothesis that the balance board as a control method would be less sickening than using joysticks, we designed a user study (N=32, 15 women) in which the participants drove a simulated differential drive robot in a virtual environment with either a Nintendo Wii Balance Board or joysticks. However, our pre-registered main hypotheses were not supported; the joystick did not cause any more VR sickness on the participants than the balance board, and the board proved to be statistically significantly more difficult to use, both subjectively and objectively. Analyzing the open-ended questions revealed these results to be likely connected, meaning that the difficulty of use seemed to affect sickness; even unlimited training time before the test did not make the use as easy as the familiar joystick. Thus, making the board easier to use is a key to enable its potential; we present a few possibilities towards this goal.},
  keywords={Training;Heart;Headphones;Solid modeling;Telepresence;Head-mounted displays;Virtual environments;Telepresence robot;VR sickness;Balance board;User comfort},
  doi={10.1109/ISMAR55827.2022.00074},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995104,
  author={Waidhofer, John and Gadgil, Richa and Dickson, Anthony and Zollmann, Stefanie and Ventura, Jonathan},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PanoSynthVR: Toward Light-weight 360-Degree View Synthesis from a Single Panoramic Input}, 
  year={2022},
  volume={},
  number={},
  pages={584-592},
  abstract={We investigate how real-time, 360° view synthesis can be achieved on current virtual reality hardware from a single panoramic image input. We introduce a light-weight method to automatically convert a single panoramic input into a multi-cylinder image representation that supports real-time, free-viewpoint view synthesis rendering for virtual reality. We apply an existing convolutional neural network trained on pinhole images to a cylindrical panorama with wrap padding to ensure agreement between the left and right edges. The network outputs a stack of semi-transparent panoramas at varying depths which can be easily rendered and composited with over blending. Quantitative experiments and a user study show that the method produces convincing parallax and fewer artifacts than a textured mesh representation.},
  keywords={Image edge detection;Image representation;Rendering (computer graphics);Real-time systems;Hardware;Convolutional neural networks;Augmented reality;Computing methodologies;Artificial intelligence;Computer vision;Human-centered computing;Human computer interaction (HCI);Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00075},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995511,
  author={Thomas, Jerald and Yong, Seraphina and Rosenberg, Evan Suma},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Inverse Kinematics Assistance for the Creation of Redirected Walking Paths}, 
  year={2022},
  volume={},
  number={},
  pages={593-602},
  abstract={Virtual reality interactions that require a specific relationship between the virtual and physical coordinate systems, such as passive haptic interactions, are not possible with locomotion techniques using redirected walking. To address this limitation, recent research has introduced environmental alignment, which is the notion of using redirected walking techniques to align the virtual and physical coordinate systems such that these interactions are possible. However, the previous research has only implemented environmental alignment in a reactive manner, and the authors posited that better results could be achieved if a predictive algorithm was instead used. In this work, we introduce a novel way to model the environmental alignment problem as a version of the inverse kinematics problem which can be incorporated into several existing predictive algorithms, as well as a simple proof-of-concept implementation. An exploratory human subject study (N=17) was conducted to evaluate this implementation’s usability as a tool for authoring planned path redirected walking scenarios that incorporate physical interactivity. To our knowledge, this is the first study to evaluate redirected walking experience design tools and provides a possible framework for future studies. Our qualitative analysis of the results generated both guidance for integrating automatic solvers and broad recommendations for designing redirected path authoring tools.},
  keywords={Legged locomotion;Solid modeling;Authoring systems;Kinematics;Predictive models;Prediction algorithms;Haptic interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Computer graphics;Graphics systems and interfaces},
  doi={10.1109/ISMAR55827.2022.00076},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994980,
  author={You, Christopher and Benda, Brett and Rosenberg, Evan Suma and Ragan, Eric and Lok, Benjamin and Thomas, Jerald},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Strafing Gain: Redirecting Users One Diagonal Step at a Time}, 
  year={2022},
  volume={},
  number={},
  pages={603-611},
  abstract={Redirected walking can effectively utilize a user’s physical space when traversing larger virtual environments by using virtual self-motion gains for a user’s physical motions. In particular, curvature gain presents unique advantages in redirection but can lead to suboptimal orientations. To prevent this and add additional utility in redirected walking, we formally present strafing gain. Strafing gain seeks to add incremental lateral movements to a user’s position causing the user to walk along a diagonal trajectory while maintaining the original orientation of the user. In a study with 27 participants, we tested 11 values to determine the detection thresholds of strafing gain. The study, which was modeled on prior detection threshold studies, found that strafing gain could successfully redirect participants to walk along a 5.57° diagonal to the right and a 4.68° diagonal to the left. Furthermore, a supplementary study with 10 participants was conducted, verifying that orientation was maintained throughout redirection and validating the obtained detection thresholds. We discuss the implications of these findings and potential ways of improving these quantities in real-world applications.},
  keywords={Legged locomotion;Sensitivity;Virtual environments;Trajectory;Augmented reality;redirected walking;virtual reality;gain;strafing gain;detection threshold},
  doi={10.1109/ISMAR55827.2022.00077},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995431,
  author={Clarence, Aldrich and Knibbe, Jarrod and Cordeil, Maxime and Wybrow, Michael},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating The Effect of Direction on The Limits of Haptic Retargeting}, 
  year={2022},
  volume={},
  number={},
  pages={612-621},
  abstract={Haptic Retargeting enables spatially decoupled physical objects to provide haptic feedback for multiple virtual objects in Virtual Reality (VR). By decoupling the virtual hand from its real position, through Hand Redirection, multiple virtual objects can be mapped to a single physical proxy. However, redirection beyond a detectable level is disruptive to the user experience. The limits of haptic retargeting have mainly been explored in one primary direction—the user reaching forwards. We designed an experiment with participants performing reaching movements across 8 reaching directions in the horizontal plane, with a hand redirection of up to 30°. We identify an overall haptic retargeting limit and find that a physical proxy can be remapped to virtual objects of up to 16.14° away. We find a significant effect of reaching direction on the limit. In practice, however, these differences are small, measuring only a couple of degrees, translating to approximately 1cm across a 30cm reach. We argue that, while the psychology literature might suggest the need for specific directional limits and while we do find an effect of direction on retargeting limits, interaction designers can mitigate these requirements by applying slightly conservative global retargeting limits. Our contributions further the community’s knowledge of both how to deploy haptic retargeting in interaction without compromising the user’s experience and how visual and proprioceptive cues interact in peripersonal space in VR.},
  keywords={Visualization;Psychology;Propioception;User experience;Haptic interfaces;Object recognition;Augmented reality;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Interaction techniques;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00078},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995600,
  author={Lehrbaum, Valeriya and MacWilliams, Asa and Newman, Joseph and Sudharsan, Nischita and Bien, Seongjin and Karas, Konstantin and Eghtebas, Chloe and Weber, Sandro and Klinker, Gudrun},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enabling Customizable Workflows for Industrial AR Applications}, 
  year={2022},
  volume={},
  number={},
  pages={622-630},
  abstract={Augmented Reality (AR) is increasingly considered for use in real industrial applications [8]. In our industrial research lab at Siemens Technology, we are continuously discussing suitable AR scenarios in business sectors involving power plants, wind turbine plants, and oil and gas factories. We have developed a company-internal AR system architecture, Hololayer, which provides AR facilities in a reusable manner to development engineers such that they can build their own AR applications for their specialized use cases. The integration of AR technology into industrial processes encounters many complex issues: we need to adhere to many safety, security and quality guarantees while also adapting quickly to the rapidly changing state of the art of AR devices (HMDs, tablets). To increase flexibility, it might be good to integrate Hololayer with one of the open frameworks that have recently been proposed [11], [29], [38]. Yet, care must be taken when converting an already existing, large company-owned framework like Hololayer to such platforms. Some of the proposed standardized APIs and communication protocols may be difficult to abide by, without requiring significant rewriting efforts or even violating company-internal regulations. In this paper, we report on our efforts to combine Hololayer with one such platform, Ubi-Interact [38]. This is a collaboration between Siemens technology and the FAR group at TU Munich. After exemplary descriptions of typical application scenarios, we present the underlying principles of Hololayer to support this range of applications. We then describe rudimentary concepts of Ubi-Interact, followed by an elaboration on the efforts to combine both systems for a selected application scenario and a discussion of the results achieved thus far.},
  keywords={Protocols;Collaboration;Systems architecture;Regulation;Production facilities;Wind turbines;Safety;Human-centered computingUbiquitous and mobile computing;Human-centered computingHuman computer interaction (HCI)Interaction ParadigmsMixed / Augmented Reality},
  doi={10.1109/ISMAR55827.2022.00079},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995439,
  author={Nasiri, Moloud and Anaraky, Reza Ghaiumy and Babu, Sabarish V. and Robb, Andrew},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Gait Differences in the Real World and Virtual Reality: The Effect of Prior Virtual Reality Experience}, 
  year={2022},
  volume={},
  number={},
  pages={631-636},
  abstract={Walking through immersive virtual environments is one of the important parts of Virtual Reality (VR) applications. Prior research has established that users’ gait in virtual and real environments differs; however, little research has evaluated how users’ gait differs as users gain more experience with VR. We conducted experiments measuring novice and experienced subjects’ gait parameters in VR and real environments. Results showed that subjects’ performance in VR and Real World was more similar in the last trials than in the first trials; their walking dissimilarity in the start trials diminished by walking more trials. We found trial as a significant variable affecting the walking speed, step length, and trunk angle for both groups of users. While the main effect of expertise was not observed, an interaction effect between expertise and the trial number was shown. Trunk angle increased over time for novices but decreased for experts.},
  keywords={Legged locomotion;Design methodology;Virtual environments;Gain measurement;Augmented reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR55827.2022.00080},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995359,
  author={Yu, Difeng and Zhou, Qiushi and Dingler, Tilman and Velloso, Eduardo and Goncalves, Jorge},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Blending On-Body and Mid-Air Interaction in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={637-646},
  abstract={On-body interfaces, which leverage the human body’s surface as an input or output platform, can provide new opportunities for designing VR interaction. However, it remains unclear how on-body interfaces can best support current VR systems that mainly rely on mid-air interaction. We propose BodyOn, a collection of six design patterns that leverage combined on-body and mid-air interfaces to achieve more effective 3D interaction. Specifically, a user may use thumb-on-finger gestures, finger-on-arm gestures, or on-body displays with mid-air input, including hand movement and orientation, to complete an interaction task. To test our design concepts, we implemented example interaction techniques based on BodyOn that can assist users in various 3D interaction tasks. We further conducted an expert evaluation using the techniques as probes to elicit immediate design issues that emerge from the novel combination of on-body and midair interaction. We provide insights that can inspire and inform the design of future 3D user interfaces.},
  keywords={Visualization;Three-dimensional displays;Focusing;User interfaces;Task analysis;Probes;Augmented reality;Human-centered computing;Human Computer Interaction (HCI);Interaction Paradigms;Virtual Reality},
  doi={10.1109/ISMAR55827.2022.00081},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995604,
  author={Liu, Wenlei and Fei, Jiajun and Zhu, Ziyu},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MFF-PR: Point Cloud and Image Multi-modal Feature Fusion for Place Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={647-655},
  abstract={Place recognition technology can eliminate cumulative errors and thus plays a vital role in autonomous driving. In this paper, the composite feature of point cloud and image data is obtained by multi-modal feature fusion, thereby improving positioning accuracy. Semantic features, instance features, topological features, and image texture features are integrated to obtain comprehensive features, presenting strong robustness and complex scene expression abilities. Topological features consist of intra-class features and inter-instance features, which allow users to obtain more comprehensive scene structure information. The place recognition methods of data-level fusion and feature-level fusion based on point cloud and image are compared. This paper verifies the proposed method on SemanticKitti and nuScenes datasets. The results show that it outperforms state-of-the-art place recognition methods.},
  keywords={Point cloud compression;Image texture;Image recognition;Semantics;Pose estimation;Data preprocessing;Sensor phenomena and characterization;Place recognition;Multi-modal fusion;SLAM;Point cloud and image},
  doi={10.1109/ISMAR55827.2022.00082},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995250,
  author={Souchet, Alexis D. and Diallo, Mamadou Lamarana and Lourdeaux, Domitile},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Cognitive load Classification with a Stroop task in Virtual Reality based on Physiological data}, 
  year={2022},
  volume={},
  number={},
  pages={656-666},
  abstract={Cognitive load (CL) when using Virtual Reality (VR) requires more experimental inputs, especially to determine how VR affects human psychophysiology depending on the task. Classifying humans’ physiological variations in a controlled setup is essential. We randomly assigned 92 participants to three experimental conditions: control, stereoscopy, and dual-task. Participants fulfilled a rest-baseline period, a Stroop task (25 congruent, 25 incongruent words), and a NASA-TLX questionnaire. We recorded behavioral and physiological data from eye-tracking(ET), electrocardiogram(ECG), and electrodermal activity (EDA). NASA-TLX scores of control and stereoscopy were statistically different with dual-task conditions. We used NASA-TLX scores to create three classes and train a CL classifier based on physiological variations. We deployed linear models penalized with the L1 norm to select the most relevant features correlated with subjective CL levels. The ECG sensor provided the most selected features compared to EDA and ET. We compared SVM, Logistic Regression, and Gradient boosting classifier models. The Gradient boosting method with 87.23% accuracy and an 87.13% F1 score is the most performant. Future works will try to compare such an approach with stressful stimuli.},
  keywords={Support vector machines;Solid modeling;Electrocardiography;Cognitive load;Boosting;Physiology;Behavioral sciences;Virtual Reality;Cognitive load;Physiology;Machine Learning},
  doi={10.1109/ISMAR55827.2022.00083},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995369,
  author={Makled, Elhassan and Weidner, Florian and Broll, Wolfgang},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Investigating User Embodiment of Inverse-Kinematic Avatars in Smartphone Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={666-675},
  abstract={Smartphone Augmented Reality (AR) has already provided us with a plethora of social applications such as Pokemon Go or Harry Potter Wizards Unite. However, to enable smartphone AR for social applications similar to VRChat or AltspaceVR, proper user tracking is necessary to accurately animate the avatars. In Virtual Reality (VR), avatar tracking is rather easy due to the availability of hand-tracking, controllers, and HMD whereas smartphone AR has only the back-(and front) camera and IMUs available for this task. In this paper we propose ARIKA, a tracking solution for avatars in smartphone AR. ARIKA uses tracking information from ARCore to track the users hand position and to calculate a pose using Inverse Kinematics (IK). We compare the accuracy of our system against a commercial motion tracking system and compare both systems with respect to sense of agency, self-location, and body-ownership. For this, 20 participants observed their avatars in an augmented virtual mirror and executed a navigation and a pointing task. Our results show that participants felt a higher sense of agency and self location when using the full body tracked avatar as opposed to IK avatars. Interestingly and in favor of ARIKA, there were no significant differences in body-ownership between our solution and the full-body tracked avatars. Thus, ARIKA and it’s single-camera approach is valid solution for smartphone AR applications where body-ownership is essential.},
  keywords={Tracking;Navigation;Avatars;Multimedia systems;Collaboration;Resists;Cameras;Augmented reality;inverse kinematics;embodiment;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities; I.4. S [Image Processing and Computer Vision]: Scene Analysis-Tracking},
  doi={10.1109/ISMAR55827.2022.00084},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995375,
  author={Liu, Jingyu and Jindal, Akshay and Mantel, Claire and Forchhammer, Søren and Mantiuk, Rafal K.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={How bright should a virtual object be to appear opaque in optical see-through AR?}, 
  year={2022},
  volume={},
  number={},
  pages={676-684},
  abstract={Reproduction of occlusions and opaque surfaces are the major challenges of additive optical see-through (OST) displays. This is because the user of an OST display sees a linear mixture of display and environment light, which creates an impression of transparency unless the displayed color is sufficiently bright. The primary goal of this work is to determine how bright a displayed surface needs to be in relation to environment light to be perceived as opaque. We test multiple factors that could affect the perception of opacity: background luminance, contrast, spatial frequency, and accommodation depth in foveal vision. The subjective results, collected on a high-dynamic-range multi-focal stereo display, indicate that a virtual object needs to be, on average, 60 times brighter than the background environment light to be perceived as opaque. A higher contrast of the texture of the virtual object and a background that is out of focus can reduce the required luminance ratio. We demonstrate that a model of visual perception based on Weber’s law and accounting for contrast masking and defocus blur can predict the experimental data with an averaged prediction error of 8.29%. Existing perceptual image difference metrics (PSNR, FovVideoVDP and HDR-VDP-3) can also predict the effect of major factors, but with lower accuracy (e.g. prediction error of 34% for PSNR with PU21 encoding).},
  keywords={Measurement;Visualization;Image coding;Image color analysis;Computational modeling;Predictive models;Optical imaging;Human-centered computing;HCI theory;concepts and models},
  doi={10.1109/ISMAR55827.2022.00085},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995051,
  author={Vatavu, Radu-Daniel},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Sensorimotor Realities: Formalizing Ability-Mediating Design for Computer-Mediated Reality Environments}, 
  year={2022},
  volume={},
  number={},
  pages={685-694},
  abstract={We introduce “Sensorimotor Realities,” a new concept in the XR landscape and corresponding technology-agnostic framework for computer-mediated perception and motor action. Sensorimotor Realities capitalize on the heterogeneity of human sensorimotor abilities to support conceptualization, characterization, and design of computer technology that leverages existing abilities in new, computer-mediated worlds. We introduce a conceptual space for Sensorimotor Realities with six dimensions, discuss examples of interactive systems in this space, and show how Sensorimotor Realities are distinct in nature and goal from Augmented, Mixed, Virtual, and Mediated Reality. We capitalize on Sensorimotor Realities to propose “ability-mediating design,” an approach to designing accessible interactive computer systems that complements ability-based design. We discuss how Sensorimotor Realities offer new opportunities for research and development at the intersection of XR, wearable computing, ambient intelligence, and accessible computing, and draw a research roadmap with three major directions of scientific investigation.},
  keywords={Wearable computers;Source coding;Multimedia systems;Interactive systems;Focusing;Ambient intelligence;X reality;H.5.1 [Multimedia Information Systems]:;Artificial;augmented;and virtual realities;;K.4.2 [Social Issues]:;Assistive technologies for persons with disabilities},
  doi={10.1109/ISMAR55827.2022.00086},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995201,
  author={Rettinger, Maximilian and Rigoll, Gerhard},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Defuse the Training of Risky Tasks: Collaborative Training in XR}, 
  year={2022},
  volume={},
  number={},
  pages={695-701},
  abstract={Extensive training is crucial but challenging in certain areas such as explosive ordnance disposal. Past conflicts have shown that not only military personal but also civilians have to learn how to disarm unexploded ordnance. The preparation for dangerous situations is difficult and limited in the real world. Extended reality (XR) offers new possibilities to enhance the training of explosive ordnance disposal experts due to its immersive capabilities. This paper presents a comparative study (n = 75) of three distinct training methods: 1) Real-world (Real)-Training with a tangible replica object, 2) Virtual Reality (VR)-Training with a non-see-through Head-Mounted-Display (HMD), and 3) Mixed Reality (MR)-Training in a Cave Automatic Virtual Environment (CAVE). All training methods are collaborative, i.e., an instructor teaches the training content to a participant in the real or virtual world. We evaluate the suitability of these approaches in terms of usability, cognitive workload, training motivation, and training success. Our results indicate that the virtual methods, VR-Training and MR-Training, provide significantly superior results in the evaluated aspects compared to the real-world training. These results can also be applied to other collaborative training methods, as the training concept of this use case was non-specific. Therefore, these virtual technologies can increase the safety of explosive ordnance disposal personnel, and we recommend establishing this in future training.},
  keywords={Training;Social computing;Collaboration;Virtual environments;Explosives;Safety;Personnel;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Collaborative and social computing;Empirical studies in collaborative and social computing},
  doi={10.1109/ISMAR55827.2022.00087},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994904,
  author={Shen, Junxiao and Hu, Jinghui and Dudley, John J. and Kristensson, Per Ola},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Personalization of a Mid-Air Gesture Keyboard using Multi-Objective Bayesian Optimization}, 
  year={2022},
  volume={},
  number={},
  pages={702-710},
  abstract={We present AdaptiKeyboard, a mid-air gesture keyboard that uses multi-objective Bayesian optimization to adaptively change layout size to simultaneously optimize speed and accuracy. Gesture keyboards are well suited for enabling mid-air text entry in augmented reality (AR) due to their relative robustness to articulation inaccuracy. However, transplanting gesture keyboards to AR involves a larger design and operational space compared to touchscreen interactions. One potential advantage of this larger design and operational space is that mid-air keyboards presented in AR can be more versatile than their touchscreen equivalents. A key component of a mid-air gesture keyboard is the layout size, which can be made adaptive in order to optimize text entry speed and accuracy at the individual user level. This adaptive personalization can refine the keyboard design to reflect the differences users exhibit in motor behaviors and personal preferences. In this paper, we propose a multi-objective Bayesian optimization approach for adapting the layout size of a mid-air gesture keyboard to individual users. We show that this process can deliver a 14.4% improvement in speed and a 13.8% improvement in accuracy relative to a baseline design with a constant size derived from the default system keyboard on the HoloLens 2.},
  keywords={Design methodology;Layout;Keyboards;Touch sensitive screens;Robustness;Bayes methods;Behavioral sciences;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Text input;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/ISMAR55827.2022.00088},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995672,
  author={Karpov, Aleksei and Makarov, Ilya},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring Efficiency of Vision Transformers for Self-Supervised Monocular Depth Estimation}, 
  year={2022},
  volume={},
  number={},
  pages={711-719},
  abstract={Depth estimation is a crucial task for the creation of depth maps, one of the most important components for augmented reality (AR) and other applications. However, the most widely used hardware for AR and smartphones has only sparse depth sensors with different ground truth depth acquisition methods. Thus, depth estimation models that are robust for downstream AR tasks performance can only be trained reliably using self-supervised learning based on camera information. Previous works in the field mostly focus on self-supervised models with pure convolutional architectures, without taking global spatial context into account.In this paper, we utilize vision transformer architectures for self-supervised monocular depth estimation and propose VTDepth, a vision transformer-based model, which provides a solution to the problem of the global spatial context. We compare various combinations of convolutional and transformer architectures for self-supervised depth estimation and show that the best combination of models is an encoder with a transformer basis and convolutional decoder. Our experiments demonstrate the efficiency of VTDepth for self-supervised depth estimation. Our set of models achieves state-of-the-art performance for self-supervised learning on NYUv2 and KITTI datasets. Our code is available at https://github.com/ahbpp/VTDepth.},
  keywords={Convolutional codes;Computational modeling;Estimation;Self-supervised learning;Predictive models;Transformers;Decoding;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Artificial intelligence;Computer vision;Localization;spatial registration and tracking;3D reconstruction},
  doi={10.1109/ISMAR55827.2022.00089},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995573,
  author={Zins, Matthieu and Simon, Gilles and Berger, Marie-Odile},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={OA-SLAM: Leveraging Objects for Camera Relocalization in Visual SLAM}, 
  year={2022},
  volume={},
  number={},
  pages={720-728},
  abstract={In this work, we explore the use of objects in Simultaneous Localization and Mapping in unseen worlds and propose an object-aided system (OA-SLAM). More precisely, we show that, compared to low-level points, the major benefit of objects lies in their higher-level semantic and discriminating power. Points, on the contrary, have a better spatial localization accuracy than the generic coarse models used to represent objects (cuboid or ellipsoid). We show that combining points and objects is of great interest to address the problem of camera pose recovery. Our main contributions are: (1) we improve the relocalization ability of a SLAM system using high-level object landmarks; (2) we build an automatic system, capable of identifying, tracking and reconstructing objects with 3D ellipsoids; (3) we show that object-based localization can be used to reinitialize or resume camera tracking. Our fully automatic system allows on-the-fly object mapping and enhanced pose tracking recovery, which we think, can significantly benefit to the AR community. Our experiments show that the camera can be relocalized from viewpoints where classical methods fail. We demonstrate that this localization allows a SLAM system to continue working despite a tracking loss, which can happen frequently with an uninitiated user. Our code and test data are released at gitlab.inria.fr/tangram/oa-slam.},
  keywords={Location awareness;Visualization;Simultaneous localization and mapping;Three-dimensional displays;Codes;Semantics;Cameras;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/ISMAR55827.2022.00090},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995334,
  author={Song, Guoxian and Cham, Tat-Jen and Cai, Jianfei and Zheng, Jianmin},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Real-time Shadow-aware Portrait Relighting in Virtual Backgrounds for Realistic Telepresence}, 
  year={2022},
  volume={},
  number={},
  pages={729-738},
  abstract={While using virtual backgrounds has recently become a very popular feature in videoconferencing, there often exists a jarring mismatch between the lighting of the user and the illumination condition of the virtual background. Existing portrait relighting methods can alleviate the problem, but do not have the capacity to deal with difficult shadow effects. In this paper, we present a new shadow-aware portrait relighting system that can relight an input portrait to be consistent with a given desired background image with shadow effects. Our system consists of four major components: portrait neutralization, illumination estimation, shadow generation and hierarchical neural rendering, which are all based on deep neural networks, and the whole system is end-to-end trainable. In addition, we created a large-scale photorealistic synthetic dataset with shadow, illumination and depth annotations for training, which allows our model to generalize well to real images. The extensive experiments demonstrate that our shadow-aware relight system outperforms the state-of-the-art portrait relighting solutions in terms of producing more lighting-consistent relighted images with shadow effects.},
  keywords={Training;Deep learning;Telepresence;Neural networks;Lighting;Estimation;Rendering (computer graphics);Shadow Generation;Relighting;Neural Rendering},
  doi={10.1109/ISMAR55827.2022.00091},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994878,
  author={Liu, Xiaolong and Wang, Lili and Luan, Shuai and Shi, Xuehuai and Liu, Xinda},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Distant Object Manipulation with Adaptive Gains in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={739-747},
  abstract={Object Manipulation is a fundamental interaction in virtual reality (VR). The efficiency and accuracy of object manipulation are important to provide immersion to users. We propose a manipulation method with adaptive gains to improve the efficiency and accuracy of object manipulation in VR applications. First, we introduce manipulation gains. We then design an experiment to collect user behavior during manipulation to determine fitting functions for calculating manipulation gain. At last, we design a user study to evaluate the performance of our distant object manipulation method with adaptive gains. The results show that, compared with the state of the art methods, our method has a significant improvement in the completion time, and the manipulation accuracy of the tasks. Moreover, our method significantly increases usability and reduces task load.},
  keywords={Fitting;Behavioral sciences;Task analysis;Usability;Augmented reality;Virtual reality;Object manipulation;Manipulation gains;Visual perception},
  doi={10.1109/ISMAR55827.2022.00092},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995335,
  author={Ban, Reigo and Matsumoto, Keigo and Narumi, Takuji and Kuzuoka, Hideaki},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Wormholes in VR: Teleporting Hands for Flexible Passive Haptics}, 
  year={2022},
  volume={},
  number={},
  pages={748-757},
  abstract={Presenting haptic feedback in virtual reality (VR) is a long-standing challenge, with passive haptics being one way of presenting haptic feedback inexpensively. However, passive haptics requires props in physical environments that are co-located with their virtual counterparts, which is often not the case in the real world. Although redirected hands and other methods have previously been proposed to solve this problem, significant differences between the displayed and actual hand positions can cause the degradation of presence and sense of embodiment, limiting the range of presentable environments. In this study, we present a new hand displacement method called wormholes, in which the virtual hand is teleported discontinuously as the user inserts their hand into the hole. The experiment showed that the wormhole could maintain the sense of embodiment, presence, and task performance even with large hand displacements. Our method enables to apply passive haptics even when the actual and virtual environments are quite different, contributing to the realization of inexpensive and flexible haptic presentation in VR applications.},
  keywords={Performance evaluation;Degradation;Limiting;Virtual environments;Teleportation;Haptic interfaces;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction devices;Haptic devices},
  doi={10.1109/ISMAR55827.2022.00093},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995281,
  author={Kwon, Soon-Uk and Jeon, Sang-Bin and Hwang, June-Young and Cho, Yong-Hun and Park, Jinhyung and Lee, In-Kwon},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Infinite Virtual Space Exploration Using Space Tiling and Perceivable Reset at Fixed Positions}, 
  year={2022},
  volume={},
  number={},
  pages={758-767},
  abstract={A simultaneous walking experience in virtual and real spaces can provide a high sense of presence. However, users may face challenges when walking within a large virtual space while walking in a small and complex real space. Several methods such as Redirected Walking (RDW) and Substitutional Reality (SR) have been proposed as different approaches to this problem. However, the users must “reset” their movement direction at unpredictable moments to avoid collision in a small and complex real space when using subtle RDW that does not maintain the correspondence between virtual and real space. Contrarily, exploration through the SR has a limitation in that the VR scene is restricted to a controlled area. In this paper, we propose Reset at Fixed Positions (RFP), a method that combines RDW with the advantage of the SR and matches walkable real space with walkable virtual space. To utilize RFP, we defined Guaranteed Space Block (GSB), a unit space that constitutes a walkable virtual space. This space is obtained through the point reflection of the GSB utilizing the reset position within the GSB. RFPs can be implemented by two methods: Generating Virtual Space Using RFP (G-RFP) and Implementing Given Virtual Space Using RFP (I-RFP). G-RFP can create an infinitely large virtual space for exploration. On the other hand, I-RFP can conFigure a given virtual environment to make users walk. We observed that G-RFP provides higher presence, immersion and a higher mean distance traveled between resets compared to the existing RDW method in a complex real space through a user study. In addition, exploration through I-RFP provided a higher immersion, a comparable presence, and a similar number of resets.},
  keywords={Legged locomotion;Virtual environments;Aerospace electronics;Reflection;Space exploration;Task analysis;Faces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00094},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995326,
  author={Jamalian, Nima and Gillies, Marco and Leymarie, Frederic Fol and Pan, Xueni},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={The Effects of Hand Tracking on User Performance: an experimental study of an object selection based memory game}, 
  year={2022},
  volume={},
  number={},
  pages={768-776},
  abstract={Until recently, Virtual Reality (VR) applications relied on controllers to enable user interaction in virtual environments. With advances in tracking technology, HMDs are now able to track the movements of users’ hands in real-time with significantly greater accuracy, allowing us to interact with the digital world directly with our hands. However, it is not entirely clear how hand tracking affects users’ performance. In this study, we investigate user performance using an in-game analytics-based assessment methodology for a VR memory puzzle task. We conducted a within-subjects experiment with 30 participants in three conditions: 1- Hand-tracking, 2-Controller Without Haptics, and 3- Controller With Haptics. In all our measurements (correct order and pattern, correct pattern only, and trial completion) except for the initial selection time, participants performed best with hand tracking. The use of controllers with haptics did not outperform controllers without haptics in most measures, possibly because other feedback cues compensated for the lack of haptics. This study helps us better understand the three selected interactivity methods when used in VR, as well as the importance of naturalistic experience in interaction design.},
  keywords={Tracking;Atmospheric measurements;Virtual environments;Games;Particle measurements;Time measurement;Real-time systems;Virtual Reality;Hand Tracking;Cognition;Haptic;Multisensory Feedback},
  doi={10.1109/ISMAR55827.2022.00095},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995115,
  author={Kundu, Ripan Kumar and Islam, Rifatul and Calyam, Prasad and Hoque, Khaza Anuarul},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={TruVR: Trustworthy Cybersickness Detection using Explainable Machine Learning}, 
  year={2022},
  volume={},
  number={},
  pages={777-786},
  abstract={Cybersickness can be characterized by nausea, vertigo, headache, eye strain, and other discomforts when using virtual reality (VR) systems. The previously reported machine learning (ML) and deep learning (DL) algorithms for detecting (classification) and predicting (regression) VR cybersickness use black-box models; thus, they lack explainability. Moreover, VR sensors generate a massive amount of data, resulting in complex and large models. Therefore, having inherent explainability in cybersickness detection models can significantly improve the model’s trustworthiness and provide insight into why and how the ML/DL model amved at a specific decision. To address this issue, we present three explainable machine learning (xML) models to detect and predict cybersickness: 1) explainable boosting machine (EBM), 2) decision tree (DT), and 3) logistic regression (LR). We evaluate xML-based models with publicly available physiological and gameplay datasets for cybersickness. The results show that the EBM can detect cybersickness with an accuracy of 99.75% and 94.10% for the physiological and gameplay datasets, respectively. On the other hand, while predicting the cybersickness, EBM resulted in a Root Mean Square Error (RMSE) of 0.071 for the physiological dataset and 0.27 for the gameplay dataset. Furthermore, the EBM-based global explanation reveals exposure length, rotation, and acceleration as key features causing cybersickness in the gameplay dataset. In contrast, galvanic skin responses and heart rate are most significant in the physiological dataset. Our results also suggest that EBM-based local explanation can identify cybersickness-causing factors for individual samples. We believe the proposed xML-based cybersickness detection method can help future researchers understand, analyze, and design simpler cybersickness detection and reduction models.},
  keywords={Solid modeling;Cybersickness;XML;Predictive models;Sensor phenomena and characterization;Physiology;Skin;Virtual Reality;Cybersickness;Explainable Machine Learning;Cybersickness Detection;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;HCI design and evaluation methods},
  doi={10.1109/ISMAR55827.2022.00096},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995681,
  author={Lee, Geonsun and Healey, Jennifer and Manocha, Dinesh},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VRDoc: Gaze-based Interactions for VR Reading Experience}, 
  year={2022},
  volume={},
  number={},
  pages={787-796},
  abstract={Virtual reality (VR) offers the promise of an infinite office and remote collaboration, however, existing interactions in VR do not strongly support one of the most essential tasks for most knowledge workers, reading. This paper presents VRDoc, a set of gaze-based interaction methods designed to improve the reading experience in VR. We introduce three key components: Gaze Select-and-Snap for document selection, Gaze MagGlass for enhanced text legibility, and Gaze Scroll for ease of document traversal. We implemented each of these tools using a commodity VR headset with eye-tracking. In a series of user studies with 13 participants, we show that VRDoc makes VR reading both more efficient (p < 0.01) and less demanding (p < 0.01), and when given a choice, users preferred to use our tools over the current VR reading methods.},
  keywords={Headphones;Collaboration;Gaze tracking;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques},
  doi={10.1109/ISMAR55827.2022.00097},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9994913,
  author={Wieland, Jonathan and Garcia, Rudolf C. Hegemann and Reiterer, Harald and Feuchtner, Tiare},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Arrow, Bézier Curve, or Halos? – Comparing 3D Out-of-View Object Visualization Techniques for Handheld Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={797-806},
  abstract={Handheld augmented reality (AR) applications allow users to interact with their virtually augmented environment on the screen of their tablet or smartphone by simply pointing its camera at nearby objects or “points of interest” (POIs). However, this often requires users to carefully scan their surroundings in search of POIs that are out of view. Proposed 2D guides for out-of-view POIs can, unfortunately, be ambiguous due to the projection of a 3D position to 2D screen space. We address this by using 3D visualizations that directly encode the POI’s 3D direction and distance. Based on related work, we implemented three such visualization techniques: (1) 3D Arrow, (2) 3D Bézier Curve, and (3) 3D Halos. We confirmed the applicability of these three techniques in a case study and then compared them in a user study, evaluating performance, workload, and user experience. Participants performed best using 3D Arrow, while surprisingly, 3D Halos led to poor results. We discuss the design implications of these results that can inform future 3D out-of-view object visualization techniques.},
  keywords={Human computer interaction;Visualization;Three-dimensional displays;Cameras;User experience;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / Augmented Reality;Visualization;Visualization techniques},
  doi={10.1109/ISMAR55827.2022.00098},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995325,
  author={Zhu, Fengyuan and Lyu, Zhuoyue and Sousa, Mauricio and Grossman, Tovi},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Touching The Droid: Understanding and Improving Touch Precision With Mobile Devices in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={807-816},
  abstract={Touch interaction with physical smartphones and tablets in Virtual Reality offers interesting opportunities for cross-device input. Unfortunately, any imprecision in the alignment of the visual representation of either the hand or device can impact the precision of touch and the realism of the experience. We first study a user’s ability to rely solely on preoperative feedback to perform touch interaction in VR, where no rendering of the hand is provided. Results indicate that touch in VR is possible without a visual representation of the hand, but accuracy is influenced by how the device is held and the distance traveled to the target. We then introduce a dynamic calibration algorithm to minimize the offset between the physical hand and its virtual representation. In a second study, we show that this algorithm can increase touch accuracy by 43%, and minimize depth-based “screen penetration” or “floating touch” errors.},
  keywords={Performance evaluation;Visualization;Heuristic algorithms;User interfaces;Rendering (computer graphics);Calibration;Reliability;Human-Centered Computing;Human computer interaction (HCI);Interaction Paradigms;Virtual Reality},
  doi={10.1109/ISMAR55827.2022.00099},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995624,
  author={Somraj, Nagabhushan and Sancheti, Pranali and Soundararajan, Rajiv},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images}, 
  year={2022},
  volume={},
  number={},
  pages={817-826},
  abstract={The challenge of graphically rendering high frame-rate videos on low compute devices can be addressed through periodic prediction of future frames to enhance the user experience in virtual reality applications. This is studied through the problem of temporal view synthesis (TVS), where the goal is to predict the next frames of a video given the previous frames and the head poses of the previous and the next frames. In this work, we consider the TVS of dynamic scenes in which both the user and objects are moving. We design a framework that decouples the motion into user and object motion to effectively use the available user motion while predicting the next frames. We predict the motion of objects by isolating and estimating the 3D object motion in the past frames and then extrapolating it. We employ multi-plane images (MPI) as a 3D representation of the scenes and model the object motion as the 3D displacement between the corresponding points in the MPI representation. In order to handle the sparsity in MPIs while estimating the motion, we incorporate partial convolutions and masked correlation layers to estimate corresponding points. The predicted object motion is then integrated with the given user or camera motion to generate the next frame. Using a disocclusion infilling module, we synthesize the regions uncovered due to the camera and object motion. We develop a new synthetic dataset for TVS of dynamic scenes consisting of 800 videos at full HD resolution. We show through experiments on our dataset and the MPI Sintel dataset that our model outperforms all the competing methods in the literature.},
  keywords={Solid modeling;Three-dimensional displays;TV;Correlation;Dynamics;Cameras;Rendering (computer graphics);View synthesis;Video prediction;Interleaved reprojection;3D optical flow},
  doi={10.1109/ISMAR55827.2022.00100},
  ISSN={1554-7868},
  month={Oct},}
@INPROCEEDINGS{9995285,
  author={Pointecker, Fabian and Friedl, Judith and Schwajda, Daniel and Jetter, Hans-Christian and Anthes, Christoph},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Bridging the Gap Across Realities: Visual Transitions Between Virtual and Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={827-836},
  abstract={Cross-Virtuality applications enabling users to move between different stages of Milgram’s reality-virtuality continuum are a rapidly growing field of research. Modern video see-through head-mounted displays allow users to switch between augmented and virtual reality without removing the headset. This enables for the first time a fluent transition between augmented and virtual reality. Based on insights from literature and preliminary experiments we designed and implemented four transitions: Fade, SimpleCut, TeleportBeam and Portal. These techniques were expected to represent the best suitable concepts for transitioning seamlessly between augmented and virtual reality. After incorporating results from a pre-study, the transition techniques were evaluated in a qualitative user study regarding user experience, simulator sickness, continuity and applicability. Participants were able to freely move between both realities during the study in an immersive analytics scenario for logistics data. In the user study, users preferred Fade in a workplace setting due to its efficiency and simplicity when transitioning frequently between realities. The Portal technique was deemed visually exciting and suitable for infrequent transitions between realities that differ greatly.},
  keywords={Headphones;Visualization;Head-mounted displays;Employment;Switches;User experience;Haptic interfaces;Human-centered computing;Mixed / augmented reality;Virtual reality;User studies},
  doi={10.1109/ISMAR55827.2022.00101},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995367,
  author={Jing, Allison and Gupta, Kunal and McDade, Jeremy and Lee, Gun A. and Billinghurst, Mark},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparing Gaze-Supported Modalities with Empathic Mixed Reality Interfaces in Remote Collaboration}, 
  year={2022},
  volume={},
  number={},
  pages={837-846},
  abstract={In this paper, we share real-time collaborative gaze behaviours, hand pointing, gesturing, and heart rate visualisations between remote collaborators using a live 360 ° panoramic-video based Mixed Reality (MR) system. We first ran a pilot study to explore visual designs to combine communication cues with biofeedback (heart rate), aiming to understand user perceptions of empathic collaboration. We then conducted a formal study to investigate the effect of modality (Gaze+Hand, Hand-only) and interface (Near-Gaze, Embodied). The results show that the Gaze+Hand modality in a Near-Gaze interface is significantly better at reducing task load, improving co-presence, enhancing understanding and tightening collaborative behaviours compared to the conventional Embodied hand-only experience. Ranked as the most preferred condition, the Gaze+Hand in Near-Gaze condition is perceived to reduce the need for dividing attention to the collaborator’s physical location, although it feels slightly less natural compared to the embodied visualisations. In addition, the Gaze+Hand conditions also led to more joint attention and less hand pointing to align mutual understanding. Lastly, we provide a design guideline to summarize what we have learned from the studies on the representation between modality, interface, and biofeedback.},
  keywords={Heart rate;Visualization;Design methodology;Collaboration;Mixed reality;Real-time systems;Biological control systems;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR55827.2022.00102},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995441,
  author={Mahmud, M. Rasel and Stewart, Michael and Cordova, Alberto and Quarles, John},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Auditory Feedback to Make Walking in Virtual Reality More Accessible}, 
  year={2022},
  volume={},
  number={},
  pages={847-856},
  abstract={The objective of this study is to investigate the impact of several auditory feedback modalities on gait (i.e., walking patterns) in virtual reality (VR). Prior research has substantiated gait disturbances in VR users as one of the primary obstacles to VR usability. However, minimal research has been done to mitigate this issue. We recruited 39 participants (with mobility impairments: 18, without mobility impairments: 21) who completed timed walking tasks in a real-world environment and the same tasks in a VR environment with various types of auditory feedback. Within-subject results showed that each auditory condition significantly improved gait performance while in VR $(p \lt$.001) compared to the no auditory condition in VR for both groups of participants with and without mobility impairments. Moreover, spatial audio improved gait performance significantly $(p \lt$.001) compared to other auditory conditions for both groups of participants. This research could help to make walking in VR more accessible for people with and without mobility impairments.},
  keywords={Legged locomotion;Design methodology;Spatial audio;Assistive technologies;Usability;Task analysis;Augmented reality;Virtual reality;auditory feedback;gait disturbances;accessibility;usability;gait improvement;Head-Mounted Displays;Human-centered computing;Human computer interaction (HCI);interaction paradigms;Virtual reality; Human-centered computing;HCI design and evaluation methods;User studies},
  doi={10.1109/ISMAR55827.2022.00103},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995039,
  author={Huynh, Brandon and Wysopal, Abby and Ross, Vivian and Orlosky, Jason and Höllerer, Tobias},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Layerable Apps: Comparing Concurrent and Exclusive Display of Augmented Reality Applications}, 
  year={2022},
  volume={},
  number={},
  pages={857-863},
  abstract={Current augmented reality (AR) interfaces are often designed for interacting with one application at a time, significantly limiting a user’s ability to concurrently interact with and switch between multiple applications or modalities that could run in parallel. In this work, we introduce an application model called Layerable Apps, which supports a variety of AR application types while enabling multitasking through concurrent execution, fast application switching, and the ability to layer application views to adjust the degree of augmentation to the user’s preference. We evaluated Layerable Apps through a within-subjects user study (n=44), compared against a traditional single-focus application model on a split-information task involving the simultaneous use of multiple applications. We report the results of our study, where we found differences in quantitative task performance, favoring Layerable mode. We also analyzed app usage patterns, spatial awareness, and overall preferences between both modes as well as between experienced and novice AR users.},
  keywords={Limiting;Prototypes;Switches;Multitasking;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/ISMAR55827.2022.00104},
  ISSN={1554-7868},
  month={Oct},}@INPROCEEDINGS{9995649,
  author={Song, Zhaomou and Dudley, John J. and Kristensson, Per Ola},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Efficient Special Character Entry on a Virtual Keyboard by Hand Gesture-Based Mode Switching}, 
  year={2022},
  volume={},
  number={},
  pages={864-871},
  abstract={The need to support efficient text input in virtual reality continues to attract significant research attention. However, much of this research understandably focuses exclusively on core text input tasks involving the entry of standard alphabetic characters. The less common though still critical task of entering special characters is often ignored. In this paper we focus on this niche use case as chiefly encountered when entering passwords. Current commercial virtual keyboards allow users to switch between different layers of the keyboard in order to access capital letters, numerals and special characters by pressing an explicit mode-switch button. We propose a new method of switching between layers of a virtual keyboard using hand gestures. Critically, these hand gestures are seamlessly performed in conjunction with key selections to deliver an efficient and intuitive interaction. We report on a user study with 16 participants entering standard passwords comparing our gesture-based mode switching approach to a conventional button-based baseline. We find that with practice our proposed method results in significantly faster entry rates without any deterioration in accuracy. Feedback from users also indicates that our technique is considered efficient and comfortable to use.},
  keywords={Keyboards;Switches;Passwords;Pressing;Task analysis;Standards;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Text input Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/ISMAR55827.2022.00105},
  ISSN={1554-7868},
  month={Oct},}
