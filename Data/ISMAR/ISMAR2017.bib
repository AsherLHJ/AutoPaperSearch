@INPROCEEDINGS{8115399,
  author={Zioulis, Nikolaos and Papachristou, Alexandros and Zarpalas, Dimitris and Daras, Petros},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Improving Camera Pose Estimation via Temporal EWA Surfel Splatting}, 
  year={2017},
  volume={},
  number={},
  pages={1-10},
  abstract={Camera pose estimation is a fundamental problem of Augmented Reality and 3D reconstruction systems. Recently, despite the new better performing direct methods being developed, state-of-the-art methods are still estimating erroneous poses due to sensor noise, environmental conditions and challenging trajectories. Adding a back-end mapping process, SLAM systems achieve better performance and are more robust, but require higher computational resources, limiting their applicability. Therefore, lighter solutions to improve the accuracy of pose estimates are required. In this work we demonstrate the effectiveness of lighter data structures, namely surface elements, and exploit the temporality of sensor data streams to accumulate moving camera frames and improve tracking. This representation allows us to splat a photometric and geometric model simultaneously and use it to improve the performance of dense RGB-D camera pose estimation methods. Exploiting Elliptical Weighted Average splatting to produce high quality photometric results also allows us to detect erroneous poses through a novel visual quality analysis process. We show evidence of the EWA temporal model's effectiveness in publicly available datasets and argue that point-based representations are a good candidate for building lighter systems that should be further explored.},
  keywords={Cameras;Pose estimation;Image color analysis;Simultaneous localization and mapping;Three-dimensional displays;Colored noise;Solid modeling;Camera pose estimation;Surface elements (surfels);Elliptical Weighted Average (EWA);Point-based rendering (PBR);Splatting;Tracking;SLAM;AR;3D reconstruction;Visual quality analysis (VQA)},
  doi={10.1109/ISMAR.2017.17},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115400,
  author={Li, Peiliang and Qin, Tong and Hu, Botao and Zhu, Fengyuan and Shen, Shaojie},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Monocular Visual-Inertial State Estimation for Mobile Augmented Reality}, 
  year={2017},
  volume={},
  number={},
  pages={11-21},
  abstract={Mobile phones equipped with a monocular camera and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications, but the lack of direct metric distance measurement and the existence of aggressive motions pose significant challenges on the localization of the AR device. In this work, we propose a tightly-coupled, optimization-based, monocular visual-inertial state estimation for robust camera localization in complex indoor and outdoor environments. Our approach does not require any artificial markers, and is able to recover the metric scale using the monocular camera setup. The whole system is capable of online initialization without relying on any assumptions about the environment. Our tightly-coupled formulation makes it naturally robust to aggressive motions. We develop a lightweight loop closure module that is tightly integrated with the state estimator to eliminate drift. The performance of our proposed method is demonstrated via comparison against state-of-the-art visual-inertial state estimators on public datasets and real-time AR applications on mobile devices. We release our implementation on mobile devices as open source software1.},
  keywords={Cameras;Feature extraction;Mobile handsets;Measurement;State estimation;Visualization;Robustness},
  doi={10.1109/ISMAR.2017.18},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115401,
  author={Orlosky, Jason and Kim, Peter and Kiyokawa, Kiyoshi and Mashita, Tomohiro and Ratsamee, Photchara and Uranishi, Yuki and Takemura, Haruo},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={VisMerge: Light Adaptive Vision Augmentation via Spectral and Temporal Fusion of Non-visible Light}, 
  year={2017},
  volume={},
  number={},
  pages={22-31},
  abstract={Low light situations pose a significant challenge to individuals working in a variety of different fields such as firefighting, rescue, maintenance and medicine. Tools like flashlights and infrared (IR) cameras have been used to augment light in the past, but they must often be operated manually, provide a field of view that is decoupled from the operator's own view, and utilize color schemes that can occlude content from the original scene. To help address these issues, we present VisMerge, a framework that combines a thermal imaging head mounted display (HMD) and algorithms that temporally and spectrally merge video streams of different light bands into the same field of view. For temporal synchronization, we first develop a variant of the time warping algorithm used in virtual reality (VR), but redesign it to merge video see-through (VST) cameras with different latencies. Next, using computer vision and image compositing we develop five new algorithms designed to merge non-uniform video streams from a standard RGB camera and small form-factor infrared (IR) camera. We then implement six other existing fusion methods, and conduct a series of comparative experiments, including a system level analysis of the augmented reality (AR) time warping algorithm, a pilot experiment to test perceptual consistency across all eleven merging algorithms, and an in-depth experiment on performance testing the top algorithms in a VR (simulated AR) search task. Results showed that we can reduce temporal registration error due to inter-camera latency by an average of 87.04%, that the wavelet and inverse stipple algorithms were perceptually rated the highest, that noise modulation performed best, and that freedom of user movement is significantly increased with visualizations engaged.},
  keywords={Cameras;Streaming media;Calibration;Resists;Algorithm design and analysis;Image color analysis;Head;Vision augmentation;augmented reality;infrared;image fusion;timewarping},
  doi={10.1109/ISMAR.2017.19},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115402,
  author={Fond, Antoine and Berger, Marie-Odile and Simon, Gilles},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Facade Proposals for Urban Augmented Reality}, 
  year={2017},
  volume={},
  number={},
  pages={32-41},
  abstract={We introduce a novel object proposals method specific to building facades. We define new image cues that measure typical facade characteristics such as semantic, symmetry and repetitions. They are combined to generate a few facade candidates in urban environments fast. We show that our method outperforms state-of-the-art object proposals techniques for this task on the 1000 images of the Zurich Building Database. We demonstrate the interest of this procedure for augmented reality through facade recognition and camera pose initialization. In a very time-efficient pipeline we classify the candidates and match them to a facade references database using CNN-based descriptors. We prove that this approach is more robust to severe changes of viewpoint and occlusions than standard object recognition methods.},
  keywords={Proposals;Semantics;Image edge detection;Three-dimensional displays;Cameras;Windows},
  doi={10.1109/ISMAR.2017.20},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115403,
  author={Willi, Simon and Grundh√∂fer, Anselm},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Robust Geometric Self-Calibration of Generic Multi-Projector Camera Systems}, 
  year={2017},
  volume={},
  number={},
  pages={42-51},
  abstract={Calibration of multi-projector-camera systems (MPCS) is a cumbersome and time-consuming process. It is of great importance to have robust, fast and accurate calibration procedures at hand for a wide variety of practical applications. We propose a fully automated self-calibration method for arbitrarily complex MPCS. It enables reliable and accurate intrinsic and extrinsic calibration without any human parameter tuning. We evaluated the proposed methods using more than ten multi-projection datasets ranging from a toy castle set up consisting of three cameras and one projector up to a half dome display system with more than 30 devices. Comparisons to reference calibrations, which were generated using the standard checkerboard calibration approach [44], show the reliability of our proposed pipeline, while a ground truth evaluation also shows that the resulting reconstructed point cloud accurately matches the shape of the reference geometry. Besides being fully automatic without the necessity of parameter fine tuning, the proposed method also significantly reduces the installation time of MPCS compared to checkerboard-based methods and makes it more suitable for real-world applications.},
  keywords={Cameras;Calibration;Robustness;Surface treatment;Three-dimensional displays;Geometry;Projector-camera systems;Calibration and registration of sensing systems;Display hardware;including 3D;stereoscopic and multi-user Entertainment;broadcast},
  doi={10.1109/ISMAR.2017.21},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115404,
  author={Watanabe, Yoshihiro and Kato, Toshiyuki and ishikawa, Masatoshi},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Extended Dot Cluster Marker for High-speed 3D Tracking in Dynamic Projection Mapping}, 
  year={2017},
  volume={},
  number={},
  pages={52-61},
  abstract={The technique of Projection Mapping, which is useful for merging real-world geometry with an augmented appearance, is a promising core technology for augmented reality (AR). In recent years, dynamically changing environments, mainly a consequence of the growing demand for interactive user experiences, have contributed to a new style of AR applications. However, performance levels of current systems for realizing 3D effects, in terms of the tracking speed and projection ability, are insufficient to meet these demands. In this paper, we present a high-speed, occlusion-robust marker-based 3D tracking method achieved by only using a monocular monochrome image. The objective of our research is to develop an automatic marker design method for any 3D shape and an effective framework for stabilizing tracking at high throughput by extending the latest promising work based on a deformable dot cluster marker [46]. Furthermore, this tracking method was used in combination with a high-speed projector, both of which can achieve high throughput and low latency, on the order of milliseconds. This enabled us to realize a high-quality computational display capable of representing the material appearance of a dynamically moving target. The demonstration showed that the effect of a dynamically changing appearance with nearly imperceptible latency drastically enriches the sense of immersion in the recognition of augmented materials with the naked eye.},
  keywords={Three-dimensional displays;Target tracking;Solid modeling;Robustness;Shape;Heuristic algorithms;Augmented reality},
  doi={10.1109/ISMAR.2017.22},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115405,
  author={Hebborn, Anna Katharina and H√∂hner, Nils and M√ºller, Stefan},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Occlusion Matting: Realistic Occlusion Handling for Augmented Reality Applications}, 
  year={2017},
  volume={},
  number={},
  pages={62-71},
  abstract={Nowadays, visualizations in Augmented Reality have to be as realistic as possible with lowest possible computational cost. In this paper, we present a real-time solution to realize dynamic occlusions. Sometimes virtual objects are softly, partially or totally occluded by real objects. Incorrect and inaccurate occlusion handling breaks the illusion of co-existence between the real and virtual world on the one hand and can result in wrong depth perception on the other hand. Our approach formulates the occlusion problem as alpha matting problem. Instead of calculating the visibility for each pixel of the virtual objects we estimate a blending coefficient. This enables a seamless integration of virtual objects in the real world, even for fuzzy foreground objects (like hair). Our approach takes raw depth information of the real scene (e.g. obtained by a low cost depth sensor) to realize rough foreground background segmentation. The blending coefficient between transitions where depth values are typically noisy is estimated based on the color image. Experimental evaluations of several scenes demonstrate that our algorithm produces consistent and visually appealing occlusions between the real and virtual scene with low computational cost. Furthermore, we compare the results with related depth-based approaches and show that our algorithm overcomes previous limitations.},
  keywords={Image color analysis;Image edge detection;Solid modeling;Augmented reality;Real-time systems;Color;Noise measurement;Realistic Occlusion;Dynamic Occlusion Handling;Augmented Reality;Alpha Matting;Natural Image Matting},
  doi={10.1109/ISMAR.2017.23},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115406,
  author={Walton, David R. and Thomas, Diego and Steed, Anthony and Sugimoto, Akihiro},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Synthesis of Environment Maps for Mixed Reality}, 
  year={2017},
  volume={},
  number={},
  pages={72-81},
  abstract={When rendering virtual objects in a mixed reality application, it is helpful to have access to an environment map that captures the appearance of the scene from the perspective of the virtual object. It is straightforward to render virtual objects into such maps, but capturing and correctly rendering the real components of the scene into the map is much more challenging. This information is often recovered from physical light probes, such as reflective spheres or fisheye cameras, placed at the location of the virtual object in the scene. For many application areas, however, real light probes would be intrusive or impractical. Ideally, all of the information necessary to produce detailed environment maps could be captured using a single device. We introduce a method using an RGBD camera and a small fisheye camera, contained in a single unit, to create environment maps at any location in an indoor scene. The method combines the output from both cameras to correct for their limited field of view and the displacement from the virtual object, producing complete environment maps suitable for rendering the virtual content in real time. Our method improves on previous probeless approaches by its ability to recover high-frequency environment maps. We demonstrate how this can be used to render virtual objects which shadow, reflect and refract their environment convincingly.},
  keywords={Cameras;Probes;Lighting;Rendering (computer graphics);Real-time systems;Light sources;Virtual reality},
  doi={10.1109/ISMAR.2017.24},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115407,
  author={Mandl, David and Yi, Kwang Moo and Mohr, Peter and Roth, Peter M. and Fua, Pascal and Lepetit, Vincent and Schmalstieg, Dieter and Kalkofen, Denis},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Learning Lightprobes for Mixed Reality Illumination}, 
  year={2017},
  volume={},
  number={},
  pages={82-89},
  abstract={This paper presents the first photometric registration pipeline for Mixed Reality based on high quality illumination estimation using convolutional neural networks (CNNs). For easy adaptation and deployment of the system, we train the CNNs using purely synthetic images and apply them to real image data. To keep the pipeline accurate and efficient, we propose to fuse the light estimation results from multiple CNN instances and show an approach for caching estimates over time. For optimal performance, we furthermore explore multiple strategies for the CNN training. Experimental results show that the proposed method yields highly accurate estimates for photo-realistic augmentations.},
  keywords={Lighting;Cameras;Training;Three-dimensional displays;Image reconstruction;Virtual reality;Rendering (computer graphics)},
  doi={10.1109/ISMAR.2017.25},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115408,
  author={Regenbrecht, Holger and Meng, Katrin and Reepen, Arne and Beck, Stephan and Langlotz, Tobias},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mixed Voxel Reality: Presence and Embodiment in Low Fidelity, Visually Coherent, Mixed Reality Environments}, 
  year={2017},
  volume={},
  number={},
  pages={90-99},
  abstract={Mixed Reality aims at combining virtual reality with the user's surrounding real environment in a way that they form one, coherent reality. A coherent visual quality is of utmost importance, expressed in measures of e.g. resolution, framerate, and latency for both the real and the virtual domains. For years, researchers have focused on maximizing the quality of the virtual visualization mimicking the real world to get closer to visual coherence. This however, makes Mixed Reality systems overly complex and requires high computational power. In this paper, we propose a different approach by decreasing the realism of one or both visual realms, real and virtual, to achieve visual coherence. Our system coarsely voxelizes the real and virtual environments, objects, and people to provide a believable, coherent mixed voxel reality. In this paper we present the general idea, the current implementation and demonstrate the effectiveness of our approach by technical and empirical evaluations. Our mixed voxel reality system serves as a platform for low-cost presence research and studies on human perception and cognition, a host of diagnostic and therapeutic applications, and for a variety of Mixed Reality applications where users' embodiment is important. Our findings challenge some commonplace assumptions on more is better approaches in mixed reality research and practice-sometimes less can be more.},
  keywords={Virtual reality;Visualization;Cameras;Rendering (computer graphics);Hardware;Coherence;mixed reality;augmented reality;believability;presence;voxel grid},
  doi={10.1109/ISMAR.2017.26},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115409,
  author={Barba, Evan and Marroquin, Ramon Zamora},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Primer on Spatial Scale and Its Application to Mixed Reality}, 
  year={2017},
  volume={},
  number={},
  pages={100-110},
  abstract={As mixed reality grows in popularity, the concepts and language we use to describe it will need to evolve as well. Having such concepts will allow for better interdisciplinary collaboration in both the arts and sciences, help to inform the creation of new software tools that enable the further evolution of the field, and will enable mixed reality research to advance scientific understanding in other disciplines. We provide an explication of the concept of spatial scale, including its relevant history in the fields of psychology and geography, and demonstrate its relevance to mixed reality. Through two case studies we show that spatial scale can operate effectively as a system of classification and analysis for mixed reality, and identify two concepts-scale transitions and the scale/complexity tradeoff-as critical to using this concept in future discussions of mixed reality.},
  keywords={Virtual reality;Psychology;Cognition;Geography;Collaboration;Media;mixed reality;augmented reality;virtual reality;spatial scale;multiscale analysis;scale transitions},
  doi={10.1109/ISMAR.2017.27},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115410,
  author={Diaz, Catherine and Walker, Michael and Szafir, Danielle Albers and Szafir, Daniel},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Designing for Depth Perceptions in Augmented Reality}, 
  year={2017},
  volume={},
  number={},
  pages={111-122},
  abstract={Augmented reality technologies allow people to view and interact with virtual objects that appear alongside physical objects in the real world. For augmented reality applications to be effective, users must be able to accurately perceive the intended real world location of virtual objects. However, when creating augmented reality applications, developers are faced with a variety of design decisions that may affect user perceptions regarding the real world depth of virtual objects. In this paper, we conducted two experiments using a perceptual matching task to understand how shading, cast shadows, aerial perspective, texture, dimensionality (i.e., 2D vs. 3D shapes) and billboarding affected participant perceptions of virtual object depth relative to real world targets. The results of these studies quantify trade-offs across virtual object designs to inform the development of applications that take advantage of users' visual abilities to better blend the physical and virtual world.},
  keywords={Augmented reality;Visualization;Two dimensional displays;Virtual environments;Legged locomotion},
  doi={10.1109/ISMAR.2017.28},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115411,
  author={Chen, Long and Day, Thomas W and Tang, Wen and John, Nigel W},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Recent Developments and Future Challenges in Medical Mixed Reality}, 
  year={2017},
  volume={},
  number={},
  pages={123-135},
  abstract={Mixed Reality (MR) is of increasing interest within technology-driven modern medicine but is not yet used in everyday practice. This situation is changing rapidly, however, and this paper explores the emergence of MR technology and the importance of its utility within medical applications. A classification of medical MR has been obtained by applying an unbiased text mining method to a database of 1,403 relevant research papers published over the last two decades. The classification results reveal a taxonomy for the development of medical MR research during this period as well as suggesting future trends. We then use the classification to analyse the technology and applications developed in the last five years. Our objective is to aid researchers to focus on the areas where technology advancements in medical MR are most needed, as well as providing medical practitioners with a useful source of reference.},
  keywords={Virtual reality;Market research;Surgery;Mobile communication;Training;Databases},
  doi={10.1109/ISMAR.2017.29},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115412,
  author={Wiesner, Christian A. and Ruf, Mike and Sirim, Demet and Klinker, Gudrun},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={3D-FRC: Depiction of the future road course in the Head-Up-Display}, 
  year={2017},
  volume={},
  number={},
  pages={136-143},
  abstract={The introduction of Head-Up-Displays (HUDs) have opened up avenues for a whole range of novel AR applications. However, until these applications become available for the mass market a number of problems need to be tackled. For example, the field of view (FoV) of current HUDs is extremely limited, and real world tracking and 3D reconstruction are still not precise enough to show driving information embedded into wide areas of complex traffic environment. It is not possible to show true AR-visualizations in the display areas provided by the current FoVs. In this paper, we investigate how an AR-like visualization approach in current HUDs (with a limited FoV) can support drivers in foreseeing the future road course. This visualisation uses the already established concept of an electronic horizon. By complying with automotive standards, our application can be easily adapted for series production. With this visualisation we performed a user study, investigating the effect on drivers' gaze behaviour. For this reason the test subjects were equipped with an eye tracking system. The results showed a decrease in both, the number of gazes as well as total glance time on the head unit and the instrument cluster. We also investigated the test subjects' braking behaviour around sharp bends of the road which showed an overall improvement when the visualisation was enabled. Furthermore it showed an increase of the mean glance duration in the area of the HUD. Note that the eye tracking system is not capable of distinguishing between glances at the visualisation in the HUD and the users' glance at objects behind the visualisation - overlapping with the HUD. This would require tracking the test persons' depth of focus. The study showed that developers need to be concerned about not displaying excessively in the HUD, so as not to distract drivers. It furthermore showed that AR-like visualizations have the potential to decrease the time the driver is not looking at the road creating a safer driving experience.},
  keywords={Visualization;Roads;Automobiles;Instruments;Navigation;Gaze tracking},
  doi={10.1109/ISMAR.2017.30},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115413,
  author={Cidota, Marina A. and Bank, Paulina J. M. and Ouwehand, P. W. and Lukosch, Stephan G.},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Assessing Upper Extremity Motor Dysfunction Using an Augmented Reality Game}, 
  year={2017},
  volume={},
  number={},
  pages={144-154},
  abstract={Advances in technology offer new opportunities for a better understanding of how different disorders affect motor function. In this paper, we explore the potential of an augmented reality (AR) game implemented using free hand and body tracking to develop a uniform, cost-effective and objective methods for evaluation of upper extremity motor dysfunction in different patient groups. We conducted a study with 20 patients (10 Parkinson's Disease patients and 10 stroke patients) who performed hand/arm movement tasks in four different conditions in AR and one condition in real world. Despite usability issues mainly due to non-robust hand tracking, the patients were moderately engaged while playing the AR game. Our findings show that moving virtual objects was less targeted, took more time and was associated with larger trunk displacement and a lower variability of elbow angle and upper arm angle than moving real objects. No significant correlations were observed between characteristics of movements in AR and movements in the real world. Still, our findings suggest that the AR game may be suitable for assessing the hand and arm function of mildly affected patients if usability can be further improved.},
  keywords={Games;Diseases;Visualization;Tracking;Thumb;Augmented Reality Games;Engagement;Upper Extremity Motor Dysfunction;Assessment;Parkinson's Disease;Stroke patients},
  doi={10.1109/ISMAR.2017.31},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115414,
  author={Harborth, David and Pape, Sebastian},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Hype: Investigating Technology Acceptance Factors of Pok√©mon Go}, 
  year={2017},
  volume={},
  number={},
  pages={155-168},
  abstract={We investigate the technology acceptance factors of the AR smart-phone game Pok√©mon Go with a PLS-SEM approach based on the UTAUT2 model by Venkatesh et al. [1]. Therefore, we conducted an online study in Germany with 683 users of the game. Many other studies rely on the users' imagination of the application's functionality or laboratory environments. In contrast, we asked a relatively large user base already interacting in the natural environment with the application. Not surprisingly, the strongest predictor of behavioral intention to play Pok√©mon Go is hedonic motivation, i.e. fun and pleasure due to playing the game. Additionally, we find medium-sized effects of effort expectancy on behavioral intention, and of habit on behavioral intention and use behavior. These results imply that AR applications - besides needing to be easily integrable in the users' daily life - should be designed in an intuitive and easily understandable way. We contribute to the understanding of the phenomenon of Pok√©mon Go by investigating established acceptance factors that potentially fostered the massive adoption of the game.},
  keywords={Games;Mobile communication;Augmented reality;Information systems;Laboratories;Solid modeling;Mathematical model},
  doi={10.1109/ISMAR.2017.32},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115415,
  author={Bork, Felix and Barmaki, Roghayeh and Eck, Ulrich and Yu, Kevin and Sandor, Christian and Navab, Nassir},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Empirical Study of Non-Reversing Magic Mirrors for Augmented Reality Anatomy Learning}, 
  year={2017},
  volume={},
  number={},
  pages={169-176},
  abstract={Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)? In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains.},
  keywords={Mirrors;Education;Augmented reality;Medical diagnostic imaging;Games},
  doi={10.1109/ISMAR.2017.33},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115416,
  author={},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Author index}, 
  year={2017},
  volume={},
  number={},
  pages={177-177},
  abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
  keywords={},
  doi={10.1109/ISMAR.2017.34},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8115417,
  author={},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={[Publisher's information]}, 
  year={2017},
  volume={},
  number={},
  pages={178-178},
  abstract={Provides a listing of current committee members and society officers.},
  keywords={},
  doi={10.1109/ISMAR.2017.35},
  ISSN={},
  month={Oct},}
