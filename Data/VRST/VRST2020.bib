@inproceedings{10.1145/3385956.3418964,
author = {Tanaka, Yudai and Horie, Arata and Chen, Xiang 'Anthony'},
title = {DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418964},
doi = {10.1145/3385956.3418964},
abstract = {We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {1},
numpages = {10},
keywords = {Virtual Reality, Vibration, Mass Perception, Haptics},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418943,
author = {Steed, Anthony and Friston, Sebastian and Pawar, Vijay and Swapp, David},
title = {Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations of Grounded and Worn Devices},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418943},
doi = {10.1145/3385956.3418943},
abstract = {Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user’s location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {2},
numpages = {11},
keywords = {virtual reality, haptics, force feedback},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418953,
author = {Elsayed, Hesham and Barrera Machuca, Mayra Donaji and Schaarschmidt, Christian and Marky, Karola and M\"{u}ller, Florian and Riemann, Jan and Matviienko, Andrii and Schmitz, Martin and Weigel, Martin and M\"{u}hlh\"{a}user, Max},
title = {VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418953},
doi = {10.1145/3385956.3418953},
abstract = {Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen – a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1)&nbsp;pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2)&nbsp;vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1)&nbsp;pneumatic, (2)&nbsp;vibrotactile and (3)&nbsp;a combination of both with (4)&nbsp;snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {3},
numpages = {11},
keywords = {Virtual Reality, Vibrotactile Actuation, Sketching, Pneumatic Actuation, Haptics, 3D User Interfaces},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418973,
author = {Kocur, Martin and Graf, Sarah and Schwind, Valentin},
title = {The Impact of Missing Fingers in Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418973},
doi = {10.1145/3385956.3418973},
abstract = {Avatars in virtual reality (VR) can have body structures that differ from the physical self. Game designers, for example, often stylize virtual characters by reducing the number of fingers. Previous work found that the sensation of presence in VR depends on avatar realism and the number of limbs. However, it is currently unknown how the removal of individual fingers affects the VR experience, body perception, and how fingers are used instead. In a study with 24 participants, we investigate the effects of missing fingers and avatar realism on presence, phantom pain perception, and finger usage. Our results show that particularly missing index fingers decrease presence, show the highest phantom pain ratings, and significantly change hand interaction behavior. We found that relative usage of thumb and index fingers in contrast to middle, ring, and little finger usage was higher with abstract hands than with realistic ones – even when the fingers were missing. We assume that dominant fingers are firstly integrated into the own body schema when an avatar does not resemble one’s own appearance. We discuss cognitive mechanisms in experiencing virtual limb loss.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {4},
numpages = {5},
keywords = {virtual reality, presence, phantom pain, missing fingers, avatars},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418955,
author = {Rodil, Kasper and Maasz, Donovan and Winschiers-Theophilus, Heike},
title = {Moving Virtual Reality out of its Comfort Zone and Into the African Kalahari Desert Field: Experiences From Technological Co-Exploration With an Indigenous San Community in Namibia},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418955},
doi = {10.1145/3385956.3418955},
abstract = {Indigenous people (IP) living in remote areas, at the margins of mainstream society, are often the last ones to experience emerging technologies and even less to shape those experiences. It could be argued technology exposure and experience is necessary for IP to gain agency in making informed decisions on the rejection or appropriation of novel technologies. In this paper, VR is introduced to a remote San community within a broader community-based research collaboration considering political and ethical perspectives of technology inclusion. The intent was to familiarise the community with the technology through the development and playthrough of a game, to explore future opportunities for joint co-designs of VR applications, meanwhile gauging the barriers for how VR operates outside of its intended setting. The community members expressed their excitement about the experience and the desire to re-create traditional San games in VR. The paper reflects on the community experiences, the setup and use of VR in remote settings, and the choices made to facilitate the familiarization of emerging technology.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {5},
numpages = {10},
keywords = {Virtual Reality, User Experiences, San People, Namibia, Indigenous People, Indigenous Knowledge, Cultural Heritage},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418957,
author = {Zable, Alexander and Hollenberg, Lloyd and Velloso, Eduardo and Goncalves, Jorge},
title = {Investigating Immersive Virtual Reality as an Educational Tool for Quantum Computing},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418957},
doi = {10.1145/3385956.3418957},
abstract = {Quantum computing (QC) is an intrinsically complex yet exciting discipline with increasing practical relevance. A deep understanding of QC requires the integration of knowledge across numerous technical fields, such as physics, computing and mathematics. This work aims to investigate how immersive Virtual Reality (VR) compares to a desktop environment (‘web-applet’) as an educational tool to help teach individuals QC fundamentals. We developed two interactive learning tutorials, one utilising the ‘Bloch sphere’ visualisation to represent a single-qubit system, and the other exploring multi-qubit systems through the lens of ‘quantum entanglement’. We evaluate the effectiveness of each medium to teach QC fundamentals in a user study with 24 participants. We find that the Bloch sphere visualisation was well-suited to VR over a desktop environment. Our results also indicate that mathematics literacy is an important factor in facilitating greater learning with this effect being notably more pronounced when using VR. However, VR did not significantly improve learning in a multi-qubit context. Our work provides valuable insights which contribute to the emerging field of Quantum HCI (QHCI) and VR for education.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {6},
numpages = {11},
keywords = {Virtual Reality, Quantum Computing, Learning, Education},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418963,
author = {Gil, Hyunjae and Shin, Yonghwan and Son, Hyungki and Hwang, Inwook and Oakley, Ian and Kim, Jin Ryong},
title = {Characterizing In-Air Eyes-Free Typing Movements in VR},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418963},
doi = {10.1145/3385956.3418963},
abstract = {We empirically explore fundamental requirements for achieving VR in-air typing by observing the unconstrained eyes-free in-air typing of touch typists. We show that unconstrained typing movements differ substantively from previously observed constrained in-air typing movements and introduce a novel binary categorization of typing strategies: typists who use finger movements alone (FINGER) and those who combine finger movement with gross hand movement (HAND). We examine properties of finger kinematics, correlated movement of fingers, interrelation in consecutive key-strokes, and 3D distribution of key-stroke movements. We report that, compared to constrained typing, unconstrained typing generates shorter (49 mm) and faster (764 mm/s) key-strokes with a high correlation of finger movement and that the HAND strategy group exhibits more dynamic key-strokes. We discuss how these findings can inform the design of future in-air typing systems.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {7},
numpages = {10},
keywords = {Typing in VR, Text Entry, In-Air Typing},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418947,
author = {Pirker, Johanna and Dengel, Andreas and Holly, Michael and Safikhani, Saeed},
title = {Virtual Reality in Computer Science Education: A Systematic Review},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418947},
doi = {10.1145/3385956.3418947},
abstract = {Virtual reality (VR) technologies have become more affordable and accessible in recent years. This is opening up new methods and opportunities in the field of digital learning. VR can offer new forms of interactive learning and working, especially for subjects from the STEM (Science, technology, engineering, and mathematics) area. In this context we investigate the potential and application of VR for computer science education with a systematic review in this paper. We present a formal literature review on the use of VR technologies in computer science education. We focus on the identification of factors such as learning objectives, technologies used, interaction characteristics, and challenges and advantages of using fully immersive VR for computer science education.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {8},
numpages = {8},
keywords = {Virtual Reality, VR, Literature Review, Computer Science Education},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418949,
author = {Zhang, Yiran and Ladeveze, Nicolas and Nguyen, Huyen and Fleury, Cedric and Bourdot, Patrick},
title = {Virtual Navigation considering User Workspace: Automatic and Manual Positioning before Teleportation},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418949},
doi = {10.1145/3385956.3418949},
abstract = {Teleportation is a navigation technique widely used in virtual reality applications using head-mounted displays. Basic teleportation usually moves a user’s viewpoint to a new destination of the virtual environment without taking into account the physical space surrounding them. However, considering the user’s real workspace is crucial for preventing them from reaching its limits and thus managing direct access to multiple virtual objects. In this paper, we propose to display a virtual representation of the user’s real workspace before the teleportation, and compare manual and automatic techniques for positioning such a virtual workspace. For manual positioning, the user adjusts the position and orientation of their future virtual workspace. A first controlled experiment compared exocentric and egocentric manipulation techniques with different virtual workspace representations, including or not an avatar at the user’s future destination. Although exocentric and egocentric techniques result in a similar level of performance, representations with an avatar help the user to understand better how they will land after teleportation. For automatic positioning, the user selects their future virtual workspace among relevant options generated at runtime. A second controlled experiment shows that the manual technique selected from the first experiment and the automatic technique are more efficient than the basic teleportation. Besides, the manual technique seems to be more suitable for crowded scenes than the automatic one.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {9},
numpages = {11},
keywords = {virtual workspace, virtual object access, teleportation, spatial awareness., real workspace, Locomotion},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418966,
author = {Thomas, Jerald and Hutton Pospick, Courtney and Suma Rosenberg, Evan},
title = {Towards Physically Interactive Virtual Environments: Reactive Alignment with Redirected Walking},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418966},
doi = {10.1145/3385956.3418966},
abstract = {Interactions with the physical environment, such as passive haptic feedback, have been previously shown to provide richer and more immersive virtual reality experiences. A strict correspondence between the virtual and real world coordinate systems is a staple requirement for physical interaction. However, many of the commonly employed VR locomotion techniques allow for, or even require, this relationship to change as the experience progresses. The outcome is that experience designers frequently have to choose between flexible locomotion or physical interactivity, as the two are often mutually exclusive. To address this limitation, this paper introduces reactive environmental alignment, a novel framework that leverages redirected walking techniques to achieve a desired configuration of the virtual and real world coordinate systems. This approach can transition the system from a misaligned state to an aligned state, thereby enabling the user to interact with physical proxy objects or passive haptic surfaces. Simulation-based experiments demonstrate the effectiveness of reactive alignment and provide insight into the mechanics and potential applications of the proposed algorithm. In the future, reactive environmental alignment can enhance the interactivity of virtual reality systems and inform new research vectors that combine redirected walking and passive haptics.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {10},
numpages = {10},
keywords = {virtual reality, redirected walking, locomotion, alignment},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418961,
author = {Lee, Jong-In and Asente, Paul and Kim, Byungmoon and Kim, Yeojin and Stuerzlinger, Wolfgang},
title = {Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418961},
doi = {10.1145/3385956.3418961},
abstract = {Virtual environments with a wide range of scales are becoming commonplace in Virtual Reality applications. Methods to control locomotion parameters can help users explore such environments more easily. For multi-scale virtual environments, point-and-teleport locomotion with a well-designed distance control method can enable mid-air teleportation, which makes it competitive to flying interfaces. Yet, automatic distance control for point-and-teleport has not been studied in such environments. We present a new method to automatically control the distance for point-and-teleport. In our first user study, we used a solar system environment to compare three methods: automatic distance control for point-and-teleport, manual distance control for point-and-teleport, and automatic speed control for flying. Results showed that automatic control significantly reduces overshoot compared with manual control for point-and-teleport, but the discontinuous nature of teleportation made users prefer flying with automatic speed control. We conducted a second study to compare automatic-speed-controlled flying and two versions of our teleportation method with automatic distance control, one incorporating optical flow cues. We found that point-and-teleport with optical flow cues and automatic distance control was more accurate than flying with automatic speed control, and both were equally preferred to point-and-teleport without the cues.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {11},
numpages = {10},
keywords = {multiscale virtual environments, VR navigation, Point-and-teleport, Automatic control},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418959,
author = {Lee, Juyoung and Lee, Myungho and Kim, Gerard Jounghyun and Hwang, Jae-In},
title = {Effects of Synchronized Leg Motion in Walk-in-Place Utilizing Deep Neural Networks for Enhanced Body Ownership and Sense of Presence in VR},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418959},
doi = {10.1145/3385956.3418959},
abstract = {We investigate the effects of different ways of visualizing the virtual gait of the avatar in the context of Walk-in-Place (WIP) based navigation in a virtual environment (VE). In Study&nbsp;1, participants navigated through a VE using the WIP method while inhabiting an avatar. We varied the visualization of the avatar’s leg motion while performing the WIP gesture: (1) Fixed Body: the legs stood still; (2) Pre-recorded Animation: the legs moved in a fixed predetermined pace (plausible but not in accordance to that of the user in general); (3) Synchronized Motion the legs moved according (synchronized) to those of the user. Our results indicated that the sense of presence and body ownership improved significantly when the leg motion was rendered synchronized to that of the user (Synchronized Motion). In addition, we developed a deep neural network (DNN) that predicted the users’ leg postures only with the head position tracking, eliminating the need for any external sensors. We carried out Study&nbsp;2, to assess the effects of different gait visualizations, under two new factors: (1) virtual gait seen directly by the user looking down, or already visible by one’s shadow (i.e., no need to look down); and (2) playing a pre-recorded animation, or pre-recorded animation whose playback speed was adjusted to match with pace of the users’ actual leg motions as predicted by the DNN. The results of Study&nbsp;2 showed that the virtual gait temporally synchronized with that of the user greatly improved the sense of body ownership, whether it was witnessed directly or indirectly with the shadow. However, the effect of virtual gait on presence was less marked when indirectly observed. We discuss our findings and the implications for representing the avatar locomotion in immersive virtual environments.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {12},
numpages = {10},
keywords = {walk-in-place, presence, machine learning, deep neural network, body ownership illusion},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418956,
author = {Schott, Ephraim and Kulik, Alexander and Froehlich, Bernd},
title = {Virtual Projection Planes for the Visual Comparison of Photogrammetric 3D Reconstructions with Photo Footage},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418956},
doi = {10.1145/3385956.3418956},
abstract = {Image-based 3D reconstructions and their visualization in virtual reality promise novel opportunities to explore and analyze 3D reconstructions of real objects, buildings and places. However, the faithfulness of the presented data is not always obvious and, in most cases, a 3D reconstruction cannot be compared directly to its corresponding real world instance. However, in case of reconstruction methods based on structure from motion (SFM), a large number of raw photos is available. This motivated us to develop a novel interaction technique for the visual comparison of details of 3D models with projections of the corresponding image sections, e.g. in order to rapidly verify the authenticity of perceived features. The results of a formal user study (n=18) demonstrate the general usability of such visual provenance information as well as benefits of the comparison in vicinity of the features in question over a separate image gallery. Further observations informed our iterative design process and led to the development of an improved interactive visualization. Our final implementation provides a spatial and content-related overview while retaining the efficiency of the original approach.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {13},
numpages = {9},
keywords = {visual comparison, virtual reality, spatially registered images, magic lenses, interaction design, image browsing},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418946,
author = {Pohl, Henning and Dalsgaard, Tor-Salve and Krasniqi, Vesa and Hornb\ae{}k, Kasper},
title = {Body LayARs: A Toolkit for Body-Based Augmented Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418946},
doi = {10.1145/3385956.3418946},
abstract = {Technological advances are enabling a new class of augmented reality (AR) applications that use bodies as substrates for input and output. In contrast to sensing and augmenting objects, body-based AR applications track people around the user and layer information on them. However, prototyping such applications is complex, time-consuming, and cumbersome, due to a lack of easily accessible tooling and infrastructure. We present Body LayARs, a toolkit for fast development of body-based AR prototypes. Instead of directly programming for a device, Body LayARs provides an extensible graphical programming environment with a device-independent runtime abstraction. We focus on face-based experiences for headset AR, and show how Body LayARs makes a range of body-based AR applications fast and easy to prototype.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {14},
numpages = {11},
keywords = {toolkit, body-based augmentation, Augmented reality},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418951,
author = {Kim, Byeol and Nguyen, Phong Danh and Nar, Pratham and Liu, Xiaolong and Loke, Yue-Hin and Mass, Paige and Hibino, Narutoshi and Olivieri, Laura and Krieger, Axel},
title = {CorFix: Virtual Reality Cardiac Surgical Planning System for Designing Patient Specific Vascular Grafts},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418951},
doi = {10.1145/3385956.3418951},
abstract = {Patients with single ventricle heart defect undergo Fontan surgery to reroute the blood flow from the lower body to the lung by connecting the inferior vena cava to the pulmonary artery using a vascular graft. Since each patient has an unique anatomical structure and blood flow dynamics, the graft design is a critical factor for maximizing the long-term survival rate of Fontan patients. Currently, designing and evaluating grafts involve computer aided design (CAD) and computational fluid dynamics (CFD) skills. CAD incorporates numerous tools for design but lacks depth perception, surgical features, and design parameters for creating vascular grafts while visualizing and modifying patient anatomies. These limitations may lead to long lead times, inconsistent workflow, and surgically infeasible graft designs. In this paper, we introduce a novel virtual reality vascular graft modeling software - CorFix, that provides solutions to these challenges. CorFix includes several visualization features for performing diagnostics and surgical features with design guidelines for creating patient specific tube-shaped grafts in 3D. The designed vascular graft can be exported into a 3D model, which can be utilized for performing computational fluid dynamic analysis and 3D printing. The patient specific vascular graft designs in CorFix were compared to an engineering CAD software, SolidWorks (Dassault Syst\`{e}mes, V\'{e}lizy-Villacoublay, France), by 8 participants. Through all participants had only received one time 10-minute tutorial on CorFix, CorFix had a higher success rate and 3.4 times faster performance in designing surgically feasible grafts than CAD. CorFix also scored higher in usability and lower in perceived workload than CAD. CorFix may be the tool that can enable medical doctors without 3D modeling background to design patient specific grafts.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {15},
numpages = {5},
keywords = {Virtual Reality, Usability Study, Prototyping/Implementation, Applications, 3D Modeling},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418960,
author = {Cloete, Richard and Norval, Chris and Singh, Jatinder},
title = {A Call for Auditable Virtual, Augmented and Mixed Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418960},
doi = {10.1145/3385956.3418960},
abstract = {XR (Virtual, Augmented and Mixed Reality) technologies are growing in prominence. However, they are increasingly being used in sectors and in situations that can result in harms. As such, this paper argues the need for auditability to become a key consideration of XR systems. Auditability entails capturing information of a system’s operation to enable oversight, inspection or investigation. Things can and will go wrong, and information that helps unpack situations of failure or harm, and that enables accountability and recourse, will be crucial to XR’s adoption and acceptance. In drawing attention to the urgent need for auditability, we illustrate some risks associated with XR technology and their audit implications, and present some initial findings from a survey with developers indicating the current ‘haphazard’ approach towards such concerns. We also highlight some challenges and considerations of XR audit in practice, as well as areas of future work for taking this important area of research forward.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {16},
numpages = {6},
keywords = {transparency, reviewability, responsibility, audit, accountability},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418950,
author = {Nguyen, Anh and Rothacher, Yannick and Efthymiou, Evdokia and Lenggenhager, Bigna and Brugger, Peter and Imbach, Lukas and Kunz, Andreas},
title = {Effect of Cognitive Load on Curvature Redirected Walking Thresholds},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418950},
doi = {10.1145/3385956.3418950},
abstract = {To allow users to perform real walking in a virtual environment larger than the physical space, redirected walking (RDW) techniques could be employed. Users do not notice this manipulation and immersion remains intact when RDW is applied within certain thresholds. Although many studies on RDW detection thresholds exists, in none of these studies, users were performing an additional task during the threshold identification process. These existing thresholds could be only conservative estimates and the potential of RDW may not be fully utilized. In this paper, we present an experiment to investigate the effect of cognitive load on curvature RDW thresholds. The cognitive load was imposed using a dual task of serial seven subtraction. Results showed that gender and cognitive load have significant effects on curvature RDW thresholds. More specifically, men are on average more sensitive to RDW than women, and being engaged in a dual task increases users’ RDW thresholds.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {17},
numpages = {5},
keywords = {threshold identification, cognitive load, Redirected walking},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418945,
author = {Huang, Jiawei and Klippel, Alexander},
title = {The Effects of Visual Realism on Spatial Memory and Exploration Patterns in Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418945},
doi = {10.1145/3385956.3418945},
abstract = {Understanding the effects of environmental features such as visual realism on spatial memory can inform a human-centered design of virtual environments. This paper investigates the effects of visual realism on object location memory in virtual reality, taking account of individual differences, gaze, and locomotion. Participants freely explored two environments which varied in visual realism, and then recalled the locations of objects by returning the misplaced objects back to original locations. Overall, we did not find a significant relationship between visual realism and object location memory. We found, however, that individual differences such as spatial ability and gender accounted for more variance than visual realism. Gaze and locomotion analysis suggest that participants exhibited longer gaze duration and more clustered movement patterns in the low realism condition. Preliminary inspection further found that locomotion hotspots coincided with objects that showed a significant gaze time difference between high and low visual realism levels. These results suggest that high visual realism still provides positive spatial learning affordances but the effects are more intricate.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {18},
numpages = {11},
keywords = {Visual realism, Visual fidelity, Virtual reality, Spatial memory, Spatial ability},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418971,
author = {Batmaz, Anil Ufuk and Sun, Xintian and Taskiran, Dogu and Stuerzlinger, Wolfgang},
title = {Eye-Hand Coordination Training for Sports with Mid-air VR},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418971},
doi = {10.1145/3385956.3418971},
abstract = {A relatively recent application area for Virtual Reality (VR) systems is sports training and user performance assessment. One of these applications is eye-hand coordination training systems (EHCTSs). Previous research identified that VR-based training systems have great potential for EHCTSs. While previous work investigated 3D targets on a 2D plane, here we aim to study full 3D movements and extend the application of throughput analysis to EHCTSs. We conducted two user studies to investigate how user performance is affected by different target arrangements, feedback conditions, and handedness in VR-based EHCTSs. In the first study, we explored handedness as well as vertical and horizontal target arrangements, and showed that user performance increases with the dominant hand and a vertical target plane. In the second study, we investigated different combinations of visual and haptic feedback and how they affect user performance with different target and cursor sizes. Results illustrate that haptic feedback did not increase user performance when it is added to visual feedback. Our results inform the creation of better EHCTSs with mid-air VR systems.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {19},
numpages = {10},
keywords = {reaction test, performance assessment, mid-air interaction, haptic feedback, Virtual Reality, Fitts’ task},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418962,
author = {Pfeuffer, Ken and Mecke, Lukas and Delgado Rodriguez, Sarah and Hassib, Mariam and Maier, Hannah and Alt, Florian},
title = {Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418962},
doi = {10.1145/3385956.3418962},
abstract = {Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {20},
numpages = {11},
keywords = {Virtual Reality, Pointing, Menu, Manual input, Gaze, Design},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418965,
author = {M\"{u}hlhausen, Moritz and Kappel, Moritz and Kassubeck, Marc and Bittner, Paul M. and Castillo, Susana and Magnor, Marcus},
title = {Temporal Consistent Motion Parallax for Omnidirectional Stereo Panorama Video},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418965},
doi = {10.1145/3385956.3418965},
abstract = {We present a new pipeline to enable head-motion parallax in omnidirectional stereo (ODS) panorama video rendering using a neural depth decoder. While recent ODS panorama cameras record short-baseline horizontal stereo parallax to offer the impression of binocular depth, they do not support the necessary translational degrees-of-freedom (DoF) to also provide for head-motion parallax in virtual reality (VR) applications. To overcome this limitation, we propose a pipeline that enhances the classical ODS panorama format with 6 DoF free-viewpoint rendering by decomposing the scene into a multi-layer mesh representation. Given a spherical stereo panorama video, we use the horizontal disparity to store explicit depth information for both eyes in a simple neural decoder architecture. While this approach produces reasonable results for individual frames, video rendering usually suffers from temporal depth inconsistencies. Thus, we perform successive optimization to improve temporal consistency by fine-tuning our depth decoder for both temporal and spatial smoothness. Using a consumer-grade ODS camera, we evaluate our approach on a number of real-world scene recordings and demonstrate the versatility and robustness of the proposed pipeline.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {21},
numpages = {9},
keywords = {virtual reality, rendering, neural network},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418952,
author = {Chen, Chih-Fan and Suma Rosenberg, Evan},
title = {Capture to Rendering Pipeline for Generating Dynamically Relightable Virtual Objects with Handheld RGB-D Cameras},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418952},
doi = {10.1145/3385956.3418952},
abstract = {We present a complete end-to-end pipeline for generating dynamically relightable virtual objects captured using a single handheld consumer-grade RGB-D camera. The proposed system plausibly replicates the geometry, texture, illumination, and surface reflectance properties of non-Lambertian objects, making them suitable for integration within virtual reality scenes that contain arbitrary illumination. First, the geometry of the target object is reconstructed from depth images captured using a handheld camera. To get nearly drift-free texture maps of the virtual object, a set of selected images from the original color stream is used for camera pose optimization. Our approach further separates these images into diffuse (view-independent) and specular (view-dependent) components using low-rank decomposition. The lighting conditions during capture and reflectance properties of the virtual object are subsequently estimated from the computed specular maps. By combining these parameters with the diffuse texture, the reconstructed model can then be rendered in real-time virtual reality scenes that plausibly replicate real world illumination at the point of capture. Furthermore, these objects can interact with arbitrary virtual lights that vary in direction, intensity, and color.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {22},
numpages = {11},
keywords = {virtual reality, scanning, reconstruction, content creation},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418972,
author = {Wagener, Nadine and Stamer, Mareike and Sch\"{o}ning, Johannes and T\"{u}mler, Johannes},
title = {Investigating Effects and User Preferences of Extra- and Intradiegetic Virtual Reality Questionnaires},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418972},
doi = {10.1145/3385956.3418972},
abstract = {Virtual realities (VR) are becoming an integral part of product development across many industries, for example to assess aesthetics and usability of new features in the automotive industry. The recording of the evaluation is typically conducted by filling out questionnaires after the study participants left the virtual environment. In this paper, we investigate how questionnaires can be best embedded within the virtual environment and compare how VR-questionnaires differ from classical post-test evaluations regarding preference, presence, and questionnaire completion time. In the first study (N = 11), experts rated four design concepts of questionnaires embedded in VR, of which two were designed as extradiegetic and two as intradiegetic user interfaces. We show that intradiegetic UIs have a significantly higher perceived user experience and presence while the usability remains similar. Intradiegetic UIs are preferred by the majority. Based on these findings, we compared intradiegetic VR-questionnaires with paper-based evaluations in a follow up study (N = 24). 67\% of the participants preferred the evaluation in VR, even though it takes significantly longer. We found no effect on presence.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {23},
numpages = {11},
keywords = {Virtual Reality (VR), User Interface (UI), Presence, INVR- questionnaires, Evaluation},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418942,
author = {Min, Seulki and Moon, Jung-geun and Cho, Chul-Hyun and Kim, Gerard J.},
title = {Effects of Immersive Virtual Reality Content Type to Mindfulness and Physiological Parameters},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418942},
doi = {10.1145/3385956.3418942},
abstract = {Virtual reality (VR) has been applied as a complimentary way to conventional treatment for mental disorders successfully. On the other hand, it has not been clearly shown what type of immersive media such as VR can directly affect one’s physiological parameters, associated with the state of mindfulness. We sought to assess how being subjected to differently designed VR contents can affect and modulate one’s anxiety both psychologically and more importantly physiologically. We empirically tested the comparative effects of two polarizing VR content types to this effect: (1) “calm/soothing” content and (2) “disturbing”. Twenty-five adults participated and their mental state, anxiety level and physiological signals were measured before and after experiencing the respective VR content type. The experiment found a statistically significant effect of the content type to the changes in these measures and confirmed that the “calm” content was helpful for one to self-regulate to lower heart rate and blood pressure, stable GSR, and the “disturbing” content in the opposite way. We applied this result to calm down and stabilize vital signs of patients during actual coronary angiography and catheterization operations. We were able to observe the same effect with positive comments from the patients and operating team.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {24},
numpages = {9},
keywords = {Virtual Reality, Nervous System, Heart Rate, Haptic feedback, Galvanic Skin Response (Skin Conductance), Blood Pressure},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418967,
author = {Maloney, Divine and Zamanifard, Samaneh and Freeman, Guo},
title = {Anonymity vs. Familiarity: Self-Disclosure and Privacy in Social Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418967},
doi = {10.1145/3385956.3418967},
abstract = {Understanding how and why users reveal information about their self in online social spaces and what they perceive as privacy online is a central research agenda in HCI. Drawing on 30 in-depth interviews, in this paper we focus on what type of information users disclose, to whom they reveal information, and concerns they had regarding self-disclosure in social Virtual Reality (VR) - where multiple users can interact with one another through VR head-mounted displays in 3D virtual spaces. Our findings show that overall, users felt comfortable to disclose their emotions, personal experience, and personal information in social VR. However, they also acknowledged that disclosing personal information in social VR was an inevitable trade-off: giving up bio-metric information in order to better use the system. We contribute to existing literature on self-disclosure and privacy online by focusing on social VR as an emerging novel online social space. We also explicate implications for designing and developing future social VR applications.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {25},
numpages = {9},
keywords = {social virtual reality, self-disclosure, online social interaction, digital privacy},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418968,
author = {McGill, Mark and Gugenheimer, Jan and Freeman, Euan},
title = {A Quest for Co-Located Mixed Reality: Aligning and Assessing SLAM Tracking for Same-Space Multi-User Experiences},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418968},
doi = {10.1145/3385956.3418968},
abstract = {Current solutions for creating co-located Mixed Reality (MR) experiences typically rely on platform-specific synchronisation of spatial anchors or Simultaneous Localisation and Mapping (SLAM) data across clients, often coupled to cloud services. This introduces significant costs (in development and deployment), constraints (with interoperability across platforms often limited), and privacy concerns. For practitioners, support is needed for creating platform-agnostic co-located MR experiences. This paper explores the utility of aligned SLAM solutions by 1) surveying approaches toward aligning disparate device coordinate spaces, formalizing their theoretical accuracy and limitations; 2) providing skeleton implementations for audience-based, small-scale and large-scale co-location using said alignment approaches; and 3) detailing how we can assess the accuracy and safety of 6DoF/SLAM tracking solutions for any arbitrary device and dynamic environment without the need for an expensive ground truth optical tracking, by using trilateration and a $30 laser distance meter. Through this, we hope to further democratise the creation of cross-platform co-located MR experiences.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {26},
numpages = {10},
keywords = {VR, SLAM, Multi-User, Mixed Reality, Co-location, AR;},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418969,
author = {Kocur, Martin and Schauhuber, Philipp and Schwind, Valentin and Wolff, Christian and Henze, Niels},
title = {The Effects of Self- and External Perception of Avatars on Cognitive Task Performance in Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418969},
doi = {10.1145/3385956.3418969},
abstract = {Virtual reality (VR) allows embodying any possible avatar. Known as the Proteus effect, avatars can change users’ behavior and attitudes. Previous work found that embodying Albert Einstein can increase cognitive task performance. The behavioral confirmation paradigm, however, predicts that our behavior is also affected by others’ perception of us. Therefore, we investigated the cognitive performance in collaborative VR when self-perception and external perception of the own avatar differ. 32 male participants performed a Tower of London task in pairs. One participant embodied Einstein or a young adult while the other perceived the participant as Einstein or a young adult. We show that the perception by others affects cognitive performance. The Einstein avatar also decreased the perceived workload. Results imply that avatars’ appearance to both, the user and the others must be considered when designing for cognitively demanding tasks.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {27},
numpages = {11},
keywords = {virtual reality, cognitive performance, body ownership, avatar embodiment, Proteus effect},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418941,
author = {Schwind, Valentin and Halbhuber, David and Fehle, Jakob and Sasse, Jonathan and Pfaffelhuber, Andreas and T\"{o}gel, Christoph and Dietz, Julian and Henze, Niels},
title = {The Effects of Full-Body Avatar Movement Predictions in Virtual Reality using Neural Networks},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418941},
doi = {10.1145/3385956.3418941},
abstract = {Motion tracking technologies and avatars in virtual reality (VR) showing the movements of the own body enable high levels of presence and a strong illusion of body ownership (IBO) – key features of immersive systems and gaming experiences in virtual environments. Previous work suggests using software-based algorithms that can not only compensate system latency but also predict future movements of the user to increase input performance. However, the effects of movement prediction in VR on input performance are largely unknown. In this paper, we investigate neural network-based predictions of full-body avatar movements in two scenarios: In the first study, we used a standardized 2D Fitts’ Law task to examine the information throughput in VR. In the second study, we utilized a full-body VR game to determine the users’ performance. We found that both performance and subjective measures in a standardized 2D Fitts’ law task could not benefit from the predicted avatar movements. In an immersive gaming scenario, however, the perceived accuracy of the own body location improved. Presence and body assessments remained more stable and were higher than during the Fitts’ task. We conclude that machine-learning-based predictions could be used to compensate system-related latency but participants only subjectively benefit under certain conditions.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {11},
keywords = {neural networks., movement prediction, avatars, Virtual reality},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418940,
author = {Wenninger, Stephan and Achenbach, Jascha and Bartl, Andrea and Latoschik, Marc Erich and Botsch, Mario},
title = {Realistic Virtual Humans from Smartphone Videos},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418940},
doi = {10.1145/3385956.3418940},
abstract = {This paper introduces an automated 3D-reconstruction method for generating high-quality virtual humans from monocular smartphone cameras. The input of our approach are two video clips, one capturing the whole body and the other providing detailed close-ups of head and face. Optical flow analysis and sharpness estimation select individual frames, from which two dense point clouds for the body and head are computed using multi-view reconstruction. Automatically detected landmarks guide the fitting of a virtual human body template to these point clouds, thereby reconstructing the geometry. A graph-cut stitching approach reconstructs a detailed texture. Our results are compared to existing low-cost monocular approaches as well as to expensive multi-camera scan rigs. We achieve visually convincing reconstructions that are almost on par with complex camera rigs while surpassing similar low-cost approaches. The generated high-quality avatars are ready to be processed, animated, and rendered by standard XR simulation and game engines such as Unreal or Unity.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {29},
numpages = {11},
keywords = {Virtual Reality, Avatars, 3D Reconstruction},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418944,
author = {Sykownik, Philipp and Masuch, Maic},
title = {The Experience of Social Touch in Multi-User Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418944},
doi = {10.1145/3385956.3418944},
abstract = {We present user study results on virtual body contact experience in a two-user VR scenario, in which participants performed different touches with a research assistant. The interaction evoked different emotional reactions in perceived relaxation, happiness, desire, anxiety, disgust, and fear. Congruent to physical social touch, the evaluation of virtual body contact was modulated by intimacy, touch direction, and sex. Further, individual comfort with interpersonal touch was positively associated with perceived relaxation and happiness. We discuss the results regarding implications for follow-up studies and infer implications for the use of social touch in social VR applications.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {30},
numpages = {11},
keywords = {virtual reality, social touch, social VR, multi-user VR, mediated social touch},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418974,
author = {Lampen, Eva and Lehwald, Jannes and Pfeiffer, Thies},
title = {Virtual Humans in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418974},
doi = {10.1145/3385956.3418974},
abstract = {Embedding virtual humans in educational settings enables the transfer of the approved concepts of learning by observation and imitation of experts to extended reality scenarios. Whilst various presentation concepts of virtual humans for learning have been investigated in sports and rehabilitation, little is known regarding industrial use cases. In prior work on manual assembly, Lampen et al.&nbsp;[21] show that three-dimensional (3D) registered virtual humans can provide assistance as effective as state-of-the-art HMD-based AR approaches. We extend this work by conducting a comparative user study (N=30) to verify implementation costs of assistive behavior features and 3D registration. The results reveal that the basic concept of a 3D registered virtual human is limited and comparable to a two-dimensional screen aligned presentation. However, by incorporating additional assistive behaviors, the 3D assistance concept is enhanced and shows significant advantages in terms of cognitive savings and reduced errors. Thus, it can be concluded, that this presentation concept is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {5},
keywords = {Virtual Human, Expert-Based Learning, Augmented Reality},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418948,
author = {Kim, Hanseob and Kim, TaeHyung and Lee, Myungho and Kim, Gerard Jounghyun and Hwang, Jae-In},
title = {Don’t Bother Me: How to Handle Content-Irrelevant Objects in Handheld Augmented Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418948},
doi = {10.1145/3385956.3418948},
abstract = {In this paper, we applied the concept of diminished reality to remove content-irrelevant pedestrian (i.e., real object) in the context of handheld augmented reality (AR). We prepared three view conditions: in Transparent (TP) condition, we removed the pedestrian entirely; in Semi-transparent (STP) condition, the pedestrian became semi-transparent; lastly, in Default (DF) condition, the pedestrian appeared as is. We conducted a user study to compare the effects of the three conditions on users’ engagement and perception of a virtual pet in the AR content. Our findings revealed that users felt less distracted to the AR content in TP and STP conditions, compared to the DF condition. Furthermore, users felt the virtual pet as more life-like, its behavior more plausible, and felt a higher spatial presence in the real environment, in the TP condition.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {32},
numpages = {5},
keywords = {Perceptual issue, Online user study, Diminished reality},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418954,
author = {Armstrong, Mark and Tsuchiya, Keitaro and Liang, Feng and Kunze, Kai and Pai, Yun Suen},
title = {Multiplex Vision: Understanding Information Transfer and F-Formation With Extended 2-Way FOV},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418954},
doi = {10.1145/3385956.3418954},
abstract = {Research in sociology shows that effective conversation relates to people’s spatial and orientational relationship, namely the proxemics (distance, eye contact, synchrony) and the F-formation (orientation and arrangement). In this work, we introduce novel conversational paradigms that effects conventional F-formation by introducing the concept of multi-directional conversation. Multiplex Vision is a head-mounted device capable of providing a 360° field-of-view (FOV) and facilitating multi-user interaction multi-directionally, thereby providing novel methods on how people can interact with each other. We propose 3 possible new forms of interactions from our prototype: one-to-one, one-to-many, and many-to-many. To facilitate them, we manipulate 2 key variables, which are the viewing parameter and the display parameter. To gather feedback for our system, we conducted a study to understand information transfer between various modes, as well as a user study on how different proposed paradigms effect conversation. Finally, we discuss present and future use cases that can benefit from our system.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {33},
numpages = {9},
keywords = {vision augmentation, conversation, F-formation, 360 field-of-view},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418958,
author = {Creel, Benjamin and Rinz-Jones, Caitlin J. and Jones, Adam and Jackson, Colin},
title = {Bacterial Load of Virtual Reality Headsets},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418958},
doi = {10.1145/3385956.3418958},
abstract = {As commodity virtual reality (VR) systems become more common, they are rapidly gaining popularity for entertainment, education, and training purposes. VR utilizes headsets which come in contact with or close proximity to the user’s eyes, nose, and forehead. In this study, the potential for these headsets to become contaminated with bacteria was analyzed. To the best of our knowledge, this study is the first to address the potential for microorganisms to be transmitted via VR headsets. The data discussed herein were collected roughly one year prior to the outbreak of the COVID-19 pandemic in the United States. We feel it is important to be clear that this study focuses exclusively on bacteria, as opposed to viruses like those responsible for the present pandemic. The nosepieces and foreheads of two HTC Vive headsets were sampled over the course of a seven-week period in a VR software development course. Serial dilutions were performed, and samples were plated on various culture media. Following incubation, counts of bacteria were determined. DNA was extracted from bacterial colonies and the 16S rRNA gene was sequenced to identify bacterial contaminates present on the headsets. Chief among these contaminates was Staphylococcus aureus. The results of these tests indicated that the Staphylococcus aureus strains isolated from the headsets possessed high levels of antibiotic resistance. Other notable bacterial isolates included Moraxella osloensis, the bacteria responsible for foul odors in laundry and, Micrococcus luteus, a communalistic bacterial species capable of causing opportunistic infections. Other bacterial isolates were detected in variable amounts throughout the trial.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {34},
numpages = {8},
keywords = {virtual reality, sanitation, pathogen, hygine, bacteria},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3385956.3418970,
author = {Ibrahim, Muhammad Twaha and Meenakshisundaram, Gopi and Majumder, Aditi},
title = {Dynamic Projection Mapping of Deformable Stretchable Materials},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418970},
doi = {10.1145/3385956.3418970},
abstract = {We present a method for dynamic projection mapping on deformable, stretchable and elastic materials (e.g. cloth) using a time of flight (ToF) depth camera (e.g. Azure Kinect or Pico-Flexx) that come equipped with an IR camera. We use Bezier surfaces to model the projection surface without explicitly modeling the deformation. We devise an efficient tracking method that tracks the boundary of the surface material using the IR-Depth camera. This achieves realistic mapping even in the interior of the surface, with simple markers (e.g. black dots or squares) or without markers entirely, such that the projection appears to be printed on the material. The surface representation is updated in real-time using GPU based computations. Further, we also show that the speed of these updates is limited by the camera frame rate and therefore can be adopted for higher speed cameras as well. This technique can be used to project on several stretchable moving materials to change their appearance.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {35},
numpages = {5},
keywords = {Spatially Augmented Reality, Dynamic Projection Mapping, Deformable Materials, Appearance Editing},
location = {Virtual Event, Canada},
series = {VRST '20}
}

