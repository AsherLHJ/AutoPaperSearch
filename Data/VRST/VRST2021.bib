@inproceedings{10.1145/3489849.3489859,
author = {Sugimori, Ken and Mitake, Hironori and Sato, Hirohito and Oguri, Kensho and Hasegawa, Shoichi},
title = {Avatar Tracking Control with Generations of Physically Natural Responses on Contact to Reduce Performers’ Loads},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489859},
doi = {10.1145/3489849.3489859},
abstract = {The real-time performance of motion-captured avatars in virtual space is becoming increasingly popular, especially within applications including social virtual realities (VRs), virtual performers (e.g., virtual YouTubers), and VR games. Such applications often include contact between multiple avatars or between avatars and objects as communication or gameplay. However, most current applications do not solve the effects of contact for avatars, causing penetration or unnatural behavior to occur. In reality, no contact with the player’s body occurs; nevertheless, the player must perform as if contact occurred. While physics simulation can solve the contact issue, the naive use of physics simulation causes tracking delay. We propose a novel avatar tracking controller with feedforward control. Our method enables quick, accurate tracking and flexible motion in response to contacts. Furthermore, the technique frees avatar performers from the loads of performing as if contact occurred. We implemented our method and experimentally evaluated the naturalness of the resulting motions and our approach’s effectiveness in reducing performers’ loads.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {1},
numpages = {5},
keywords = {avatar, character, motion capture, physics simulation},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489882,
author = {Yu, Peilin and Guo, Chi and Liu, yang and Zhang, Huyin},
title = {Fusing Semantic Segmentation and Object Detection for Visual SLAM in Dynamic Scenes},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489882},
doi = {10.1145/3489849.3489882},
abstract = {The assumption of static scenes limits the performance of traditional visual SLAM. Many existing solutions adopt deep learning methods or geometric constraints to solve the problem of dynamic scenes, but these schemes are either low efficiency or lack of robustness to a certain extent. In this paper, we propose a solution combining object detection and semantic segmentation to obtain the prior contours of potential dynamic objects. With this prior information, geometric constraints techniques are utilized to assist with removing dynamic feature points. Finally, the evaluation with the public datasets demonstrates that our proposed method can improve the accuracy of pose estimation and robustness of visual SLAM with no efficiency loss in high dynamic scenarios.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {2},
numpages = {7},
keywords = {objection detection, pose estimation, semantic segmentation, visual simultaneous and mapping},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489865,
author = {Misiak, Martin and Fuhrmann, Arnulph and Latoschik, Marc Erich},
title = {Impostor-based Rendering Acceleration for Virtual, Augmented, and Mixed Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489865},
doi = {10.1145/3489849.3489865},
abstract = {This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {3},
numpages = {10},
keywords = {image-based rendering, impostors, rendering acceleration},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489863,
author = {Gsaxner, Christina and Li, Jianning and Pepe, Antonio and Schmalstieg, Dieter and Egger, Jan},
title = {Inside-Out Instrument Tracking for Surgical Navigation in Augmented Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489863},
doi = {10.1145/3489849.3489863},
abstract = {Surgical navigation requires tracking of instruments with respect to the patient. Conventionally, tracking is done with stationary cameras, and the navigation information is displayed on a stationary display. In contrast, an augmented reality (AR) headset can superimpose surgical navigation information directly in the surgeon’s view. However, AR needs to track the headset, the instruments and the patient, often by relying on stationary infrastructure. We show that 6DOF tracking can be obtained without any stationary, external system by purely utilizing the on-board stereo cameras of a HoloLens 2 to track the same retro-reflective marker spheres used by current optical navigation systems. Our implementation is based on two tracking pipelines complementing each other, one using conventional stereo vision techniques, the other relying on a single-constraint-at-a-time extended Kalman filter. In a technical evaluation of our tracking approach, we show that clinically relevant accuracy of 1.70 mm/1.11°&nbsp;and real-time performance is achievable. We further describe an example application of our system for untethered end-to-end surgical navigation.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {4},
numpages = {11},
keywords = {Augmented Reality, HoloLens 2, Surgical Navigation, Tracking},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489867,
author = {Baloup, Marc and Pietrzak, Thomas and Hachet, Martin and Casiez, G\'{e}ry},
title = {Non-isomorphic Interaction Techniques for Controlling Avatar Facial Expressions in VR},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489867},
doi = {10.1145/3489849.3489867},
abstract = {The control of an avatar’s facial expressions in virtual reality is mainly based on the automated recognition and transposition of the user’s facial expressions. These isomorphic techniques are limited to what users can convey with their own face and have recognition issues. To overcome these limitations, non-isomorphic techniques rely on interaction techniques using input devices to control the avatar’s facial expressions. Such techniques need to be designed to quickly and easily select and control an expression, and not disrupt a main task such as talking. We present the design of a set of new non-isomorphic interaction techniques for controlling an avatar facial expression in VR using a standard VR controller. These techniques have been evaluated through two controlled experiments to help designing an interaction technique combining the strengths of each approach. This technique was evaluated in a final ecological study showing it can be used in contexts such as social applications.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {5},
numpages = {10},
keywords = {Avatar, Emoji, Emoticons, Emotion, Facial expression, VR},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489871,
author = {Friston, Sebastian J and Congdon, Ben J and Swapp, David and Izzouzi, Lisa and Brandst\"{a}tter, Klara and Archer, Daniel and Olkkonen, Otto and Thiel, Felix Johannes and Steed, Anthony},
title = {Ubiq: A System to Build Flexible Social Virtual Reality Experiences},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489871},
doi = {10.1145/3489849.3489871},
abstract = {While they have long been a subject of academic study, social virtual reality (SVR) systems are now attracting increasingly large audiences on current consumer virtual reality systems. The design space of SVR systems is very large, and relatively little is known about how these systems should be constructed in order to be usable and efficient. In this paper we present Ubiq, a toolkit that focuses on facilitating the construction of SVR systems. We argue for the design strategy of Ubiq and its scope. Ubiq is built on the Unity platform. It provides core functionality of many SVR systems such as connection management, voice, avatars, etc. However, its design remains easy to extend. We demonstrate examples built on Ubiq and how it has been successfully used in classroom teaching. Ubiq is open source (Apache License) and thus enables several use cases that commercial systems cannot.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {6},
numpages = {11},
keywords = {avatars, communication tools, networking, open source, social virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489877,
author = {Vu, Thanh Long and Le, Dinh Tung and Nguyen, Dac Dang Khoa and Sutjipto, Sheila and Paul, Gavin},
title = {Investigating the Effect of Sensor Data Visualization Variances in Virtual Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489877},
doi = {10.1145/3489849.3489877},
abstract = {This paper investigates the effect of real-time sensor data variances on humans performing straightforward assembly tasks in a Virtual Reality-based (VR-based) training system. A VR-based training system has been developed to transfer color and depth images, and constructs colored point clouds data to represent objects in real-time. Various parameters that affect sensor data acquisition and visualization of remotely operated robots in the real-world are varied. Afterward, the associated task performance is observed. Experimental results from 12 participants performed a total of 95 VR-guided puzzle assembly tasks demonstrated that a combination of low resolution and uncolored points has the most significant effect on participants’ performance. Participants mentioned that they needed to rely upon tactile feedback when the perceptual feedback was minimal. The most insignificant parameter determined was the resolution of the data representations, which, when varied within the experimental bounds, only resulted in a 5\% average change in completion time. Participants also indicated in surveys that they felt their performance had improved and frustration was reduced when provided with color information of the scene.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {7},
numpages = {5},
keywords = {Rendering, Usability Study, Virtual Reality, Visual Perception},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489868,
author = {Walsh, James A and Baumeister, James and Thomas, Bruce H},
title = {Spatial Augmented Reality Visibility and Line-of-Sight Cues for Building Design},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489868},
doi = {10.1145/3489849.3489868},
abstract = {Despite the technological advances in building design, visualizing 3D building layouts can be especially difficult for novice and expert users alike, who must take into account design constraints including line-of-sight and visibility. Using CADwalk, a commercial building design tool that utilizes floor-facing projectors to show 1:1 scale building plans, this work presents and evaluates two floor-based visual cues for assisting with evaluating line-of-sight and visibility. Additionally, we examine the impact of using virtual cameras looking from the inside-out (from user’s location to objects of interest) and outside-in (looking from an object of interest’s location back towards the user). Results show that floor-based cues led to participants more correctly rating visibility, despite taking longer to complete the task. This is an effective tradeoff, given the final outcome (the building design) where accuracy is paramount.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {8},
numpages = {5},
keywords = {CAD, XR, augmented reality, construction, design, spatial augmented reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489851,
author = {Gall, Alexander and Gr\"{o}ller, Eduard and Heinzl, Christoph},
title = {ImNDT: Immersive Workspace for the Analysis of Multidimensional Material Data From Non-Destructive Testing},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489851},
doi = {10.1145/3489849.3489851},
abstract = {An analysis of large multidimensional volumetric data as generated by non-destructive testing (NDT) techniques, e.g., X-ray computed tomography (XCT), can hardly be evaluated using standard 2D visualization techniques on desktop monitors. The analysis of fiber-reinforced polymers (FRPs) is currently a time-consuming and cognitively demanding task, as FRPs have a complex spatial structure, consisting of several hundred thousand fibers, each having more than twenty different extracted features. This paper presents ImNDT, a novel visualization system, which offers material experts an immersive exploration of multidimensional secondary data of FRPs. Our system is based on a virtual reality (VR) head-mounted device (HMD) to enable fluid and natural explorations through embodied navigation, the avoidance of menus, and manual mode switching. We developed immersive visualization and interaction methods tailored to the characterization of FRPs, such as a Model in Miniature, a similarity network, and a histo-book. An evaluation of our techniques with domain experts showed advantages in discovering structural patterns and similarities. Especially novices can strongly benefit from our intuitive representation and spatial rendering.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {9},
numpages = {11},
keywords = {Multidimensional data visualization, fibre-reinforced polymers, immersive analytics, interaction techniques, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489885,
author = {Wei\ss{}, Sebastian and Klassen, Nelly and Heuten, Wilko},
title = {Effects of Image Realism on the Stress Response in Virtual Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489885},
doi = {10.1145/3489849.3489885},
abstract = {Safety critical situations, as they occur in professions such as medicine, nursing, and aviation, are often trained in simulators to prevent damages to personnel and material. These jobs often come with a high amount of stress, to which prolonged exposure can have devastating effects. Over the past years, stress inoculation training in conjunction with Virtual Reality has become focus of the research community and software companies. Especially the nursing profession can benefit from it, since stress-related illnesses are often the reason for an early exit from the workforce. However, since training facilities often need to compromise on their simulations due to monetary reasons, trade-offs must be made in the degree of detail of such simulations in order to keep development and acquisition costs low. One such possibility is in using low graphical fidelity. We present a psycho-physiological study on the influence of image realism of virtual environments on the stress response. In a within subject design study, we ask participants to complete nursing related, virtually recreated tasks in an artificial intensive care unit, whilst exposed to different stress factors. We provide our findings in the form of objective and subjective measures. Results show that one can elicit different stress responses by manipulating image realism in a sufficiently drastic manner. However, a life-like reaction does not seem to depend on a highly realistic environment.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {10},
numpages = {10},
keywords = {image realism, nursing, stress, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489852,
author = {Kimmel, Simon and Cobus, Vanessa and Heuten, Wilko},
title = {opticARe - Augmented Reality Mobile Patient Monitoring in Intensive Care Units},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489852},
doi = {10.1145/3489849.3489852},
abstract = {German Intensive Care Units (ICUs) are in crisis, struggling with an increasing shortage of skilled workers, ultimately putting patients’ safety at risk. To counteract this process, researchers are increasingly concerned with finding digital solutions which aim to support healthcare professionals by enhancing the efficiency of reoccurring critical caring tasks and thus, improve working conditions. In this regard, this paper evaluates the application of Augmented Reality (AR) for patient monitoring for critical care nursing. Grounded on an observational study, semi-structured interviews, as well as a quantitative analysis, mobile patient monitoring scenarios, present particularly during patient transport, were identified as an innovative context of use of AR in the field. Additionally, user requirements such as high wearability, hands-free operability, and clear data representation could be derived from the obtained study results. For validation of these and identification of further requirements, three prototypes differing in their data illustration format were subsequently developed and quantitatively, as well as qualitatively evaluated by conducting an online survey. Thereby, it became evident that future implementations of a corresponding system for patient monitoring ought to integrate a context-dependent data presentation in particular, as this combines high navigability and availability of required data. Identifying patient monitoring during patient transport as a potential context of use, as well as distinguishing a context-dependent design approach as favorable constitute two key contributions of this work and provide a foundation on which future implementations of AR systems in the nursing domain and other related contexts can be established.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {11},
numpages = {11},
keywords = {augmented reality, critical care, head-mounted display, patient monitoring, wearable},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489884,
author = {Safikhani, Saeed and Holly, Michael and Kainz, Alexander and Pirker, Johanna},
title = {The Influence of in-VR Questionnaire Design on the User Experience},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489884},
doi = {10.1145/3489849.3489884},
abstract = {Researchers study the user experience in Virtual Reality (VR) typically by collecting either sensory data or using questionnaires. While traditional questionnaire formats present it through web-based survey tools (out-VR), recent studies investigate the effects of presenting questionnaires directly in the virtual environment (in-VR). The in-VR questionnaire can be defined as an implemented user-interface object that allows interaction with questionnaires in VR that do not break the immersion. Integrating questionnaires directly into the virtual environment, however, also challenges design decisions. While most previous research presents in-VR questionnaires in the form of 2D panels in the virtual environment, we want to investigate the difference from such traditional formats to a presentation of a questionnaire format in the form of an interactive object as part of the environment. Accordingly, we evaluate and compare two different in-VR questionnaire designs and a traditional web-based form (out-VR) to assess user experience, the effect on presence, duration of completing the questionnaires, and users’ preferences. As the means for achieving this goal, we developed an immersive questionnaire toolkit that provides a general solution for implementing in-VR questionnaires and exchanging data with popular survey services. This toolkit enables us to run our study both on-site and remotely. As a first small study, 16 users, either on-site or remotely, attended by completing the System Usability Scale, NASA TLX, and the iGroup Presence Questionnaire after a playful activity. The first results indicate that there is no significant difference in the case of usability and presence between different design layouts. Furthermore, we could not find a significant difference also for the task load except between 2D and web-based layout for mental demand and frustration as well as the duration of completing the questionnaire. The results also indicate that users generally prefer in-VR questionnaire designs to the traditional ones. The study can be expanded to include more participants in user studies as a means of gaining more concrete results. Furthermore, additional questionnaire design alternatives can also help to provide us with a more usable and accurate questionnaire design in VR.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {12},
numpages = {8},
keywords = {3D User Interface, Presence, Questionnaires, Virtual Reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489887,
author = {Li, Yaxuan and Yoo, Yongjae and Weill-Duflos, Antoine and Cooperstock, Jeremy},
title = {Towards Context-aware Automatic Haptic Effect Generation for Home Theatre Environments},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489887},
doi = {10.1145/3489849.3489887},
abstract = {The application of haptic technology in entertainment systems, such as Virtual Reality and 4D cinema, enables novel experiences for users and drives the demand for efficient haptic authoring systems. Here, we propose an automatic multimodal vibrotactile content creation pipeline that substantially improves the overall hapto-audiovisual (HAV) experience based on contextual audio and visual content from movies. Our algorithm is implemented on a low-cost system with nine actuators attached to a viewing chair and extracts significant features from video files to generate corresponding haptic stimuli. We implemented this pipeline and used the resulting system in a user study (n = 16), quantifying user experience according to the sense of immersion, preference, harmony, and discomfort. The results indicate that the haptic patterns generated by our algorithm complement the movie content and provide an immersive and enjoyable HAV user experience. This further suggests that the pipeline can facilitate the efficient creation of 4D effects and could therefore be applied to improve the viewing experience in home theatre environments.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {13},
numpages = {11},
keywords = {4D effect generation, Haptics, automatic haptic effect authoring, home theatre, immersive experience},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489888,
author = {Friedrich, Thibault and Prouzeau, Arnaud and McGuffin, Michael},
title = {The Effect of Increased Body Motion in Virtual Reality on a Placement-Retrieval Task},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489888},
doi = {10.1145/3489849.3489888},
abstract = {Previous work has shown that increased effort and use of one’s body can improve memory. When positioning windows inside a virtual reality, does the use of a larger volume, and using one’s legs to move around, improve ability to later find the windows? The results of our experiment indicate there can be a modest benefit for spatial memory and retrieval time, but at the cost of increased time spent initially positioning the windows.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {14},
numpages = {5},
keywords = {Virtual reality, controlled experiment, locomotion},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489874,
author = {Wang, Zhu and Arie, Liraz and Lubetzky, Anat V. and Perlin, Ken},
title = {VRGaitAnalytics: Visualizing Dual Task Cost for VR Gait Assessment},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489874},
doi = {10.1145/3489849.3489874},
abstract = {Among its many promising applications, Virtual Reality (VR) can simulate diverse real-life scenarios and therefore help experimenters assess individuals’ gait performance (i.e., walking) under controlled functional contexts. VR-based gait assessment may provide low-risk, reproducible and controlled virtual environments, enabling experimenters to investigate underlying causes for imbalance by manipulating experimental conditions such as multi-sensory loads, mental processing loads (cognitive load), and/or motor tasks. We present a low-cost novel VR gait assessment system that simulates virtual obstacles, visual, auditory, and cognitive loads while using motion tracking to assess participants’ walking performance. The system utilizes in-situ spatial visualization for trial playback and instantaneous outcome measures which enable experimenters and participants to observe and interpret their performance. The trial playback can visualize any moment in the trial with embodied graphic segments including the head, waist, and feet. It can also replay two trials at the same time frame for trial-to-trial comparison, which helps visualize the impact of different experimental conditions. The outcome measures, i.e., the metrics related to walking performance, are calculated in real-time and displayed as data graphs in VR. The system can help experimenters get specific gait information on balance performance beyond a typical clinical gait test, making it clinically relevant and potentially applicable to gait rehabilitation. We conducted a feasibility study with physical therapy students, research graduate students, and licensed physical therapists. They evaluated the system and provided feedback on the outcome measures, the spatial visualizations, and the potential use of the system in the clinic. The study results indicate that the system was feasible for gait assessment, and the immediate spatial visualization features were seen as clinically relevant and useful. Limitations and considerations for future work are discussed.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {15},
numpages = {10},
keywords = {Spatial visualization, gait balance, obstacle crossing, playback, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489864,
author = {Gomes de Siqueira, Alexandre and Yao, Heng and Bafna, Anokhi and Bloch-Elkouby, Sarah and Richards, Jenelle and Lloveras, Lauren B. and Feeney, Kathleen and Morris, Stephanie and Musser, Erica D. and Lok, Benjamin and Galynker, Igor},
title = {Investigating the Effects of Virtual Patients’ Nonsensical Responses on Users’ Facial Expressions in Mental Health Training Scenarios},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489864},
doi = {10.1145/3489849.3489864},
abstract = {This report investigates how clinician-participants react to virtual patients’ sensical vs. nonsensical responses in a training simulation that aims to help clinicians acquire empathetic skills toward high-risk patients with symptoms of the Suicide Crisis Syndrome (SCS). Two suicidal virtual patients were developed, and clinician-participants interactions with them were recorded. Their facial emotions were analysed in three key moments: after a baseline sensical response, after a nonsensical response, and after the following sensical response. We compared their basic facial emotions aggregated into Negative and Positive facial affective behaviors (FABs). We describe our study involving ten clinician-participants and the results of the facial expression analysis with Noldus FaceReader. Our results suggest that nonsensical responses from virtual humans have an overall impact in both Positive and Negative facial affective emotions, and may lead to an increased percent of time participants demonstrate Negative facial affective behaviors when interacting with virtual humans. We discuss several aspects regarding the impacts and importance of considering nonsensical responses in the context of virtual human-based interactions.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {16},
numpages = {10},
keywords = {empathy skills, nonsensical responses, sensical responses, suicide crises syndrome, virtual human, virtual patient},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489861,
author = {Elor, Aviv and Thang, Tiffany and Hughes, Benjamin Paul and Crosby, Alison and Phung, Amy and Gonzalez, Everardo and Katija, Kakani and Haddock, Steven H. D. and Martin, Eric J. and Erwin, Benjamin Eric and Takayama, Leila},
title = {Catching Jellies in Immersive Virtual Reality: A Comparative Teleoperation Study of ROVs in Underwater Capture Tasks},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489861},
doi = {10.1145/3489849.3489861},
abstract = {Remotely Operated Vehicles (ROVs) are essential to human-operated underwater expeditions in the deep sea. However, piloting an ROV to safely interact with live ecosystems is an expensive and cognitively demanding task, requiring extensive maneuvering and situational awareness. Immersive Virtual Reality (VR) Head-Mounted Displays (HMDs) could address some of these challenges. This paper investigates how VR HMDs influence operator performance through a novel telepresence system for piloting ROVs in real-time. We present an empirical user study [N=12] that examines common midwater creature capture tasks, comparing Stereoscopic-VR, Monoscopic-VR, and Desktop teleoperation conditions. Our findings indicate that Stereoscopic-VR can outperform Monoscopic-VR and Desktop ROV capture tasks, effectively doubling the efficacy of operators. We also found significant differences in presence, task load, usability, intrinsic motivation, and cybersickness. Our research points to new opportunities towards VR with ROVs.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {17},
numpages = {10},
keywords = {Cybersickness, Head-Mounted Display, Human-Operated Vehicles, Immersive Applications, Immersive Virtual Reality, Remotely Operated Vehicle, Teleoperation, Telepresence, Usability},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489883,
author = {Somin, Lior and McKendrick, Zachary and Finn, Patrick and Sharlin, Ehud},
title = {BreachMob: Detecting Vulnerabilities in Physical Environments Using Virtual Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489883},
doi = {10.1145/3489849.3489883},
abstract = {BreachMob is a virtual reality (VR) tool that applies open design principles from information security to physical buildings and structures. BreachMob uses a detailed 3D digital model of a property owner's building. The model is then published as a virtual environment (VE), complete with all applicable security measures and released to the public to test the building's security and find any potential vulnerabilities by completing specified objectives. Our paper contributes a new method of applying VR to crowd source detection of physical environment vulnerabilities. We detail the technical realization of two BreachMob prototypes (a home and an airport) reflecting on static and dynamic vulnerabilities. Our design critique suggests that&nbsp;BreachMob&nbsp;promotes user immersion by allowing participants the freedom to behave in ways that align with the experience of breaching physical security protocols.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {18},
numpages = {6},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489872,
author = {Chen, Mengyu and Peljhan, Marko and Sra, Misha},
title = {EntangleVR: A Visual Programming Interface for Virtual Reality Interactive Scene Generation},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489872},
doi = {10.1145/3489849.3489872},
abstract = {Entanglement is a unique phenomenon in quantum physics that describes a correlated relationship in the measurement of a group of spatially separated particles. In the fields of science fiction, game design, art and philosophy, it has inspired the creation of numerous innovative works. We present EntangleVR, a novel method to create entanglement-inspired virtual scenes with the goal to simplify representing this phenomenon in the design of interactive VR games and experiences. By providing a reactive visual programming interface, users can integrate entanglement into their design without requiring prior knowledge of quantum computing or quantum physics. Our system enables fast creation of complex scenes composed of virtual objects with manipulable correlated behaviors.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {19},
numpages = {6},
keywords = {3D scene creation, art, creativity, entanglement, quantum computing, virtual reality, visual programming},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489854,
author = {Lim, Beomsu and Han, Sangyoon and Choi, Seungmoon},
title = {Image-Based Texture Styling for Motion Effect Rendering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489854},
doi = {10.1145/3489849.3489854},
abstract = {A motion platform provides the vestibular stimuli that elicit the sensations of self-motion and thereby improves the immersiveness. A representative example is a 4D Ride, which presents a video of POV shots and motion effects synchronized with the camera motion in the video. Previous research efforts resulted in a few automatic motion effect synthesis algorithms for POV shots. Although effective in generating gross motion effects, they do not consider fine features on the ground, such as a rough or bumpy road. In this paper, we propose an algorithm for styling the gross motion effects using a texture image. Our algorithm transforms a texture image into a high-frequency style motion and merges it with the original motion while respecting both perceptual and device constraints. A user study demonstrated that texture styling could increase immersiveness, realism, and harmony.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {20},
numpages = {10},
keywords = {4D, automatic generation, image-based, motion effect, vestibular},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489893,
author = {Bimberg, Pauline and Weissker, Tim and Kulik, Alexander and Froehlich, Bernd},
title = {Virtual Rotations for Maneuvering in Immersive Virtual Environments},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489893},
doi = {10.1145/3489849.3489893},
abstract = {In virtual navigation, maneuvering around an object of interest is a common task which requires simultaneous changes in both rotation and translation. In this paper, we present Anchored Jumping, a teleportation technique for maneuvering that allows the explicit specification of a new viewing direction by selecting a point of interest as part of the target specification process. A first preliminary study showed that na\"{\i}ve Anchored Jumping can be improved by an automatic counter rotation that preserves the user’s relative orientation towards their point of interest. In our second, qualitative study, this extended technique was compared with two common approaches to specifying virtual rotations. Our results indicate that Anchored Jumping allows precise and comfortable maneuvering and is compatible with techniques that primarily support virtual exploration and search tasks. Equipped with a combination of such complementary techniques, seated users generally preferred virtual over physical rotations for indoor navigation.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {21},
numpages = {10},
keywords = {3D navigation, Jumping, Maneuvering, Target-based travel, Teleportation, Virtual reality, Virtual rotation},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489880,
author = {Liebers, Jonathan and Horn, Patrick and Burschik, Christian and Gruenefeld, Uwe and Schneegass, Stefan},
title = {Using Gaze Behavior and Head Orientation for Implicit Identification in Virtual Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489880},
doi = {10.1145/3489849.3489880},
abstract = {Identifying users of a Virtual Reality (VR) headset provides designers of VR content with the opportunity to adapt the user interface, set user-specific preferences, or adjust the level of difficulty either for games or training applications. While most identification methods currently rely on explicit input, implicit user identification is less disruptive and does not impact the immersion of the users. In this work, we introduce a biometric identification system that employs the user’s gaze behavior as a unique, individual characteristic. In particular, we focus on the user’s gaze behavior and head orientation while following a moving stimulus. We verify our approach in a user study. A hybrid post-hoc analysis results in an identification accuracy of up to 75&nbsp;\% for an explainable machine learning algorithm and up to 100&nbsp;\% for a deep learning approach. We conclude with discussing application scenarios in which our approach can be used to implicitly identify users.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {22},
numpages = {9},
keywords = {eye tracking, gaze-based authentication, implicit identification, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489879,
author = {Hilton, Clarice and Plant, Nicola and Gonz\'{a}lez D\'{\i}az, Carlos and Perry, Phoenix and Gibson, Ruth and Martelli, Bruno and Zbyszynski, Michael and Fiebrink, Rebecca and Gillies, Marco},
title = {InteractML: Making machine learning accessible for creative practitioners working with movement interaction in immersive media},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489879},
doi = {10.1145/3489849.3489879},
abstract = {Interactive Machine Learning offers a method for designing movement interaction that supports creators in implementing even complex movement designs in their immersive applications by simply performing them with their bodies. We introduce a new tool, InteractML, and an accompanying ideation method, which makes movement interaction design faster, adaptable and accessible to creators of varying experience and backgrounds, such as artists, dancers and independent game developers. The tool is specifically tailored to non-experts as creators configure and train machine learning models via a node-based graph and VR interface, requiring minimal programming. We aim to democratise machine learning for movement interaction to be used in the development of a range of creative and immersive applications.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {23},
numpages = {10},
keywords = {artists, creative virtual reality, dancers, machine learning, movement interaction},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489876,
author = {Krau\ss{}, Veronika and Jasche, Florian and Sa\ss{}mannshausen, Sheree May and Ludwig, Thomas and Boden, Alexander},
title = {Research and Practice Recommendations for Mixed Reality Design – Different Perspectives from the Community},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489876},
doi = {10.1145/3489849.3489876},
abstract = {Over the last decades, different kinds of design guides have been created to maintain consistency and usability in interactive system development. However, in the case of spatial applications, practitioners from research and industry either have difficulty finding them or perceive such guides as lacking relevance, practicability, and applicability. This paper presents the current state of scientific research and industry practice by investigating currently used design recommendations for mixed reality (MR) system development. We analyzed and compared 875 design recommendations for MR applications elicited from 89 scientific papers and documentation from six industry practitioners in a literature review. In doing so, we identified differences regarding four key topics: Focus on unique MR design challenges, abstraction regarding devices and ecosystems, level of detail and abstraction of content, and covered topics. Based on that, we contribute to the MR design research by providing three factors for perceived irrelevance and six main implications for design recommendations that are applicable in scientific and industry practice.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {24},
numpages = {13},
keywords = {Augmented Reality, Design Recommendations, Design Theory and Practice, Guidelines, Mixed Reality, User Interface Design},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489869,
author = {Akbas, Saliha and Kuscu, Kemal and Yantac, Asim Evren and Erdem, Gizem and Semsioglu, Sinem and Gurkan, Onur and G\"{u}nay, Asli and Vatansever, Ali and Eskenazi, Terry},
title = {Qualitative Dimensions of Technology-Mediated Reflective Learning: The Case of VR Experience of Psychosis},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489869},
doi = {10.1145/3489849.3489869},
abstract = {Self-reflection is evaluation of one’s inferential processes often triggered by complex social and emotional experiences, characterized by their ambiguity and unpredictability, pushing one to re-interpret the experience, and update existing knowledge. Using immersive Virtual Reality (VR), we aimed to support social and emotional learning (SEL) through reflection in psychology education. We used the case of psychosis as it involves ambiguous perceptual experiences. With a codesign workshop, we designed a VR prototype that simulates the perceptual, cognitive, affective, and social elements of psychotic experiences, followed by a user-study with psychology students to evaluate the potential of this technology to support reflection. Our analyses suggested that technology-mediated reflection in SEL involves two dimensions: spontaneous perspective-taking and shared state of affect. By exploring the subjective qualities of reflection with the said dimensions, our work contributes to the literature on technology-supported learning and VR developers designing for reflection.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {25},
numpages = {10},
keywords = {ambiguous design, experiential learning, reflection, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489886,
author = {Salemi Parizi, Farshid and Kienzle, Wolf and Whitmire, Eric and Gupta, Aakar and Benko, Hrvoje},
title = {RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489886},
doi = {10.1145/3489849.3489886},
abstract = {We introduce RotoWrist, an infrared (IR) light based solution for continuously and reliably tracking 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {26},
numpages = {11},
keywords = {hand tracking, time-of-flight sensor, virtual and augmented reality, wearable device, wrist pose, wristband.},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489878,
author = {Unlu, Arda Ege and Xiao, Robert},
title = {PAIR: Phone as an Augmented Immersive Reality Controller},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489878},
doi = {10.1145/3489849.3489878},
abstract = {Immersive head-mounted augmented reality allows users to overlay 3D digital content on a user’s view of the world. Current-generation devices primarily support interaction modalities such as gesture, gaze and voice, which are readily available to most users yet lack precision and tactility, rendering them fatiguing for extended interactions. We propose using smartphones, which are also readily available, as companion devices complementing existing AR interaction modalities. We leverage user familiarity with smartphone interactions, coupled with their support for precise, tactile touch input, to unlock a broad range of interaction techniques and applications - for instance, turning the phone into an interior design palette, touch-enabled catapult or AR-rendered sword. We describe a prototype implementation of our interaction techniques using an off-the-shelf AR headset and smartphone, demonstrate applications, and report on the results of a positional accuracy study.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {27},
numpages = {6},
keywords = {Augmented Reality, Input Techniques, Smartphone, Touch},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489889,
author = {Singhal, Yatharth and Wang, Haokun and Gil, Hyunjae and Kim, Jin Ryong},
title = {Mid-Air Thermo-Tactile Feedback using Ultrasound Haptic Display},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489889},
doi = {10.1145/3489849.3489889},
abstract = {This paper presents a mid-air thermo-tactile feedback system using an ultrasound haptic display. We design a proof-of-concept thermo-tactile feedback system with an open-top chamber, heat modules, and an ultrasound display. Our approach is to provide heated airflow along the path to the focused pressure point created from the ultrasound display to generate thermal and vibrotactile cues in mid-air simultaneously. We confirm that our system can generate the thermo-tactile stimuli up to 54.2°C with 3.43 mN when the ultrasonic haptic signal was set to 100 Hz with a 12 mm radius of the cue size. We also confirm that our system can provide a stable temperature (mean error=0.25\%). We measure the warm detection threshold (WDT) and the heat-pain detection threshold (HPDT). The results show that the mean WDT was 32.8°C (SD=1.12), and the mean HPDT was 44.6°C (SD=1.64), which are consistent with the contact-based thermal thresholds. We also found that the accuracy of haptic pattern identification is similar for non-thermal (98.1\%) and thermal conditions (97.2\%), showing a non-significant effect of high temperature. We finally confirmed that thermo-tactile feedback further enhances the user experiences.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {11},
keywords = {Mid-air haptic feedback, thermal feedback, thermo-tactile feedback},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489890,
author = {Bhardwaj, Ayush and Chae, Junghoon and Noeske, Richard Huynh and Kim, Jin Ryong},
title = {TangibleData: Interactive Data Visualization with Mid-Air Haptics},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489890},
doi = {10.1145/3489849.3489890},
abstract = {In this paper, we investigate the effects of mid-air haptics in interactive 3D data visualization. We build an interactive 3D data visualization tool that adapts hand gestures and mid-air haptics to provide tangible interaction in VR using ultrasound haptic feedback on 3D data visualization. We consider two types of 3D visualization datasets and provide different data encoding methods for haptic representations. Two user experiments are conducted to evaluate the effectiveness of our approach. The first experimental results show that adding a mid-air haptic modality can be beneficial regardless of noise conditions and useful for handling occlusion or discerning density and volume information. The second experiment results further show the strengths and weaknesses of direct touch and indirect touch modes. Our findings can shed light on designing and implementing a tangible interaction on 3D data visualization with mid-air haptic feedback.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {29},
numpages = {11},
keywords = {Data visualization, haptics, immersive analytics, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489857,
author = {Zhang, Bowen and Sra, Misha},
title = {PneuMod: A Modular Haptic Device with Localized Pressure and Thermal Feedback},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489857},
doi = {10.1145/3489849.3489857},
abstract = {Humans have tactile sensory organs distributed all over the body. However, haptic devices are often only created for one part (e.g., hands, wrist, or face). We propose PneuMod, a wearable modular haptic device that can simultaneously and independently present pressure and thermal (warm and cold) cues to different parts of the body. The module in PneuMod is a pneumatically-actuated silicone bubble with an integrated Peltier device that can render thermo-pneumatic feedback through shapes, locations, patterns, and motion effects. The modules can be arranged with varying resolutions on fabric to create sleeves, headbands, leg wraps, and other forms that can be worn on multiple parts of the body. In this paper, we describe the system design, the module implementation, and applications for social touch interactions and in-game thermal and pressure feedback.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {30},
numpages = {7},
keywords = {fabrication, haptic communication, modular device, multimodal haptics, pneumatic feedback, thermal feedback, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489856,
author = {Sueishi, Tomohiro and Ishikawa, Masatoshi},
title = {Ellipses Ring Marker for High-speed Finger Tracking},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489856},
doi = {10.1145/3489849.3489856},
abstract = {High-speed finger tracking is necessary for augmented reality and operation in human-machine cooperation without latency discomfort, but conventional markerless finger tracking methods are not fast enough and the marker-based methods have low wearability. In this paper, we propose an ellipses ring marker (ERM), a finger-ring marker consisting of multiple ellipses and its high-speed image recognition algorithm. The finger-ring shape has highly wearing continuity, and the surface shape is suitable for various viewing angle observation. The invariance of the ellipse in the perspective projection enables accurate and low-latency posture estimation. We have experimentally investigated the advantage in normal distribution, validated the sufficient accuracy and computational cost in the marker tracking, and showed a demonstration of dynamic projection mapping on a palm.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {5},
keywords = {dynamic projection mapping, hand tracking, high-speed image processing},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489881,
author = {Chiu, Pascal and Huang, Jiawei and Kitamura, Yoshifumi},
title = {Enabling Robot-assisted Motion Capture with Human Scale Tracking Optimization},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489881},
doi = {10.1145/3489849.3489881},
abstract = {Motion tracking systems with viewpoint concerns or whose marker data include unreliable states have proven difficult to use despite many impactful benefits. We propose a technique inspired by active vision and using a customized hill-climbing approach to control a robot-sensor setup and apply it to a magnetic induction system capable of occlusion-free motion tracking. Our solution reduces the impact of displacement and orientation issues for markers which inherently present a dead-angle range that disturbs usability and accuracy. The resulting interface is successful in stabilizing previously unexploitable data while preventing sub-optimal states for up to hundreds of occurrences per recording and featuring an approximate 40\% decrease in tracking error.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {32},
numpages = {6},
keywords = {VR, active sensing, motion tracking, robotic interface, viewpoint optimization},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489891,
author = {Shimomura, Yuki and Ban, Yuki and Warisawa, Shin'ichi},
title = {Presenting Sense of Loud Vocalization Using Vibratory Stimuli to the Larynx and Auditory Stimuli},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489891},
doi = {10.1145/3489849.3489891},
abstract = {In recent years, technologies related to virtual reality (VR) have continued to advance. As a method to enhance the VR experience, we focused on loud vocalization. This is because we believe that loud vocalization can enable us to engage with the VR environment in a more interactive way. Also, as loud vocalization is an action that is thought to be closely related to stress reduction and a sense of exhilaration, the stress reduction through VR with loud vocalization is also expected. But loud vocalization itself has disadvantages for physical, mental, and social reasons. Then, we hypothesized that loud vocalization itself is not necessary for such benefits; but the sense of loud vocalization plays an important role. Therefore, we focused on a method of substituting experience by presenting sensory stimuli. In this paper, we proposed a way to present the sense of loud vocalization through vibratory stimuli to the larynx and auditory stimuli to users who are actually vocalizing quietly with the expectation for the sense of loud vocalization. Our user study showed that the proposed method can extend the sense of vocalization and realize pseudo-loud vocalization. In addition, it was also shown that the proposed method can cause a sense of exhilaration. By contrast, excessively strong vibratory stimuli spoil the sense of loud vocalization, and thus the intensity of the vibration should be appropriately determined.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {33},
numpages = {10},
keywords = {cross modal, loud vocalization, sense of vocalization, vibratory stimuli},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489870,
author = {Lee, Jiwan and Park, Jaejun and Choi, Seungmoon},
title = {Absolute and Differential Thresholds of Motion Effects in Cardinal Directions},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489870},
doi = {10.1145/3489849.3489870},
abstract = {In this paper, we report both absolute and differential thresholds for motion in the six cardinal directions as comprehensively as possible. As with general 4D motion effects, we used sinusoidal motions with low intensity and large frequency as stimuli. Hence, we could also compare the effectiveness of motion types in delivering motion effects. We found that the thresholds for the z-axis (up-down) were higher than those for the x-axis (front-back) and y-axis (left-right) in both kinds of thresholds and that the type of motion significantly affected both thresholds. Further, between differential thresholds and reference intensities, we found a strong linear relationship for roll, yaw and, surge. Compared to them, a relatively weak linear relationship was observed for the rest of the motion types. Our results can be useful for generating motion effects for 4D contents while considering the human sensitivity to motion feedback.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {34},
numpages = {10},
keywords = {4D, Detection threshold, Discrimination threshold, Motion effect, Psychometric function, Psychophysics, Self-motion, Vestibular stimulation, Vestibular threshold},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489866,
author = {Hartfill, Judith and Gabel, Jenny and Kruse, Lucie and Schmidt, Susanne and Riebandt, Kevin and K\"{u}hn, Simone and Steinicke, Frank},
title = {Analysis of Detection Thresholds for Hand Redirection during Mid-Air Interactions in Virtual Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489866},
doi = {10.1145/3489849.3489866},
abstract = {Avatars in virtual reality (VR) with fully articulated hands enable users to naturally interact with the virtual environment (VE). Interactions are often performed in a one-to-one mapping between the movements of the user’s real body, for instance, the hands, and the displayed body of the avatar. However, VR also allows manipulating this mapping to introduce non-isomorphic techniques. In this context, research on manipulations of virtual hand movements typically focuses on increasing the user’s interaction space to improve the overall efficiency of hand-based interactions. In this paper, we investigate a hand retargeting method for decelerated hand movements. With this technique, users need to perform larger movements to reach for an object in the VE, which can be utilized, for example, in therapeutic applications. If these gain-based redirections of virtual hand movements are small enough, users become unable to reliably detect them due to the dominance of the visual sense. In a psychophysical experiment, we analyzed detection thresholds for six different motion paths in mid-air for both hands. We found significantly different detection thresholds between movement directions on each spatial axis. To verify our findings, we applied the identified gains in a playful application in a confirmatory study.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {35},
numpages = {10},
keywords = {avatar, detection thresholds, hand redirection},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489894,
author = {Gauthier, Baptiste and Albert, Louis and Martuzzi, Roberto and Herbelin, Bruno and Blanke, Olaf},
title = {Virtual Reality platform for functional magnetic resonance imaging in ecologically valid conditions},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489894},
doi = {10.1145/3489849.3489894},
abstract = {Functional magnetic resonance Brain Imaging (fMRI) is a key non-invasive imaging technique for the study of human brain activity. Its millimetric spatial resolution is at the cost of several constraints: participants must remain static and experience artificial stimuli, making it difficult to generalize neuroscientific results to naturalistic and ecological conditions. Immersive Virtual Reality (VR) provides alternatives to such stimuli through simulation, but still requires an active first-person exploration of the environment to evoke a strong sense of presence in the virtual environment. Here, we report how to compensate for the inability to freely move in VR by leveraging on principles of embodiment for a virtual avatar, to eventually evoke a strong sense of presence with a minimal motion of the participant. We validated the functionality of the platform in a study where healthy participants performed several basic research tasks in an MR-specific immersive virtual environment. Our results show that our approach can lead to high sense of presence, strong body ownership, and sense of agency for a virtual avatar, with low movement-related MRI artifacts. Moreover, to exemplify the versatility of the platform, we reproduced several behavioral and fMRI results in the perceptual, motor, and cognitive domains. We discuss how to leverage such technology for neuroscience research and provide recommendations on efficient ways to implement and develop it successfully.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {36},
numpages = {12},
keywords = {Ecological, Immersive Virtual Reality, Presence, fMRI},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489862,
author = {Wo\'{z}niak, Miko\l{}aj P. and Sikorski, Piotr and Wr\'{o}bel-Lachowska, Magdalena and Bart\l{}omiejczyk, Natalia and Dominiak, Julia and Grudzie\'{n}, Krzysztof and Romanowski, Andrzej},
title = {Enhancing In-game Immersion Using BCI-controlled Mechanics},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489862},
doi = {10.1145/3489849.3489862},
abstract = {Due to multimodal approach, the virtual reality experiences become increasingly more immersive and entertaining. New control modalities, such as brain-computer interfaces (BCIs), enable the players to engage in the game with both their bodies and minds. In our work, we investigate the influence of employing BCI-driven mechanics on player’s in-game immersion. We designed and implemented an escape room-themed game which employed player’s mental states of focus and relaxation as input for selected game mechanisms. Through a between-subject user study, we found that controlling the game with mental states enhances the in-game immersion and attracts the player’s engagement. At the same time, using BCIs did not impose additional cognitive workload. Our work contributes qualitative insights on psychocognitive effects of using BCIs in gaming and describing immersive gaming experiences.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {37},
numpages = {6},
keywords = {brain-computer interface, virtual reality games},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489860,
author = {Giunchi, Daniele and Bovo, Riccardo and Charalambous, Panayiotis and Liarokapis, Fotis and Shipman, Alastair and James, Stuart and Steed, Anthony and Heinis, Thomas},
title = {Perceived Realism of Pedestrian Crowds Trajectories in VR},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489860},
doi = {10.1145/3489849.3489860},
abstract = {Crowd simulation algorithms play an essential role in populating Virtual Reality (VR) environments with multiple autonomous humanoid agents. The generation of plausible trajectories can be a significant computational cost for real-time graphics engines, especially in untethered and mobile devices such as portable VR devices. Previous research explores the plausibility and realism of crowd simulations on desktop computers but fails to account the impact it has on immersion. This study explores how the realism of crowd trajectories affects the perceived immersion in VR. We do so by running a psychophysical experiment in which participants rate the realism of real/synthetic trajectories data, showing similar level of perceived realism.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {38},
numpages = {5},
keywords = {crowd simulation, perception, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489873,
author = {Lang, Florian and Machulla, Tonja},
title = {Pressing a Button You Cannot See: Evaluating Visual Designs to Assist Persons with Low Vision through Augmented Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489873},
doi = {10.1145/3489849.3489873},
abstract = {Partial vision loss occurs in several medical conditions and affects persons of all ages. It compromises many daily activities, such as reading, cutting vegetables, or identifying and accurately pressing buttons, e.g., on ticket machines or ATMs. Touchscreen interfaces pose a particular challenge because they lack haptic feedback from interface elements and often require people with impaired vision to rely on others for help. We propose a smartglasses-based solution to utilize the user’s residual vision. Together with visually-impaired individuals, we designed assistive augmentations for touchscreen interfaces and evaluated their suitability to guide attention towards interface elements and to increase the accuracy of manual inputs. We show that augmentations improve interaction performance and decrease cognitive load, particularly for unfamiliar interface layouts.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {39},
numpages = {10},
keywords = {Accessibility, Augmented Reality, Low Vision},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489855,
author = {Auda, Jonas and Verheyen, Nils and Mayer, Sven and Schneegass, Stefan},
title = {Flyables: Haptic Input Devices for Virtual Reality using Quadcopters},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489855},
doi = {10.1145/3489849.3489855},
abstract = {Virtual Reality (VR) has made its way into everyday life. While VR delivers an ever-increasing level of immersion, controls and their haptics are still limited. Current VR headsets come with dedicated controllers that are used to control every virtual interface element. However, the controller input mostly differs from the virtual interface. This reduces immersion. To provide a more realistic input, we present Flyables, a toolkit that provides matching haptics for virtual user interface elements using quadcopters. We took five common virtual UI elements and built their physical counterparts. We attached them to quadcopters to deliver on-demand haptic feedback. In a user study, we compared Flyables to controller-based VR input. While controllers still outperform Flyables in terms of precision and task completion time, we found that Flyables present a more natural and playful way to interact with VR environments. Based on the results from the study, we outline research challenges that could improve interaction with Flyables in the future.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {40},
numpages = {11},
keywords = {Drones, Flyables, Haptics, Quadcopter, Toolkit., Virtual Reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489858,
author = {Maldonado, Jaime and Zetzsche, Christoph},
title = {Object Manipulations in VR Show Task- and Object-Dependent Modulation of Motor Patterns},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489858},
doi = {10.1145/3489849.3489858},
abstract = {Humans can perform object manipulations in VR in spite of missing haptic and acoustic information. Whether their movements under these artificial conditions do still rely on motor programs based on natural experience or are impoverished due to the restrictions imposed by VR is unclear. We investigated whether reach-to-place and reach-to-grasp movements in VR can still be adapted to the task and to the specific properties of the objects being handled, or whether they reflect a stereotypic, task- and object-independent motor program. We analyzed reach-to-grasp and reach-to-place movements from participants performing an unconstrained ”set-the-table” task involving a variety of different objects in virtual reality. These actions were compared based on their kinematic features. We encountered significant differences in peak speed and the duration of the deceleration phase which are modulated depending on the action and on the manipulated object. The flexibility of natural human sensorimotor control thus is at least partially transferred and exploited in impoverished VR conditions. We discuss possible explanations of this behavior and the implications for the design of object manipulations in VR.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {41},
numpages = {9},
keywords = {Virtual reality, kinematics, motor control, motor skill, object manipulation phases, reach and place movements},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489853,
author = {Dalsgaard, Tor-Salve and Knibbe, Jarrod and Bergstr\"{o}m, Joanna},
title = {Modeling Pointing for 3D Target Selection in VR},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489853},
doi = {10.1145/3489849.3489853},
abstract = {Virtual reality (VR) allows users to interact similarly to how they do in the physical world, such as touching, moving, and pointing at objects. To select objects at a distance, most VR techniques rely on casting a ray through one or two points located on the user’s body (e.g., on the head and a finger), and placing a cursor on that ray. However, previous studies show that such rays do not help users achieve optimal pointing accuracy nor correspond to how they would naturally point. We seek to find features, which would best describe natural pointing at distant targets. We collect motion data from seven locations on the hand, arm, and body, while participants point at 27 targets across a virtual room. We evaluate the features of pointing and analyse sets of those for predicting pointing targets. Our analysis shows an 87\% classification accuracy between the 27 targets for the best feature set and a mean distance of 23.56&nbsp;cm in predicting pointing targets across the room. The feature sets can inform the design of more natural and effective VR pointing techniques for distant object selection.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {42},
numpages = {10},
keywords = {Virtual reality, pointing, target selection},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489875,
author = {Blaga, Andreea Dalia and Frutos-Pascual, Maite and Creed, Chris and Williams, Ian},
title = {Virtual Object Categorisation Methods: Towards a Richer Understanding of Object Grasping for Virtual Reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489875},
doi = {10.1145/3489849.3489875},
abstract = {Object categorisation methods have been historically used in literature for understanding and collecting real objects together into meaningful groups and can be used to define human interaction patterns (i. e grasping). When investigating grasping patterns for Virtual Reality (VR), researchers used Zingg’s methodology which categorises objects based on shape and form. However, this methodology is limited and does not take into consideration other object attributes that might influence grasping interaction in VR. To address this, our work presents a study into three categorisation methods for virtual objects. We employ Zingg’s object categorisation as a benchmark against existing real and virtual object interaction work and introduce two new categorisation methods that focus on virtual object equilibrium and virtual object component parts. We evaluate these categorisation methods using a dataset of 1872 grasps from a VR docking task on 16 virtual representations of real objects and report findings on grasp patterns. We report on findings for each virtual object categorisation method showing differences in terms of grasp classes, grasp type and aperture. We conclude by detailing recommendations and future ideas on how these categorisation methods can be taken forward to inform a richer understanding of grasping in VR.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {43},
numpages = {5},
keywords = {Grasping, Interaction, Object Categorisation, Virtual Reality},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3489849.3489892,
author = {Ratcliffe, Jack and Ballou, Nick and Tokarchuk, Laurissa},
title = {Actions, not gestures: contextualising embodied controller interactions in immersive virtual reality},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489892},
doi = {10.1145/3489849.3489892},
abstract = {Modern immersive virtual reality (IVR) often uses embodied controllers for interacting with virtual objects. However, it is not clear how we should conceptualise these interactions. They could be considered either gestures, as there is no interaction with a physical object; or as actions, given that there is object manipulation, even if it is virtual. This distinction is important, as literature has shown that in the physical world, action-enabled and gesture-enabled learning produce distinct cognitive outcomes. This study attempts to understand whether sensorimotor-embodied interactions with objects in IVR can cognitively be considered as actions or gestures. It does this by comparing verb-learning outcomes between two conditions: (1) where participants move the controllers without touching virtual objects (gesture condition); and (2) where participants move the controllers and manipulate virtual objects (action condition). We found that (1) users can have cognitively distinct outcomes in IVR based on whether the interactions are actions or gestures, with actions providing stronger memorisation outcomes; and (2) embodied controller actions in IVR behave more similarly to physical world actions in terms of verb memorization benefits.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {44},
numpages = {11},
keywords = {HCI, cognition, embodiment, immersive virtual reality, learning, sensorimotor, virtual reality},
location = {Osaka, Japan},
series = {VRST '21}
}

