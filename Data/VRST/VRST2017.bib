@inproceedings{10.1145/3139131.3139132,
author = {Lin, Chaolan and Faas, Travis and Dombrowski, Lynn and Brady, Erin},
title = {Beyond cute: exploring user types and design opportunities of virtual reality pet games},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139132},
doi = {10.1145/3139131.3139132},
abstract = {Virtual pet games, such as handheld games like Tamagotchi or video games like Petz, provide players with artificial pet companions or entertaining pet-raising simulations. Prior research has found that virtual pets have the potential to promote learning, collaboration, and empathy among users. While virtual reality (VR) has become an increasingly popular game medium, litle is known about users' expectations regarding game avatars, gameplay, and environments for VR-enabled pet games. We surveyed 780 respondents in an online survey and interviewed 30 participants to understand users' motivation, preferences, and game behavior in pet games played on various medium, and their expectations for VR pet games. Based on our findings, we generated three user types that reflect users' preferences and gameplay styles in VR pet games. We use these types to highlight key design opportunities and recommendations for VR pet games.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {1},
numpages = {10},
keywords = {virtual reality, virtual pet, user types, pet game},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139145,
author = {Rietzler, Michael and Geiselhart, Florian and Rukzio, Enrico},
title = {The matrix has you: realizing slow motion in full-body virtual reality},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139145},
doi = {10.1145/3139131.3139145},
abstract = {While we perceive time as a constant factor in the real world, it can be manipulated in media. Being quite easy for linear media, this is used for various aspects of storytelling e.g., by applying slow motion in movies or TV. Interactive media like VR however poses additional challenges, because user interaction speed is independent from media speed. While it is still possible to change the speed of the environment, for interaction it is also necessary to deal with the emerging speed mismatch, e.g., by slowing down visual feedback of user movements. In this paper, we explore the possibility of such manipulations of visual cues, with the goal of enabling the use of slow motion also in immersive interactive media like VR. We conducted a user study to investigate the impact of limiting angular velocity of a virtual character in first person view in VR. Our findings show that it is possible to use slow motion in VR while maintaining the same levels of presence, enjoyment and susceptibility to motion sickness, while users adjust to the maximum speed quickly. Moreover, our results also show an impact of slowing down user movements on their time estimations.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {2},
numpages = {10},
keywords = {virtual reality, time perception, slow motion, evaluation},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139161,
author = {Kono, Michinari and Miyaki, Takashi and Rekimoto, Jun},
title = {JackIn Airsoft: localization and view sharing for strategic sports},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139161},
doi = {10.1145/3139131.3139161},
abstract = {We present JackIn Airsoft, a system for generating maps for multi-player sport activities and sharing their first-person views (FPVs). In first-person shooting (FPS) games, maps are generated for the visualization of the player's location; these maps are used for strategic play and discussions. FPS games are a virtual experience of shooting or of military activities; the displaying of maps on a screen is a technique designed for game playing. We address the challenge of adapting this effective map visualization technique to real-world strategic sports. In this paper, we introduce our prototype map generator for airsoft sports that is based on the usage of wearable cameras and simultaneous localization and mapping (SLAM). The system enables users to switch between the FPV and the generated map in order to share a player's experience and a strategic overview for team sports. We applied ORB-SLAM2 to multiple recorded FPV videos to discuss the situations and conditions appropriate for our system.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {3},
numpages = {4},
keywords = {wearable camera, first-person view, augmented sports, airsoft, SLAM},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139166,
author = {Kj\ae{}r, Tina and Lillelund, Christoffer B. and Moth-Poulsen, Mie and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
title = {Can you cut it? an exploration of the effects of editing in cinematic virtual reality},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139166},
doi = {10.1145/3139131.3139166},
abstract = {The advent of affordable virtual reality (VR) displays and 360° video cameras has sparked an interest in bringing cinematic experiences from the screen and into VR. However, it remains uncertain whether traditional approaches to filmmaking can be directly applied to cinematic VR. Historically editing has provided filmmakers with a powerful tool for shaping stories and guiding the attention of audiences. However, will an immersed viewer, experiencing the story from inside the fictional world, find cuts disorienting? This paper details two studies exploring how cut frequency influences viewers' sense of disorientation and their ability to follow the story, during exposure to fictional 360° films experienced using a head-mounted display. The results revealed no effects of increased cut frequency which leads us to conclude that editing need not pose a problem in relation to cinematic VR, as long as the participants' attention is appropriately guided at the point of the cut.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {4},
numpages = {4},
keywords = {editing, cinematic virtual reality, 360 degree film},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139148,
author = {Uji, Takahiro and Zhang, Yiting and Oku, Hiromasa},
title = {Edible retroreflector},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139148},
doi = {10.1145/3139131.3139148},
abstract = {A retroreflector is a kind of optical device that can reflect incident light back to the light source. A retroreflector is often used as an optical marker for machine vision since it becomes very bright in the scene and is easy detect by image processing.Most conventional retroreflectors are made of glass or plastic. Thus, it is difficult to apply them to some fields such as foods or inspection of the inner wall of digestive organs, because there is the risk of accidental ingestion of the device, which may cause severe health problems. However, the visual recognition of foods seems to have many potential applications, such as projection mapping on foods. Also, the recognition of the inner wall of digestive organs is important for medical inspection. If it were possible to make an edible retroreflector, such a retroreflector would be expected to be suitable for these applications.Based on this idea, this paper proposes an edible retroreflector made from transparent foodstuffs. We found that kanten, or Japan agar, which is a traditional Japanese cooking ingredient used to form a transparent jelly, was suitable for forming such optical devices. A recipe for an edible retroreflector using kanten was developed. A prototype made from kanten showed a retroreflective function in reflectance measurement experiments. Dynamic projection mapping on a Swiss roll was also successfully demonstrated using the prototype as an optical marker.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {5},
numpages = {8},
keywords = {retroreflector, optical marker, food, edible},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139171,
author = {Mallaro, Sophia and Rahimian, Pooya and O'Neal, Elizabeth E. and Plumert, Jodie M. and Kearney, Joseph K.},
title = {A comparison of head-mounted displays vs. large-screen displays for an interactive pedestrian simulator},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139171},
doi = {10.1145/3139131.3139171},
abstract = {This investigation compared how people performed a complex perception-action task - crossing traffic-filled roadways - in a CAVE vs. an HMD virtual environment. Participants physically crossed a virtual roadway with continuous cross traffic in either a CAVE-like or an HTC Vive pedestrian simulator. The 3D model and traffic scenario were identical in both simulators, allowing for a direct comparison between the two display systems. We found that participants in the Vive group accepted smaller gaps for crossing than participants in the CAVE group. They also timed their entry into the gap more precisely and tended to cross somewhat more quickly. As a result, participants in the Vive group had a somewhat larger margin of safety when they exited the roadway than those in the CAVE group. The results provide a foundation for future studies of pedestrian behavior and other tasks involving full-body motion using HMD-based VR.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {6},
numpages = {4},
keywords = {virtual reality, pedestrian road crossing, HTC vive, CAVE},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139136,
author = {Savkin, Pavel A. and Saito, Shunsuke and Vansteenberge, Jarich and Fukusato, Tsukasa and Wilson, Lochlainn and Morishima, Shigeo},
title = {Outside-in monocular IR camera based HMD pose estimation via geometric optimization},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139136},
doi = {10.1145/3139131.3139136},
abstract = {Accurately tracking a Head Mounted Display (HMD) with a 6 degree of freedom is essential to achieve a comfortable and a nausea free experience in Virtual Reality. Existing commercial HMD systems using synchronized Infrared (IR) camera and blinking IR-LEDs can achieve highly accurate tracking. However, most of the off-the-shelf cameras do not support frame synchronization. In this paper, we propose a novel method for real time HMD pose estimation without using any camera synchronization or LED blinking. We extended over the state of the art pose estimation algorithm by introducing geometrically constrained optimization. In addition, we propose a novel system to increase robustness to the blurred IR-LEDs patterns appearing at high-velocity movements. The quantitative evaluations showed significant improvements in pose stability and accuracy over wide rotational movements as well as a decrease in runtime.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {7},
numpages = {9},
keywords = {vision-based pose estimation, position tracking, perspective-n-point problem, monocular IR camera},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139164,
author = {Nguyen, Minh and Tran, Huy and Le, Huy and Yan, Wei Qi},
title = {A tile based colour picture with hidden QR code for augmented reality and beyond},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139164},
doi = {10.1145/3139131.3139164},
abstract = {Most existing Augmented Reality (AR) applications use either template (picture) markers or bar-code markers to overlay computer-generated graphics on the real world surfaces. The use of template markers is computationally expensive and unreliable. On the other hand, bar-code markers display only black and white blocks; thus, they look uninteresting and uninformative. In this short paper, we describe a new way to optically hide a QR code inside a tile based colour picture. Each AR marker is built from hundreds of small tiles (just like tiling a bathroom), and the unique gaps between the tiles are used to determine the elements of the hidden QR Code. This novel type of AR marker presents not only a realistic-looking colour picture but also contains self-Correcting information (stored in QR code). In this article, we demonstrate that this tile based colour picture with hidden QR code is relatively robust under various conditions and scaling. We believe many nowadays' AR challenges could be solved with this type of marker. AR-enabled medias could then be easily generated. For instance, it would be capable of storing and displaying virtual figures of an entire book or magazine. Thus, it provides a promising AR approach to be used in many different AR applications; and beyond, it may even replace the barcodes and QR Codes in some cases.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {8},
numpages = {4},
keywords = {computer vision, augmented reality, QR code},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139141,
author = {Mottelson, Aske and Hornb\ae{}k, Kasper},
title = {Virtual reality studies outside the laboratory},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139141},
doi = {10.1145/3139131.3139141},
abstract = {Many user studies are now conducted outside laboratories to increase the number and heterogeneity of participants. These studies are conducted in diverse settings, with the potential to give research greater external validity and statistical power at a lower cost. The feasibility of conducting virtual reality (VR) studies outside laboratories remains unclear because these studies often use expensive equipment, depend critically on the physical context, and sometimes study delicate phenomena concerning body awareness and immersion. To investigate, we explore pointing, 3D tracing, and body-illusions both in-lab and out-of-lab. The in-lab study was carried out as a traditional experiment with state-of-the-art VR equipment; 31 completed the study in our laboratory. The out-of-lab study was conducted by distributing commodity cardboard VR glasses to participants; 57 completed the study anywhere they saw fit. The effects found in-lab were comparable to those found out-of-lab, with much larger variations in the settings in the out-of-lab condition. A follow-up study showed that performance metrics are mostly governed by the technology used, where more complex VR phenomena depend more critically on the internal control of the study. We argue that conducting VR studies outside the laboratory is feasible, and that certain types of VR studies may advantageously be run this way. From the results, we discuss the implications and limitations of running VR studies outside the laboratory.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {9},
numpages = {10},
keywords = {user studies, google cardboard, crowdsourcing, consumer VR},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139152,
author = {Moulec, Gwendal Le and Argelaguet, Ferran and Gouranton, Val\'{e}rie and Blouin, Arnaud and Arnaldi, Bruno},
title = {Agent: automatic generation of experimental protocol runtime},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139152},
doi = {10.1145/3139131.3139152},
abstract = {Due to the nature of Virtual Reality (VR) research, conducting experiments in order to validate the researcher's hypotheses is a must. However, the development of such experiments is a tedious and time-consuming task. In this work, we propose to make this task easier, more intuitive and faster with a method able to describe and generate the most tedious components of VR experiments. The main objective is to let experiment designers focus on their core tasks: designing, conducting, and reporting experiments. To that end, we propose the use of Domain-Specific Languages (DSLs) to ease the description and generation of VR experiments. An analysis of published VR experiments is used to identify the main properties that characterize VR experiments. This allowed us to design AGENT (Automatic Generation of ExperimeNtal proTocol runtime), a DSL for specifying and generating experimental protocol runtimes. We demonstrated the feasibility of our approach by using AGENT on two experiments published in the VRST'16 proceedings.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {10},
numpages = {10},
keywords = {reusability, domain-specific language, automatic generation of experiments},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139153,
author = {Walton, David R. and Steed, Anthony},
title = {Accurate real-time occlusion for mixed reality},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139153},
doi = {10.1145/3139131.3139153},
abstract = {Properly handling occlusion between real and virtual objects is an important property for any mixed reality (MR) system. Existing methods have typically required known geometry of the real objects in the scene, either specified manually, or reconstructed using a dense mapping algorithm. This limits the situations in which they can be applied. Modern RGBD cameras are cheap and widely available, but the depth information they provide is typically too noisy and incomplete to use directly to provide quality results.In this paper, a method is proposed which makes use of both the colour and depth information provided by an RGBD camera to provide improved occlusion. This method, Cost Volume Filtering Occlusion, is capable of running in real time, and can also handle occlusion of virtual objects by dynamic, moving objects - such as the user's hands. The method operates on individual RGBD frames as they arrive, meaning it can function immediately in unknown environments, and respond appropriately to sudden changes. The accuracy of the presented method is quantified using a novel approach capable of comparing the results of algorithms such as this to dense SLAM-based approaches. The proposed approach is shown to be capable of producing superior results to both previous image-based approaches and dense RGBD reconstruction, at lower computational cost.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {11},
numpages = {10},
keywords = {user interfaces, mixed and augmented reality, image processing},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139154,
author = {Achenbach, Jascha and Waltemate, Thomas and Latoschik, Marc Erich and Botsch, Mario},
title = {Fast generation of realistic virtual humans},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139154},
doi = {10.1145/3139131.3139154},
abstract = {In this paper we present a complete pipeline to create ready-to-animate virtual humans by fitting a template character to a point set obtained by scanning a real person using multi-view stereo reconstruction. Our virtual humans are built upon a holistic character model and feature a detailed skeleton, fingers, eyes, teeth, and a rich set of facial blendshapes. Furthermore, due to the careful selection of techniques and technology, our reconstructed humans are quite realistic in terms of both geometry and texture. Since we represent our models as single-layer triangle meshes and animate them through standard skeleton-based skinning and facial blendshapes, our characters can be used in standard VR engines out of the box. By optimizing for computation time and minimizing manual intervention, our reconstruction pipeline is capable of processing whole characters in less than ten minutes.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {12},
numpages = {10},
keywords = {virtual humans, virtual characters, avatars, 3D scanning},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139169,
author = {Wagner, Daniel and Hofmann, Christian and Hamann, Heiko and von Mammen, Sebastian},
title = {Design and exploration of braiding swarms in VR},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139169},
doi = {10.1145/3139131.3139169},
abstract = {Swarm-based braiding of structures represents a novel research direction in the domain of building architecture. The idea is that autonomous agents, for instance robots that unroll threads or plants that grow, are programmed or influenced to braid. It is an aspect of biohybrid systems where organisms and robots join forces. In order to harness this idea, we have developed a swarm-based model that allows architects to explore the resulting design spaces in virtual reality. In this paper, we present (1) the model of our swarm-based simulation that aims at growing braided structures, (2) the design elements to guide the otherwise self-organising virtual agents, and (3) the user interface that allows the user to configure, place and grow the swarms of braiding agents. We also present results of a first user study with students and faculty from architecture, in which we tried to capture the usability of our first prototype based on a survey and an analysis of the built results.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {13},
numpages = {4},
keywords = {virtual reality, braiding, architecture, agent-based modeling},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139170,
author = {Ma, Zhixin and Shen, Xukun and Cao, Chong},
title = {A hybrid CRF framework for semantic 3D reconstruction},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139170},
doi = {10.1145/3139131.3139170},
abstract = {Nowadays, in order to achieve an immersive experience, virtual reality systems usually require vivid 3D models and a good understanding of particular scenes. The limitations of separately optimizing image segmentation and 3D modeling from images have gradually been seen by more and more researchers, so plenty of novel methods on how to combine them for a better result begin to be put forward widely. In this paper, we propose a new hybrid framework to generate semantic 3D dense models from monocular images. Based on the available hierarchical CRFs model, we make full use of the correlation between voxels and their corresponding pixels from different images. Naturally, valuable information from 3D space can be added as one of the important energy items in the model. Either pixels, segments or voxles are all regarded as a node in the huge graph we build. Our ultimate goal is to realize a joint optimization for both 3D dense reconstruction and image segmentation. Experiments have been done on four real challenging datasets and all of the results prove the efficiency of our proposed hybrid framework.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {14},
numpages = {4},
keywords = {semantic reconstruction, image segmentation, graphics/3D, dense 3D modeling},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139142,
author = {Lacoche, J\'{e}r\'{e}my and Pallamin, Nico and Boggini, Thomas and Royan, J\'{e}r\^{o}me},
title = {Collaborators awareness for user cohabitation in co-located collaborative virtual environments},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139142},
doi = {10.1145/3139131.3139142},
abstract = {In a co-located collaborative virtual environment, multiple users share the same physical tracked space and the same virtual workspace. When the virtual workspace is larger than the real workspace, navigation interaction techniques must be deployed to let the users explore the entire virtual environment. When a user navigates in the virtual space while remaining static in the real space, his/her position in the physical workspace and in the virtual workspace are no longer the same. Thus, in the context where each user is immersed in the virtual environment with a Head-Mounted-Display, a user can still perceive where his/her collaborators are in the virtual environment but not where they are in real world. In this paper, we propose and compare three methods to warn users about the position of collaborators in the shared physical workspace to ensure a proper cohabitation and safety of the collaborators. The frst one is based on a virtual grid shaped as a cylinder, the second one is based on a ghost representation of the user and the last one displays the physical safe-navigation space on the foor of the virtual environment. We conducted a user-study with two users wearing a Head-Mounted-Display in the context of a collaborative First-Person-Shooter game. Our three methods were compared with a condition where the physical tracked space was separated into two zones, one per user, to evaluate the impact of each condition on safety, displacement freedom and global satisfaction of users. Results suggest that the ghost avatar and the cylinder grid can be good alternatives to the separation of the tracked space.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {15},
numpages = {9},
keywords = {virtual reality, collaborative virtual environment},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139163,
author = {Le, Khanh-Duy and Fjeld, Morten and Alavi, Ali and Kunz, Andreas},
title = {Immersive environment for distributed creative collaboration},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139163},
doi = {10.1145/3139131.3139163},
abstract = {While videoconferencing has been available for years, tools for distributed creative collaboration such as brainstorming have not yet reached professional use. Unlike regular conferencing, brainstorming relies on information exchange across multiple channels in shared task and communication spaces. If a participant joins a facilitated brainstorming session remotely through his/her mobile device, perceiving these spaces is challenging. By proposing an immersive environment for the remote participant, this tech note addresses how to provide him/her a stronger engagement. Offered as a client application for the remote participant's tablet device, it enables him/her to see all channels and select which one to interact with. Our proof-of-concept client allows us to examine how this immersive environment for distributed creative collaboration can provide the remote participant with an increased engagement.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {16},
numpages = {4},
keywords = {team work, mobile device, immersion},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139165,
author = {Clergeaud, Damien and Roo, Joan Sol and Hachet, Martin and Guitton, Pascal},
title = {Towards seamless interaction between physical and virtual locations for asymmetric collaboration},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139165},
doi = {10.1145/3139131.3139165},
abstract = {Virtual Reality allows rapid prototyping and simulation of physical artefacts, which would be difficult and expensive to perform otherwise. On the other hand, when the design process is complex and involves multiple stakeholders, decisions are taken in meetings hosted in the physical world. In the case of aerospace industrial designs, the process is accelerated by having asymmetric collaboration between the two locations: experts discuss the possibilities in a meeting room while a technician immersed in VR tests the selected alternatives. According to experts, the current approach is not without limitations, and in this work, we present prototypes designed to tackle them. The described artefacts were created to address the main issues: awareness of the remote location, remote interaction and manipulation, and navigation between locations. First feedback from experts regarding the prototypes is also presented. The resulting design considerations can be used in other asymmetric collaborative scenarios.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {17},
numpages = {4},
keywords = {virtual reality, tangible user interfaces, spatial augmented reality, mixed reality, head mounted display, asymmetric collaboration},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139133,
author = {Bhandari, Jiwan and Tregillus, Sam and Folmer, Eelke},
title = {Legomotion: scalable walking-based virtual locomotion},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139133},
doi = {10.1145/3139131.3139133},
abstract = {Using real walking for virtual navigation generally delivers the most natural and immersive virtual reality experience, but its usage is generally bounded by available tracking space. To navigate beyond the confines of available tracking space, users need to switch to an artificial locomotion technique, such as controller input. However, having to switch from leg-based input to hand-based input is considered to break presence. We present a hybrid handsfree locomotion technique called legomotion that lets users seamlessly switch between real walking input and walking-in-place input to enable navigation at scale. A user study with 18 participants compared legomotion to full locomotion using a controller. Legomotion led to higher presence as switching to controller input was found to be more tedious. Because controller input is also faster than walking, we observed most users to abandon positional tracking input altogether and primary use a controller for navigation - which then led to a lower presence. This finding could have major implications for the design of VR locomotion.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {18},
numpages = {8},
keywords = {walking-in-place, virtual reality, presence, locomotion, VR sickness},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139146,
author = {Yamashita, Koki and Kikuchi, Takashi and Masai, Katsutoshi and Sugimoto, Maki and Thomas, Bruce H. and Sugiura, Yuta},
title = {CheekInput: turning your cheek into an input surface by embedded optical sensors on a head-mounted display},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139146},
doi = {10.1145/3139131.3139146},
abstract = {In this paper, we propose a novel technology called "CheekInput" with a head-mounted display (HMD) that senses touch gestures by detecting skin deformation. We attached multiple photo-reflective sensors onto the bottom front frame of the HMD. Since these sensors measure the distance between the frame and cheeks, our system is able to detect the deformation of a cheek when the skin surface is touched by fingers. Our system uses a Support Vector Machine to determine the gestures: pushing face up and down, left and right. We combined these 4 directional gestures for each cheek to extend 16 possible gestures. To evaluate the accuracy of the gesture detection, we conducted a user study. The results revealed that CheekInput achieved 80.45 \% recognition accuracy when gestures were made by touching both cheeks with both hands, and 74.58 \% when by touching both cheeks with one hand.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {19},
numpages = {8},
keywords = {skin interface, photo-reflective sensor, OST-HMD},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139167,
author = {Nguyen, Anh and Cervellati, Federico and Kunz, Andreas},
title = {Gain compensation in redirected walking},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139167},
doi = {10.1145/3139131.3139167},
abstract = {Redirected Walking Techniques (RWTs) enable a user to immersively explore a virtual environment larger than the available physical space by real walking. RWTs are based on the use of gains (translational, rotational and curvature), which introduce a mismatch between the virtual and physical trajectories. When these gains are applied within certain thresholds, the "manipulation" is unnoticeable and immersion is maintained. Numerous research has been carried out to identify these thresholds and factors that affect them such as walking speed, environment structure or tasks involved. However, it has not been known whether users change their walking behavior when RWTs are applied and if this in turn influences ftheir perception thresholds.In this paper, we investigate the change in users' walking behavior, particularly their walking speed, when translational gains are applied. We call this behavior gain compensation. 17 subjects were invited to play a shopping game where they had to walk 50 straight segments to fetch the ingredients. During each segment, one of the five different translational gains (0.7, 0.9, 1.0, 1.2, 1.4) was randomly applied and users' walking speeds were measured. Results show that there is a negative correlation between walking speed and translational gain.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {20},
numpages = {4},
keywords = {redirected walking, gain compensation},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139168,
author = {Nguyen, Anh and Kunz, Andreas and Rothacher, Yannick and Brugger, Peter and Lenggenhager, Bigna},
title = {Spontaneous alternation behavior in humans},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139168},
doi = {10.1145/3139131.3139168},
abstract = {Redirected walking refers to a number of techniques that enable users to explore a virtual environment larger than the physical space by real walking. The efficiency of these techniques has been shown to improve when predictions about the user's future trajectory are incorporated. Predictions can be made not only based on the knowledge about the environment but also on how humans behave in it. In a maze-like environment, it is known that most animal species show a strong preference to alternate their turning direction. This is called spontaneous alternation behavior (SAB). Although such behavior has also been observed in humans during maze tracing tasks, little is known whether they also exhibit this behavior during real walking, and if they do, what their alternation rate is.In the experiment described in this paper, 60 right-handed subjects were invited to walk freely through a virtual maze consisting of a primary 90° forced turn followed by three consecutive T-junctions. Results show that, on average, humans exhibited an alternation rate of 72\%. When looking only at the junction after the forced turn, subjects alternated with 76\%. After two consecutive turns of the same direction subjects alternated with 93\%. The alternation rates obtained not only clearly confirm the existence of SAB in humans but also could be used to improve the accuracy of existing prediction models in human walking.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {21},
numpages = {4},
keywords = {spontaneous alternation behaviour (SAB), redirected walking, model predictive control},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139138,
author = {Nuernberger, Benjamin and Turk, Matthew and H\"{o}llerer, Tobias},
title = {Evaluating snapping-to-photos virtual travel interfaces for 3D reconstructed visual reality},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139138},
doi = {10.1145/3139131.3139138},
abstract = {Navigating through a virtual, 3D reconstructed scene has recently become very important in many applications. A popular approach is to virtually travel to the photos used in reconstructing the scene; such an approach may be generally termed a "snapping-to-photos" virtual travel interface. While previous work has either used fully constrained interfaces (always at the photos) or minimally constrained interfaces (free-flight navigation), in this paper we introduce new snapping-to-photos interfaces that lie in between these two extremes. Our snapping-to-photos interfaces snap the view to a photo in 3D based on viewpoint similarity and optionally the user's mouse cursor or finger-tap position. Experimental results, with both indoor and outdoor scene reconstructions, found that our snapping-to-photos interfaces are preferred over the baseline fully constrained-to-photos interface, that there exist differences between indoor and outdoor scenes, and that users preferred and were able to reach target photos better with click-to-snap point-of-interest snapping compared to automatic point-of-view snapping.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {22},
numpages = {11},
keywords = {virtual navigation, user experiments, 3D user interfaces},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139140,
author = {Casarin, Julien and Bechmann, Dominique and Keller, Marilyn},
title = {A unified model for interaction in 3D environment},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139140},
doi = {10.1145/3139131.3139140},
abstract = {The Virtual (VR), Augmented (AR) and Mixed Reality (MR) devices are currently evolving at a very fast pace. This rapid evolution affects significantly the maintainability and portability of the applications. In this paper, we present a model for designing VR, AR and MR applications independently of any device. To do this, we use degrees of freedom to define an abstraction layer between the tasks to be performed and the interaction device.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {23},
numpages = {7},
keywords = {virtual worlds, model-based interactive system development, input techniques},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139144,
author = {Jeanne, Florian and Thouvenin, Indira and Lenglet, Alban},
title = {A study on improving performance in gesture training through visual guidance based on learners' errors},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139144},
doi = {10.1145/3139131.3139144},
abstract = {Gesture training, especially for technical gestures, requires supervisors to point out errors made by trainees. Virtual reality (VR) makes it possible to reduce reliance on supervisors (fewer interventions and of shorter duration) and to reduce the length of training, using extrinsic feedback that provides training or learning assistance using different modalities (visual, auditory, and haptic). Visual feedback has received much attention in recent decades. Users can be guided by a metaphor in a virtual environment. This metaphor may be a 3D trace of canonical movements, a visual cue pointing in the right direction, or gestures by an avatar that the trainee must mimic. However, with many kinds of feedback, trainees are not aware of their errors while performing gestures. Our hypothesis is that guiding users with a dynamic metaphor based on the visualization of errors will reduce these errors and improve performance. To this end, in a previous work we designed and implemented a new 3D metaphor called EBAGG to guide users in real time.In the present paper we evaluate EBAGG in relation to two other visual cues: first, a feedforward technique that displays the trace of a reference movement, and, second, a concurrent orientation feedback. The results of the user study show that EBAGG outperformed the others in improving users' performances over a training session. Moreover, the information assimilated during training with this dynamic feedback had a persistent effect when the metaphor was no longer displayed.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {24},
numpages = {10},
keywords = {visual feedback, virtual reality, user study, performance, guidance, gestures},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139135,
author = {K\'{a}n, Peter and Kaufmann, Hannes},
title = {Automated interior design using a genetic algorithm},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139135},
doi = {10.1145/3139131.3139135},
abstract = {In this paper, we present a system that automatically populates indoor virtual scenes with furniture objects and optimizes their positions and orientations with respect to aesthetic, ergonomic and functional rules called interior design guidelines. These guidelines are represented as mathematical expressions which form the cost function. Our system optimizes the set of multiple interior designs by minimizing the cost function using a genetic algorithm. Moreover, we extend the optimization to transdimensional space by enabling automatic selection of furniture objects. Finally, we optimize the assignment of materials to the furniture objects to achieve a unified design and harmonious color distribution. We investigate the capability of our system to generate sensible and livable interior designs in a perceptual study.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {25},
numpages = {10},
keywords = {virtual environments, scene modeling, interior design, furniture arrangement, computational design},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139139,
author = {Sra, Misha and Maes, Pattie and Vijayaraghavan, Prashanth and Roy, Deb},
title = {Auris: creating affective virtual spaces from music},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139139},
doi = {10.1145/3139131.3139139},
abstract = {Affective virtual spaces are of interest in many virtual reality applications such as education, wellbeing, rehabilitation, and entertainment. In this paper we present Auris, a system that attempts to generate affective virtual environments from music. We use music as input because it inherently encodes emotions that listeners readily recognize and respond to. Creating virtual environments is a time consuming and labor-intensive task involving various skills like design, 3D modeling, texturing, animation, and coding. Auris helps make this easier by automating the virtual world generation task using mood and content extracted from song audio and lyrics data respectively. Our user study results indicate virtual spaces created by Auris successfully convey the mood of the songs used to create them and achieve high presence scores with the potential to provide novel experiences of listening to music.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {26},
numpages = {11},
keywords = {virtual reality, music, generative models, deep neural networks},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139157,
author = {Mendes, Daniel and Sousa, Maur\'{\i}cio and Lorena, Rodrigo and Ferreira, Alfredo and Jorge, Joaquim},
title = {Using custom transformation axes for mid-air manipulation of 3D virtual objects},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139157},
doi = {10.1145/3139131.3139157},
abstract = {Virtual Reality environments are able to offer natural interaction metaphors. However, it is difficult to accurately place virtual objects in the desired position and orientation using gestures in mid-air. Previous research concluded that the separation of degrees-of-freedom (DOF) can lead to better results, but these benefits come with an increase in time when performing complex tasks, due to the additional number of transformations required. In this work, we assess whether custom transformation axes can be used to achieve the accuracy of DOF separation without sacrificing completion time. For this, we developed a new manipulation technique, MAiOR, which offers translation and rotation separation, supporting both 3-DOF and 1-DOF manipulations, using personalized axes for the latter. Additionally, it also has direct 6-DOF manipulation for coarse transformations, and scaled object translation for increased placement. We compared MAiOR against an exclusively 6-DOF approach and a widget-based approach with explicit DOF separation. Results show that, contrary to previous research suggestions, single DOF manipulations are not appealing to users. Instead, users favored 3-DOF manipulations above all, while keeping translation and rotation independent.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {27},
numpages = {8},
keywords = {virtual reality, mid-air object manipulation, custom manipulation axis, DOF separation, 3D user interfaces},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139160,
author = {Fujinawa, Eisuke and Yoshida, Shigeo and Koyama, Yuki and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
title = {Computational design of hand-held VR controllers using haptic shape illusion},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139160},
doi = {10.1145/3139131.3139160},
abstract = {Humans are capable of haptically perceiving the shape of an object by simply wielding it, even without seeing it. On the other hand, typical hand-held controllers for virtual reality (VR) applications are pre-designed for general applications, and thus not capable of providing appropriate haptic shape perception when wielding specific virtual objects. Contradiction between haptic and visual shape perception causes a lack of immersion and leads to inappropriate object handling in VR. To solve this problem, we propose a novel method for designing hand-held VR controllers which illusorily represent haptic equivalent of visual shape in VR. In ecological psychology, it has been suggested that the perceived shape can be modeled using the limited mass properties of wielded objects. Based on this suggestion, we built a shape perception model using a data-driven approach; we aggregated data of perceived shapes against various hand-held VR controllers with different mass properties, and derived the model using regression techniques. We implemented a design system which enables automatic design of hand-held VR controllers whose actual shapes are smaller than target shapes while maintaining their haptic shape perception. We verified that controllers designed with our system can present aimed shape perception irrespective of their actual shapes.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {10},
keywords = {virtual reality, perception, data-driven, computational design},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139150,
author = {Avveduto, Giovanni and Tecchia, Franco and Fuchs, Henry},
title = {Real-world occlusion in optical see-through AR displays},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139150},
doi = {10.1145/3139131.3139150},
abstract = {In this work we describe a system composed by an optical see-through AR headset---a Microsoft HoloLens---, stereo projectors and shutter glasses. Projectors are used to add to the device the capability of occluding real-world surfaces to make the virtual objects to appear more solid and less transparent.A framework was developed in order to allow us to evaluate the importance of occlusion capabilities in optical see-through AR headset. We designed and conducted two experiment to test whether making virtual elements solid would improve the performance of certain tasks with an AR system. Results suggest that making virtual objects to appear more solid by projecting an occlusion mask onto the real-world is useful in some situations.Using an occlusion mask it is also possible to eliminate ambiguities that could arise when enhancing user's perception in some ways that are not possible in real-life, like when a "x-ray vision" is enabled. In this case we wanted to investigate if using an occlusion mask to eliminate perceptual conflicts will hit user's performance in some AR applications.The framework that allowed us to conduct our experiments is made freely available to anyone interested in conducting future studies.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {29},
numpages = {10},
keywords = {optical see-through displays, occlusion mask, X-ray vision},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139159,
author = {Kasperi, Johan and Edwardsson, Malin Picha and Romero, Mario},
title = {Occlusion in outdoor augmented reality using geospatial building data},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139159},
doi = {10.1145/3139131.3139159},
abstract = {Aligning virtual and real objects in Augmented Reality (AR) is essential for the user experience. Without alignment, the user loses suspension of disbelief and the sense of depth, distance, and size. Occlusion is a key feature to be aligned. Virtual content should be partially or fully occluded if real world objects are in its line-of-sight. The challenge for simulating occlusion is to construct the geometric model of the environment. Earlier studies have aimed to create realistic occlusions, yet most have either required depth-sensing hardware or a static predefined environment. This paper proposes and evaluates an alternative model-based method for dynamic outdoor AR of virtual buildings rendered on non depth-sensing smartphones. It uses geospatial data to construct the geometric model of real buildings surrounding the virtual building. The method removes the target regions from the virtual building using masks constructed from real buildings. While the method is not pixel-perfect, meaning that the simulated occlusion is not fully realistic, results from the user study indicate that it fulfilled its goal. A majority of the participants expressed that their experience and depth perception improved with the method activated. The result from this study has applications to mobile AR since the majority of smartphones are not equipped with depth sensors. Using geospatial data for simulating occlusions is a sufficiently effective solution until depth-sensing AR devices are more widely available.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {30},
numpages = {10},
keywords = {physical simulation, open street maps, occlusion, geospatial data, augmented reality, AR},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139151,
author = {Kim, Kangsoo and Bruder, Gerd and Welch, Greg},
title = {Exploring the effects of observed physicality conflicts on real-virtual human interaction in augmented reality},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139151},
doi = {10.1145/3139131.3139151},
abstract = {Augmented reality (AR) enables the illusion of computer-generated virtual objects and humans co-existing with us in the real world. Virtual humans (VHs) in AR can further induce an illusion of physicality in the real world due to their form of presentation and their behavior, such as showing awareness of their surroundings. However, certain behaviors can cause a conflict that breaks this illusion, for example, when we see a VH passing through a physical object.In this paper we describe a human-subject study that we performed to test the hypothesis that participants experience higher copresence in conflict-free circumstances, and we investigate the magnitude of this effect and behavioral manifestations. Participants perceived a social situation in a room that they shared with a VH as seen through a HoloLens head-mounted display. The behavior of the VH either caused conflicts with (occupied the same space as) physical entities, or avoided them. Our results show that the conflicts in physicality significantly reduced subjective reports of copresence. Moreover, we observed that participants were more likely to cause a conflict (occupy the same space as) virtual entities in case the VH had avoided the conflict. We discuss implications for future research and shared AR setups with real-virtual human interactions.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {7},
keywords = {virtual humans, physicality, copresence, augmented reality},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139158,
author = {Sayyad, Ehsan and Sen, Pradeep and H\"{o}llerer, Tobias},
title = {PanoTrace: interactive 3D modeling of surround-view panoramic images in virtual reality},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139158},
doi = {10.1145/3139131.3139158},
abstract = {Full-surround panoramic imagery can provide a viewer with a high-resolution visual impression of a pictured real or realistically rendered environment, but it does not provide as high a level of immersion as modeled 3D geometry can, when viewed with virtual reality (VR) headsets or projection-based setups. In this paper, we demonstrate that augmenting panorama images with geometrical models can be done simply in VR itself and can significantly increase the feeling of immersion a viewer experiences. We propose a novel interactive modeling tool that allows users to model geometry depicted in a surround-panoramic scene directly in VR, utilizing projection mapping of the panorama on top of the evolving geometry. The user interface is intuitive and allows novice users to produce geometry that approximates ground truth models sufficiently to enhance a user's VR viewing experience. We designed a user study that compares users' self-reported levels of immersion, scene realism, and discomfort on a set of created models and comparison cases. Our results indicate that our modeled scenes produce a significantly higher sense of immersion than a basic dome geometry for the panorama when viewed in VR with head orientation and position tracking.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {32},
numpages = {10},
keywords = {virtual reality, panorama imaging, geometric modeling, VR modeling tools},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139155,
author = {Trescak, Tomas and Bogdanovych, Anton},
title = {Case-based planning for large virtual agent societies},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139155},
doi = {10.1145/3139131.3139155},
abstract = {In this paper we discuss building large scale virtual reality reconstructions of historical heritage sites and populating it with crowds of virtual agents. Such agents are capable of performing complex actions, while respecting the cultural and historical accuracy of agent behaviour. In many commercial video games such agents either have very limited range of actions (resulting primitive behaviour) or are manually designed (resulting high development costs). In contrast, we follow the principles of automatic goal generation and automatic planning. Automatic goal generation in our approach is achieved through simulating agent needs and then producing a goal in response to those needs that require satisfaction. Automatic planning refers to techniques that are concerned with producing sequences of actions that can successfully change the state of an agent to the state where its goals are satisfied. Classical planning algorithms are computationally costly and it is difficult to achieve real-time performance for our problem domain with those. We explain how real-time performance can be achieved with Case-Based Planning, where agents build plan libraries and learn how to reuse and combine existing plans to archive their dynamically changing goals. We illustrate the novelty of our approach, its complexity and associated performance gains through a case-study focused on developing a virtual reality reconstruction of an ancient Mesopotamian settlement in 5000 B.C.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {33},
numpages = {10},
keywords = {virtual agents, social simulations, case-based planning},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139134,
author = {McDuff, Daniel and Hurter, Christophe and Gonzalez-Franco, Mar},
title = {Pulse and vital sign measurement in mixed reality using a HoloLens},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139134},
doi = {10.1145/3139131.3139134},
abstract = {Cardiography, quantitative measurement of the functioning of the heart, traditionally requires customized obtrusive contact sensors. Using new methods photoplethysmography and ballistocardiography signals can be captured using ubiquitous sensors, such as webcams and accelerometers. However, these signals are not visible to the unaided eye. We present Cardiolens - a mixed reality system that enables real-time, hands-free measurement and visualization of blood flow and vital signs from multiple people. The system combines a front-facing webcam, imaging ballistocardiography, and remote imaging photoplethysmography methods for recovering pulse signals. A heads up display allows users to view their own heart rate whenever they are wearing the device and the heart rate and heart rate variability of another person simply by looking at them. Cardiolens provides the wearer with a new way to understand physiological signals and has applications in human-computer interaction and in the study of social psychology.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {34},
numpages = {9},
keywords = {remote sensing, physiology, mixed reality, interoception, health},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139162,
author = {Rapetti, Lorenzo and Crivellaro, Simone and De Momi, Elena and Ferrigno, Giancarlo and Niederberger, Craig and Luciano, Cristian},
title = {Virtual reality navigation system for prostate biopsy},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139162},
doi = {10.1145/3139131.3139162},
abstract = {Prostate cancer is the most common non-cutaneous cancer in America. Tumor detection involves non-invasive screening tests, but positive results must be confirmed by a prostate biopsy. About twelve random samples are obtained during the biopsy, which is a systematic procedure traditionally performed with trans-rectal ultrasound (TRUS) guidance to determine prostate location. Recently, methods of fusion between TRUS and preoperative MRI have been introduced in order to perform targeted biopsies aimed to reduce the number of samples to few suspicious areas. Since the TRUS displaces the prostate during the procedure, the preoperative MRI does not match patient anatomy. Therefore, complex MRI deformation algorithms are needed. However, despite the substantial increase in complexity and cost, there is no strong evidence that the TRUS-MRI fusion actually improves accuracy and surgical outcomes.This paper presents an innovative virtual reality surgical navigation system for performing targeted prostate biopsies, without the need of the uncomfortable TRUS. Both biopsy needle and patient anatomy are constantly tracked by an electromagnetic tracking system that provides their 3D position and orientation with respect to the surgical bed. Multiple fiducial markers are placed on the patient skin (at the iliac crest and pubic bone) during MRI scanning. Once in the operative room, the surgeon is presented a stereoscopic 3D volumetric rendering and multiple orthogonal views of the patient anatomy, as well as a virtual representation of the tracked needle. After a simple registration process between the MRI and the tracker coordinate system, the navigation system guides the needle insertion in the patient perineum through several anatomical layers towards the biopsy targets.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {35},
numpages = {4},
keywords = {prototyping/implementation, medical and health support, graphics/3D},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139137,
author = {Kim, Hak Gu and Baddar, Wissam J. and Lim, Heoun-taek and Jeong, Hyunwook and Ro, Yong Man},
title = {Measurement of exceptional motion in VR video contents for VR sickness assessment using deep convolutional autoencoder},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139137},
doi = {10.1145/3139131.3139137},
abstract = {This paper proposes a new objective metric of exceptional motion in VR video contents for VR sickness assessment. In VR environment, VR sickness can be caused by several factors which are mismatched motion, field of view, motion parallax, viewing angle, etc. Similar to motion sickness, VR sickness can induce a lot of physical symptoms such as general discomfort, headache, stomach awareness, nausea, vomiting, fatigue, and disorientation. To address the viewing safety issues in virtual environment, it is of great importance to develop an objective VR sickness assessment method that predicts and analyses the degree of VR sickness induced by the VR content. The proposed method takes into account motion information that is one of the most important factors in determining the overall degree of VR sickness. In this paper, we detect the exceptional motion that is likely to induce VR sickness. Spatio-temporal features of the exceptional motion in the VR video content are encoded using a convolutional autoencoder. For objectively assessing the VR sickness, the level of exceptional motion in VR video content is measured by using the convolutional autoencoder as well. The effectiveness of the proposed method has been successfully evaluated by subjective assessment experiment using simulator sickness questionnaires (SSQ) in VR environment.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {36},
numpages = {7},
keywords = {virtual reality, machine learning, cybersickness},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139143,
author = {Chowdhury, Tanvir Irfan and Ferdous, Sharif Mohammad Shahnewaz and Quarles, John},
title = {Information recall in a virtual reality disability simulation},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139143},
doi = {10.1145/3139131.3139143},
abstract = {The purpose of this paper is to investigate the effect of the sense of presence on one aspect of learning, information recall, in an immersive virtual reality (VR) disability simulation. Previous research has shown that the use of VR technology in education may facilitate improved learning outcomes, however, it is still an active research topic as the learning outcomes can vary widely. We hypothesized that a higher level of immersion and involvement in a VR disability simulation that leads to a high sense of presence will help the user improve information recall. To investigate this hypothesis, we conducted a between subjects experiment in which participants were presented information about multiple sclerosis in different immersive conditions and afterwards they attempted to recall the information. We also looked into whether there is any adverse effect of cybersickness on the information recall task in our disability simulation. The results from our study suggest that participants who were in immersive conditions were able to recall the information more effectively than the participants who experienced a non-immersive condition.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {37},
numpages = {10},
keywords = {virtual/augmented reality, user studies, persuasion, or change (primary keyword), learning, information recall, games for health},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139147,
author = {Kim, Youngwon Ryan and Kim, Gerard J.},
title = {Presence and immersion of "easy" mobile VR with open flip-on lenses},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139147},
doi = {10.1145/3139131.3139147},
abstract = {Mobile virtual reality (M-VR) uses an inexpensive and light headset into which the smartphone is inserted to conveniently experience immersive contents. Even so the headset is bulky and difficult to carry around, and makes the smartphone inaccessible. Recently, an alternative form of M-VR has appeared in the market in which the magnifying lenses are simply clipped on the smartphone (dubbed "EasyVR"). Despite being open and the user'speripheral view not shut from the outside world, it still gives a good level of immersion with a wide magnified field of view. EasyVR has the added advantages of quick switch between with the regular smartphone usage mode and access to the touch screen for the seamless interaction. In this paper, we examine and compare the level of presence and immersion as provided by three different display configurations of M-VR: (1) EasyVR, (2) the usual headset which completely isolates the user from the outer-world with the vignetted view (ClosedVR), and (3) completely open hand-held smartphone view (OpenVR). We also control the environment condition, static or dynamic, as seen and perceived through the peripheral view and possibly having an effect on the level of presence and immersion in the respective display configuration. Our findings first show the easily expected, namely, both ClosedVR and EasyVR clearly exhibiting a much higher level of presence and immersion than OpenVR. The results also show that even though there is a substantial extent within the peripheral view showing the outer environment, the level of presence and immersion of EasyVR is nearly comparable to that of ClosedVR. Only when the environment was dynamic (as visible in the peripheral view ends), EasyVR showed a lower level of presence and immersion than ClosedVR, but still significantly higher than OpenVR. Therefore, EasyVR is a very attractive alternative to the usual ClosedVR, especially as a "use-anywhere" VR considering its clearly improved convenience and sufficient level of immersion beyond just for casual purposes.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {38},
numpages = {7},
keywords = {presence, open/closed VR display, mobile virtual reality, immersion, distraction},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139156,
author = {Latoschik, Marc Erich and Roth, Daniel and Gall, Dominik and Achenbach, Jascha and Waltemate, Thomas and Botsch, Mario},
title = {The effect of avatar realism in immersive social virtual realities},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139156},
doi = {10.1145/3139131.3139156},
abstract = {This paper investigates the effect of avatar realism on embodiment and social interactions in Virtual Reality (VR). We compared abstract avatar representations based on a wooden mannequin with high fidelity avatars generated from photogrammetry 3D scan methods. Both avatar representations were alternately applied to participating users and to the virtual counterpart in dyadic social encounters to examine the impact of avatar realism on self-embodiment and social interaction quality. Users were immersed in a virtual room via a head mounted display (HMD). Their full-body movements were tracked and mapped to respective movements of their avatars. Embodiment was induced by presenting the users' avatars to themselves in a virtual mirror. Afterwards they had to react to a non-verbal behavior of a virtual interaction partner they encountered in the virtual space. Several measures were taken to analyze the effect of the appearance of the users' avatars as well as the effect of the appearance of the others' avatars on the users. The realistic avatars were rated significantly more human-like when used as avatars for the others and evoked a stronger acceptance in terms of virtual body ownership (VBO). There also was some indication of a potential uncanny valley. Additionally, there was an indication that the appearance of the others' avatars impacts the self-perception of the users.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {39},
numpages = {10},
keywords = {virtual reality, social interaction, lifelike, avatars},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139149,
author = {Tran, Tanh Quang and Shin, HyunJu and Stuerzlinger, Wolfgang and Han, JungHyun},
title = {Effects of virtual arm representations on interaction in virtual environments},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139149},
doi = {10.1145/3139131.3139149},
abstract = {Many techniques for visualization and interaction that potentially increase user performance have been studied in the growing field of virtual reality. However, the effects of virtual-arm representations on users' performance and perception in selection tasks have not been studied before. This paper presents the results of a user study of three different representations of the virtual arm: "hand only," "hand+forearm," and "whole arm" which includes the upper arm. In addition to the representations' effects on performance and perception in selection tasks, we investigate how the users' performance changes depending on whether collisions with objects are allowed or not. The relationship between the virtual-arm representations and the senses of agency and ownership are also explored. Overall, we found that the "whole arm" condition performed worst.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {40},
numpages = {9},
keywords = {virtual reality, virtual arm, selection performance, natural hand interaction, 3D interaction},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@inproceedings{10.1145/3139131.3139172,
author = {Legkov, Petr and Izdebski, Krzysztof and K\"{a}rcher, Silke and K\"{o}nig, Peter},
title = {Dual task based cognitive stress induction and its influence on path integration},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139172},
doi = {10.1145/3139131.3139172},
abstract = {Current stress induction methods are often too theoretical and do not reflect to real life scenarios. In this study, we used a dual task paradigm in virtual reality, combining a navigation task with a reaction task. With this setup, we aimed at creating a novel benchmark stress induction approach utilizing modern virtual reality technology. Results show that our paradigm induced small scale physiological and subjective state changes. Lastly, we discuss our paradigm and experimental results from the perspective of ecological validity and present suggestions for improving our stress induction methodology as well as potential areas of use.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {41},
numpages = {5},
keywords = {virtual reality, navigation, ecological validity, cognitive stress},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

