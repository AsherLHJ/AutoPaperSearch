@inproceedings{10.1145/3281505.3281514,
author = {Armengol-Urpi, Alexandre and Sarma, Sanjay E.},
title = {Sublime: a hands-free virtual reality menu navigation system using a high-frequency SSVEP-based brain-computer interface},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281514},
doi = {10.1145/3281505.3281514},
abstract = {In this work we present Sublime, a new concept of Steady-State Visually Evoked Potential (SSVEP) based Brain-Computer Interface (BCI) where brain-computer communication occurs by capturing imperceptible visual stimuli integrated in the virtual scene and effortlessly conveying subliminal information to a computer. The technology was tested in a Virtual Reality (VR) environment, where the subject could navigate between the different menus by just gazing at them. The ratio between the stimuli frequencies and the refresh rate of the VR display creates an undesired perception of beats for which different solutions are proposed. To inform the user of target activation, real-time feedback in the form of loading bars is incorporated under each selectable object. We conducted experiments with several subjects and though the system is slower than a conventional joystick, users reported a satisfactory overall experience, in part due to the unexpected responsiveness of the system, as well as due to the fact that virtual objects flickered at a rate that did not cause annoyance. Since the imperceptible visual stimuli can be integrated unobtrusively to any element of the virtual world, we conclude that the potential applications of Sublime are extensive, especially in situations where knowing user's visual focus can be relevant.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {1},
numpages = {8},
keywords = {virtual reality, steady-state visually evoked potentials, electroencephalography, brain-computer interface},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281525,
author = {Marquardt, Alexander and Kruijff, Ernst and Trepkowski, Christina and Maiero, Jens and Schwandt, Andrea and Hinkenjann, Andr\'{e} and Stuerzlinger, Wolfgang and Sch\"{o}ning, Johannes},
title = {Audio-tactile proximity feedback for enhancing 3D manipulation},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281525},
doi = {10.1145/3281505.3281525},
abstract = {In presence of conflicting or ambiguous visual cues in complex scenes, performing 3D selection and manipulation tasks can be challenging. To improve motor planning and coordination, we explore audio-tactile cues to inform the user about the presence of objects in hand proximity, e.g., to avoid unwanted object penetrations. We do so through a novel glove-based tactile interface, enhanced by audio cues. Through two user studies, we illustrate that proximity guidance cues improve spatial awareness, hand motions, and collision avoidance behaviors, and show how proximity cues in combination with collision and friction cues can significantly improve performance.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {2},
numpages = {10},
keywords = {tactile feedback, hand guidance, 3D user interface},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281526,
author = {Marquardt, Alexander and Maiero, Jens and Kruijff, Ernst and Trepkowski, Christina and Schwandt, Andrea and Hinkenjann, Andr\'{e} and Sch\"{o}ning, Johannes and Stuerzlinger, Wolfgang},
title = {Tactile hand motion and pose guidance for 3D interaction},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281526},
doi = {10.1145/3281505.3281526},
abstract = {We present a novel forearm-and-glove tactile interface that can enhance 3D interaction by guiding hand motor planning and coordination. In particular, we aim to improve hand motion and pose actions related to selection and manipulation tasks. Through our user studies, we illustrate how tactile patterns can guide the user, by triggering hand pose and motion changes, for example to grasp (select) and manipulate (move) an object. We discuss the potential and limitations of the interface, and outline future work.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {3},
numpages = {10},
keywords = {tactile feedback, hand guidance, 3D user interface},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281539,
author = {Wijnants, Maarten and Lievens, Hendrik and Michiels, Nick and Put, Jeroen and Quax, Peter and Lamotte, Wim},
title = {Standards-compliant HTTP adaptive streaming of static light fields},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281539},
doi = {10.1145/3281505.3281539},
abstract = {Static light fields are an effective technology to precisely visualize complex inanimate objects or scenes, synthetic and real-world alike, in Augmented, Mixed and Virtual Reality contexts. Such light fields are commonly sampled as a collection of 2D images. This sampling methodology inevitably gives rise to large data volumes, which in turn hampers real-time light field streaming over best effort networks, particularly the Internet. This paper advocates the packaging of the source images of a static light field as a segmented video sequence so that the light field can then be interactively network streamed in a quality-variant fashion using MPEG-DASH, the standardized HTTP Adaptive Streaming scheme adopted by leading video streaming services like YouTube and Netflix. We explain how we appropriate MPEG-DASH for the purpose of adaptive static light field streaming and present experimental results that prove the feasibility of our approach, not only from a networking but also a rendering perspective. In particular, real-time rendering performance is achieved by leveraging video decoding hardware included in contemporary consumer-grade GPUs. Important trade-offs are investigated and reported on that impact performance, both network-wise (e.g., applied sequencing order and segmentation scheme for the source images of the static light field) and rendering-wise (e.g., disk-versus-GPU caching of source images). By adopting a standardized transmission scheme and by exclusively relying on commodity graphics hardware, the net result of our work is an interoperable and broadly deployable network streaming solution for static light fields.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {4},
numpages = {12},
keywords = {video compression, static light fields, experimental evaluation, MPEG-DASH, JPEG, IBR, HTTP adaptive streaming, H.264},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281540,
author = {Fafard, Dylan Brodie and Zhou, Qian and Chamberlain, Chris and Hagemann, Georg and Fels, Sidney and Stavness, Ian},
title = {Design and implementation of a multi-person fish-tank virtual reality display},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281540},
doi = {10.1145/3281505.3281540},
abstract = {A mixed reality experience with a physical display, that situates 3D virtual content within the real world, has the potential to help people work and play with 3D information. However, almost all of such "fish tank virtual reality" (FTVR) systems have been isolated to a single-person experience, making them unsuitable for collaborative tasks. In this paper, we present a display system that allows two people to have unobstructed 3D perspective views into a spherical display while still being able to see and talk to one another. We evaluated the system through qualitative observation at a four-day exhibition and found it was effective for providing a convincing, shared 3D experience.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {5},
numpages = {9},
keywords = {stereo, spherical displays, fish tank virtual reality, collaboration, co-location, 3D displays},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281517,
author = {Ichikawa, Shotaro and Takashima, Kazuki and Tang, Anthony and Kitamura, Yoshifumi},
title = {VR safari park: a concept-based world building interface using blocks and world tree},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281517},
doi = {10.1145/3281505.3281517},
abstract = {We present a concept-based world building approach, realized in a system called VR Safari Park, which allows users to rapidly create and manipulate a world simulation. Conventional world building tools focus on the manipulation and arrangement of entities to set up the simulation, which is time consuming as it requires frequent view and entity manipulations. Our approach focuses on a far simpler mechanic, where users add virtual blocks which represent world entities (e.g. animals, terrain, weather, etc.) to a World Tree, which represents the simulation. In so doing, the World Tree provides a quick overview of the simulation, and users can easily set up scenarios in the simulation without having to manually perform fine-grain manipulations on world entities. A preliminary user study found that the proposed interface is effective and usable for novice users without prior immersive VR experience.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {6},
numpages = {5},
keywords = {interaction design, entertainment, concept-based modeling},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281532,
author = {Toyohara, Soichiro and Sato, Toshiki and Koike, Hideki},
title = {Balloonygen: extended tabletop display embedded with balloon-like deformable spherical screen},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281532},
doi = {10.1145/3281505.3281532},
abstract = {Balloonygen, an extended tabletop display embedded with a balloon-like deformable spherical screen, is a display that can seamlessly expose a spherical screen for three-dimensional contents, such as omnidirectional images, in a conventional flat display. By continuously morphing between a two-dimensional shape called tabletop and a three-dimensional shape called sphere, we render the benefits of a flat display and a spherical display to coexist and propose a smoother approach for information sharing. Balloonygen dynamically provides an optimal way to display the contents by inflating the rubber membrane installed at the center of a tabletop display and morphing between the two- and three-dimensional shapes. In this study, by prototyping and designing the application scenario, we discuss the advantages and disadvantages of this display and possible interactions involved.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {7},
numpages = {5},
keywords = {tabletop display, shape-changing display, omnidirectional display, deformable display, actuated surface},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281512,
author = {Bolder, Anna and Gr\"{u}nvogel, Stefan M. and Angelescu, Emanuel},
title = {Comparison of the usability of a car infotainment system in a mixed reality environment and in a real car},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281512},
doi = {10.1145/3281505.3281512},
abstract = {Instead of installing new control modes for infotainments systems in a real vehicle for testing, it is an attractive idea (saving time and cost) to evaluate and develop these systems in a mixed reality (MR) environment. The central question of the study is whether the usability evaluation of a car entertainment system within a MR environment provides the same results as the evaluation of the car entertainment system within a real car. For this purpose a prototypical car infotainment system was built and integrated into a real car and into a MR environment. The MR environment represents the interior of the car and uses finger tracking and real haptic control elements of the center console of a car. Two test groups were assigned to the two different test environments. The study shows, that the usability is rated similar in both environments although readability and representation within the infotainment system is problematic.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {8},
numpages = {10},
keywords = {user experience, usability, study, mixed reality, car infotainment system},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281521,
author = {Freiwald, Jann Philipp and Katzakis, Nicholas and Steinicke, Frank},
title = {Camera time warp: compensating latency in video see-through head-mounted-displays for reduced cybersickness effects},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281521},
doi = {10.1145/3281505.3281521},
abstract = {We introduce Camera Time Warp (CamWarp), a novel reprojection technique for video-see-through augmented reality, which reduces the registration error between captured real-world videos and rendered virtual images. Instead of rendering the image plane locked to the virtual camera, CamWarp renders the image plane at the real-world position it was captured at, and compensates for potential artifacts. We conducted two experiments to evaluate the effectiveness of CamWarp. In the first experiment participants were asked to report subjective discomfort while moving their head in a pattern inspired by the ISO 9241-9 Fitts' Law task at different speeds while the video feed was rendered at varying frame rates. The results show that the technique can significantly reduce subjective levels of discomfort and cybersickness symptoms for all tested configurations. In the second experiment participants were asked to move physical objects on a projected path as quickly and precisely as possible. Results show a positive effect of CamWarp on speed and accuracy.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {9},
numpages = {7},
keywords = {latency compensation, cybersickness, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281541,
author = {Alallah, Fouad and Neshati, Ali and Sakamoto, Yumiko and Hasan, Khalad and Lank, Edward and Bunt, Andrea and Irani, Pourang},
title = {Performer vs. observer: whose comfort level should we consider when examining the social acceptability of input modalities for head-worn display?},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281541},
doi = {10.1145/3281505.3281541},
abstract = {The popularity of head-worn displays (HWD) technologies such as Virtual Reality (VR) and Augmented Reality (AR) headsets is growing rapidly. To predict their commercial success, it is essential to understand the acceptability of these new technologies, along with new methods to interact with them. In this vein, the evaluation of social acceptability of interactions with these technologies has received significant attention, particularly from the performer's (i.e., user's) viewpoint. However, little work has considered social acceptability concerns from observers' (i.e., spectators') perspective. Although HWDs are designed to be personal devices, interacting with their interfaces are often quite noticeable, making them an ideal platform to contrast performer and observer perspectives on social acceptability. Through two studies, this paper contrasts performers' and observers' perspectives of social acceptability interactions with HWDs under different social contexts. Results indicate similarities as well as differences, in acceptability, and advocate for the importance of including both perspectives when exploring social acceptability of emerging technologies. We provide guidelines for understanding social acceptability specifically from the observers' perspective, thus complementing our current practices used for understanding the acceptability of interacting with these devices.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {10},
numpages = {9},
keywords = {social acceptance, input modalities, augmented reality, HWDs},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281544,
author = {Congdon, Ben J. and Wang, Tuanfeng and Steed, Anthony},
title = {Merging environments for shared spaces in mixed reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281544},
doi = {10.1145/3281505.3281544},
abstract = {In virtual reality a real walking interface limits the extent of a virtual environment to our local walkable space. As local spaces are specific to each user, sharing a virtual environment with others for collaborative work or games becomes complicated. It is not clear which user's walkable space to prefer, or whether that space will be navigable for both users.This paper presents a technique which allows users to interact in virtual reality while each has a different walkable space. With this method mappings are created between pairs of environments. Remote users are then placed in the local environment as determined by the corresponding mapping.A user study was conducted with 38 participants. Pairs of participants were invited to collaborate on a virtual reality puzzle-solving task while in two different virtual rooms. An avatar representing the remote user was mapped into the local user's space. The results suggest that collaborative systems can be based on local representations that are actually quite different.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {11},
numpages = {8},
keywords = {virtual reality, virtual environments, virtual co-location, remote collaboration, planar map, mixed reality, head-mounted display, computer graphics, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281545,
author = {Hashiguchi, Satoshi and Mori, Shohei and Tanaka, Miho and Shibata, Fumihisa and Kimura, Asako},
title = {Perceived weight of a rod under augmented and diminished reality visual effects},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281545},
doi = {10.1145/3281505.3281545},
abstract = {We can use augmented reality (AR) and diminished reality (DR) in combination, in practice. However, to the best of our knowledge, there is no research on the validation of the cross-modal effects in AR and DR. Our research interest here is to investigate how this continuous visual changes between AR and DR would change our weight sensation of an object. In this paper, we built a system that can continuously extend and reduce the amount of visual entity of real objects using AR and DR renderings to confirm that users can perceive things heavier and lighter than they actually are in the same manner as SWI. Different from the existing research where either AR or DR visual effects were used, we validated one of cross-modal effects in the context of both continuous AR and DR visuo-haptic. Regarding the weight sensation, we found that such cross-modal effect can be approximated with a continuous linear relationship between the weight and length of real objects. Our experimental results suggested that the weight sensation is closely related to the positions of the center of gravity (CoG) and perceived CoG positions lie within the object's entity under the examined conditions.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {12},
numpages = {6},
keywords = {weight sensation, visuo-haptic system, sense of ownership, diminished reality, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281535,
author = {Nitta, Masashi and Sueishi, Tomohiro and Ishikawa, Masatoshi},
title = {Tracking projection mosaicing by synchronized high-speed optical axis control},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281535},
doi = {10.1145/3281505.3281535},
abstract = {Projectors, as information display devices, have improved substantially and to achieve both the wide range and high resolution is desired for the dynamic human gaze. However, a fixed projector has a trade-off between the angle of projection and a resolution with limited pixels. Conventional methods with dynamic optical axis control lack the potential speed of the devices. We propose a tracking projection mosaicing with a high-speed projector and a high-speed optical axis controller for a randomly moving position, such as the gaze. We also propose a synchronization strategy by queuing and alternating operations to reduce motion-based artifacts, which realize a high-quality static image projection during the dynamic optical axis control. We have experimentally validated the geometric and temporal consistency of the proposed synchronization method and have attempted a demonstration of the tracking projection mosaicing for the dynamically moving bright spot of a laser pointer.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {13},
numpages = {5},
keywords = {synchronization, projection-based augmented reality, high-speed visual feedback, high-speed projector, high-speed mirror},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281537,
author = {Miyamoto, Junpei and Koike, Hideki and Amano, Toshiyuki},
title = {Gaze navigation in the real world by changing visual appearance of objects using projector-camera system},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281537},
doi = {10.1145/3281505.3281537},
abstract = {This paper proposes a method for gaze navigation in the real world by projecting an image onto a real object and changing its appearance. In the proposed method, a camera captures an image of objects in the real world. Next all the pixels in the image but those in a specified region are slightly shifted to left and right. Then the obtained image is projected onto the original objects. As a result, the objects not in the specified region looks blurred. We conducted user experiments and showed that the users' gaze were navigated to the specified region.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {14},
numpages = {5},
keywords = {shift filter, procam, gaze navigation, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281509,
author = {Souchet, Alexis D. and Philippe, St\'{e}phanie and Zobel, Dimitri and Ober, Floriane and L\'{e}v\v{e}que, Aur\'{e}lien and Leroy, Laure},
title = {Eyestrain impacts on learning job interview with a serious game in virtual reality: a randomized double-blinded study},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281509},
doi = {10.1145/3281505.3281509},
abstract = {Purpose: This study explores eyestrain and its possible impacts on learning performances and quality of experience using different apparatuses and imaging. Materials and Methods: 69 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD) or a computer screen. The study was conducted according to a double-blinded protocol. Participants were randomly assigned to 3 groups: PC, HMD biocular and HMD stereoscopy (S3D). Participants played the game twice, allowing between group analyses. Eyestrain was assessed pre- and post-exposure on a chin-head rest with optometric measures. Learning traces were obtained in-game by registering response time and scores. Quality of experience was measured with questionnaires assessing Presence, Flow and Visual Comfort. Results: eyestrain was significantly higher with HMDs than PC based on Punctum Proximum of accommodation and visual acuity variables and tends to be higher with S3D. Learning was more efficient in HMDs conditions based on time for answering but the group with stereoscopy performed lower than the binocular imaging one. Quality of Experience was better based on visual discomfort with the PC condition than with HMDs. Conclusion: learning expected answers from a job interview is more efficient while using HMDs than a computer screen. However, eyestrain tends to be higher while using HMDs and S3D. The quality of experience was also negatively impacted with HMDs compared to computer screen. Not using S3D or lowering its impact should be explored to provide comfortable learning experience.1},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {15},
numpages = {12},
keywords = {virtual reality, stereoscopy, serious game, learning, head mounted display, eyestrain},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281511,
author = {Medeiros, Daniel and dos Anjos, Rafael K. and Mendes, Daniel and Pereira, Jo\~{a}o Madeiras and Raposo, Alberto and Jorge, Joaquim},
title = {Keep my head on my shoulders! why third-person is bad for navigation in VR},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281511},
doi = {10.1145/3281505.3281511},
abstract = {Head-Mounted Displays are useful to place users in virtual reality (VR). They do this by totally occluding the physical world, including users' bodies. This can make self-awareness problematic. Indeed, researchers have shown that users' feeling of presence and spatial awareness are highly influenced by their virtual representations, and that self-embodied representations (avatars) of their anatomy can make the experience more engaging. On the other hand, recent user studies show a penchant towards a third-person view of one's own body to seemingly improve spatial awareness. However, due to its unnaturality, we argue that a third-person perspective is not as effective or convenient as a first-person view for task execution in VR. In this paper, we investigate, through a user evaluation, how these perspectives affect task performance and embodiment, focusing on navigation tasks, namely walking while avoiding obstacles. For each perspective, we also compare three different levels of realism for users' representation, specifically a stylized abstract avatar, a mesh-based generic human, and a real-time point-cloud rendering of the users' own body. Our results show that only when a third-person perspective is coupled with a realistic representation, a similar sense of embodiment and spatial awareness is felt. In all other cases, a first-person perspective is still better suited for navigation tasks, regardless of representation.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {16},
numpages = {10},
keywords = {virtual reality, travel, full-body tracking, embodiment, avatar, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281513,
author = {Zhou, Yi and Cao, Mingjun and You, Jingdi and Meng, Ming and Wang, Yuehua and Zhou, Zhong},
title = {MR video fusion: interactive 3D modeling and stitching on wide-baseline videos},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281513},
doi = {10.1145/3281505.3281513},
abstract = {A major challenge facing camera networks today is how to effectively organizing and visualizing videos in the presence of complicated network connection and overwhelming and even increasing amount of data. Previous works focus on 2D stitching or dynamic projection to 3D models, such as panorama and Augmented Virtual Environment (AVE), and haven't given an ideal solution. We present a novel method of multiple video fusion in 3D environment, which produces a highly comprehensive imagery and yields a spatio-temporal consistent scene. User initially interact with a newly designed background model named video model to register and stitch videos' background frames offline. The method then fuses the offline results to render videos in a real time manner. We demonstrate our system on 3 real scenes, each of which contains dozens of wide-baseline videos. The experimental results show that, our 3D modeling interface developed with the our presented model and method can efficiently assist the users to seamlessly integrate videos by comparing to commercial-off-the-shelf software with less operating complexity and more accurate 3D environment. The stitching method proposed by us is much more robust against the position, orientation, attribute differences among videos than the start-of-the-art methods. More importantly, this study sheds light on how to use the 3D techniques to solve 2D problems in realistic and we validate its feasibility.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {17},
numpages = {11},
keywords = {video tourism, video popup, video fusion, immersive videos, augmented virtual environment},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281531,
author = {Walton, David R. and Steed, Anthony},
title = {Dynamic HDR environment capture for mixed reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281531},
doi = {10.1145/3281505.3281531},
abstract = {Rendering accurate and convincing virtual content into mixed reality (MR) scenes requires detailed illumination information about the real environment. In existing MR systems, this information is often captured using light probes [1, 8, 9, 17, 19--21], or by reconstructing the real environment as a preprocess [31, 38, 54]. We present a method for capturing and updating a HDR radiance map of the real environment and tracking camera motion in real time using a self-contained camera system, without prior knowledge about the real scene. The method is capable of producing plausible results immediately and improving in quality as more of the scene is reconstructed. We demonstrate how this can be used to render convincing virtual objects whose illumination changes dynamically to reflect the changing real environment around them.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {18},
numpages = {11},
keywords = {mixed reality, HDR, 3D reconstruction},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281538,
author = {John, Brendan and Raiturkar, Pallavi and Banerjee, Arunava and Jain, Eakta},
title = {An evaluation of pupillary light response models for 2D screens and VR HMDs},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281538},
doi = {10.1145/3281505.3281538},
abstract = {Pupil diameter changes have been shown to be indicative of user engagement and cognitive load for various tasks and environments. However, it is still not the preferred physiological measure for applied settings. This reluctance to leverage the pupil as an index of user engagement stems from the problem that in scenarios where scene brightness cannot be controlled, the pupil light response confounds the cognitive-emotional response. What if we could predict the light response of an individual's pupil, thus creating the opportunity to factor it out of the measurement? In this work, we lay the groundwork for this research by evaluating three models of pupillary light response in 2D, and in a virtual reality (VR) environment. Our results show that either a linear or an exponential model can be fit to an individual participant with an easy-to-use calibration procedure. This work opens several new research directions in VR relating to performance analysis and inspires the use of eye tracking beyond gaze as a pointer and foveated rendering.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {19},
numpages = {11},
keywords = {virtual reality, videos, pupil dilation, light response, eyetracking},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281546,
author = {Roxas, Menandro and Hori, Tomoki and Fukiage, Taiki and Okamoto, Yasuhide and Oishi, Takeshi},
title = {Occlusion handling using semantic segmentation and visibility-based rendering for mixed reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281546},
doi = {10.1145/3281505.3281546},
abstract = {Real-time occlusion handling is a major problem in outdoor mixed reality system because it requires great computational cost mainly due to the complexity of the scene. Using only segmentation, it is difficult to accurately render a virtual object occluded by complex objects such as vegetation. In this paper, we propose a novel occlusion handling method for real-time mixed reality given a monocular image and an inaccurate depth map. We modify the intensity of the overlayed CG object based on the texture of the underlying real scene using visibility-based rendering. To determine the appropriate level of visibility, we use CNN-based semantic segmentation and assign labels to the real scene based on the complexity of object boundary and texture. Then we combine the segmentation results and the foreground probability map from the depth image to solve the appropriate blending parameter for visibility-based rendering. Our results show improvement in handling occlusions for inaccurate foreground segmentation compared to existing blending-based methods.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {20},
numpages = {8},
keywords = {semantic segmentation, occlusion handling, mixed reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281523,
author = {Podkosova, Iana and Kaufmann, Hannes},
title = {Co-presence and proxemics in shared walkable virtual environments with mixed colocation},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281523},
doi = {10.1145/3281505.3281523},
abstract = {The purpose of the experiment presented in this paper is to investigate co-presence and locomotory patterns in a walkable shared virtual environment. In particular, trajectories of users that use a walkable tracking space alone are compared to those of users who use the tracking space in pairs. Co-presence, in a sense of perception of another person being present in the same virtual space is analyzed through subjective responses and behavioral markers. The results indicate that both perception and proxemics in relation to co-located and distributed players differ. The effect on the perception is however mitigated if participants do not collide with the avatars of distributed co-players.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {21},
numpages = {11},
keywords = {shared virtual environments, co-presence},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281527,
author = {Moustafa, Fares and Steed, Anthony},
title = {A longitudinal study of small group interaction in social virtual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281527},
doi = {10.1145/3281505.3281527},
abstract = {Now that high-end consumer phones can support immersive virtual reality, we ask whether social virtual reality is a promising medium for supporting distributed groups of users. We undertook an exploratory in-the-wild study using Samsung Gear VR headsets to see how existing social groups that had become geographically dispersed could use VR for collaborative activities. The study showed a strong propensity for users to feel present and engaged with group members. Users were able to bring group behaviors into the virtual world. To overcome some technical limitations, they had to create novel forms of interaction. Overall, the study found that users experience a range of emotional states in VR that are broadly similar to those that they would experience face-to-face in the same groups. The study highlights the transferability of existing social group dynamics in VR interactions but suggests that more work would need to be done on avatar representations to support some intimate conversations.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {22},
numpages = {10},
keywords = {virtual reality, social VR, in-the-wild study, avatar representation, affective states},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281529,
author = {Parger, Mathias and Mueller, Joerg H. and Schmalstieg, Dieter and Steinberger, Markus},
title = {Human upper-body inverse kinematics for increased embodiment in consumer-grade virtual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281529},
doi = {10.1145/3281505.3281529},
abstract = {Having a virtual body can increase embodiment in virtual reality (VR) applications. However, comsumer-grade VR falls short of delivering sufficient sensory information for full-body motion capture. Consequently, most current VR applications do not even show arms, although they are often in the field of view. We address this shortcoming with a novel human upper-body inverse kinematics algorithm specifically targeted at tracking from head and hand sensors only. We present heuristics for elbow positioning depending on the shoulder-to-hand distance and for avoiding reaching unnatural joint limits. Our results show that our method increases the accuracy compared to general inverse kinematics applied to human arms with the same tracking input. In a user study, participants preferred our method over displaying disembodied hands without arms, but also over a more expensive motion capture system. In particular, our study shows that virtual arms animated with our inverse kinematics system can be used for applications involving heavy arm movement. We demonstrate that our method can not only be used to increase embodiment, but can also support interaction involving arms or shoulders, such as holding up a shield.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {23},
numpages = {10},
keywords = {virtual reality, presence, motion capture, inverse kinematics, embodiment, animation},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281530,
author = {Skarbez, Richard and Brooks, Frederick P. and Whitton, Mary C.},
title = {Immersion and coherence in a stressful virtual environment},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281530},
doi = {10.1145/3281505.3281530},
abstract = {We report on the design and results of two experiments investigating Slater's Place Illusion (PI) and Plausibility Illusion (Psi) in a virtual visual cliff environment. PI (the illusion of being in a place) and Psi (the illusion that the depicted events are actually happening) were proposed by Slater as orthogonal components of virtual experience which contribute to realistic response in a VE. To that end, we identified characteristics of a virtual reality experience that we expected to influence one or the other of PI and Psi. We designed two experiments in which each participant experienced a given VE in one of four conditions chosen from a 2\texttimes{}2 design: high or low levels of PI-eliciting characteristics (that is, immersion) and high or low levels of Psi-eliciting characteristics. Following Skarbez, we use the term "coherence" for those characteristics which contribute to Psi, parallel to the use of "immersion" for characteristics that contribute to PI. We collected both questionnaire-based and physiological metrics. Several existing presence questionnaires could not reliably distinguish the effects of PI from those of Psi. They did, however, indicate that high levels of PI-eliciting characteristics and Psi-eliciting characteristics together result in higher presence, compared any of the other three conditions. This suggests that "breaks in PI" and "breaks in Psi" belong to a broader category of "breaks in experience," any of which result in a degraded user experience. Participants' heart rates, however, responded markedly differently in the two Psi conditions; no such difference was observed across the PI conditions. This indicates that a VE that exhibits unusual or confusing behavior can cause stress in a user that affects physiological responses, and that one must take care to eliminate such confusing behaviors if one is using physiological measurement as a proxy for subjective experience in a VE.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {24},
numpages = {11},
keywords = {virtual reality, user studies, presence, plausibility illusion (Psi), place illusion(PI), physiological metrics, immersion, coherence},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281533,
author = {Lee, Myungho and Norouzi, Nahal and Bruder, Gerd and Wisniewski, Pamela J. and Welch, Gregory F.},
title = {The physical-virtual table: exploring the effects of a virtual human's physical influence on social interaction},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281533},
doi = {10.1145/3281505.3281533},
abstract = {In this paper, we investigate the effects of the physical influence of a virtual human (VH) in the context of face-to-face interaction in augmented reality (AR). In our study, participants played a tabletop game with a VH, in which each player takes a turn and moves their own token along the designated spots on the shared table. We compared two conditions as follows: the VH in the virtual condition moves a virtual token that can only be seen through AR glasses, while the VH in the physical condition moves a physical token as the participants do; therefore the VH's token can be seen even in the periphery of the AR glasses. For the physical condition, we designed an actuator system underneath the table. The actuator moves a magnet under the table which then moves the VH's physical token over the surface of the table. Our results indicate that participants felt higher co-presence with the VH in the physical condition, and participants assessed the VH as a more physical entity compared to the VH in the virtual condition. We further observed transference effects when participants attributed the VH's ability to move physical objects to other elements in the real world. Also, the VH's physical influence improved participants' overall experience with the VH. We discuss potential explanations for the findings and implications for future shared AR tabletop setups.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {25},
numpages = {11},
keywords = {virtual humans, mediated physicality, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281542,
author = {Kolkmeier, Jan and Harmsen, Emiel and Giesselink, Sander and Reidsma, Dennis and Theune, Mari\"{e}t and Heylen, Dirk},
title = {With a little help from a holographic friend: the OpenIMPRESS mixed reality telepresence toolkit for remote collaboration systems},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281542},
doi = {10.1145/3281505.3281542},
abstract = {Remote mixed reality (MR) collaboration systems allow for multimodal, real-time support from remote experts. We present our open toolkit that provides a flexible end-to-end solution for building such systems using off-the-shelf hardware. From related work, three core design aspects have been identified: 1) the independence of the viewpoint that the visitor (the remote expert) can take in relation to position and viewpoint of the visitee, 2) the immersiveness of the presentation technology for visitor and visitee, and 3) the extent to which the visitor's body is represented in the visitee's environment. This paper describes the implementation of our system, which includes these aspects. In a study aimed at validating whether we implemented these core aspects to good effect, conducted with a collaborative puzzle application built with our toolkit, we examine how variations of these aspects contribute to usability, performance and social presence related metrics.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {26},
numpages = {11},
keywords = {telepresence, mixed reality, embodiment, collaboration},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281543,
author = {George, Ceenu and Spitzer, Michael and Hussmann, Heinrich},
title = {Training in IVR: investigating the effect of instructor design on social presence and performance of the VR user},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281543},
doi = {10.1145/3281505.3281543},
abstract = {We investigate instructor representations (IRs) in the context of virtual trainings with head mounted displays (HMD). Despite the recently increased industry and research focus on virtual training in immersive virtual reality (IVR), the effect of IRs on the performer (VR user) has received little attention. We present the results of a study (N=33), evaluating the effect of three IRs - webcam, avatar and sound-only - on social presence (SP) and performance (PE) of the VR user during task completion. Our results show that instructor representation has an effect on SP and that, contrary to our assumption based on prior work, it affects performance negatively.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {27},
numpages = {5},
keywords = {social presence, instructor design, immersive virtual reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281508,
author = {He, Linjia and Li, Hongsong and Xue, Tong and Sun, Deyuan and Zhu, Shoulun and Ding, Gangyi},
title = {Am I in the theater? usability study of live performance based virtual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281508},
doi = {10.1145/3281505.3281508},
abstract = {Duplicating the audience experience of an art performance with VR technology is a promising VR application, which is considered to provide better viewer experience than the conventional video. As various forms of art performances are recorded by the panoramic camera and broadcasted on the Internet, the impact of this new VR-based media to the viewers needs to be systematically studied. In this work, a two-level usability framework is proposed, which combines the traditional concepts of presence and the quality evaluation of art performances, aiming to systematically study the usability of such VR application. Both the conventional video and the panoramic video of a theatre performance were captured simultaneously, and were replayed to two groups of viewers in a cinematic setup and through an HMD respectively. The psychological measurement methods, including the questionnaire and the interview, as well as the psychophysical measurement methods, including the EEG and the motion capture techniques were both used in the study. The results show that the such VR application duplicates the live performance better by providing a higher sense of presence, higher engagement levels, and stronger desire to see live performance. For visual intensive performance contents, the new VR-based media can provide a better user experience. The future development of the new media forms based on the panoramic video technique could benefit from this work.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {11},
keywords = {virtual reality, viewer experience, usability evaluation, live performance, EEG},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281515,
author = {Nguyen, Anh and Kunz, Andreas},
title = {Discrete scene rotation during blinks and its effect on redirected walking algorithms},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281515},
doi = {10.1145/3281505.3281515},
abstract = {Moving through a virtual environment (VE) by real walking is beneficial to user immersion, feeling of presence and way finding. However, the available physical spaces are of limited size and usually much smaller than the VE. One solution to this problem is using redirection techniques (RDTs). While the focus of existing research has been mostly on continuous RDTs, work on discrete RDTs is still limited.In this paper, we present our research results on the discrete rotation of a virtual scene during walking. A study with 14 subjects was conducted to identify the detection threshold of the scene rotation in two conditions: during blinking and when eyes are open. Results showed that on average, users failed to detect a scene rotation of 9.1 degrees during blinking, as compared to 2.4 degrees when eyes are open. Simulations were then performed to investigate the effects of incorporating discrete scene orientation during blinks into existing algorithms such as steer-to-center and steer-to-orbit when different predefined paths are followed. Results showed that on average the number of resets is reduced by 13\%, and the minimum space required for encountering no reset is reduced by 20\%. A reset technique was also proposed and shown to give better performance than the existing two-one turn reset technique.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {29},
numpages = {10},
keywords = {visual suppression, virtual reality, redirected walking, discrete rotation, blink},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281519,
author = {Hong, Yang and MacQuarrie, Andrew and Steed, Anthony},
title = {The effect of chair type on users' viewing experience for 360-degree video},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281519},
doi = {10.1145/3281505.3281519},
abstract = {The consumption of 360-degree videos with head-mounted displays (HMDs) is increasing rapidly. A large number of HMD users watch 360-degree videos at home, often on non-swivel seats; however videos are frequently designed to require the user to turn around. This work explores how the difference in users' chair type might influence their viewing experience. A between-subject experiment was conducted with 41 participants. Three chair conditions were used: fixed, half-swivel and full-swivel. A variety of measures were explored using eye-tracking, questionnaires, tasks and semi-structured interviews. Results suggest that the fixed and half-swivel chairs discouraged exploration for certain videos compared with the full-swivel chair. Additionally, participants in the fixed chair had worse spatial awareness and greater concern about missing something for certain video than those in the full-swivel chair. No significant differences were found in terms of incidental memory, general engagement and simulator sickness among the three chair conditions. Furthermore, thematic analysis of post-experiment interviews revealed four themes regarding the restrictive chairs: physical discomfort, difficulty following moving objects, reduced orientation and guided attention. Based on the findings, practical implications, limitations and future work are discussed.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {30},
numpages = {11},
keywords = {user study, panoramic video, cinematic virtual reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281524,
author = {Bera, Aniket and Randhavane, Tanmay and Kubin, Emily and Shaik, Husam and Gray, Kurt and Manocha, Dinesh},
title = {Data-driven modeling of group entitativity in virtual environments},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281524},
doi = {10.1145/3281505.3281524},
abstract = {We present a data-driven algorithm to model and predict the socio-emotional impact of groups on observers. Psychological research finds that highly entitative i.e. cohesive and uniform groups induce threat and unease in observers. Our algorithm models realistic trajectory-level behaviors to classify and map the motion-based entitativity of crowds. This mapping is based on a statistical scheme that dynamically learns pedestrian behavior and computes the resultant entitativity induced emotion through group motion characteristics. We also present a novel interactive multi-agent simulation algorithm to model entitative groups and conduct a VR user study to validate the socio-emotional predictive power of our algorithm. We further show that model-generated high-entitativity groups do induce more negative emotions than low-entitative groups.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {10},
keywords = {virtual reality, pedestrian behavior, motion model, group dynamics, data driven simulation, crowd simulation},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281528,
author = {Nuernberger, Benjamin and H\"{o}llerer, Tobias and Turk, Matthew},
title = {Hybrid orbiting-to-photos in 3D reconstructed visual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281528},
doi = {10.1145/3281505.3281528},
abstract = {Virtually navigating through photos from a 3D image-based reconstruction has recently become very popular in many applications. In this paper, we consider a particular virtual travel maneuver that is important for this type of virtual navigation---orbiting to photos that can see a point-of-interest (POI). The main challenge with this particular type of orbiting is how to give appropriate feedback to the user regarding the existence and information of each photo in 3D while allowing the user to manipulate three degrees-of-freedom (DoF) for orbiting around the POI. We present a hybrid approach that combines features from two baselines---proxy plane and thumbnail approaches. Experimental results indicate that users rated our hybrid approach more favorably for several qualitative questionnaire statements, and that the hybrid approach is preferred over both baselines for outdoor scenes.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {32},
numpages = {10},
keywords = {virtual navigation, orbiting, 3d reconstructed visual reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281534,
author = {Han, Sangyoon and Bhardwaj, Amit and Choi, Seungmoon},
title = {Automatic transfer of musical mood into virtual environments},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281534},
doi = {10.1145/3281505.3281534},
abstract = {This paper presents a method that automatically transforms a virtual environment (VE) according to the mood of input music. We use machine learning to extract a mood from the music. We then select images exhibiting the mood and transfer their styles to the textures of objects in the VE photorealistically or artistically. Our user study results indicate that our method is effective in transferring valence-related aspects, but not arousal-related ones. Our method can still provide novel experiences in virtual reality and speed up the production of VEs by automating its procedure.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {33},
numpages = {5},
keywords = {virtual environment, transfer, music, mood, affect},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281536,
author = {Wang, Congzhi and Dogaru, Oana A. and Strandholt, Patrick L. and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
title = {Step aside: an initial exploration of gestural input for lateral movement during walking-in-place locomotion},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281536},
doi = {10.1145/3281505.3281536},
abstract = {Walking-in-place (WIP) techniques provide users with a relatively natural way of walking in virtual reality. However, previous research has primarily focused on WIP during forward movement and tasks involving turning. Thus, little is known about what gestures to use in combination with WIP in order to enable sidestepping. This paper presents two user studies comparing three different types of gestures based on movement of the hip, leaning of the torso, and actual sidesteps. The first study focuses on purely lateral movement while the second involves both forward and lateral movement. The results of both studies suggest that leaning yielded significantly more natural walking experiences and this gesture also produced significantly less positional drift.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {34},
numpages = {5},
keywords = {virtual reality, travel, locomotion},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281507,
author = {Han, Ping-Hsuan and Chen, Yang-Sheng and Lee, Kong-Chang and Wang, Hao-Cheng and Hsieh, Chiao-En and Hsiao, Jui-Chun and Chou, Chien-Hsing and Hung, Yi-Ping},
title = {Haptic around: multiple tactile sensations for immersive environment and interaction in virtual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281507},
doi = {10.1145/3281505.3281507},
abstract = {In this paper, we present Haptic Around, a hybrid-haptic feedback system, which utilizes fan, hot air blower, mist creator and heat light to recreate multiple tactile sensations in virtual reality for enhancing the immersive environment and interaction. This system consists of a steerable haptic device rigged on the top of the user head and a handheld device also with haptics feedbacks to simultaneously provide tactile sensations to the users in a 2m x 2m space. The steerable haptic device can enhance the immersive environment for providing full body experience, such as heat in the desert or cold in the snow mountain. Additionally, the handheld device can enhance the immersive interaction for providing partial body experience, such as heating the iron or quenching the hot iron. With our system, the users can perceive visual, auditory and haptic when they are moving around in virtual space and interacting with virtual object. In our study, the result has shown the potential of the hybrid-haptic feedback system, which the participants rated the enjoyment, realism, quality, immersion higher than the other.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {35},
numpages = {10},
keywords = {virtual reality, multiple tactile sensation, immersive experience, immersive environment, haptics},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281510,
author = {Tran, Tanh Quang and Tran, Thanh Dat Ngoc and Nguyen, Tam Duy and Regenbrecht, Holger and Tran, Minh-Triet},
title = {Can we perceive changes in our moving speed: a comparison between directly and indirectly powering the locomotion in virtual environments},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281510},
doi = {10.1145/3281505.3281510},
abstract = {Many categories of the illusion of self-motion have been widely studied with the potential support of virtual reality. However, the effects of directly and indirectly powering the movement on the possibility of perceiving changes in moving speed and their relationship with sensory feedback on users' speed change perception have not been investigated before. In this paper, we present the results of our user study on the difference in perceiving changes in moving speed between two different movement techniques: "pedaling" and "throttling". We also explore the effects of different velocity gains, accelerations and speeds of airflow, and their interactions with the movement techniques on users' perception of speed changes in addition to user performance and perception. We built a bike simulator that supports both of the movement techniques and provides sensory feedback. In general, "pedaling" gave users more possibility to perceive changes in moving velocity than "throttling".},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {36},
numpages = {10},
keywords = {virtual reality, speed perception, locomotion, bike simulator},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281518,
author = {Danieau, Fabien and Guillotel, Philippe and Dumas, Olivier and Lopez, Thomas and Leroy, Bertrand and Mollet, Nicolas},
title = {HFX studio: haptic editor for full-body immersive experiences},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281518},
doi = {10.1145/3281505.3281518},
abstract = {Current virtual reality systems enable users to explore virtual worlds, fully embodied in avatars. This new type of immersive experience requires specific authoring tools. The traditional ones used in the movie and the video games industries were modified to support immersive visual and audio content. However, few solutions exist to edit haptic content, especially when the whole user's body is involved. To tackle this issue we propose HFX Studio, a haptic editor based on haptic perceptual models. Three models of pressure, vibration and temperature were defined to allow the spatialization of haptic effects on the user's body. These effects can be designed directly on the body (egocentric approach), or specified as objects of the scene (allocentric approach). The perceptual models are also used to describe capabilities of haptic devices. This way the created content is generic, and haptic feedback is rendered on the available devices. The concept has been implemented with the Unity®game engine, a tool already used in VR production. A qualitative pilot user study was conducted to analyze the usability of our tool with expert users. Results shows that the edition of haptic feedback is intuitive for these users.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {37},
numpages = {9},
keywords = {immersive experience, haptics, full body, edition},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281522,
author = {Pan, Dandan and Xu, Qing and Ma, Shiqiang and Zhang, Kunlong},
title = {The impact of fear of the sea on working memory performance: a research based on virtual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281522},
doi = {10.1145/3281505.3281522},
abstract = {The sea has been manifested to cause the emotion of fear to people when it comes to a very depth, especially to those who have thalassophobia. Many people have to work in the sea while nearly no research on influence of fear of the sea to cognition has been carried out. This study explores the impact of fear of the sea induced by immersive virtual reality on working memory which is a cognitive system with a limited capacity. Participants were required to complete n-back working memory task of three difficulty levels in the non-emotional environment and the undersea environment respectively by means of virtual reality. Pupil diameter changes were recorded along with the task performance. In addition to reaction times and accuracy (correctly press a button in response to targets) as two task performance indices used in most researches, the commission errors (incorrectly press a button in response to non-targets) and omission errors (incorrectly do not press a button in response to targets) were also differentiated herein. The results of the study indicated that the virtual undersea environment did induce the emotion of fear. As for the task performance, except that the performance of low-level task did not differ much between the two environments, the fear of the sea increased the accuracy of the medium level n-back task but decreased it of high-level n-back task. Result of omission errors was just the opposite and commission errors were increased in both levels of task. The findings, including the positive role of a moderate level of fear of the sea in the performance of working memory task, make a lot of sense for future cognitive work in the sea.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {38},
numpages = {9},
keywords = {working memory, virtual reality, task performance, fear of the sea},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281547,
author = {Ferdous, Sharif Mohammad Shahnewaz and Chowdhury, Tanvir Irfan and Arafat, Imtiaz Muhammad and Quarles, John},
title = {Investigating the reason for increased postural instability in virtual reality for persons with balance impairments},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281547},
doi = {10.1145/3281505.3281547},
abstract = {The objective of this study is to investigate how different visual components of Virtual Reality (VR), such as field of view, frame rate, and display resolution affect postural stability in VR. Although previous studies identified these visual components as some of the primary factors that differ significantly in VR from reality, the effect of each component on postural stability is yet unknown. While most people experience postural instability in VR, it is worse for people with balance impairments (BIs). This may be because they depend more on their visual cues to maintain postural stability. We conducted a study with ten people with balance impairments due to Multiple Sclerosis (MS) and seven people without balance impairments to investigate the effect of different visual components on postural stability. In each condition, we varied one of the visual components and kept all other components fixed. Each participant explored the virtual environment (VE) in a controlled fashion to make sure that the effect of the visual components was consistent for all participants. Results from our study suggest that for people with BIs, decreased field of view and decreased frame rate have significant negative effects on postural stability, but the effect of display resolution is inconclusive. However, for people without BIs, there were no significant differences for any of the visual components. Therefore, VR systems targeting people with balance impairments should focus on improving field of view and frame rate before improving display resolution.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {39},
numpages = {7},
keywords = {virtual reality, postural stability, head-mounted display, balance, accessibility},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281506,
author = {Kono, Michinari and Miyaki, Takashi and Rekimoto, Jun},
title = {In-pulse: inducing fear and pain in virtual experiences},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281506},
doi = {10.1145/3281505.3281506},
abstract = {Researchers have attempted to increase the realism of virtual reality (VR) applications in many ways. Combinations of the visual, auditory and haptic feedback have successfully simulated experiences in VR, however, multimedia contents may also stimulate emotions. In this paper, we especially paid attention to negative emotions that may be perceived in such experiences (e.g., fear). We hypothesized that volunteering, visual, mechanical, and electrical feedback may induce negative emotional feedback to users. In-Pulse is a novel system and approach to explore the potential of bringing this emotional feedback to users. We designed a head-mounted display (HMD) combined with mechanical and electrical muscle stimulation (EMS) actuators. A user study was performed to explore the effect of our approaches with combinations with VR contents. The results suggest that mechanical actuators and EMS can improve the experience of virtual experiences.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {40},
numpages = {5},
keywords = {wearables, virtual reality, pain, head-mounted display, fear, emotion, electrical muscle stimulation},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281516,
author = {Chen, Taizhou and Wu, Yi-Shiun and Zhu, Kening},
title = {Investigating different modalities of directional cues for multi-task visual-searching scenario in virtual reality},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281516},
doi = {10.1145/3281505.3281516},
abstract = {In this study, we investigated and compared the effectiveness of visual, auditory, and vibrotactile directional cues on multiple simultaneous visual-searching tasks in an immersive virtual environment. Effectiveness was determined by the task-completion time, the range of head movement, the accuracy of the identification task, and the perceived workload. Our experiment showed that the on-head vibrotactile display can effectively guide users towards virtual visual targets, without affecting their performance on the other simultaneous tasks, in the immersive VR environment. These results can be applied to numerous applications (e.g. gaming, driving, and piloting) in which there are usually multiple simultaneous tasks, and the user experience and performance could be vulnerable.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {41},
numpages = {5},
keywords = {visual, virtual reality, vibration, multi-task, directional cue, auditory},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3281505.3281520,
author = {Shin, Sunghwan and Choi, Seungmoon},
title = {Effects of haptic texture rendering modalities on realism},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281520},
doi = {10.1145/3281505.3281520},
abstract = {In haptics, two major modalities, force and vibration, are used to model real textures and recreate them in a virtual environment. This paper compares the perceptual advantages and disadvantages between the two approaches by a user study. In particular, the perceptual similarity of a virtual texture to a real texture is rated using five criteria of geometry, roughness, hardness, friction and overall similarity. These categorical comparisons allowed us to provide general guidelines to appropriate uses of the two approaches.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {42},
numpages = {5},
keywords = {vibrotactile feedback, texture, perception, haptics, force feedback},
location = {Tokyo, Japan},
series = {VRST '18}
}

