@inproceedings{10.1145/2993369.2993406,
author = {Kim, Jae-Woo and Lee, Kang-Kyu and Ryu, Je-Ho and Kim, Jong-Ok},
title = {Localized color correction for optical see-through displays via weighted linear regression},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993406},
doi = {10.1145/2993369.2993406},
abstract = {Visual consistency in augmented reality displays requires truthful color reproduction of virtual images. However, the color distortion of Optical See-Through Displays hinders truthful color reproduction. We propose a color correction method for Optical See-Through Displays with three contributions. First, we handle non-linearity of color distortion by localized regression. Second, we model the color distortion in CIE XYZ domain, a device-independent representation of color, based on color measurements. This supports the locally linear modeling of color distortion. Finally, we introduce Hue-constrained gamut mapping for color correction. Experimental results validate the three contributions by showing critically meaningful performance gain.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {11–14},
numpages = {4},
keywords = {local linear regression, gamut mapping, color distortion correction, OST-HMD, CIE XYZ color space},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993393,
author = {Wang, Zeyu and Jin, Xiaohan and Xue, Fei and Li, Renju and Zha, Hongbin and Ikeuchi, Katsushi},
title = {Perceptual enhancement for stereoscopic videos based on horopter consistency},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993393},
doi = {10.1145/2993369.2993393},
abstract = {Audience discomfort, such as eye strain and dizziness, is one of the urgent issues that virtual reality and 3D movie technologies should tackle. Except for inappropriate horizontal and vertical disparity, one major problem is that people's binocular vergence and focal length in the cinema remain inconsistent from normal visual habits. Psychologists discovered the horopter and Panum's fusional area to describe zero-disparity points projected on the retinas based on accommodation-convergence consistency. In this paper, inspired by these concepts, we propose a stereoscopic effect correction system for perceptual enhancement according to fixated region and scene information. As a preprocessing step, tracking and stereo matching algorithms are implemented to prepare cues for further transformation in 3D space. Then in order to accomplish certain visual effects, we describe a geometric framework for disparity refinement and image warping based on parameter adjustment of the virtual stereoscopic rig. For evaluation, subjective experiments have been conducted to prove the effectiveness of our method. Therefore, our work provides a possibility to improve the audience experience from a formerly underexplored perspective.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {15–18},
numpages = {4},
keywords = {virtual rig modification, stereoscopic videos, perceptual enhancement, image warping, horopter consistency},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993407,
author = {Greenwald, Scott W. and Loreti, Luke and Funk, Markus and Zilberman, Ronen and Maes, Pattie},
title = {Eye gaze tracking with google cardboard using purkinje images},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993407},
doi = {10.1145/2993369.2993407},
abstract = {Mobile phone-based Virtual Reality (VR) is rapidly growing as a platform for stereoscopic 3D and non-3D digital content and applications. The ability to track eye gaze in these devices would be a tremendous opportunity on two fronts: firstly, as an interaction technique, where interaction is currently awkward and limited, and secondly, for studying human visual behavior. We propose a method to add eye gaze tracking to these existing devices using their on-board display and camera hardware, with a minor modification to the headset enclosure. We present a proof-of-concept implementation of the technique and show results demonstrating its feasibility. The software we have developed will be made available as open source to benefit the research community.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {19–22},
numpages = {4},
keywords = {virtual reality, purkinje images, low cost, eye tracking},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993376,
author = {Geronazzo, Michele and Fantin, Jacopo and Sorato, Giacomo and Baldovino, Guido and Avanzini, Federico},
title = {Acoustic selfies for extraction of external ear features in mobile audio augmented reality},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993376},
doi = {10.1145/2993369.2993376},
abstract = {Virtual and augmented realities are expected to become more and more important in everyday life in the next future; the role of spatial audio technologies over headphones will be pivotal for application scenarios which involve mobility. This paper introduces the SelfEar project, aimed at low-cost acquisition and personalization of Head-Related Transfer Functions (HRTFs) on mobile devices. This first version focuses on capturing individual spectral features which characterize external ear acoustics, through a self-adjustable procedure which guides users in collecting such information: their mobile device must be held with the stretched arm and positioned at several specific elevation points; acoustic data are acquired by an audio augmented reality headset which embeds a pair of microphones at listener ear-canals. A preliminary measurement session assesses the ability of the system to capture spectral features which are crucial for elevation perception. Moreover, a virtual experiment using a computational auditory model predicts clear vertical localization cues in the measured features.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {23–26},
numpages = {4},
keywords = {mobile augmented reality, headphones, head-related transfer function, computational auditory model, binaural audio},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993381,
author = {Waltemate, Thomas and Senna, Irene and H\"{u}lsmann, Felix and Rohde, Marieke and Kopp, Stefan and Ernst, Marc and Botsch, Mario},
title = {The impact of latency on perceptual judgments and motor performance in closed-loop interaction in virtual reality},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993381},
doi = {10.1145/2993369.2993381},
abstract = {Latency between a user's movement and visual feedback is inevitable in every Virtual Reality application, as signal transmission and processing take time. Unfortunately, a high end-to-end latency impairs perception and motor performance. While it is possible to reduce feedback delay to tens of milliseconds, these delays will never completely vanish. Currently, there is a gap in literature regarding the impact of feedback delays on perception and motor performance as well as on their interplay in virtual environments employing full-body avatars. With the present study at hand, we address this gap by performing a systematic investigation of different levels of delay across a variety of perceptual and motor tasks during full-body action inside a Cave Automatic Virtual Environment. We presented participants with their virtual mirror image, which responded to their actions with feedback delays ranging from 45 to 350 ms. We measured the impact of these delays on motor performance, sense of agency, sense of body ownership and simultaneity perception by means of psychophysical procedures. Furthermore, we looked at interaction effects between these aspects to identify possible dependencies. The results show that motor performance and simultaneity perception are affected by latencies above 75 ms. Although sense of agency and body ownership only decline at a latency higher than 125 ms, and deteriorate for a latency greater than 300 ms, they do not break down completely even at the highest tested delay. Interestingly, participants perceptually infer the presence of delays more from their motor error in the task than from the actual level of delay. Whether or not participants notice a delay in a virtual environment might therefore depend on the motor task and their performance rather than on the actual delay.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {27–35},
numpages = {9},
keywords = {body ownership, full-body motion capture, latency, sense of agency, simultaneity perception, virtual mirror},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993375,
author = {van Waveren, J. M. P.},
title = {The asynchronous time warp for virtual reality on consumer hardware},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993375},
doi = {10.1145/2993369.2993375},
abstract = {To help create a true sense of presence in a virtual reality experience, a so called "time warp" may be used. This time warp does not only correct for the optical aberration of the lenses used in a virtual reality headset, it also transforms the stereoscopic images based on the very latest head tracking information to significantly reduce the motion-to-photon delay (or end-to-end latency). The time warp operates as close as possible to the display refresh, retrieves updated head tracking information and transforms a stereoscopic pair of images from representing a view at the time it was rendered, to representing the correct view at the time it is displayed. When run asynchronously to the stereoscopic rendering, the time warp can be used to increase the perceived frame rate and to smooth out inconsistent frame rates. Asynchronous operation can also improve the overall graphics hardware utilization by not requiring the stereoscopic rendering to be synchronized with the display refresh cycle. However, on today's consumer hardware it is challenging to implement a high quality time warp that is fast, has predictable latency and throughput, and runs asynchronously. This paper discusses the various challenges and the different trade-offs that need to be considered when implementing an asynchronous time warp on consumer hardware.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {37–46},
numpages = {10},
keywords = {image warping, latency, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993402,
author = {Stauffert, Jan-Philipp and Niebling, Florian and Latoschik, Marc Erich},
title = {Towards comparable evaluation methods and measures for timing behavior of virtual reality systems},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993402},
doi = {10.1145/2993369.2993402},
abstract = {A low latency is a fundamental timeliness requirement to reduce the potential risks of cyber sickness and to increase effectiveness, efficiency, and user experience of Virtual Reality Systems. The effects of uniform latency degradation based on mean or worst-case values are well researched. In contrast, the effects of latency jitter, the distribution pattern of latency changes over time has largely been ignored so far although today's consumer VR systems are extremely vulnerable in this respect. We investigate the applicability of the Walsh, generalized ESD, and the modified z-score test for the detection of outliers as one central latency distribution aspect. The tests are applied to well defined test cases mimicking typical timing behavior expected from concurrent architectures of today. We introduce accompanying graphical visualization methods to inspect, analyze and communicate the latency behavior of VR systems beyond simple mean or worst-case values. As a result, we propose a stacked modified z-score test for more detailed analysis.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {47–50},
numpages = {4},
keywords = {cyber sickness, latency, outlier, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993383,
author = {Arafat, Imtiaz Muhammad and Ferdous, Sharif Mohammad Shahnewaz and Quarles, John},
title = {The effects of cybersickness on persons with multiple sclerosis},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993383},
doi = {10.1145/2993369.2993383},
abstract = {Cybersickness is commonly experienced by the users in immersive Virtual Environments (VE). It has symptoms similar to Motion Sickness, such as dizziness, nausea etc. Although there have been many cybersickness experiments conducted with persons without disabilities, persons with disabilities, such as Multiple Sclerosis (MS), have been minimally studied. This is an important area of research because cybersickness could have negative effects on virtual rehabilitation effectiveness and the accessibility of VEs. For this experiment, we recruited 16 participants - 8 persons with MS and 8 persons without MS from similar demographics (e.g. age, race). Two participants from population without MS could not complete the experiment due to severe cybersickness. We asked each participant to experience a VE. We collected Galvanic Skin response (GSR) data before and during VR exposure; GSR is commonly used as an objective measure of cybersickness. Also, Simulator Sickness Questionnaire (SSQ) feedback was recorded before and after the experiment. SSQ results show that the VE induced cybersickness in the participants. The GSR data suggests that the cybersickness may have induced similar physiological changes in participants with MS as participants without MS, albeit with greater variability in participants without MS. However, participants with MS had significantly lower GSR during VR exposure. In this paper, we compare the effects of cybersickness between the people with MS and the people without MS with respect to SSQ score and GSR data.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {51–59},
numpages = {9},
keywords = {accessibility, cybersickness, multiple sclerosis, user studies, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993395,
author = {Faita, Claudia and Vanni, Federico and Tanca, Camilla and Ruffaldi, Emanuele and Carrozzino, Marcello and Bergamasco, Massimo},
title = {Investigating the process of emotion recognition in immersive and non-immersive virtual technological setups},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993395},
doi = {10.1145/2993369.2993395},
abstract = {This paper investigates the use of Immersive Virtual Environment (IVE) to evaluate the process of emotion recognition from faces (ERF). ERF has been mostly probed by using still photographs resembling universal expressions. However, this approach does not reflect the vividness of faces. Virtual Reality (VR) makes use of animated agents, trying to overcome this issue by reproducing the inherent dynamic of facial expressions, but outside a natural environment. We suggest that a setup using IVE technology simulating a real scene in combination with virtual agents (VAs) displaying dynamic facial expressions should improve the study of ERF. To support our claim we carried out an experiment in which two groups of subjects had to recognize VAs facial expression of universal and basic emotions in IVE and No-IVE condition. The goal was to evaluate the impact of the immersion in VE for ERF investigation. Results showed that the level of immersion in IVE does not interfere with the recognition task and a high level of accuracy in facial recognition suggests that IVE can be used to investigate the process of ERF.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {61–64},
numpages = {4},
keywords = {ekman basic emotion, emotion recognition, emotional virtual agents, facial expression, immersive virtual environment, virtual environments, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993410,
author = {Moulec, Gwendal Le and Argelaguet, Ferran and L\'{e}cuyer, Anatole and Gouranton, Val\'{e}rie},
title = {Take-over control paradigms in collaborative virtual environments for training},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993410},
doi = {10.1145/2993369.2993410},
abstract = {The main objective of this paper is to study and formalize the Take-Over Control in Collaborative Virtual Environments for Training (CVET). The Take-Over Control represents the transfer (the take over) of the interaction control of an object between two or more users. This paradigm is particularly useful for training scenarios, in which the interaction control could be continuously exchanged between the trainee and the trainer, e.g. the latter guiding and correcting the trainee's actions. The paper presents the formalization of the Take-Over Control followed by an illustration focusing in a use-case of collaborative maritime navigation. In the presented use-case, the trainee has to avoid an under-water obstacle with the help of a trainer who has additional information about the obstacle. The use-case allows to highlight the different elements a Take-Over Control situation should enforce, such as user's awareness. Different Take-Over Control techniques were provided and evaluated focusing on the transfer exchange mechanism and the visual feedback. The results show that participants preferred the Take-Over Control technique which maximized the user awareness.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {65–68},
numpages = {4},
keywords = {CVET, awareness, take-over control},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993397,
author = {Wijewickrema, Sudanthi and Zhou, Yun and Bailey, James and Kennedy, Gregor and O'Leary, Stephen},
title = {Provision of automated step-by-step procedural guidance in virtual reality surgery simulation},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993397},
doi = {10.1145/2993369.2993397},
abstract = {One of the roadblocks to the wide-spread use of virtual reality simulation as a surgical training platform is the need for expert supervision during training to ensure proper skill acquisition. To fully utilize the capacity of virtual reality in surgical training, it is imperative that the guidance process is automated. In this paper, we discuss a method of providing one aspect of performance guidance: advice on the steps of a surgery or procedural guidance. We manually segment the surgical trajectory of an expert surgeon into steps and present them one at a time to guide trainees through a surgical procedure. We show, using a randomized controlled trial, that this form of guidance is effective in moving trainee behavior towards an expert ideal.To support practice variation and different surgical styles adopted by experts, separate guidance templates have to be generated. To enable this, we introduce a method of automatically segmenting a surgical trajectory into steps. We propose a pre-processing step that uses domain knowledge specific to our application to reduce the solution space. We show how this can be incorporated into existing trajectory segmentation methods, as well as a greedy approach that we propose. We compare this segmentation method to existing techniques and show that it is accurate and efficient.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {69–72},
numpages = {4},
keywords = {automated guidance, surgery simulation, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993399,
author = {Latoschik, Marc Erich and Lugrin, Jean-Luc and Roth, Daniel},
title = {FakeMi: a fake mirror system for avatar embodiment studies},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993399},
doi = {10.1145/2993369.2993399},
abstract = {This paper introduces a fake mirror system as a research tool to study the effect of avatar embodiment with non-visually immersive virtual environments. The system combines marker-less face and body tracking to animate the individual avatars seen in a stereoscopic display with a correct perspective projection. The display dimensions match typical dimensions of a real physical mirror and the animated avatars are rendered based on a geometrically correct reflection as expected from a real mirror including correct body and face animations. The first evaluation of the system reveals the high acceptance of the setup as well as a convincing illusion of a real mirror with different types of avatars.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {73–76},
numpages = {4},
keywords = {avatar, tool, virtual body ownership, virtual mirror},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993385,
author = {Westhoven, Martin and Paul, Dennis and Alexander, Thomas},
title = {Head turn scaling below the threshold of perception in immersive virtual environments},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993385},
doi = {10.1145/2993369.2993385},
abstract = {Immersive virtual environments allow to experience presence, the feeling of being present in a virtual environment. When accessing virtual reality with virtual reality goggles, head tracking is used to update the virtual viewpoint according to the user's head movement. While typically used unmodified, the extent to which the virtual viewpoint follows the real head motion can be scaled. In this paper, the effect of scaling below the threshold of perception on presence during a target acquisition task was studied. It was assumed, that presence is reduced when head motion is scaled. No effect on presence, simulator sickness and performance was found. A significant effect on physical task load was found. The results yield information for further work and for the required verification of the used concept of presence. It can be assumed, that load can be modified by the scaling without significantly influencing the quality of presence.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {77–86},
numpages = {10},
keywords = {empirical study, head tracking manipulation, immersive virtual environments, perception, presence, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993398,
author = {Kern, Angelika C. and Ellermeier, Wolfgang and Wojtusch, Janis},
title = {Noise-cancelling, steps and soundscapes: the effect of auditory stimulation on presence in virtual realities while walking},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993398},
doi = {10.1145/2993369.2993398},
abstract = {This study investigates the influence of different auditory stimuli on perceived presence, the feeling of "being there", on a walk through a park-like virtual environment. A single-factorial design with five levels varying the surrounding sound impressions was employed, including the conditions "No Headphones", "Noise-Cancelling", "Steps", "Soundscape" and "Steps \&amp; Soundscape", in order to find out which of the conditions would enhance the feeling of presence most. 36 participants rated their impression of presence using a questionnaire after walking on a treadmill while wearing a head-mounted display and Noise-Cancelling headphones. Statistical analysis of the data showed that the conditions including soundscapes resulted in significantly higher ratings of presence and realism, compared to all other auditory conditions. The results are placed into the context of findings in other studies and point to further research needs regarding the auditory enhancement of virtual environments.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {87–90},
numpages = {4},
keywords = {acoustic, head-mounted display, multimodality, noise-cancelling, presence, self-motion, soundscape, treadmill, virtual reality, visual},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993378,
author = {Narang, Sahil and Best, Andrew and Randhavane, Tanmay and Shapiro, Ari and Manocha, Dinesh},
title = {PedVR: simulating gaze-based interactions between a real user and virtual crowds},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993378},
doi = {10.1145/2993369.2993378},
abstract = {We present a novel interactive approach, PedVR, to generate plausible behaviors for a large number of virtual humans, and to enable natural interaction between the real user and virtual agents. Our formulation is based on a coupled approach that combines a 2D multi-agent navigation algorithm with 3D human motion synthesis. The coupling can result in plausible movement of virtual agents and can generate gazing behaviors, which can considerably increase the believability. We have integrated our formulation with the DK-2 HMD and demonstrate the benefits of our crowd simulation algorithm over prior decoupled approaches. Our user evaluation suggests that the combination of coupled methods and gazing behavior can considerably increase the behavioral plausibility.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {91–100},
numpages = {10},
keywords = {crowds, human agents, multi-agent simulation, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993373,
author = {Li, Fu-Shun and Wong, Sai-Keung},
title = {Animating agents based on radial view in crowd simulation},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993373},
doi = {10.1145/2993369.2993373},
abstract = {We present a simple agent-based approach for crowd simulation in a two-dimensional space. Our approach computes a set of collision-free feasible movement directions for each agent. The movement directions of the agent are determined according to a radial view of the agent in the local region. Subsequently, the direction with the least effort is selected for moving the agent. To efficiently compute feasible movement directions for each agent, we discretize the movement direction field of the agent and use a bit string to estimate the set of movement directions. Each bit of the bit string represents a span of movement directions. The proposed approach enables the agents to move around simple objects without using expensive global search techniques. Experimental results show that our approach can simulate agents that can alleviate the collision problem. The agents can move to their destinations efficiently.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {101–109},
numpages = {9},
keywords = {agent-based simulation, crowd simulation, radial view},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993370,
author = {Maia, Lu\'{\i}s Fernando and Viana, Windson and Trinta, Fernando},
title = {A real-time x-ray mobile application using augmented reality and google street view},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993370},
doi = {10.1145/2993369.2993370},
abstract = {X-ray view can be defined as the ability one has to see through real surfaces. Although this skill is often associated with superheroes and medical examination, there are several researches conducted to employ X-ray view in numerous applications. However, the generation of X-ray visualization includes numerous challenges regarding occlusion, realistic appearance, and depth perception.In this paper, we present a mobile application that uses Augmented Reality and Google Street View to allow users experience real-time X-ray vision. The proposed application was designed to enhance previous Augmented Reality X-ray systems, by introducing a silhouette computation method to provide visual context from the occluder and a perspective estimation system that improves the projection of occluded images into the real scene.Additionally, we implemented two usability studies to assess qualitative aspects of both silhouettes and perspective estimation to generate better X-ray effects. Results indicate good acceptance of the novel X-ray visualization method and a great usability score on the SUS scale for the mobile application.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {111–119},
numpages = {9},
keywords = {augmented reality, image and video processing in UI, mobile and embedded devices},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993380,
author = {Samini, Ali and Palmerius, Karljohan Lundin},
title = {A study on improving close and distant device movement pose manipulation for hand-held augmented reality},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993380},
doi = {10.1145/2993369.2993380},
abstract = {Hand-held smart devices are equipped with powerful processing units, high resolution screens and cameras, that in combination makes them suitable for video see-through Augmented Reality. Many Augmented Reality applications require interaction, such as selection and 3D pose manipulation. One way to perform intuitive, high precision 3D pose manipulation is by direct or indirect mapping of device movement.There are two approaches to device movement interaction; one fixes the virtual object to the device, which therefore becomes the pivot point for the object, thus makes it difficult to rotate without translate. The second approach avoids latter issue by considering rotation and translation separately, relative to the object's center point. The result of this is that the object instead moves out of view for yaw and pitch rotations.In this paper we study these two techniques and compare them with a modification where user perspective rendering is used to solve the rotation issues. The study showed that the modification improves speed as well as both perceived control and intuitiveness among the subjects.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {121–128},
numpages = {8},
keywords = {augmented reality, device interaction, device perspective, user study, user-perspective, video see-through},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993371,
author = {Nuernberger, Benjamin and Lien, Kuo-Chin and Grinta, Lennon and Sweeney, Chris and Turk, Matthew and H\"{o}llerer, Tobias},
title = {Multi-view gesture annotations in image-based 3D reconstructed scenes},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993371},
doi = {10.1145/2993369.2993371},
abstract = {We present a novel 2D gesture annotation method for use in image-based 3D reconstructed scenes with applications in collaborative virtual and augmented reality. Image-based reconstructions allow users to virtually explore a remote environment using image-based rendering techniques. To collaborate with other users, either synchronously or asynchronously, simple 2D gesture annotations can be used to convey spatial information to another user. Unfortunately, prior methods are either unable to disambiguate such 2D annotations in 3D from novel viewpoints or require relatively dense reconstructions of the environment.In this paper, we propose a simple multi-view annotation method that is useful in a variety of scenarios and applicable to both very sparse and dense 3D reconstructions. Specifically, we employ interactive disambiguation of the 2D gestures via a second annotation drawn from another viewpoint, triangulating two drawings to achieve a 3D result. Our method automatically chooses an appropriate second viewpoint and uses image-based rendering transitions to keep the user oriented while moving to the second viewpoint. User experiments in an asynchronous collaboration scenario demonstrate the usability of the method and its superiority over a baseline method. In addition, we showcase our method running on a variety of image-based reconstruction datasets and highlight its use in a synchronous local-remote user collaboration system.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {129–138},
numpages = {10},
keywords = {3D reconstruction, annotations, augmented reality, collaboration, image-based reconstruction, image-based rendering, interactive disambiguation, virtual navigation, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993400,
author = {von Mammen, Sebastian and Hamann, Heiko and Heider, Michael},
title = {Robot gardens: an augmented reality prototype for plant-robot biohybrid systems},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993400},
doi = {10.1145/2993369.2993400},
abstract = {Robot Gardens are an augmented reality concept allowing a human user to design a biohybrid, plant-robot system. Plants growing from deliberately placed seeds are directed by robotic units that the user can position, configure and activate. For example, the robotic units may serve as physical shields or frames but they may also guide the plants' growth through emission of light. The biohybrid system evolves over time to redefine architectural spaces. This gives rise to the particular challenge of designing a biohybrid system before its actual implementation and potentially long before its developmental processes unfold. Here, an augmented reality interface featuring according simulation models of plants and robotic units allows one to explore the design space a priori. In this work, we present our first functional augmented reality prototype to design biohybrid systems. We provide details about its workings and elaborate on first empirical studies on its usability.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {139–142},
numpages = {4},
keywords = {augmented reality, biohybrids, interactive simulation},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993377,
author = {Tong, Qianqian and Yuan, Zhiyong and Zheng, Mianlun and Zhu, Weixu and Zhang, Guian and Liao, Xiangyun},
title = {A novel magnetic levitation haptic device for augmentation of tissue stiffness perception},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993377},
doi = {10.1145/2993369.2993377},
abstract = {In medical training especially in palpation surgery, it is important for surgeons to perceive tissue stiffness. We design a novel magnetic levitation haptic device based on electromagnetic principles to enhance the perception of tissue stiffness in a virtual environment. The user can directly sense virtual tissues by moving a magnetic stylus in the magnetic field generated by the coil array of our device. To fully use the effective magnetic field, we devise an adjustable coil array and provide a reasonable explanation for such design. Moreover, we design a control interface circuit and present a self-adaptive fuzzy proportion integration differentiation (PID) algorithm to precisely control the coil current. The quantitative experiment shows that the experimental and simulation data of our device are consistent and the proposed control algorithm contributes to increasing the accuracy of tissue stiffness perception. In qualitative experiment, we recruit 22 participants to distinguish tissues of different stiffness and detect tissue abnormality. The experimental results demonstrate that our magnetic levitation haptic device can provide accurate perception of tissue stiffness.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {143–152},
numpages = {10},
keywords = {adjustable coil array, magnetic levitation haptic device, self-adaptive fuzzy PID, stiffness perception augmentation},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993386,
author = {Sagardia, Mikel and Hulin, Thomas and Hertkorn, Katharina and Kremer, Philipp and Sch\"{a}tzle, Simon},
title = {A platform for bimanual virtual assembly training with haptic feedback in large multi-object environments},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993386},
doi = {10.1145/2993369.2993386},
abstract = {We present a virtual reality platform which addresses and integrates some of the currently challenging research topics in the field of virtual assembly: realistic and practical scenarios with several complex geometries, bimanual six-DoF haptic interaction for hands and arms, and intuitive navigation in large workspaces. We put an especial focus on our collision computation framework, which is able to display stiff and stable forces in 1 kHz using a combination of penalty- and constraint-based haptic rendering methods. Interaction with multiple arbitrary geometries is supported in realtime simulations, as well as several interfaces, allowing for collaborative training experiences. Performance results for an exemplary car assembly sequence which show the readiness of the system are provided.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {153–162},
numpages = {10},
keywords = {haptic devices, haptic rendering, interaction techniques, virtual assembly},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993374,
author = {Sagardia, Mikel and Hulin, Thomas},
title = {A fast and robust Six-DoF god object heuristic for haptic rendering of complex models with friction},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993374},
doi = {10.1145/2993369.2993374},
abstract = {Collision detection and force computation between complex geometries are essential technologies for virtual reality and robotic applications. Penalty-based haptic rendering algorithms provide a fast collision computation solution, but they cannot avoid the undesired interpenetration between virtual objects, and have difficulties with thin non-watertight geometries. God object methods or constraint-based haptic rendering approaches have shown to solve this problem, but are typically complex to implement and computationally expensive. This paper presents an easy-to-implement god object approach applied to six-DoF penalty-based haptic rendering algorithms. Contact regions are synthesized to penalty force and torque values and these are used to compute the position of the god object on the surface. Then, the pose of this surface proxy is used to render stiff and stable six-DoF contacts with friction. Independently of the complexity of the used geometries, our implementation runs in only around 5 μs and the results show a maximal penetration error of the resolution used in the penalty-based haptic rendering algorithm.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {163–172},
numpages = {10},
keywords = {haptic devices, haptic rendering, interaction techniques, virtual assembly},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993392,
author = {Bowyer, Stuart A. and Baena, Ferdinando Rodriguez y},
title = {A hybrid constraint-penalty proxy method for six degree-of-freedom haptic display of deforming objects},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993392},
doi = {10.1145/2993369.2993392},
abstract = {There are many applications and tasks in which the precise, high-fidelity haptic display of deforming objects is required. A crucial element in haptic rendering is the definition of a proxy pose that follows the motion of the user, while respecting the geometry of the object being displayed. Conventional methods for computing the dynamics of a proxy interacting with a deforming object suffer from several issues relating to numerical instabilities when the proxy becomes over-constrained and high computational demands. This paper presents a novel hybrid proxy that combines modified versions of constraint-based and penalty-based proxies together to give high fidelity rendering with reduced computational requirements and enhanced robustness to situations where the proxy becomes enclosed. Experimental analysis of the proposed method shows that it can efficiently compute proxy dynamics that faithfully render the required object. This research forms a basis for further development of novel hybrid dynamic proxies for haptics and allows for increasingly complex deforming geometries to be rendered.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {173–182},
numpages = {10},
keywords = {deforming objects, gauss' principle of least constraint, god-object, haptics, proxy},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993404,
author = {Israr, Ali and Schwemler, Zachary and Mars, John and Krainer, Brian},
title = {VR360HD: a VR360° player with enhanced haptic feedback},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993404},
doi = {10.1145/2993369.2993404},
abstract = {We present a VR360° video player with haptic feedback playback. The VR360HD application enhances VR viewing experience by triggering customized haptic effects associated with user's activities, biofeedback, network messages and customizable timeline triggers incorporated in the VR media. The app is developed in the Unity3D game engine and tested using a GearVR headset, therefore allowing users to add animations to VR gameplay and to the VR360° streams. A custom haptic plugin allows users to author and associate animated haptic effects to the triggers, and playback these effects on a custom haptic hardware, the Haptic Chair. We show that the VR360HD app creates rich tactile effects and can be easily adapted to other media types.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {183–186},
numpages = {4},
keywords = {VR viewing, haptic feedback, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993403,
author = {Arnaud, Adrien and Christophe, Julien and Gouiffes, Mich\`{e}le and Ammi, Mehdi},
title = {3D reconstruction of indoor building environments with new generation of tablets},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993403},
doi = {10.1145/2993369.2993403},
abstract = {This paper presents a mobile platform that uses a new generation of tablets equipped with a depth sensor to perform a real time 3D reconstruction of an indoor environment. The platform generates a 3D model where the strucutral elements are identified: ground, ceiling, walls and openings. The 3D model is used for the evaluation of both the geometric features of the building, but also for the assessment of the building's energetic performance. Also, a series of edition and visualization tools are proposed within the platform to assist the user in modifying and exploring the 3D reconstructed environment.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {187–190},
numpages = {4},
keywords = {3D edition, 3D indoor reconstruction, mobile devices, visualization},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993372,
author = {Sra, Misha and Garrido-Jurado, Sergio and Schmandt, Chris and Maes, Pattie},
title = {Procedurally generated virtual reality from 3D reconstructed physical space},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993372},
doi = {10.1145/2993369.2993372},
abstract = {We present a novel system for automatically generating immersive and interactive virtual reality (VR) environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas (WA) to enable real-walking in the generated virtual environment (VE). Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our approach is new, in that it allows a casual user to easily create virtual reality worlds in any indoor space of arbitrary size and shape without requiring specialized equipment or training. We demonstrate our approach through a fully working system implemented on the Google Project Tango tablet device.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {191–200},
numpages = {10},
keywords = {3D reconstruction, computer vision, depth cameras, locomotion, mobile computing, obstacle avoidance, procedural generation, tracking, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993384,
author = {Mossel, Annette and Koessler, Christian},
title = {Large scale cut plane: an occlusion management technique for immersive dense 3D reconstructions},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993384},
doi = {10.1145/2993369.2993384},
abstract = {Dense 3D reconstructions of real-world environments become wide spread and are foreseen to act as data base to solve real world problems, such as remote inspections. Therefore not only scene viewing is required but also the ability to interact with the environment, such as selection of a user-defined part of the reconstruction for later usage. However, inter-object occlusion is inherent to large dense 3D reconstructions, due to scene geometry or reconstruction artifacts that might result in object containment. Since prior art lacks approaches for occlusion management in environments that consist of one or multiple (large) continuous surfaces, we propose the novel technique Large Scale Cut Plane that enables segmentation and subsequent selection of visible, partly or fully occluded patches within a large 3D reconstruction, even at far distance. We combine Large Scale Cut Plane with an immersive virtual reality setup to foster 3D scene understanding and natural user interactions. We furthermore present results from a user study where we investigate performance and usability of our proposed technique compared to a baseline technique. Our results indicate Large Scale Cut Plane to be superior in terms of speed and precision, while we found need of improvement of the user interface. The presented investigations has to the authors' best knowledge not been subject to previous research.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {201–210},
numpages = {10},
keywords = {3D selection, dense 3D surface reconstruction, immersive virtual reality, occlusion management},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993387,
author = {Huang, Jiawei and Mori, Tsuyoshi and Takashima, Kazuki and Hashi, Shuichiro and Kitamura, Yoshifumi},
title = {6-DOF computation and marker design for magnetic 3D dexterous motion-tracking system},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993387},
doi = {10.1145/2993369.2993387},
abstract = {We describe our approach that derives reliable 6-DOF information including the translation and the rotation of a rigid marker in a 3D space from a set of insufficient 5-DOF measurements. As a practical example, we carefully constructed a prototype and its design and evaluated it in our 3D dexterous motion-tracking system, IM6D, which is our novel real-time magnetic 3D motion-tracking system that uses multiple identifiable, tiny, lightweight, wireless, and occlusion-free markers. The system contains two key technologies; a 6-DOF computation algorithm and a marker design for 6D marker. The 6-DOF computation algorithm computes the result of complete 6-DOF information including translation and rotation in 3D space for a single rigid marker that consists of three LC coils. We propose several possible approaches for implementation, including geometric, matrix-based kinematics, and computational approaches. In addition, we introduce workflow to find an optimal marker design for the system to achieve the best compromise between its smallness and accuracy based on the tracking principle. We experimentally compare the performances of some typical marker prototypes with different layouts of LC coils. Finally, we also show another experimental result to prove the effectiveness of the results from the solutions in these two problems.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {211–217},
numpages = {7},
keywords = {3D interaction, 3D user interface, augmented reality, input devices, motion capture, sensor, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993389,
author = {Lee, Jaebong and Han, Bohyung and Choi, Seungmoon},
title = {Interactive motion effects design for a moving object in 4D films},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993389},
doi = {10.1145/2993369.2993389},
abstract = {This paper presents an algorithm that allows for the rapid design of motion effects for 4D films. Our algorithm is based on a viewer-centered rendering strategy that matches chair motion to the movement of a viewer's visual attention. Object tracking algorithm is used to estimate the movement of visual attention under the assumption that visual attention follows an object of interest. We performed several experiments to find optimal parameters for implementation, such as the required accuracy of object tracking. Our algorithm enables motion effects design to be at least 10 times faster than the current practice of manual authoring. We also assessed the subjective quality of the motion effects generated by our algorithm, and results indicated that our algorithm can provide perceptually plausible motion effects.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {219–228},
numpages = {10},
keywords = {4D film, motion effects, multi-sensory theater},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993405,
author = {Nielsen, Lasse T. and M\o{}ller, Matias B. and Hartmeyer, Sune D. and Ljung, Troels C. M. and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
title = {Missing the point: an exploration of how to guide users' attention during cinematic virtual reality},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993405},
doi = {10.1145/2993369.2993405},
abstract = {Recent technological advances have brought Virtual Reality (VR) into the homes of consumers, and there is a growing interest in bringing cinematic experiences from the screen and into VR. However, cinematic VR limits filmmakers' ability to effectively guide the audience's attention. In this paper we present a taxonomy of approaches to guiding users' attention, and present a study comparing two such approaches with a control condition devoid of guidance. One approach guides users by controlling their body's orientation, and the other implicitly directs their attention by encouraging them to follow a firefly with their gaze. The results revealed interesting, albeit statistically insignificant, indications that assuming control of the user's action may negatively influence presence, whereas the firefly was perceived as significantly more helpful.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {229–232},
numpages = {4},
keywords = {attention, cinematic VR, film, presence, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993408,
author = {Schubert, Rebekka S. and M\"{u}ller, Mathias and Pannasch, Sebastian and Helmert, Jens R.},
title = {Depth information from binocular disparity and familiar size is combined when reaching towards virtual objects},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993408},
doi = {10.1145/2993369.2993408},
abstract = {Reaching movements towards stereoscopically presented virtual objects have been reported to be imprecise. This might be a problem for touch interaction with virtual environments. Estimating the distance to an object in personal space relies on binocular disparity and other depth cues but previous work on the influence of familiar size for reaching and grasping has produced conflicting results. We presented a virtual tennis ball and manipulated binocular disparity as well as the size of the tennis ball. The results suggest that depth information from binocular disparity and from familiar size is combined for reaching movements towards virtual objects. However, subjects differed in the weight they assigned to each depth cue.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {233–236},
numpages = {4},
keywords = {depth perception, distance estimates, reaching, stereoscopy, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993388,
author = {Medeiros, Daniel and Sousa, Maur\'{\i}cio and Mendes, Daniel and Raposo, Alberto and Jorge, Joaquim},
title = {Perceiving depth: optical versus video see-through},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993388},
doi = {10.1145/2993369.2993388},
abstract = {Head-Mounted Displays (HMDs) and similar 3D visualization devices are becoming ubiquitous. Going a step forward, HMD see-through systems bring virtual objects to real world settings, allowing augmented reality to be used in complex engineering scenarios. Of these, optical and video see-through systems differ on how the real world is captured by the device. To provide a seamless integration of real and virtual imagery, the absolute depth and size of both virtual and real objects should match appropriately. However, these technologies are still in their early stages, each featuring different strengths and weaknesses which affect the user experience. In this work we compare optical to video see-through systems, focusing on depth perception via exocentric and egocentric methods. Our study pairs Meta Glasses, an off-the-shelf optical see-through, to a modified Oculus Rift setup with attached video-cameras, for video see-through. Results show that, with the current hardware available, the video see-through configuration provides better overall results. These experiments and our results can help interaction designers for both virtual and augmented reality conditions.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {237–240},
numpages = {4},
keywords = {augmented reality, depth perception, see-through system, user evaluation},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993379,
author = {Langbehn, Eike and Raupp, Tino and Bruder, Gerd and Steinicke, Frank and Bolte, Benjamin and Lappe, Markus},
title = {Visual blur in immersive virtual environments: does depth of field or motion blur affect distance and speed estimation?},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993379},
doi = {10.1145/2993369.2993379},
abstract = {It is known for decades that users tend to significantly underestimate or overestimate distances or speed in immersive virtual environments (IVEs) compared to corresponding judgments in the real world. Although several factors have been identified in the past that could explain small portions of this effect, the main causes of these perceptual discrepancies still remain elusive. One of the factors that has received less attention in the literature is the amount of blur presented in the visual imagery, for example, when using a head-mounted display (HMD).In this paper, we analyze the impact of the visual blur effects depth-of-field and motion blur in terms of their effects on distance and speed estimation in IVEs. We conducted three psychophysical experiments in which we compared distance or speed estimation between the real world and IVEs with different levels of depth-of-field or motion blur. Our results indicate that the amount of blur added to the visual stimuli had no noticeable influence on distance and speed estimation even when high magnitudes of blur were shown. Our findings suggest that the human perceptual system is highly capable of extracting depth and motion information regardless of blur, and implies that blur can likely be ruled out as the main cause of these misperception effects in IVEs.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {241–250},
numpages = {10},
keywords = {distance and speed estimation, virtual environments, visual blur},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993382,
author = {Gebhardt, Sascha and Petersen-Krau, Till and Pick, Sebastian and Rausch, Dominik and Nowke, Christian and Knott, Thomas and Schmitz, Patric and Zielasko, Daniel and Hentschel, Bernd and Kuhlen, Torsten W.},
title = {Vista widgets: a framework for designing 3D user interfaces from reusable interaction building blocks},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993382},
doi = {10.1145/2993369.2993382},
abstract = {Virtual Reality (VR) has been an active field of research for several decades, with 3D interaction and 3D User Interfaces (UIs) as important sub-disciplines. However, the development of 3D interaction techniques and in particular combining several of them to construct complex and usable 3D UIs remains challenging, especially in a VR context. In addition, there is currently only limited reusable software for implementing such techniques in comparison to traditional 2D UIs. To overcome this issue, we present ViSTA Widgets, a software framework for creating 3D UIs for immersive virtual environments. It extends the ViSTA VR framework by providing functionality to create multi-device, multi-focus-strategy interaction building blocks and means to easily combine them into complex 3D UIs. This is realized by introducing a device abstraction layer along sophisticated focus management and functionality to create novel 3D interaction techniques and 3D widgets. We present the framework and illustrate its effectiveness with code and application examples accompanied by performance evaluations.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {251–260},
numpages = {10},
keywords = {3D interaction, 3D user interfaces, framework, multi-device, virtual reality},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993396,
author = {Mendes, Daniel and Relvas, Filipe and Ferreira, Alfredo and Jorge, Joaquim},
title = {The benefits of DOF separation in mid-air 3D object manipulation},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993396},
doi = {10.1145/2993369.2993396},
abstract = {Object manipulation is a key feature in almost every virtual environment. However, it is difficult to accurately place an object in immersive virtual environments using mid-air gestures that mimic interactions in the physical world, although being a direct and natural approach. Previous research studied mouse and touch based interfaces concluding that separation of degrees-of-freedom (DOF) led to improved results. In this paper, we present the first user evaluation to assess the impact of explicit 6 DOF separation in mid-air manipulation tasks. We implemented a technique based on familiar virtual widgets that allow single DOF control, and compared it against a direct approach and PRISM, which dynamically adjusts the ratio between hand and object motions. Our results suggest that full DOF separation benefits precision in spatial manipulations, at the cost of additional time for complex tasks. From our results we draw guidelines for 3D object manipulation in mid-air.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {261–268},
numpages = {8},
keywords = {3D user interfaces, DOF separation, immersive virtual environments, mid-air object manipulation, spatial interactions},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993391,
author = {Argelaguet, Ferran and Maignant, Morgant},
title = {GiAnt: stereoscopic-compliant multi-scale navigation in VEs},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993391},
doi = {10.1145/2993369.2993391},
abstract = {Navigation in multi-scale virtual environments (MSVE) requires the adjustment of the navigation parameters to ensure optimal navigation experiences at each level of scale. In particular, in immersive stereoscopic systems, e.g. when performing zoom-in and zoom-out operations, the navigation speed and the stereoscopic rendering parameters have to be adjusted accordingly. Although this adjustment can be done manually by the user, it can be complex, tedious and strongly depends on the virtual environment. In this work we propose a new multi-scale navigation technique named GiAnt (GIant/ANT) which automatically and seamlessly adjusts the navigation speed and the scale factor of the virtual environment based on the user's perceived navigation speed. The adjustment ensures an almost-constant perceived navigation speed while avoiding diplopia effects or diminished depth perception due to improper stereoscopic rendering configurations. The results from the conducted user evaluation shows that GiAnt is an efficient multi-scale navigation which minimizes the changes of the scale factor of the virtual environment compared to state-of-the-art multi-scale navigation techniques.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {269–277},
numpages = {9},
keywords = {3DUI, multi-scale, navigation, optical flow},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993401,
author = {Al-Kalbani, Maadh and Williams, Ian and Frutos-Pascual, Maite},
title = {Improving freehand placement for grasping virtual objects via dual view visual feedback in mixed reality},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993401},
doi = {10.1145/2993369.2993401},
abstract = {This paper presents a first study into the use of dual view visual feedback in an exocentric MR environment for assisting freehand grasping of virtual objects. Recent work has highlighted problems associated with user errors in freehand grasping, via an analysis of virtual object type, location and size. Our work presents an extension to this, where 30 participants are recruited for two experiments (one assessing object size and the second object position), giving 1710 grasps in total. We report on results following the same protocol of the aforementioned study using a dual view visual feedback method. Results show that dual view visual feedback significantly increases user z placement accuracy and improves grasp placement in the x and y axes, however completion time is significantly higher. No improvement was found in user grasp aperture using dual view visual feedback for changes in object size and position.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {279–282},
numpages = {4},
keywords = {dual view feedback, freehand interaction, grasping, human performance measurement, mixed reality, natural hand interaction, visual feedback},
location = {Munich, Germany},
series = {VRST '16}
}

@inproceedings{10.1145/2993369.2993390,
author = {Louvet, Jean-Baptiste and Fleury, C\'{e}dric},
title = {Combining bimanual interaction and teleportation for 3D manipulation on multi-touch wall-sized displays},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993390},
doi = {10.1145/2993369.2993390},
abstract = {While multi-touch devices are well established in our everyday life, they are currently becoming larger and larger. Large screens such as wall-sized displays are now equipped with multi-touch capabilities. Multi-touch wall-sized displays will become widespread in a near future in various places such as public places or meeting rooms. These new devices are an interesting opportunity to interact with 3D virtual environments: the large display surface offers a good immersion, while the multi-touch capabilities could make interaction with 3D content accessible to the general public.In this paper, we aim to explore touch-based 3D interaction in the situation where users are immersed in a 3D virtual environment and move in front of a vertical wall-sized display. We design In(SITE), a bimanual touch-based technique combined with object teleportation features which enables users to interact on a large wall-sized display. This technique is compared with a standard 3D interaction technique for performing 6 degrees of freedom manipulation tasks on a wall-sized display. The results of two controlled experiments show that participants can reach the same level of performance for completion time and a better precision for fine adjustments of object position with the In(SITE) technique. They also suggest that combining object teleportation with both techniques improves translations in terms of ease of use, fatigue, and user preference.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {283–292},
numpages = {10},
keywords = {3D manipulation, multi-touch interaction, virtual reality, wall-sized display},
location = {Munich, Germany},
series = {VRST '16}
}

