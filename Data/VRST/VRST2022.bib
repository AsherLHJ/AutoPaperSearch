@inproceedings{10.1145/3562939.3565618,
author = {Belga, Jacob and Do, Tiffany D. and Ghamandi, Ryan and McMahan, Ryan P. and LaViola, Joseph J.},
title = {Carousel: Improving the Accuracy of Virtual Reality Assessments for Inspection Training Tasks},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565618},
doi = {10.1145/3562939.3565618},
abstract = {Training simulations in virtual reality (VR) have become a focal point of both research and development due to allowing users to familiarize themselves with procedures and tasks without needing physical objects to interact with or needing to be physically present. However, the increasing popularity of VR training paradigms raises the question: Are VR-based training assessments accurate? Many VR training programs, particularly those focused on inspection tasks, employ simple pass or fail assessments. However, these types of assessments do not necessarily reflect the user’s knowledge. In this paper, we present Carousel, a novel VR-based assessment method that requires users to actively employ their training knowledge by considering all relevant scenarios during assessments. We also present a within-subject user study that compares the accuracy of our new Carousel method to a conventional pass or fail method for a series of virtual object inspection tasks involving shapes and colors. The results of our study indicate that the Carousel method affords significantly more-accurate assessments of a user’s knowledge than the binary-choice method.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {1},
numpages = {10},
keywords = {Virtual reality, training assessments., virtual inspections},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565610,
author = {Shijo, Ryota and Sakurai, Sho and Hirota, Koichi and Nojima, Takuya},
title = {Research on the Emotions Expressed by the Posture of Kemo-mimi},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565610},
doi = {10.1145/3562939.3565610},
abstract = {Kemo-mimi means the dog- or cat-like ears on a humanoid character, or the ears of the animal itself. Kemo-mimi is often used as an element of the avatar’s appearance. It is generally considered that the posture of animal ears represents the animal’s emotional state. And the idea has been used as a technique for expressing emotions in many cartoon and animation works. But despite this fact, there are few examples of studies on the emotions that can be expressed by animal ears. Therefore, we decided to investigate the relationship between the posture of the animal ears and emotions and to establish a method of expressing emotions using the ears. In the experiments, three-dimensional animations of animal ears changing posture were presented to the subjects, and they were asked to answer the emotion corresponding to the posture. The results showed that there was a certain degree of a common understanding of people’s impressions concerning the animal ears. In this paper, we report the emotions that can be expressed by the posture of the animal ears as revealed in this study.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {2},
numpages = {5},
keywords = {animal ears, emotion expression, kemo-mimi, kemonomimi},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565613,
author = {Shin, Yerin and Kim, Gerard Jounghyun},
title = {Leveraging VR Techniques for Efficient Exploration and Interaction in Large and Complex AR Space with Clipped and Small FOV AR Display},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565613},
doi = {10.1145/3562939.3565613},
abstract = {In this paper, we propose to take advantage of the digital twinned environment to interact more efficiently in the large and complex AR space in spite of the limited sized and clipped FOV of the AR display. Using the digital twin of the target environment, “magical” VR interaction techniques can be applied, as visualized and overlaid through the small window, while still maintaining the spatial association to the augmented real world. First we consider the use of amplified movement within the corresponding VR twinned space to help the user search, plan, navigate and explore efficiently by providing an effectively larger view and thereby better spatial understanding of the same AR space with less amount of physical movements. Secondly, we also apply the amplified movement and in addition, the stretchable arm to interact with relatively large objects (or largely spaced objects) which cannot be seen in their entirety at a time with the small FOV glass. The results of the experiment with the proposed methods have showed advantages with regards to the interaction performance as the scene became more complex and task more difficult. The work illustrates the concept of and potential for XR based interaction where the user can leverage the advantages of both VR and AR mode operations.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {3},
numpages = {10},
keywords = {augmented reality, digital twin, extended reality, interaction, mixed reality, navigation, object manipulation, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565624,
author = {Wang, Jiaheng and Anslow, Craig and Mccallum, Simon James Robertson and Robinson, Brian and Medeiros, Daniel and Jorge, Joaquim},
title = {VR Games for Chronic Pain Management},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565624},
doi = {10.1145/3562939.3565624},
abstract = {Chronic pain is a continuous ailment lasting for long periods after the initial injury or disease has healed. Chronic pain is challenging to treat and affects the daily lives of patients. Distraction therapy is a proven method of relieving patients’ discomfort by taking their attention away from the pain. Virtual reality (VR) is a platform for distraction therapy by immersing the user in a virtual world detached from reality. However, there is little research on how physical interactions in VR affect pain management. We present a study to evaluate the effectiveness of physically active, mentally active, and passive interventions in VR using games with chronic pain patients. Our results indicate that physical and mental activities in VR are equally effective at reducing pain. Furthermore, These actively engage patients, while the effects of observing relaxing content persist outside VR. These findings can help inform the design of future VR games targeted at chronic pain management.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {4},
numpages = {11},
keywords = {Chronic Pain Management, User Study, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565608,
author = {Kim, Bogoan and Jeong, Dayoung and Jeong, Mingon and Noh, Taehyung and Kim, Sung-In and Kim, Taewan and Jang, So-Youn and Yoo, Hee Jeong and Kim, Jennifer and Hong, Hwajung and Han, Kyungsik},
title = {VISTA: User-centered VR Training System for Effectively Deriving Characteristics of People with Autism Spectrum Disorder},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565608},
doi = {10.1145/3562939.3565608},
abstract = {Pervasive symptoms of people with autism spectrum disorder (ASD), such as a lack of social and communication skills, are major challenges to be embraced in the workplace. Although much research has proposed VR training programs, their effectiveness is somewhat unclear, since they provide limited, one-sided interactions through fixed scenarios or do not sufficiently reflect the characteristics of people with ASD (e.g., preference for predictable interfaces, sensory issues). In this paper, we present VISTA, a VR-based interactive social skill training system for people with ASD. We ran a user study with 10 people with ASD and 10 neurotypical people to evaluate user experience in VR training and to examine the characteristics of people with ASD based on their physical responses generated by sensor data. The results showed that ASD participants were highly engaged with VISTA and improved self-efficacy after experiencing VISTA. The two groups showed significant differences in sensor signals as the task complexity increased, which demonstrates the importance of considering task complexity in eliciting the characteristics of people with ASD in VR training. Our findings not only extend findings (e.g., low ROI ratio, EDA increase) in previous studies but also provide new insights (e.g., high utterance rate, large variation of pupil diameter), broadening our quantitative understanding of people with ASD.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {5},
numpages = {12},
keywords = {Autism Spectrum Disorder (ASD), Social skills training system, User study, Virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565630,
author = {Numan, Nels and Steed, Anthony},
title = {Exploring User Behaviour in Asymmetric Collaborative Mixed Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565630},
doi = {10.1145/3562939.3565630},
abstract = {A common issue for collaborative mixed reality is the asymmetry of interaction with the shared virtual environment. For example, an augmented reality (AR) user might use one type of head-mounted display (HMD) in a physical environment, while a virtual reality (VR) user might wear a different type of HMD and see a virtual model of that physical environment. To explore the effects of such asymmetric interfaces on collaboration we present a study that investigates the behaviour of dyads performing a word puzzle task where one uses AR and the other VR. We examined the collaborative process through questionnaires and behavioural measures based on positional and audio data. We identified relationships between presence and co-presence, accord and co-presence, leadership and talkativeness, head rotation velocity and leadership, and head rotation velocity and talkativeness. We did not find that AR or VR biased subjective responses, though there were interesting behavioural differences: AR users spoke more words, AR users had a higher median head rotation velocity, and VR users travelled further.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {6},
numpages = {11},
keywords = {augmented reality, collaboration, mixed reality, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565640,
author = {Kono, Michinari and Morimoto, Naohiko and Kaku, Ryoichi},
title = {VCPoser: Interactive Pose Generation of Virtual Characters Corresponding to Human Pose Input},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565640},
doi = {10.1145/3562939.3565640},
abstract = {Virtual characters (VCs) play a significant role in the entertainment industry, and AI-driven VCs are being developed to enable interaction with users. People are attracted to these VCs, resulting in a demand for them to co-exist in the same world. An approach to allow recording of the memories with the VCs is to capture videos or photos with them, where users are usually required to adapt their poses to the pre-rendered VC’s action. To allow a more seamless collaboration with VCs in photography scenarios, we propose VCPoser, which enables VCs to adapt their pose to the pose of the user. We created a deep neural network-based system that predicts a VC’s pose using the user’s pose data as input by learning the paired pose data. Our quantitative evaluations and user studies demonstrate that our system can predict and generate poses of VCs and allow them to be combined next to the posing user in a photo. We also provide an analysis of the human mindsets of paired poses for a better understanding of them and to share insights for aesthetic pose design.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {7},
numpages = {10},
keywords = {machine learning, photography, pose estimation, virtual characters},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565631,
author = {Otsuki, Mai and Wang, Tzu-Yang and Kuzuoka, Hideaki},
title = {Assessment of Instructor’s Capacity in One-to-Many AR Remote Instruction Giving},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565631},
doi = {10.1145/3562939.3565631},
abstract = {In this study, we focused on one-to-many remote collaboration which requires more mental resources from the remote instructor than the case of one-to-one since it is "multitasking". The main contribution of our study is that we assessed instructor’s capacity in one-to-many AR remote instruction giving both subjectively and objectively. We compared the remote instructor’s workload while interacting with a different number of local workers, assuming tasks at an industrial site. The results showed that the instructors perceived stronger workload and the communication quality became lower when interacting with multiple local workers. Based on the results, we discussed how to support the remote instructor in a one-to-many AR remote collaboration.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {8},
numpages = {5},
keywords = {AR, Multitasking, One-to-many, Remote Collaboration, Workload},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565636,
author = {Frampton-Clerk, Aisha and Oyekoya, Oyewole},
title = {Investigating the Perceived Realism of the Other User’s Look-Alike Avatars},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565636},
doi = {10.1145/3562939.3565636},
abstract = {There are outstanding questions regarding the fidelity of realistic look-alike avatars that show that there is still substantial development to be done, especially as the virtual world plays a more vital role in our education, work and recreation. The use of look-alike avatars could completely change how we interact virtually. This paper investigates which features of other people’s look-alike avatars influence our perceived realism. Four levels of avatar representations were assessed in this pilot study: a static avatar, a static avatar with lip sync corresponding to an audio recording, full face animation with audio and a full body animation. Results show that full-face and body animations are very important in increasing the perceived realism of avatars. More importantly, participants found the lip sync animation more unsettling (uncanny valley effect) than any of the other animations. The results have implications for the perception of other people’s look-alike avatars in collaborative virtual environments.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {9},
numpages = {5},
keywords = {Look-alike Avatar, Telepresence, Uncanny Valley, Virtual Character},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565629,
author = {Regal, Georg and Uhl, Jakob Carl and Gerhardus, Anna and Suette, Stefan and Frankus, Elisabeth and Schmid, Julia and Kriglstein, Simone and Tscheligi, Manfred},
title = {Marcus or Mira - Investigating the Perception of Virtual Agent Gender in Virtual Reality Role Play-Training},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565629},
doi = {10.1145/3562939.3565629},
abstract = {Immersive virtual training environments are used in various domains. In this work we focus on role-play training in virtual reality. In virtual role-play training conversations and interactions with virtual agents are often fundamental to the training. Therefore, the appearance and behavior of the agents plays an important role when designing role-play training. We focus on the gender appearance of agents, as gender is an important aspect for differentiation between characters. We conducted a study with 40 participants in which we investigated how agents gender appearance influences the perception of the agents´ personality traits and the self-perception of a participants’ assumed role in a training for social skills. This work contributes towards understanding the design-space of virtual agent design, virtual agent gender identity, and the design and development of immersive virtual reality role-play training.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {10},
numpages = {11},
keywords = {Gender, Training, Virtual Agents, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565623,
author = {Gao, Hong and Hasenbein, Lisa and Bozkir, Efe and G\"{o}llner, Richard and Kasneci, Enkelejda},
title = {Evaluating the Effects of Virtual Human Animation on Students in an Immersive VR Classroom Using Eye Movements},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565623},
doi = {10.1145/3562939.3565623},
abstract = {Virtual humans presented in VR learning environments have been suggested in previous research to increase immersion and further positively influence learning outcomes. However, how virtual human animations affect students’ real-time behavior during VR learning has not yet been investigated. This work examines the effects of social animations (i.e., hand raising of virtual peer learners) on students’ cognitive response and visual attention behavior during immersion in a VR classroom based on eye movement analysis. Our results show that animated peers that are designed to enhance immersion and provide companionship and social information elicit different responses in students (i.e., cognitive, visual attention, and visual search responses), as reflected in various eye movement metrics such as pupil diameter, fixations, saccades, and dwell times. Furthermore, our results show that the effects of animations on students differ significantly between conditions (20\%, 35\%, 65\%, and 80\% of virtual peer learners raising their hands). Our research provides a methodological foundation for investigating the effects of avatar animations on users, further suggesting that such effects should be considered by developers when implementing animated virtual humans in VR. Our findings have important implications for future works on the design of more effective, immersive, and authentic VR environments.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {11},
numpages = {11},
keywords = {education, eye-tracking, immersive virtual reality, virtual human animation, visual attention},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565621,
author = {Batmaz, Anil Ufuk and Hudhud Mughrabi, Moaaz and Barrera Machuca, Mayra Donaji and Stuerzlinger, Wolfgang},
title = {Effect of Stereo Deficiencies on Virtual Distal Pointing},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565621},
doi = {10.1145/3562939.3565621},
abstract = {Previous work has shown that the mismatch between disparity and optical focus cues, i.e., the vergence and accommodation conflict (VAC), affects virtual hand selection in immersive systems. To investigate if the VAC also affects distal pointing with ray casting, we ran a user study with an ISO 9241:411 multidirectional selection task where participants selected 3D targets with three different VAC conditions, no VAC, i.e., targets placed roughly at 75 cm, which matches the focal plane of the VR headset, constant VAC, i.e., at 400 cm from the user, and varying VAC, where the depth distance of targets changed between 75 cm and 400 cm. According to our results, the varying VAC condition requires the most time and decreases the throughput performance of the participants. It also takes longer for users to select targets in the constant VAC condition than without the VAC. Our results show that in distal pointing placing objects at different depth planes has detrimental effect on the user performance.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {12},
numpages = {8},
keywords = {Distal Pointing, Ray Casting, Selection, Stereo Deficiencies, Vergence-Accommodation Conflict, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565609,
author = {Hung, Ching-Wen and Chang, Ruei-Che and Chen, Hong-Sheng and Liang, Chung Han and Chan, Liwei and Chen, Bing-Yu},
title = {Puppeteer: Exploring Intuitive Hand Gestures and Upper-Body Postures for Manipulating Human Avatar Actions},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565609},
doi = {10.1145/3562939.3565609},
abstract = {Body-controlled avatars provide a more intuitive method to real-time control virtual avatars but require larger environment space and more user effort. In contrast, hand-controlled avatars give more dexterous and fewer fatigue manipulations within a close-range space for avatar control but provide fewer sensory cues than the body-based method. This paper investigates the differences between the two manipulations and explores the possibility of a combination. We first performed a formative study to understand when and how users prefer manipulating hands and bodies to represent avatars’ actions in current popular video games. Based on the top video games survey, we decided to represent human avatars’ motions. Besides, we found that players used their bodies to represent avatar actions but changed to using hands when they were too unrealistic and exaggerated to mimic by bodies (e.g., flying in the sky, rolling over quickly). Hand gestures also provide an alternative to lower-body motions when players want to sit during gaming and do not want extensive effort to move their avatars. Hence, we focused on the design of hand gestures and upper-body postures. We present Puppeteer, an input prototype system that allows players directly control their avatars through intuitive hand gestures and upper-body postures. We selected 17 avatar actions discovered in the formative study and conducted a gesture elicitation study to invite 12 participants to design best representing hand gestures and upper-body postures for each action. Then we implemented a prototype system using the MediaPipe framework to detect keypoints and a self-trained model to recognize 17 hand gestures and 17 upper-body postures. Finally, three applications demonstrate the interactions enabled by Puppeteer.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {13},
numpages = {11},
keywords = {Body Posture, Camera system, Hand Gesture, Input Techniques, User-Defined Gesture, Video Game},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565633,
author = {Ratcliffe, Jack and Tokarchuk, Laurissa},
title = {Rich virtual feedback from sensorimotor interaction may harm, not help, learning in immersive virtual reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565633},
doi = {10.1145/3562939.3565633},
abstract = {Sensorimotor interactions in the physical world and in immersive virtual reality (IVR) offer different feedback. Actions in the physical world almost always offer multi-modal feedback: pouring a jug of water offers tactile (weight-change), aural (the sound of running water) and visual (water moving out the jug) feedback. Feedback from pouring a virtual jug, however, depends on the IVR’s design. This study examines if the richness of feedback from IVR actions causes a detectable cognitive impact on users. To do this, we compared verb-learning outcomes between two conditions in which participants make actions with objects and (1) audiovisual feedback is presented; (2) audiovisual feedback is not presented. We found that participants (n = 74) had cognitively distinct outcomes based on the type of audiovisual feedback experienced, with a high feedback experience harming learning outcomes compared with a low feedback one. This result has implications for IVR system design and theories of cognition and memorisation.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {14},
numpages = {12},
keywords = {embodiment, gestural input, interactive virtual environments, language, learning, motivation, presence, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565625,
author = {Kim, Sunbum and Shim, Youngbo Aram and Lee, Geehyuk},
title = {Exploration of Form Factor and Bimanual 3D Manipulation Performance of Rollable In-hand VR Controller},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565625},
doi = {10.1145/3562939.3565625},
abstract = {Virtual reality (VR) environments are expected to become future workspaces. An effective bimanual 3D manipulation technique would be essential to support this vision. A ball-shaped tangible input device that can be rolled in a hand is known to be useful for 3D object manipulation because such devices allow users to utilize their finger dexterity. In this study, we further explored the potential of a rollable in-hand controller. First, we evaluated the effects of its form factor on user behavior and performance. Although the size and shape of a rollable controller are expected to influence user behavior and performance, their effects have not been empirically explored in prior works. Next, we evaluated a rollable controller on bimanual 3D assembly tasks. A rollable controller may incur a high mental load as it requires users to use finger dexterity; therefore, the benefit of using such a device in each hand is not obvious. We found that a 5 cm-diameter ball-shaped controller was the most effective among the sizes and forms that we considered, and that a pair of in-hand rollable controllers showed significantly faster completion time than a pair of VR controllers for complex bimanual assembly tasks involving frequent rotations.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {15},
numpages = {11},
keywords = {Bimanual Task, Finger Dexterity, Rollable In-hand Controller, Tangible Interface, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565639,
author = {Han, Dongyun and Kim, Donghoon and Cho, Isaac},
title = {PORTAL: Portal Widget for Remote Target Acquisition and Control in Immersive Virtual Environments},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565639},
doi = {10.1145/3562939.3565639},
abstract = {This paper introduces PORTAL (POrtal widget for Remote Target Acquisition and controL) that allows the user to interact with out-of-reach objects in a virtual environment. We describe the PORTAL interaction technique for placing a portal widget and interacting with target objects through the portal. We conduct two formal user studies to evaluate PORTAL for selection and manipulation functionalities. The results show PORTAL supports participants to interact with remote objects successfully and precisely. Following that, we discuss its potential and limitations, and future works.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {16},
numpages = {11},
keywords = {Empirical studies in HCI, Remote Object Interaction},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565637,
author = {Zhou, Xiaoyan and Williams, Adam Sinclair and Ortega, Francisco Raul},
title = {Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object Augmented Reality Environment},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565637},
doi = {10.1145/3562939.3565637},
abstract = {As augmented reality (AR) technology and hardware become more mature and affordable, researchers have been exploring more intuitive and discoverable interaction techniques for immersive environments. This paper investigates multimodal interaction for 3D object manipulation in a multi-object AR environment. To identify the user-defined gestures, we conducted an elicitation study involving 24 participants and 22 referents using an augmented reality headset. It yielded 528 proposals and generated a winning gesture set with 25 gestures after binning and ranking all gesture proposals. We found that for the same task, the same gesture was preferred for both one and two-object manipulation, although both hands were used in the two-object scenario. We present the gestures and speech results, and the differences compared to similar studies in a single object AR environment. The study also explored the association between speech expressions and gesture stroke during object manipulation, which could improve the recognizer efficiency in augmented reality headsets.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {17},
numpages = {10},
keywords = {augmented reality, elicitation, gesture and speech interaction, multi-object AR environment, multimodal interaction},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565619,
author = {Mutasim, Aunnoy and Batmaz, Anil Ufuk and Hudhud Mughrabi, Moaaz and Stuerzlinger, Wolfgang},
title = {Performance Analysis of Saccades for Primary and Confirmatory Target Selection},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565619},
doi = {10.1145/3562939.3565619},
abstract = {In eye-gaze-based selection, dwell suffers from several issues, e.g., the Midas Touch problem. Here we investigate saccade-based selection techniques as an alternative to dwell. First, we designed a novel user interface (UI) for Actigaze and used it with (goal-crossing) saccades for confirming the selection of small targets (i.e., &lt; 1.5-2°). We compared it with three other variants of Actigaze (with button press, dwell, and target reverse crossing) and two variants of target magnification (with button press and dwell). Magnification-dwell exhibited the most promising performance. For Actigaze, goal-crossing was the fastest option but suffered the most errors. We then evaluated goal-crossing as a primary selection technique for normal-sized targets (≥ 2°) and implemented a novel UI for such interaction. Results revealed that dwell achieved the best performance. Yet, we identified goal-crossing as a good compromise between dwell and button press. Our findings thus identify novel options for gaze-only interaction.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {18},
numpages = {12},
keywords = {Activation Methods, Eye-Gaze Tracking, Fitts’ Law, Saccade, Selection Techniques, Small Targets, Target Reverse Crossing, Throughput, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565641,
author = {Liu, Jen-Shuo and Tversky, Barbara and Feiner, Steven},
title = {Precueing Sequential Rotation Tasks in Augmented Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565641},
doi = {10.1145/3562939.3565641},
abstract = {Augmented reality has been used to improve sequential-task performance by cueing information about a current task step and precueing information about future steps. Existing work has shown the benefits of precueing movement (translation) information. However, rotation is also a major component in many real-life tasks, such as turning knobs to adjust parameters on a console. We developed an AR testbed to investigate whether and how much precued rotation information can improve user performance. We consider two unimanual tasks: one requires a user to make sequential rotations of a single object, and the other requires the user to move their hand between multiple objects to rotate them in sequence. We conducted a user study to explore these two tasks using circular arrows to communicate rotation. In the single-object task, we examined the impact of number of precues and visualization style on user performance. Results show that precues improved performance and that arrows with highlighted heads and tails, with each destination aligned with the next origin, yielded the shortest completion time on average. In the multiple-object task, we explored whether rotation precues can be helpful in conjunction with movement precues. Here, using a rotation cue without rotation precues in conjunction with a movement cue and movement precues performed the best, implying that rotation precues were not helpful when movement was also required.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {19},
numpages = {11},
keywords = {cueing, object rotation, precueing},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565627,
author = {Gottsacker, Matt and Norouzi, Nahal and Schubert, Ryan and Guido-Sanz, Frank and Bruder, Gerd and Welch, Gregory},
title = {Effects of Environmental Noise Levels on Patient Handoff Communication in a Mixed Reality Simulation},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565627},
doi = {10.1145/3562939.3565627},
abstract = {When medical caregivers transfer patients to another person’s care (a patient handoff), it is essential they effectively communicate the patient’s condition to ensure the best possible health outcomes. Emergency situations caused by mass casualty events (e.g., natural disasters) introduce additional difficulties to handoff procedures such as environmental noise. We created a projected mixed reality simulation of a handoff scenario involving a medical evacuation by air and tested how low, medium, and high levels of helicopter noise affected participants’ handoff experience, handoff performance, and behaviors. Through a human-subjects experimental design study (N = 21), we found that the addition of noise increased participants’ subjective stress and task load, decreased their self-assessed and actual performance, and caused participants to speak louder. Participants also stood closer to the virtual human sending the handoff information when listening to the handoff than they stood to the receiver when relaying the handoff information. We discuss implications for the design of handoff training simulations and avenues for future handoff communication research.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {20},
numpages = {10},
keywords = {Patient handoffs, environmental noise, human-subject research, virtual and mixed reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565612,
author = {Fouch\'{e}, Gwendal and Argelaguet Sanz, Ferran and Faure, Emmanuel and Kervrann, Charles},
title = {Timeline Design Space for Immersive Exploration of Time-Varying Spatial 3D Data},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565612},
doi = {10.1145/3562939.3565612},
abstract = {Timelines are common visualizations to represent and manipulate temporal data. However, timeline visualizations rarely consider spatio-temporal 3D data (e.g. mesh or volumetric models) directly. In this paper, leveraging the increased workspace and 3D interaction capabilities of virtual reality (VR), we first propose a timeline design space for 3D temporal data extending the timeline design space proposed by Brehmer et al.&nbsp;[7]. The proposed design space adapts the scale, layout and representation dimensions to account for the depth dimension and how the 3D temporal data can be partitioned and structured. Moreover, an additional dimension is introduced, the support, which further characterizes the 3D dimension of the visualization. The design space is complemented by discussing the interaction methods required for the efficient visualization of 3D timelines in VR. Secondly, we evaluate the benefits of 3D timelines through a formal evaluation (n=21). Taken together, our results showed that time-related tasks can be achieved more comfortably using timelines, and more efficiently for specific tasks requiring the analysis of the surrounding temporal context. Finally, we illustrate the use of 3D timelines with a use-case on morphogenetic analysis in which domain experts in cell imaging were involved in the design and evaluation process.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {21},
numpages = {11},
keywords = {3D temporal data, Multidimensional data, Timelines, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565622,
author = {Menzel, Timo and Botsch, Mario and Latoschik, Marc Erich},
title = {Automated Blendshape Personalization for Faithful Face Animations Using Commodity Smartphones},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565622},
doi = {10.1145/3562939.3565622},
abstract = {Digital reconstruction of humans has various interesting use-cases. Animated virtual humans, avatars and agents alike, are the central entities in virtual embodied human-computer and human-human encounters in social XR. Here, a faithful reconstruction of facial expressions becomes paramount due to their prominent role in non-verbal behavior and social interaction. Current XR-platforms, like Unity 3D or the Unreal Engine, integrate recent smartphone technologies to animate faces of virtual humans by facial motion capturing. Using the same technology, this article presents an optimization-based approach to generate personalized blendshapes as animation targets for facial expressions. The proposed method combines a position-based optimization with a seamless partial deformation transfer, necessary for a faithful reconstruction. Our method is fully automated and considerably outperforms existing solutions based on example-based facial rigging or deformation transfer, and overall results in a much lower reconstruction error. It also neatly integrates with recent smartphone-based reconstruction pipelines for mesh generation and automated rigging, further paving the way to a widespread application of human-like and personalized avatars and agents in various use-cases.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {22},
numpages = {9},
keywords = {blendshapes, deformation transfer, face animation, facial rigging, personalization, virtual humans},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565616,
author = {Hiroi, Yuichi and Itoh, Yuta and Rekimoto, Jun},
title = {NeARportation: A Remote Real-time Neural Rendering Framework},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565616},
doi = {10.1145/3562939.3565616},
abstract = {While presenting a photorealistic appearance plays a major role in immersion in Augmented Virtuality environment, displaying that of real objects remains a challenge. Recent developments in photogrammetry have facilitated the incorporation of real objects into virtual space. However, reproducing complex appearances, such as subsurface scattering and transparency, still requires a dedicated environment for measurement and possesses a trade-off between rendering quality and frame rate. Our NeARportation framework combines server–client bidirectional communication and neural rendering to resolve these trade-offs. Neural rendering on the server receives the client’s head posture and generates a novel-view image with realistic appearance reproduction that is streamed onto the client’s display. By applying our framework to a stereoscopic display, we confirm that it can display a high-fidelity appearance on full-HD stereo videos at 35-40 frames per second (fps) according to the user’s head motion.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {23},
numpages = {5},
keywords = {appearance reproduction, augmented virtuality, neural rendering, real-time rendering, remote rendering},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565615,
author = {Chae, Joohwan and Kim, Donghan and Jeong, Wooseok and Jo, Eunchan and Jeong, Won-Ki and Choi, JunYoung and Kim, Seung-wook and Kim, Myoung Gon and Lee, Jae-Won and Lee, Hyechan and Han, JungHyun},
title = {Virtual Air Conditioner’s Airflow Simulation and Visualization in AR},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565615},
doi = {10.1145/3562939.3565615},
abstract = {This paper presents a mobile AR system for visualizing airflow and temperature change made by virtual air conditioners. Even though there have been efforts to integrate the results of airflow/temperature simulation into the real world via AR, they support neither interactive modeling of the environments nor real-time simulation. This paper presents an AR system, where 3D mapping and air conditioner installation are made interactively, and then airflow/temperature simulation and visualization are made at real time. The proposed system is designed in a client-server architecture, where the server is in charge of simulation and the rest is taken by the client.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {24},
numpages = {11},
keywords = {Augmented reality, Flow visualization, HCI, Physics-based simulation},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565617,
author = {Javerliat, Charles and Elst, Pierre-Philippe and Saive, Anne-Lise and Baert, Patrick and Lavou\'{e}, Guillaume},
title = {Nebula: An Affordable Open-Source and Autonomous Olfactory Display for VR Headsets},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565617},
doi = {10.1145/3562939.3565617},
abstract = {The impact of olfactory cues on user experience in virtual reality is increasingly studied. However, results are still heterogeneous and existing studies difficult to replicate, mainly due to a lack of standardized olfactory displays. In that context, we present Nebula, a low-cost, open-source, olfactory display capable of diffusing scents at different diffusion rates using a nebulization process. Nebula can be used with PC VR or autonomous head-mounted displays, making it easily transportable without the need for an external computer. The device was calibrated to diffuse at three diffusion rates: no diffusion, low and high. For each level, the quantity of delivered odor was precisely characterized using a repeated weighting method. The corresponding perceived olfactory intensities were evaluated by a psychophysical experiment on sixteen participants. Results demonstrated the device capability to successfully create three significantly different perceived odor intensities (Friedman test p &lt; 10− 6, Wilcoxon tests padj &lt; 10− 3), without noticeable smell persistence and with limited noise and discomfort. For reproducibility and to stimulate further research in the area, 3D printing files, electronic hardware schemes, and firmware/software source-code are made publicly available.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {25},
numpages = {8},
keywords = {Autonomous VR experiment, Wearable olfactory display},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565632,
author = {Chang, Ziyi and Koulieris, George Alex and Shum, Hubert P. H.},
title = {3D Reconstruction of Sculptures from Single Images via Unsupervised Domain Adaptation on Implicit Models},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565632},
doi = {10.1145/3562939.3565632},
abstract = {Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep learning based 3D reconstruction approaches allow us to recover 3D shapes from 2D observations, among which single-view-based approaches can reduce the need for human intervention and specialised equipment in acquiring 3D sculptures for VR museums. However, there exist two challenges when attempting to use the well-researched human reconstruction methods: limited data availability and domain shift. Considering sculptures are usually related to humans, we propose our unsupervised 3D domain adaptation method for adapting a single-view 3D implicit reconstruction model from the source (real-world humans) to the target (sculptures) domain. We have compared the generated shapes with other methods and conducted ablation studies as well as a user study to demonstrate the effectiveness of our adaptation method. We also deploy our results in a VR application.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {26},
numpages = {10},
keywords = {3D Reconstruction, Domain Adaptation, Transfer Learning, Unsupervised Learning, VR},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565620,
author = {Rasla, Alex and Beyeler, Michael},
title = {The Relative Importance of Depth Cues and Semantic Edges for Indoor Mobility Using Simulated Prosthetic Vision in Immersive Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565620},
doi = {10.1145/3562939.3565620},
abstract = {Visual neuroprostheses (bionic eyes) have the potential to treat degenerative eye diseases that often result in low vision or complete blindness. These devices rely on an external camera to capture the visual scene, which is then translated frame-by-frame into an electrical stimulation pattern that is sent to the implant in the eye. To highlight more meaningful information in the scene, recent studies have tested the effectiveness of deep-learning based computer vision techniques, such as depth estimation to highlight nearby obstacles (DepthOnly mode) and semantic edge detection to outline important objects in the scene (EdgesOnly mode). However, nobody has yet attempted to combine the two, either by presenting them together (EdgesAndDepth) or by giving the user the ability to flexibly switch between them (EdgesOrDepth). Here, we used a neurobiologically inspired model of simulated prosthetic vision (SPV) in an immersive virtual reality (VR) environment to test the relative importance of semantic edges and relative depth cues to support the ability to avoid obstacles and identify objects. We found that participants were significantly better at avoiding obstacles using depth-based cues as opposed to relying on edge information alone, and that roughly half the participants preferred the flexibility to switch between modes (EdgesOrDepth). This study highlights the relative importance of depth cues for SPV mobility and is an important first step towards a visual neuroprosthesis that uses computer vision to improve a user’s scene understanding.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {27},
numpages = {11},
keywords = {bionic vision, indoor mobility, scene simplification, simulated prosthetic vision, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565611,
author = {Wu, Fei and Suma Rosenberg, Evan},
title = {Adaptive Field-of-view Restriction: Limiting Optical Flow to Mitigate Cybersickness in Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565611},
doi = {10.1145/3562939.3565611},
abstract = {Dynamic field-of-view (FOV) restriction is a widely used software technique to mitigate cybersickness in commercial virtual reality (VR) applications. The classical FOV restrictor is implemented using a symmetric mask that occludes the periphery in response to translational and/or angular velocity. In this paper, we introduce adaptive field-of-view restriction, a novel technique that responds dynamically based on real-time assessment of optical flow generated by movement through a virtual environment. The adaptive restrictor utilizes an asymmetric mask to obscure regions of the periphery with higher optical flow during virtual locomotion while leaving regions with lower optical flow visible. To evaluate the proposed technique, we conducted a gender-balanced user study (N = 38) in which participants completed in a navigation task in two different types of virtual scenes using controller-based locomotion. Participants were instructed to navigate through either close-quarter or open virtual environments using adaptive restriction, traditional symmetric restriction, or an unrestricted control condition in three VR sessions separated by at least 24 hours. The results showed that the adaptive restrictor was effective in mitigating cybersickness and reducing subjective discomfort, while simultaneously enabling participants to remain immersed for a longer amount of time compared to the control condition. Additionally, presence ratings were significantly higher when using the adaptive restrictor compared to symmetric restriction. In general, these results suggest that adaptive field-of-view restriction based on real-time measurement of optical flow is a promising approach for virtual reality applications that seek to provide a better cost-benefit tradeoff between comfort and a high-fidelity experience.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {11},
keywords = {Virtual reality, cybersickness, field-of-view, optical flow},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565628,
author = {Kocur, Martin and Bogon, Johanna and Mayer, Manuel and Witte, Miriam and Karber, Amelie and Henze, Niels and Schwind, Valentin},
title = {Sweating Avatars Decrease Perceived Exertion and Increase Perceived Endurance while Cycling in Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565628},
doi = {10.1145/3562939.3565628},
abstract = {Avatars are used to represent users in virtual reality (VR) and create embodied experiences. Previous work showed that avatars’ stereotypical appearance can affect users’ physical performance and perceived exertion while exercising in VR. Although sweating is a natural human response to physical effort, surprisingly little is known about the effects of sweating avatars on users. Therefore, we conducted a study with 24 participants to explore the effects of sweating avatars while cycling in VR. We found that visualizing sweat decreases the perceived exertion and increases perceived endurance. Thus, users feel less exerted while embodying sweating avatars. We conclude that sweating avatars contribute to more effective exergames and fitness applications.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {29},
numpages = {12},
keywords = {Proteus effect, avatars, body ownership, exergames, perception of effort, sweating, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565638,
author = {Mahmud, M. Rasel and Stewart, Michael and Cordova, Alberto and Quarles, John},
title = {Standing Balance Improvement Using Vibrotactile Feedback in Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565638},
doi = {10.1145/3562939.3565638},
abstract = {Virtual Reality (VR) users often encounter postural instability, i.e., balance issues, which can be a significant impediment to universal usability and accessibility, particularly for those with balance impairments. Prior research has validated imbalance issues, but little effort has been made to mitigate them. We recruited 39 participants (with balance impairments: 18, without balance impairments: 21) to examine the effect of various vibrotactile feedback techniques on balance in virtual reality, specifically spatial vibrotactile, static vibrotactile, rhythmic vibrotactile, and vibrotactile feedback mapped to the center of pressure (CoP). Participants completed standing visual exploration and standing reach and grasp tasks. According to within-subject results, each vibrotactile feedback enhanced balance in VR significantly (p &lt;.001) for those with and without balance impairments. Spatial and CoP vibrotactile feedback enhanced balance significantly more (p &lt;.001) than other vibrotactile feedback. This study presents strategies that might be used in future virtual environments to enhance standing balance and bring VR closer to universal usage.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {30},
numpages = {11},
keywords = {accessibiliity, standing balance, vibrotactile feedback, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565614,
author = {Kocur, Martin and Kalus, Alexander and Bogon, Johanna and Henze, Niels and Wolff, Christian and Schwind, Valentin},
title = {The Rubber Hand Illusion in Virtual Reality and the Real World - Comparable but Different},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565614},
doi = {10.1145/3562939.3565614},
abstract = {Feeling ownership of a virtual body is crucial for immersive experiences in VR. Knowledge about body ownership is mainly based on rubber hand illusion (RHI) experiments in the real world. Watching a rubber hand being stroked while one’s own hidden hand is synchronously stroked, humans experience the rubber hand as their own hand and underestimate the distance between the rubber hand and the real hand (proprioceptive drift). There is also evidence for a decrease in hand temperature. Although the RHI has been induced in VR, it is unknown whether effects in VR and the real world differ. We conducted a RHI experiment with 24 participants in the real world and in VR and found comparable effects in both environments. However, irrespective of the RHI, proprioceptive drift and temperature differences varied between settings. Our findings validate the utilization of the RHI in VR to increase our understanding of embodying virtual avatars.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {12},
keywords = {avatars, body ownership illusion, disownership, proprioceptive drift, rubber hand illusion, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3567818,
author = {Dietz, Dennis and Oechsner, Carl and Ou, Changkun and Chiossi, Francesco and Sarto, Fabio and Mayer, Sven and Butz, Andreas},
title = {Walk This Beam: Impact of Different Balance Assistance Strategies and Height Exposure on Performance and Physiological Arousal in VR},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3567818},
doi = {10.1145/3562939.3567818},
abstract = {Dynamic balance is an essential skill for the human upright gait; therefore, regular balance training can improve postural control and reduce the risk of injury. Even slight variations in walking conditions like height or ground conditions can significantly impact walking performance. Virtual reality is used as a helpful tool to simulate such challenging situations. However, there is no agreement on design strategies for balance training in virtual reality under stressful environmental conditions such as height exposure. We investigate how two different training strategies, imitation learning, and gamified learning, can help dynamic balance control performance across different stress conditions. Moreover, we evaluate the stress response as indexed by peripheral physiological measures of stress, perceived workload, and user experience. Both approaches were tested against a baseline of no instructions and against each other. Thereby, we show that a learning-by-imitation approach immediately helps dynamic balance control, decreases stress, improves attention focus, and diminishes perceived workload. A gamified approach can lead to users being overwhelmed by the additional task. Finally, we discuss how our approaches could be adapted for balance training and applied to injury rehabilitation and prevention.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {32},
numpages = {12},
keywords = {balance control, physiological arousal, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565607,
author = {Cauquis, Julien and Mercado, Victor Rodrigo and Casiez, G\'{e}ry and Normand, Jean-Marie and L\'{e}cuyer, Anatole},
title = {“Kapow!”: Studying the Design of Visual Feedback for Representing Contacts in Extended Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565607},
doi = {10.1145/3562939.3565607},
abstract = {In absence of haptic feedback, the perception of contact with virtual objects can rapidly become a problem in extended reality (XR) applications. XR developers often rely on visual feedback to inform the user and display contact information. However, as for today, there is no clear path on how to design and assess such visual techniques. In this paper, we propose a design space for the creation of visual feedback techniques meant to represent contact with virtual surfaces in XR. Based on this design space, we conceived a set of various visual techniques, including novel approaches based on onomatopoeia and inspired by cartoons, or visual effects based on physical phenomena. Then, we conducted an online preliminary user study with 60 participants, consisting in assessing 6 visual feedback techniques in terms of user experience. We could notably assess, for the first time, the potential influence of the interaction context by comparing the participants’ answers in two different scenarios: industrial versus entertainment conditions. Taken together, our design space and initial results could inspire XR developers for a wide range of applications in which the augmentation of contact seems prominent, such as for vocational training, industrial assembly/maintenance, surgical simulation, videogames, etc.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {33},
numpages = {11},
keywords = {Contact, Design Space, Extended Reality, User Experience, Visual Feedback},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565635,
author = {Elsayed, Hesham and Kartono, Kenneth and Sch\"{o}n, Dominik and Schmitz, Martin and M\"{u}hlh\"{a}user, Max and Weigel, Martin},
title = {Understanding Perspectives for Single- and Multi-Limb Movement Guidance in Virtual 3D Environments},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565635},
doi = {10.1145/3562939.3565635},
abstract = {Movement guidance in virtual reality has many applications ranging from physical therapy, assistive systems to sport learning. These movements range from simple single-limb to complex multi-limb movements. While VR supports many perspectives – e.g., first person and third person – it remains unclear how accurate these perspectives communicate different movements. In a user study (N=18), we investigated the influence of perspective, feedback, and movement properties on the accuracy of movement guidance. Participants had on average an angle error of 6.2° for single arm movements, 7.4° for synchronous two arm movements, and 10.3° for synchronous two arm and leg movements. Furthermore, the results show that the two variants of third-person perspectives outperform a first-person perspective for movement guidance (19.9\% and 24.3\% reduction in angle errors). Qualitative feedback confirms the quantitative data and shows users have a clear preference for third-person perspectives. Through our findings we provide guidance for designers and developers of future VR movement guidance systems.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {34},
numpages = {10},
keywords = {Movement guidance, body visualization, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3562939.3565634,
author = {Vizcay, Sebastian and Kourtesis, Panagiotis and Argelaguet, Ferran and Pacchierotti, Claudio and Marchal, Maud},
title = {Design and Evaluation of Electrotactile Rendering Effects for Finger-Based Interactions in Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565634},
doi = {10.1145/3562939.3565634},
abstract = {The use of electrotactile feedback in Virtual Reality (VR) has shown promising results for providing tactile information and sensations. While progress has been made to provide custom electrotactile feedback for specific interaction tasks, it remains unclear which modulations and rendering algorithms are preferred in rich interaction scenarios. In this paper, we propose a unified tactile rendering architecture and explore the most promising modulations to render finger interactions in VR. Based on a literature review, we designed six electrotactile stimulation patterns/effects (EFXs) striving to render different tactile sensations. In a user study (N=18), we assessed the six EFXs in three diverse finger interactions: 1) tapping on a virtual object; 2) pressing down a virtual button; 3) sliding along a virtual surface. Results showed that the preference for certain EFXs depends on the task at hand. No significant preference was detected for tapping (short and quick contact); EFXs that render dynamic intensities or dynamic spatio-temporal patterns were preferred for pressing (continuous dynamic force); EFXs that render moving sensations were preferred for sliding (surface exploration). The results showed the importance of the coherence between the modulation an the interaction being performed and the study proved the versatility of electrotactile feedback and its efficiency in rendering different haptic information and sensations.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {35},
numpages = {11},
keywords = {electrotactile feedback, human computer interaction, virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

