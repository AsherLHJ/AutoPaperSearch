@INPROCEEDINGS{8446539,
  author={Ebrahimi, Elham and Hartman, Leah S. and Robb, Andrew and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Effects of Anthropomorphic Fidelity of Self-Avatars on Near Field Depth Perception in Immersive Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Immersive Virtual Environments (IVEs) are becoming more accessible and more widely utilized for training. Previous research has shown that the matching of visual and proprioceptive information is important for calibration. While research has demonstrated that self-avatars can enhance ones' sense of presence and improve distance perception, the effects of self-avatar fidelity on near field distance estimations has yet to be investigated. This study tested the effect of avatar fidelity on the accuracy of distance estimations in the near-field. Performance with a virtual avatar was also compared to real-world performance. Three levels of fidelity were tested; 1) an immersive self-avatar with realistic limbs, 2) a low-fidelity self-avatar showing only joint locations, and 3) end-effector only. The results suggest that reach estimations become more accurate as the visual fidelity of the avatar increases, with accuracy for high fidelity avatars approaching real-world performance as compared to low-fidelity and end-effector conditions. In all conditions reach estimations became more accurate after receiving feedback during a calibration phase.},
  keywords={Visualization;Estimation;Avatars;Calibration;Legged locomotion;Tracking;Wrist;Human-centered computing-Visualization-Empirical studies in visualization-;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI},
  doi={10.1109/VR.2018.8446539},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446152,
  author={Narang, Sahil and Best, Andrew and Manocha, Dinesh},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulating Movement Interactions Between Avatars & Agents in Virtual Worlds Using Human Motion Constraints}, 
  year={2018},
  volume={},
  number={},
  pages={9-16},
  abstract={We present an interactive algorithm to generate plausible movements for human-like agents interacting with other agents or avatars in a virtual environment. Our approach takes into account high-dimensional human motion constraints and bio-mechanical constraints to compute collision-free trajectories for each agent. We present a novel full-body movement constrained-velocity computation algorithm that can easily be combined with many existing motion synthesis techniques. Compared to prior local navigation methods, our formulation reduces artefacts that arise in dense scenarios and close interactions, and results in smoother and plausible locomotive behaviors. We have evaluated the benefits of our new algorithm in single-agent and multi-agent environments. We investigated the perception of a single agent's movements in dense scenarios and observed that our algorithm has a strong positive effect on the perceived quality of the simulation. Our approach also allows the user to interact with the agents from a first-person perspective in immersive settings. We conducted a study to investigate the perception of such avatar-agent interactions, and found that interactions generated using our approach lead to an increase in the user's sense of co-presence.},
  keywords={Two dimensional displays;Avatars;Trajectory;Navigation;Computational modeling;Collision avoidance;Solid modeling;multi-agent simulation;virtual reality;avatars;human agents;interactive navigation: Human-centered computing-User studies;Human-centered computing-Virtual reality;Computing methodologies-Artificial intelligence;Computing methodologies-Motion path planning;Computing methodologies-Modeling and simulation;},
  doi={10.1109/VR.2018.8446152},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446229,
  author={Lugrin, Jean-Luc and Ertl, Maximilian and Krop, Philipp and Klüpfel, Richard and Stierstorfer, Sebastian and Weisz, Bianka and Rück, Maximilian and Schmitt, Johann and Schmidt, Nina and Latoschik, Marc Erich},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Any “Body” There? Avatar Visibility Effects in a Virtual Reality Game}, 
  year={2018},
  volume={},
  number={},
  pages={17-24},
  abstract={This article presents an experiment exploring the possible impact of avatar's body part visibility on players' experience and performance using current Virtual Reality (VR) gaming platform capacities. In an action-based VR game, a player sees an avatar in first person perspective which is replicating his/her hand, head and body motion. In contrast to the expected outcome from non-game VR contexts, our results did not reveal significant differences with an avatar presenting an increasing number of visible body parts. The body ownership, immersion, emotional and cognitive involvements as well as the perceived control and difficulty were not improved with a more coherent virtual body. This tends to confirm the strong performance aspect of action-based games, whereby control efficiency and enemy awareness is paramount, and could overcome the perceptual, behavioural or emotional effects of avatar embodiment. Digital games are indeed prone to create an intense flow state typically reducing self-awareness, and focusing on the game completion and high performance achievements. However, further experiments with full-body tracking and different game types are necessary to confirm this trend. This research outcome motivates further analysis of the mutual influence of bottom-up and top-down factors of avatar embodiment causing psychophysical effects. In addition, it provides useful indications for VR game developers and researchers on possible effects and evaluation methods.},
  keywords={Avatars;Games;Visualization;Virtual environments;Torso;Mirrors;Human-centered computing-Human computer interaction (HCl)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446229},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446364,
  author={Volonte, Matias and Robb, Andrew and Duchowski, Andrew T. and Babu, Sabarish V.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Empirical Evaluation of Virtual Human Conversational and Affective Animations on Visual Attention in Inter-Personal Simulations}, 
  year={2018},
  volume={},
  number={},
  pages={25-32},
  abstract={Creating realistic animations of virtual humans remains comparatively complex and expensive. This research explores the degree to which animation fidelity affects users' gaze behavior when interacting in virtual reality training simulations that include virtual humans. Participants were randomly assigned to one of three conditions, wherein the virtual patient either: 1) was not animated; 2) played idle animations; or 3) played idle animations, looked at the participant when speaking, and lip-synced speech and facial gestures when conversing with the participant. Each participant's gaze was recorded in an inter-personal interactive patient surveillance simulation. Results suggest that conversational and passive animations elicited visual attention in a similar manner, as compared to the no animation condition. Results also suggest that when participants face critical situations in inter-personal medical simulations, visual attention towards the virtual human decreases while gaze towards goal directed activities increases.},
  keywords={Animation;Visualization;Solid modeling;Training;Virtual reality;Task analysis;Surveillance;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Animations;Evaluation/methodology;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;},
  doi={10.1109/VR.2018.8446364},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446320,
  author={Chaconas, Nikolas and Höllerer, Tobias},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Evaluation of Bimanual Gestures on the Microsoft HoloLens}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={We developed and evaluated two-handed gestures on the Microsoft HoloLens to manipulate augmented reality annotations through rotation and scale operations. We explore the design space of bimanual interactions on head-worn AR platforms, with the intention of dedicating two-handed gestures to rotation and scaling manipulations while reserving one-handed interactions to drawing annotations. In total, we implemented five techniques for rotation and scale manipulation gestures on the Microsoft HoloLens: three two-handed techniques, one technique for one-handed rotation and two-handed scale, and one baseline one-handed technique that represents standard HoloLens UI recommendations. Two of the bimanual interaction techniques involve axis separation for rotation whereas the third technique is fully 6DOF and modeled after the successful “spindle” approach from 3DUI literature. To evaluate our techniques, we conducted a study with 48 users. We recorded multiple performance metrics for each user on each technique, as well as user preferences. Results indicate that in spite of problems due to field-of-view limitations, certain two-handed techniques perform comparatively to the one-handed baseline technique in terms of accuracy and time. Furthermore, the best-performing two-handed technique outdid all other techniques in terms of overall user preference, demonstrating that bimanual gesture interactions can serve a valuable role in the UI toolbox on head-worn AR devices such as the HoloLens.},
  keywords={Task analysis;Three-dimensional displays;Augmented reality;Switches;Standards;Mice;Bars;Bimanual;two-handed;gestures;object manipulation;rotation;scale;evaluation;user study;augmented reality;HoloLens: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input devices and strategies;},
  doi={10.1109/VR.2018.8446320},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446381,
  author={Whitlock, Matt and Harnner, Ethan and Brubaker, Jed R. and Kane, Shaun and Szafir, Danielle Albers},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interacting with Distant Objects in Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={41-48},
  abstract={Augmented reality (AR) applications can leverage the full space of an environment to create immersive experiences. However, most empirical studies of interaction in AR focus on interactions with objects close to the user, generally within arms reach. As objects move farther away, the efficacy and usability of different interaction modalities may change. This work explores AR interactions at a distance, measuring how applications may support fluid, efficient, and intuitive interactive experiences in room-scale augmented reality. We conducted an empirical study (N = 20) to measure trade-offs between three interaction modalities-multimodal voice, embodied freehand gesture, and handhelds devices-for selecting, rotating, and translating objects at distances ranging from 8 to 16 feet (2.4m-4.9m). Though participants performed comparably with embodied freehand gestures and handheld remotes, they perceived embodied gestures as significantly more efficient and usable than device-mediated interactions. Our findings offer considerations for designing efficient and intuitive interactions in room-scale AR applications.},
  keywords={Task analysis;Usability;Visualization;Pervasive computing;Augmented reality;Electronic mail;Human-centered computing-Interaction design-Interaction design process and methods-User interface design},
  doi={10.1109/VR.2018.8446381},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446295,
  author={Grandi, Jerônimo G and Debarba, Henrique G and Bemdt, Iago and Nedel, Luciana and Maciel, Anderson},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Assessment of a Collaborative 3D Interaction Technique for Handheld Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={49-56},
  abstract={We present the design of a handheld-based interface for collaborative manipulations of 3D objects in mobile augmented reality. Our approach combines touch gestures and device movements for fast and precise control of 7-DOF transformations. Moreover, the interface creates a shared medium where several users can interact through their point-of-view and simultaneously manipulate 3D virtual augmentations. We evaluated our collaborative solution in two parts. First, we assessed our interface in single user mode, comparing the user task performance in three conditions: touch gestures, device movements and hybrid. Then, we conducted a study with 30 participants to understand and classify the strategies that arise while working in pairs, when partners are free to make their task organization. Furthermore, we investigated the effectiveness of simultaneous manipulations compared with the individual approach.},
  keywords={Three-dimensional displays;Collaboration;Task analysis;Augmented reality;Performance evaluation;Cameras;Handheld computers;Human-centered computing-Human computer interaction (HCI)-Interaction techniques;Human-centered computing-Human computer interaction CHCI)-Interaction paradigms-Mixed/augmented reality Human-centered computing-Collaborative and social computing},
  doi={10.1109/VR.2018.8446295},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446498,
  author={Mousas, Christos},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Performance-Driven Dance Motion Control of a Virtual Partner Character}, 
  year={2018},
  volume={},
  number={},
  pages={57-64},
  abstract={Taking advantage of motion capture and display technologies, a method giving a user the ability to control the dance motions of a virtual partner in an immersive setup was developed and is presented in this paper. The method utilizes a dance motion dataset containing the motion of both dancers (leader and partner). A hidden Markov model (HMM) was used to learn the structure of the dance motions. The HMM was trained on the motion of a chosen dancer (leader or partner), and during runtime, the system predicts the progress of the chosen dance motion, which corresponds to the progress of the user's motion. The regular structure of the HMM was extended by utilizing a jump state transition, allowing the user to improvise dance motions during the runtime. Since the jump state addition increases the model's complexity, an effort was made to optimize the prediction process to ensure runtime efficiency. A few corrective steps were also implemented to ensure the partner character's motions appear natural. A user study was conducted to understand the naturalness of the synthesized motion as well as the control that the user has on the partner character's synthesized motion.},
  keywords={Hidden Markov models;Robot kinematics;Virtual reality;Training;Real-time systems;Runtime;Human-centered computing-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Animation-Motion capture},
  doi={10.1109/VR.2018.8446498},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446546,
  author={Quaglia, Jordan T. and Holecek, Andrew},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Lucid Virtual Dreaming: Antecedents and Consequents of Virtual Lucidity During Virtual Threat}, 
  year={2018},
  volume={},
  number={},
  pages={1-72},
  abstract={Here we report the first empirical findings on virtual lucidity (VL), a new construct similar to lucidity during dreaming, but regarding awareness that one is having a virtual experience. VL concerns the depth and breadth of this awareness, as well as the extent it affords regulatory monitoring and control. To study VL, we adapted a measure from lucid dreaming research to assess whether more VL predicted lower fear, but not less enjoyment, during a virtual reality (VR) threat scenario of walking, and being asked to step off, a wooden plank seemingly high above a city. We examined predictors of VL and related outcomes across a community sample and lucid dream trainees at a meditation retreat center. In line with hypotheses, higher VL predicted less fear, more enjoyment, and greater likelihood of stepping off the plank. Moreover, a number of dispositional factors predicted greater VL and lower fear. Lucid dream retreatants, engaged in a contemplative practice called illusory form yoga, experienced more VL and less fear compared to nonretreatants, with marginally higher likelihood of stepping off the plank. Finally, VL mediated all significant relations between predictors and outcomes. Results held controlling for presence or fear of heights. We discuss the potential validity and utility of VL, its relation to presence, and examples of how it may inform the development and application of VR and related technologies.},
  keywords={Psychology;Monitoring;Legged locomotion;Virtual environments;Training;Urban areas;Lucid dreaming;meditation;mindfulness;presence;virtual lucidity;virtual reality.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences-Psychology},
  doi={10.1109/VR.2018.8446546},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446153,
  author={Gall, Dominik and Latoschik, Marc Erich},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Haptic Prediction Accuracy on Presence}, 
  year={2018},
  volume={},
  number={},
  pages={73-80},
  abstract={This paper reports on the effect of visually-anchored prediction accuracy of haptic information on the perceived presence of virtual environments. We designed an experiment which explicitly prevented confounding factors potentially introduced by virtual body ownership and/or agency. The experimental design consisted of two main conditions defining congruent vs incongruent visual and haptic cues. Presence was measured during as well as after exposure. A distance estimation task solely based on motor action and the visually-anchored spatial model of the environment was executed to control for perceptual binding. 56 healthy volunteers were randomly assigned to one of two groups in a single-blind mixed-group design study. The study revealed increased presence for high prediction accuracy and decreased presence for low prediction accuracy, while perceptual binding still occurred. The observed effect sizes were in the medium range. The results indicate a significant correlation between prediction accuracy of haptic information and the perceived realness and presence of a virtual environment which gives rise to a discussion about models for dissociative symptom derealisation.},
  keywords={Haptic interfaces;Virtual environments;Visualization;Solid modeling;Head-mounted displays;Estimation;Task analysis;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI},
  doi={10.1109/VR.2018.8446153},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446280,
  author={de Tinguy, Xavier and Pacchierotti, Claudio and Marchal, Maud and Lécuyer, Anatole},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enhancing the Stiffness Perception of Tangible Objects in Mixed Reality Using Wearable Haptics}, 
  year={2018},
  volume={},
  number={},
  pages={81-90},
  abstract={This paper studies the combination of tangible objects and wearable haptics for improving the display of stiffness sensations in virtual environments. Tangible objects enable to feel the general shape of objects, but they are often passive or unable to simulate several varying mechanical properties. Wearable haptic devices are portable and unobtrusive interfaces able to generate varying tactile sensations, but they often fail at providing convincing stiff contacts and distributed shape sensations. We propose to combine these two approaches in virtual and augmented reality (VR/AR), becoming able of arbitrarily augmenting the perceived stiffness of real/tangible objects by providing timely tactile stimuli at the fingers. We developed a proof-of-concept enabling to simulate varying elasticity/stiffness sensations when interacting with tangible objects by using wearable tactile modules at the fingertips. We carried out a user study showing that wearable haptic stimulation can well alter the perceived stiffness of real objects, even when the tactile stimuli are not delivered at the contact point. We illustrated our approach both in VR and AR, within several use cases and different tangible settings, such as when touching surfaces, pressing buttons and pistons, or holding an object. Taken together, our results pave the way for novel haptic sensations in VR/AR by better exploiting the multiple ways of providing simple, unobtrusive, and low-cost haptic displays.},
  keywords={Haptic interfaces;Skin;Shape;Belts;Virtual environments;Mechanical factors;Augmented reality;Human-centered computing-Human computer interaction-Interaction devices-Haptic devices},
  doi={10.1109/VR.2018.8446280},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446403,
  author={Yem, Vibol and Vu, Kevin and Kon, Yuki and Kajimoto, Hiroyuki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Electrical Stimulation Haptic Feedback on Perceptions of Softness-Hardness and Stickiness While Touching a Virtual Object}, 
  year={2018},
  volume={},
  number={},
  pages={89-96},
  abstract={With the advantages of small size and light weight, electrical stimulation devices have been investigated for providing haptic feedback in relation to virtual objects. Electrical stimulation devices can directly activate sensory receptors to produce a reaction force or touch sensations. In the current study, we tested a new method of electrically inducing force sensation in the fingertip, presenting haptic feedback designed to alter perceptions of softness, hardness and stickiness. We developed a 3D virtual reality system combined with finger-motion capture and electrical stimulation devices. We conducted two experiments to evaluate our electrical stimulation method and analyzed the effects of electrical stimulation on perception. The first experiment confirmed that participants could distinguish between the directions of the illusory force sensation, reporting whether the stimulation flexed their index finger forward or extended it backward. The second experiment examined the effects of the electric current itself on the intensity of their perception of the softness, hardness and stickiness of a virtual object.},
  keywords={Electrical stimulation;Force;Electrodes;Haptic interfaces;Current;Thumb;Softness-hardness perception;stickiness perception;electrical stimulation;virtual touch.: H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptic I/O},
  doi={10.1109/VR.2018.8446403},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446620,
  author={Weißker, Tim and Kunert, André and Fröhlich, Bernd and Kulik, Alexander},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatial Updating and Simulator Sickness During Steering and Jumping in Immersive Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={97-104},
  abstract={Many recent head-mounted display applications and games implement a range-restricted variant of teleportation for exploring virtual environments. This travel metaphor referred to as jumping only allows to teleport to locations in the currently visible part of the scene. In this paper, we present a formal description and classification scheme for teleportation techniques and its application to the classification of jumping. Furthermore, we present the results of a user study (N=24) that compared jumping to the more conventional steering with respect to spatial updating and simulator sickness. Our results show that despite significantly faster travel times during jumping, a majority of participants (75%) achieved similar spatial updating accuracies in both conditions (mean difference 0.02°, =5.05°. In addition, jumping induced significantly less simulator sickness, which altogether justifies it as an alternative to steering for the exploration of immersive virtual environments. However, application developers should be aware that spatial updating during jumping may be impaired for individuals.},
  keywords={Teleportation;Games;Space exploration;Task analysis;Virtual environments;Visualization;I.3.6 [Computer Graphics]: Methodology and Techniques―Interaction techniques H.1.2 [Models and Principles]: User/Machine Systems-Human information processing},
  doi={10.1109/VR.2018.8446620},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446210,
  author={Cao, Zekun and Jerald, Jason and Kopper, Regis},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visually-Induced Motion Sickness Reduction via Static and Dynamic Rest Frames}, 
  year={2018},
  volume={},
  number={},
  pages={105-112},
  abstract={Visually-induced motion sickness (VIMS), also known as cyber-sickness, is a major challenge for wide-spread Virtual Reality (VR) adoption. VIMS can be reduced in different ways, for example by using high-quality tracking systems and reducing the user's field of view. However, there are no universal solutions for all situations, and a wide variety of techniques are needed in order for developers to choose the most appropriate options depending on their needs. One way to reduce VIMS is through the use of rest frames-portions of the virtual environment that remain fixed in relation to the real world and do not move as the user virtually moves. We report the results of two multi-day within-subjects studies with 44 subjects who used virtual travel to navigate the environment. In the first study, we investigated the influence of static rest frames with fixed opacity on user comfort. For the second study, we present an enhanced version of rest frames that we call dynamic rest frames, where the opacity of the rest frame changes in response to visually perceived motion as users virtually traversed the virtual environment. Results show that a virtual environment with a static or dynamic rest frame allowed users to travel through more waypoints before stopping due to discomfort compared to a virtual environment without a rest frame. Further, a virtual environment with a static rest frame was also found to result in more real-time reported comfort than when there was no rest frame.},
  keywords={Radio frequency;Virtual environments;Visualization;Games;Nose;Metals;Navigation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446210},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446194,
  author={Arafat, Imtiaz Muhammad and Shahnewaz Ferdous, Sharif Mohammad and Quarles, John},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cybersickness-Provoking Virtual Reality Alters Brain Signals of Persons with Multiple Sclerosis}, 
  year={2018},
  volume={},
  number={},
  pages={1-120},
  abstract={This study investigates and compares brain signals between persons with and without Multiple Sclerosis (MS) when exposed to cybersickness-provoking Virtual Reality (VR). Cybersickness is a set of discomforts and commonly triggered by VR exposure. It has symptoms similar to motion sickness, such as dizziness, nausea, and disorientation etc. Although cybersickness has been studied for decades, populations with neurological disabilities, such as MS, have remained minimally studied. Cybersickness could have negative impact on effectiveness of VR-based rehabilitation systems and limit the accessibility of VR for persons with disabilities. MS can disrupt communication between neurons (signal carrying nerve cells) from different areas of the brain. Cybersickness also can affect brain signals, for example, frequency powers may change due to cybersickness. This study investigates the combination of MS and cybersickness in terms of brain signals. To investigate the effect of cybersickness on participants' brain signals, electroencephalogram (EEG) data were recorded before, during and after exposure to a cybersickness-provoking VR driving simulation. The EEG data suggests that in response to cybersickness-provoking VR exposure, participants with MS have mostly shown similar changes in brain activity with different magnitudes than participants without MS. Also, for at least one scalp location we have found completely opposite brain signals in MS-Group when compared to Non-MS-Group. Difference in magnitude or completely different trend in brain signals can imply that cybersickness affects persons with MS differently than persons without MS and may be different cybersickness reduction techniques are required for different populations.},
  keywords={Electroencephalography;Brain modeling;Neurons;Multiple sclerosis;Task analysis;Automobiles;Virtual reality;Cybersickness;Multiple Sclerosis (MS);VR;VE;Electroencephalogram (EEG);ERSP;ERP;User studies and evaluation;Ethical issues in VR.: Social and professional topics-User characteristics-People with disabilities;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/VR.2018.8446194},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446195,
  author={Stauffert, Jan-Philipp and Niebling, Florian and Latoschik, Marc Erich},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Latency Jitter on Simulator Sickness in a Search Task}, 
  year={2018},
  volume={},
  number={},
  pages={121-127},
  abstract={Low latency is a fundamental requirement for Virtual Reality (VR) systems to reduce the potential risks of cybersickness and to increase effectiveness, efficiency and user experience. In contrast to the effects of uniform latency degradation, the influence of latency jitter on user experience in VR is not well researched, although today's consumer VR systems are vulnerable in this respect. In this work we report on the impact of latency jitter on cybersickness in HMD-based VR environments. Test subjects are given a search task in Virtual Reality, provoking both head rotation and translation. One group experienced artificially added latency jitter in the tracking data of their head-mounted display. The introduced jitter pattern was a replication of a real-world latency behavior extracted and analyzed from an existing example VR-system. The effects of the introduced latency jitter were measured based on self-reports simulator sickness questionnaire (SSQ) and by taking physiological measurements. We found a significant increase in self-reported simulator sickness. We therefore argue that measure and control of latency based on average values taken at a few time intervals is not enough to assure a required timeliness behavior but that latency jitter needs to be considered when designing experiences for Virtual Reality.},
  keywords={Jitter;Task analysis;Virtual reality;Delays;Probability distribution;Tracking;D.1.3 [Programming Techniques]: Concurrent Programming-Parallel programming;D.4.8 [Operating Systems]: Performance-Measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446195},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446432,
  author={Riem, Lara and Van Dehy, Jacob and Onushko, Tanya and Beardsley, Scott},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Inducing Compensatory Changes in Gait Similar to External Perturbations Using an Immersive Head Mounted Display}, 
  year={2018},
  volume={},
  number={},
  pages={128-135},
  abstract={Understanding the sensorimotor control mechanisms that mediate gait compensation during environmental perturbation is a crucial step in developing tailored rehabilitative therapies to restore ambulation in patient populations. Current methods to evaluate the effects of environmental perturbations involve costly systems that physically perturb patients to elicit a compensatory response. Studies have shown that visual feedback alone can elicit dramatic changes in gait; however, the impact of fully immersive visual feedback is not well studied. Here we examined whether a low cost immersive virtual reality (VR) system can elicit perturbation responses similar to a physical disruption. We examined the responses of 11 subjects as they walked through a VR environment consisting of a bridge spanning a lake. While subjects walked on a treadmill mounted to a 6 degree-of-freedom motion base, pseudorandom roll perturbations (3, 6, 11 deg.) were applied visually to the bridge with (VP trials) and without (V trials) the corresponding physical displacement of the motion base. Significant differences were found between normal (unperturbed) walking and normal walking in the VR environment (p<;.05) for average step length, width, and Margin of Stability (MoS). Significant differences were also observed between unperturbed and perturbed walking in the VR environment (p<;0.05 for VP and V trials). While the subjects' responses to visual perturbations were generally lower than to combined visual and physical perturbations, the differences were not statistically significant (p>.05). The results demonstrate that visual perturbations provided in an immersive virtual environment can induce compensatory changes in gait during treadmill walking that are consistent with a physical perturbation. The application of environmental perturbations in VR systems could provide a cost-effective approach for gait rehabilitation in patient populations.},
  keywords={Perturbation methods;Visualization;Legged locomotion;Virtual environments;Bridges;Sociology;Statistics;Virtual Reality;Head Mounted Display;Visualization.: H.5.1 [Multimedia Information Systems]: Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446432},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446180,
  author={Lynch, Sean D. and Pettré, Julien and Bruneau, Julien and Kulpa, Richard and Crétual, Armel and Olivier, Anne-Helene},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Virtual Human Gaze Behaviour During an Orthogonal Collision Avoidance Walking Task}, 
  year={2018},
  volume={},
  number={},
  pages={136-142},
  abstract={This paper presents a study performed in virtual reality on the effect of gaze interception during collision avoidance between two walkers. In such a situation, mutual gaze can be considered as a form of nonverbal communication. Additionally, gaze is believed to detail future path intentions and to be part of the nonverbal negotiation to achieve avoidance collaboratively. We considered an avoidance task between a real subject and a virtual human character and studied the influence of the character's gaze direction on the avoidance behaviour of the participant. Virtual reality provided an accurate control of the situation: seventeen participants were immersed in a virtual environment, instructed to navigate across a virtual space using a joystick and to avoid a virtual character that would appear from either side. The character would either gaze or not towards the participant. Further, the character would either perform or not a reciprocal adaptation of its trajectory to avoid a potential collision with the participant. The findings of this paper were that during an orthogonal collision avoidance task, gaze behaviour did not influence the collision avoidance behaviour of the participants. Further, the addition of reciprocal collision avoidance with gaze did not modify the collision behaviour of participants. These results suggest that for the duration of interaction in such a task, body motion cues were sufficient for coordination and regulation. We discuss the possible exploitation of these results to improve the design of virtual characters for populated virtual environments and user interaction.},
  keywords={Collision avoidance;Task analysis;Trajectory;Legged locomotion;Virtual environments;Navigation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality J.4 [Social and behavioural Sciences]: Psychology},
  doi={10.1109/VR.2018.8446180},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446177,
  author={Boldt, Mette and Bonfert, Michael and Lehne, Inga and Cahnbley, Melina and Korschinq, Kim and Bikas, Loannis and Finke, Stefan and Hanci, Martin and Kraft, Valentin and Liu, Boxuan and Nguyen, Tram and Panova, Alina and Singh, Ramneek and Steenbergen, Alexander and Malaka, Rainer and Smeddinck, Jan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={You Shall Not Pass: Non-Intrusive Feedback for Virtual Walls in VR Environments with Room-Scale Mapping}, 
  year={2018},
  volume={},
  number={},
  pages={143-150},
  abstract={Room-scale mapping facilitates natural locomotion in virtual reality (VR), but it creates a problem when encountering virtual walls. In traditional video games, player avatars can simply be prevented from moving through walls. This is not possible in VR with room-scale mapping due to the lack of physical boundaries. Game design is either limited by avoiding walls, or the players might ignore them, which endangers the immersion and the overall game experience. To prevent players from walking through walls, we propose a combination of auditory, visual, and vibrotactile feedback for wall collisions. This solution can be implemented with standard game engine features, does not require any additional hardware or sensors, and is independent of game concept and narrative. A between-group study with 46 participants showed that a large majority of players without the feedback did pass through virtual walls, while 87% of the participants with the feedback refrained from walking through walls. The study found no notable differences in game experience.},
  keywords={Games;Legged locomotion;Visualization;Hardware;Haptic interfaces;Resists;Vibrations;Virtual reality;virtual walls;tactile feedback;haptic feedback;visual feedback;auditory feedback;locomotion;game design;K.8.0 [Personal Computing]: General - games;H.5.2 [Information interfaces and presentation]: User Interfaces. - Interaction styles},
  doi={10.1109/VR.2018.8446177},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446250,
  author={Grubert, Jens and Witzani, Lukas and Ofek, Eyal and Pahud, Michel and Kranz, Matthias and Kristensson, Per Ola},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Hand Representations for Typing in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={151-158},
  abstract={Alphanumeric text entry is a challenge for Virtual Reality (VR) applications. VR enables new capabilities, impossible in the real world, such as an unobstructed view of the keyboard, without occlusion by the user's physical hands. Several hand representations have been proposed for typing in VR on standard physical keyboards. However, to date, these hand representations have not been compared regarding their performance and effects on presence for VR text entry. Our work addresses this gap by comparing existing hand representations with minimalistic fingertip visualization. We study the effects of four hand representations (no hand representation, inverse kinematic model, fingertip visualization using spheres and video inlay) on typing in VR using a standard physical keyboard with 24 participants. We found that the fingertip visualization and video inlay both resulted in statistically significant lower text entry error rates compared to no hand or inverse kinematic model representations. We found no statistical differences in text entry speed.},
  keywords={Keyboards;Visualization;Error analysis;Decoding;Virtual reality;Standards;Electronic mail;H.5.2: [User Interfaces - Input devices and strategies.]},
  doi={10.1109/VR.2018.8446250},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446059,
  author={Grubert, Jens and Witzani, Lukas and Ofek, Eyal and Pahud, Michel and Kranz, Matthias and Kristensson, Per Ola},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text Entry in Immersive Head-Mounted Display-Based Virtual Reality Using Standard Keyboards}, 
  year={2018},
  volume={},
  number={},
  pages={159-166},
  abstract={We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40-45% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing.},
  keywords={Keyboards;Visualization;Electronic mail;Virtual reality;Error analysis;Performance evaluation;User interfaces;H.5.2: [User Interfaces - Input devices and strategies.]},
  doi={10.1109/VR.2018.8446059},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446217,
  author={Batmaz, Anil Ufuk and de Mathelin, Michel and Dresp-Langley, Birgitta},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Image Size and Structural Complexity on Time and Precision of Hand Movements in Head Mounted Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={167-174},
  abstract={The effective design of virtual reality (VR) simulators requires a deeper understanding of VR mediated human actions such as hand movements, with specifically tailored experiments testing how different design parameters affect performance. The present experiment investigates the time and precision of hand (index finger) movements under varying conditions of structural complexity and image size in VR without tactile feed-back from object to hand/finger. 18 right-handed subjects followed a complex and a simple physiological structure of small, medium and large size in VR, with the index finger of one of their two hands, from right to left, and from left to right. The results show that subjects performed best with small-size-simple structures and large-size-complex structures in VR. Movement execution was generally faster and more precise on simple structures. Performance was less precise when the dominant hand was used to follow the complex structures and small object size in VR. It is concluded that both size and structural complexity critically influence task execution in VR when no tactile feed-back from object to finger is generated. Individual learning curves should be monitored from the beginning of the training as suggested by the individual speed-precision analyses.},
  keywords={Indexes;Complexity theory;Three-dimensional displays;Head;Software;Image color analysis;Virtual reality;Computing methodologies-Computer Graphics-Graphic systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices;Human-centered computing-Interaction design;Software and its engineering-Software organization and properties-Virtual worlds software-Virtual worlds training simulations},
  doi={10.1109/VR.2018.8446217},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448284,
  author={Höll, Markus and Oberweger, Markus and Arth, Clemens and Lepetit, Vincent},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Efficient Physics-Based Implementation for Realistic Hand-Object Interaction in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={175-182},
  abstract={We propose an efficient physics-based method for dexterous `real hand' - `virtual object' interaction in Virtual Reality environments. Our method is based on the Coulomb friction model, and we show how to efficiently implement it in a commodity VR engine for realtime performance. This model enables very convincing simulations of many types of actions such as pushing, pulling, grasping, or even dexterous manipulations such as spinning objects between fingers without restrictions on the objects' shapes or hand poses. Because it is an analytic model, we do not require any prerecorded data, in contrast to previous methods. For the evaluation of our method, we conduction a pilot study that shows that our method is perceived more realistic and natural, and allows for more diverse interactions. Further, we evaluate the computational complexity of our method to show real-time performance in VR environments.},
  keywords={Friction;Computational modeling;Grasping;Solid modeling;Three-dimensional displays;Real-time systems;I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling-Physically-based Modeling;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Direct Manipulation},
  doi={10.1109/VR.2018.8448284},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447555,
  author={Verschoor, Mickeal and Lobo, Daniel and Otaduy, Miguel A.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Soft Hand Simulation for Smooth and Robust Natural Interaction}, 
  year={2018},
  volume={},
  number={},
  pages={183-190},
  abstract={Natural hand-based interaction should feature hand motion that adapts smoothly to the tracked user's motion, reacts robustly to contact with objects in a virtual environment, and enables dexterous manipulation of these objects. In our work, we enable all these properties thanks to an efficient soft hand simulation model. This model integrates an articulated skeleton, nonlinear soft tissue and frictional contact, to provide the realism necessary for natural interaction. Robust and smooth interaction is made possible by simulating in a single energy minimization framework all the mechanical energy exchanges among elements of the hand: coupling between the hand's skeleton and the user's motion, constraints at skeletal joints, nonlinear soft skin deformation, coupling between the hand's skeleton and the soft skin, frictional contact between the skin and virtual objects, and coupling between a grasped object and other virtual objects. We have put our effort on describing all elements of the hand that provide for realism and natural interaction, while ensuring minimal and bounded computational cost, which is key for smooth and robust interaction. As a result, we accomplish hand simulation as an asset that can be connected to diverse input tracking devices, and seamlessly integrated in game engines for fast deployment in VR applications.},
  keywords={Skin;Skeleton;Computational modeling;Couplings;Robustness;Strain;Tracking;Hand Interaction;VR;Simulation: Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques},
  doi={10.1109/VR.2018.8447555},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446352,
  author={Wienrich, Carolin and Gross, Richard and Kretschmer, Felix and Müller-Plath, Gisela},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Developing and Proving a Framework for Reaction Time Experiments in VR to Objectively Measure Social Interaction with Virtual Agents}, 
  year={2018},
  volume={},
  number={},
  pages={191-198},
  abstract={Which features and characteristics must virtual agents have to elicit social presence in humans? In order to investigate this objectively, experimental paradigms relying on implicit cognitive phenomena can be employed. In the present study, two aims were reached: First, we built a mixed reality apparatus using affordable consumer VR hardware and other off-the-shelf components to detect implicit cognitive phenomena on the basis of small reaction times with a precision of several milliseconds. Secondly, using this apparatus, we showed that a virtual agent can indeed elicit an implicit phenomenon which was hitherto assumed to be restricted to human participants: social Inhibition of Return (sIOR). sIOR is indicated by an increase in reaction time of about 20 ms that occurs when one subject reaches to the same target to which a co-actor had just responded to [19]. Skarratt, Cole and Kingstone [17] found that only a real co-actor, but not an animated one could induce this inhibitory effect. However, their virtual agent was displayed as a video on a 2D-TV-screen, which was only weakly immersive. In contrast, we found in two experiments that an animated agent presented via a more immersive VR headset could elicit the sIOR phenomenon. Further research has to be done to identify the reasons why this agent - or why VR per se - evokes an sIOR phenomenon. Moreover, the underlying mechanism of why sIOR occurs are not fully understood yet. The current findings open the way to further investigate these questions in experimental settings relying on VR.},
  keywords={Virtual reality;Cognition;Headphones;Laboratories;Task analysis;Fingers;Light emitting diodes;Social cognition;Social inhibition of return;Agents;Mixed reality;Reaction times;Arduino.: [Human-centered] [Computing] Mixed / augmented reality;Virtual reality;[Computing methodologies] Virtual reality},
  doi={10.1109/VR.2018.8446352},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446480,
  author={Bönsch, Andrea and Radke, Sina and Overath, Heiko and Asché, Laura M. and Wendt, Jonathan and Vierjahn, Tom and Habel, Ute and Kuhlen, Torsten W.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Social VR: How Personal Space is Affected by Virtual Agents' Emotions}, 
  year={2018},
  volume={},
  number={},
  pages={199-206},
  abstract={Personal space (PS), the flexible protective zone maintained around oneself, is a key element of everyday social interactions. It, e.g., affects people's interpersonal distance and is thus largely involved when navigating through social environments. However, the PS is regulated dynamically, its size depends on numerous social and personal characteristics and its violation evokes different levels of discomfort and physiological arousal. Thus, gaining more insight into this phenomenon is important. We contribute to the PS investigations by presenting the results of a controlled experiment in a CAVE, focusing on German males in the age of 18 to 30 years. The PS preferences of 27 participants have been sampled while they were approached by either a single embodied, computer-controlled virtual agent (VA) or by a group of three VAs. In order to investigate the influence of a VA's emotions, we altered their facial expression between angry and happy. Our results indicate that the emotion as well as the number of VAs approaching influence the PS: larger distances are chosen to angry VAs compared to happy ones; single VAs are allowed closer compared to the group. Thus, our study is a foundation for social and behavioral studies investigating PS preferences.},
  keywords={Skin;Navigation;Physiology;Electronic mail;Shape;Brain;Virtual reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;J.4 [Computer Applications]: Social And Behavioral Sciences-Psychology},
  doi={10.1109/VR.2018.8446480},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446575,
  author={Wienrich, C. and Schindler, K. and Döllinqer, N. and Kock, S. and Traupe, O.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Social Presence and Cooperation in Large-Scale Multi-User Virtual Reality - The Relevance of Social Interdependence for Location-Based Environments}, 
  year={2018},
  volume={},
  number={},
  pages={207-214},
  abstract={Introduction. An increasing number of location-based entertainment centers offers the possibility of entering multi-user virtual reality (VR) scenarios. Until now, neither cognition and emotions of users nor team experience have been scientifically evaluated in such an application. The present study investigated the gain of positive social interdependence while experiencing an adventure on the Immersive Deck of Illusion Walk (Berlin, Germany). Method. The preliminary version of a VR group adventure of the company was enriched by a task establishing social interdependence (IDP condition). The impact of IDP on social presence and cooperation (i.e., mutual importance) was evaluated relative to a control task without interdependence (nIDP condition). Results. Social IDP increased social presence and cooperation among participants. Additionally, behavioral involvement (part of presence), certain aspects of the adventure experience, and the affective evaluation during the experience were positively influenced by IDP. Discussion. The present study showed that interdependence can substantially enhance social presence and cooperation (i.e., mutual importance) in a VR setting already characterized by social co-experience. Thus, it revealed one design option (social IDP) to improve the experience, particularly the social experience, of location-based entertainment. Conclusion. The present research addressed one goal of location-based VR hosts to scientifically established design principles for social and collective adventures by supporting the impact of “collectively mastering an adventurous challenge”. In addition, our evaluation demonstrated that the multi-modal tracking, the free movement, as well as the multi-user features enabled natural interaction with other users and the environment, and thereby engendered a comfortable social experience.},
  keywords={Virtual reality;Games;Task analysis;Human computer interaction;Psychology;Companies;Entertainment industry;Multi-user VR;Location-based Entertainment;Interdependence;Social Presence;Cooperation;Evaluation.: [HCI]: Collaborative interaction;Virtual Reality;Empirical studies in interaction design;[Human Centered Computing]: Mobile computing;[Hardware]: Haptic devices;[Computing Methodologies]: Tracking},
  doi={10.1109/VR.2018.8446575},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447550,
  author={Roth, Daniel and Klelnbeck, Constantin and Feigl, Tobias and Mutschler, Christopher and Latoschik, Marc Erich},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Beyond Replication: Augmenting Social Behaviors in Multi-User Virtual Realities}, 
  year={2018},
  volume={},
  number={},
  pages={215-222},
  abstract={This paper presents a novel approach for the augmentation of social behaviors in virtual reality (VR). We designed three visual transformations for behavioral phenomena crucial to everyday social interactions: eye contact, joint attention, and grouping. To evaluate the approach, we let users interact socially in a virtual museum using a large-scale multi-user tracking environment. Using a between-subject design (N = 125) we formed groups of five participants. Participants were represented as simplified avatars and experienced the virtual museum simultaneously, either with or without the augmentations. Our results indicate that our approach can significantly increase social presence in multi-user environments and that the augmented experience appears more thought-provoking. Furthermore, the augmentations seem also to affect the actual behavior of participants with regard to more eye contact and more focus on avatars/objects in the scene. We interpret these findings as first indicators for the potential of social augmentations to impact social perception and behavior in VR.},
  keywords={Avatars;Visualization;Electronic mail;Color;Human computer interaction;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR.2018.8447550},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446235,
  author={Krum, David M. and Kang, Sin-Hwa and Phan, Thai},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Influences on the Elicitation of Interpersonal Space with Virtual Humans}, 
  year={2018},
  volume={},
  number={},
  pages={223-9},
  abstract={The emergence of low cost virtual and augmented reality systems has encouraged the development of immersive training applications for medical, military, and many other fields. Many of the training scenarios for these various fields may require the presentation of realistic interactions with virtual humans. It is thus vital to determine the critical factors of fidelity required in those interactions to elicit naturalistic behavior on the part of trainees. Negative training may occur if trainees are inadvertently influenced to react in ways that are unexpected and unnatural, hindering proper learning and transfer of skills and knowledge back into real world contexts. In this research, we examined whether haptic priming (presenting an illusion of virtual human touch at the beginning of the virtual experience) and different locomotion techniques (either joystick or physical walking) might affect proxemic behavior in human users. The results of our study suggest that locomotion techniques can alter proxemic behavior in significant ways. Haptic priming did not appear to impact proxemic behavior, but did increase rapport and other subjective social measures. The results suggest that designers and developers of immersive training systems should carefully consider the impact of even simple design and fidelity choices on trainee reactions in social interactions.},
  keywords={Haptic interfaces;Training;Legged locomotion;Virtual environments;Atmospheric measurements;Particle measurements;Virtual humans;virtual reality;immersive training;fidelity;proxemics;haptic priming;locomotion techniques.: Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality;Human-centered computing-Interaction design-Interaction design process and methods-User interface design},
  doi={10.1109/VR.2018.8446235},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447549,
  author={Dong, Samuel and Höllerer, Tobias},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Re-Textured Geometry Modeling Using Microsoft HoloLens}, 
  year={2018},
  volume={},
  number={},
  pages={231-237},
  abstract={We implemented live-textured geometry model creation with immediate coverage feedback visualizations in AR on the Microsoft HoloLens. A user walking and looking around a physical space can create a textured model of the space, ready for remote exploration and AR collaboration. Out of the box, a HoloLens builds a triangle mesh of the environment while scanning and being tracked in a new environment. The mesh contains vertices, triangles, and normals, but not color. We take the video stream from the color camera and use it to color a UV texture to be mapped to the mesh. Due to the limited graphics memory of the HoloLens, we use a fixed-size texture. Since the mesh generation dynamically changes in real time, we use an adaptive mapping scheme that evenly distributes every triangle of the dynamic mesh onto the fixed-size texture and adapts to new geometry without compromising existing color data. Occlusion is also considered. The user can walk around their environment and continuously fill in the texture while growing the mesh in real-time. We describe our texture generation algorithm and illustrate benefits and limitations of our system with example modeling sessions. Having first-person immediate AR feedback on the quality of modeled physical infrastructure, both in terms of mesh resolution and texture quality, helps the creation of high-quality colored meshes with this standalone wireless device and a fixed memory footprint in real-time.},
  keywords={Image color analysis;Real-time systems;Geometry;Three-dimensional displays;Cameras;Solid modeling;Rendering (computer graphics);Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Computing methodologies-Computer graphics-Image manipulation-Texturing},
  doi={10.1109/VR.2018.8447549},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446135,
  author={Friston, Sebastian and Griffith, Elias and Swapp, David and Marshall, Alan and Steed, Anthony},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Profiling Distributed Virtual Environments by Tracing Causality}, 
  year={2018},
  volume={},
  number={},
  pages={238-245},
  abstract={Real-time interactive systems such as virtual environments have high performance requirements, and profiling is a key part of the optimisation process to meet them. Traditional techniques based on metadata and static analysis have difficulty following causality in asynchronous systems. In this paper we explore a new technique for such systems. Timestamped samples of the system state are recorded at instrumentation points at runtime. These are assembled into a graph, and edges between dependent samples recovered. This approach minimises the invasiveness of the instrumentation, while retaining high accuracy. We describe how our instrumentation can be implemented natively in common environments, how its output can be processed into a graph describing causality, and how heterogeneous data sources can be incorporated into this to maximise the scope of the profiling. Across three case studies, we demonstrate the efficacy of this approach, and how it supports a variety of metrics for comprehensively bench-marking distributed virtual environments.},
  keywords={Tools;Instruments;Synchronization;Virtual environments;Metadata;Haptic interfaces;profiling;benchmarking;tools;distributed;latency.: C.4 [Performance of Systems]-Measurement Techniques;D.2.8 [Software Engineering]: Metrics-Performance measures},
  doi={10.1109/VR.2018.8446135},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447557,
  author={Xu, Feng and Li, Dayang},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Software Based Visual Aberration Correction for HMDs}, 
  year={2018},
  volume={},
  number={},
  pages={246-250},
  abstract={When using current head-mounted displays (HMDs), users with optical aberrations need to wear the equipment on the top of their own glasses. As both the HMDs and the glasses require to be tightly attached to faces, wearing them together is very inconvenient and uncomfortable, and thus degrades user experiences heavily. In this paper, we propose a real-time image pre-correction technique to correct the aberrations purely by software. Users can take off their own glasses and enjoy the virtual reality (VR) experience through an ordinary HMD freely and comfortably. Furthermore, as our technique is not related to hardware, it is compatible with all the current commercial HMDs. Our technique is based on the observation that the refractive errors majorly cause the ideal retinal image to be convolved by certain kernels. So we pre-correct the image on the display according to the specific aberrations of a user, aiming to maximize the similarity between the convolved retinal image and the ideal image. To achieve real-time performance, we modify the energy function to have linear solutions and implement the optimization fully on GPU. The experiments and the user study indicate that without any changes on hardware, we generate better viewing experience of HMDs for users with optical aberrations.},
  keywords={Kernel;Real-time systems;Glass;Optical imaging;Resists;Deconvolution;visual aberration correction;image deconvolution;realtime rendering;image-based rendering: Computing methodologies-Computer graphics-Image manipulation-Image-based rendering;Human-centered computing-Human computer interaction-Interaction techniques},
  doi={10.1109/VR.2018.8447557},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446441,
  author={Mori, Shohei and Ikeda, Sei and Plopski, Alexander and Sandor, Christian},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={BrightView: Increasing Perceived Brightness of Optical See-Through Head-Mounted Displays Through Unnoticeable Incident Light Reduction}, 
  year={2018},
  volume={},
  number={},
  pages={251-258},
  abstract={Optical See-Through Head-Mounted Displays (OST-HMDs) lose the visibility of virtual contents under bright environment illumination due to their see-through nature. We demonstrate how a liquid crystal (LC) filter attached to an OST-HMD can be used to dynamically increase the perceived brightness of virtual content without impacting the perceived brightness of the real scene. We present a prototype OST-HMD that continuously adjusts the opacity of the LC filter to attenuate the environment light without users becoming aware of the change. Consequently, virtual content appears to be brighter. The proposed approach is evaluated in psychophysical experiments in three scenes, with 16, 31, and 31 participants, respectively. The participants were asked to compare the magnitude of brightness changes of both real and virtual objects, before and after dimming the LC filter over a period of 5, 10, and 20 seconds. The results showed that the participants felt increases in the brightness of virtual objects while they were less conscious of reductions of the real scene luminance. These results provide evidence for the effectiveness of our display design. Our design can be applied to a wide range of OST-HMDs to improve the brightness and hence realism of virtual content in augmented reality applications.},
  keywords={Brightness;Visualization;Prototypes;Lighting;Retina;Optical imaging;Optical attenuators;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality Interaction devices Displays and imagers},
  doi={10.1109/VR.2018.8446441},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448287,
  author={Wang, Duotun and Kubrlcht, James and Zhu, Yixin and Lianq, Wei and Zhu, Song-Chun and Jiang, Chenfanfu and Lu, Hongjing},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatially Perturbed Collision Sounds Attenuate Perceived Causality in 3D Launching Events}, 
  year={2018},
  volume={},
  number={},
  pages={259-266},
  abstract={When a moving object collides with an object at rest, people immediately perceive a causal event: i.e., the first object has launched the second object forwards. However, when the second object's motion is delayed, or is accompanied by a collision sound, causal impressions attenuate and strengthen. Despite a rich literature on causal perception, researchers have exclusively utilized 2D visual displays to examine the launching effect. It remains unclear whether people are equally sensitive to the spatiotemporal properties of observed collisions in the real world. The present study first examined whether previous findings in causal perception with audiovisual inputs can be extended to immersive 3D virtual environments. We then investigated whether perceived causality is influenced by variations in the spatial position of an auditory collision indicator. We found that people are able to localize sound positions based on auditory inputs in VR environments, and spatial discrepancy between the estimated position of the collision sound and the visually observed impact location attenuates perceived causality.},
  keywords={Three-dimensional displays;Visualization;Solid modeling;Delays;Spatiotemporal phenomena;Virtual environments;Causal perception;virtual reality;intuitive physics;visual capture;launching},
  doi={10.1109/VR.2018.8448287},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447562,
  author={Jung, Sungchul and Wisniewski, Pamela J. and Hughes, Charles E.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={In Limbo: The Effect of Gradual Visual Transition Between Real and Virtual on Virtual Body Ownership Illusion and Presence}, 
  year={2018},
  volume={},
  number={},
  pages={267-272},
  abstract={We present a study of the relative effects of gradual versus instantaneous transition between one's own body and a virtual surrogate body, and between one's real-world environment and a virtual environment. The approach uses a stereo camera attached to an HMD to provide the illusions of virtual body ownership and spatial presence in VR. We conducted the study in a static environment which is similar to the traditional rubber hand experiment platform. Since our transition method is a blending scheme between real and virtual contexts, our study investigates the direct use of real-world information during the transition to increase the dominant visual illusion in a virtual space. We also investigate the use of a conceptual stage, called Limbo, which is a transition phase that evokes anticipation of the virtual world, providing a psychological link between the real and virtual before we enter a totally virtual space. Our study of the transition effect shows that the Limbo state has a significant influence in one's illusions of virtual body ownership (VBOI) and presence.},
  keywords={Resists;Visualization;Cameras;Virtual reality;Legged locomotion;Rubber;Electronic mail;Transition;Virtual Body Ownership;Presence;Perception;User Study.: I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism―Virtual Reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems―Artificial;augmented;virtual realities H.5.1 [Information Interfaces and Presentation]: User Interfaces―User-centered design},
  doi={10.1109/VR.2018.8447562},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448293,
  author={Fribourg, Rebecca and Argelaguet, Ferran and Hoyet, Ludovic and Lécuyer, Anatole},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Studying the Sense of Embodiment in VR Shared Experiences}, 
  year={2018},
  volume={},
  number={},
  pages={273-280},
  abstract={In this paper, we explore the influence of sharing a virtual environment with another user on the sense of embodiment in virtual reality. For this aim, we conducted an experiment where users were immersed in a virtual environment while being embodied in an anthropomorphic virtual representation of themselves. To evaluate the influence of the presence of another user, two situations were studied: either users were immersed alone, or in the company of another user. During the experiment, participants performed a virtual version of the well-known whac-a-mole game, therefore interacting with the virtual environment, while sitting at a virtual table. Our results show that users were significantly more “efficient” (i.e., faster reaction times), and accordingly more engaged, in performing the task when sharing the virtual environment, in particular for the more competitive tasks. Also, users experienced comparable levels of embodiment both when immersed alone or with another user. These results are supported by subjective questionnaires but also through behavioural responses, e.g. users reacting to the introduction of a threat towards their virtual body. Taken together, our results show that competition and shared experiences involving an avatar do not influence the sense of embodiment, but can increase user engagement. Such insights can be used by designers of virtual environments and virtual reality applications to develop more engaging applications.},
  keywords={Avatars;Task analysis;Virtual environments;Visualization;Electronic mail;Games;Human-centered computing―Human computer interaction (HCI)―HCI design and evaluation methods─User studies;Human-centered computing―Human computer interaction (HCI)―Interaction paradigms―Virtual reality},
  doi={10.1109/VR.2018.8448293},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448292,
  author={Di Loreto, Cédric and Chardonnet, Jean-Rémy and Ryard, Julien and Rousseau, Alain},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={WoaH: A Virtual Reality Work-at-Height Simulator}, 
  year={2018},
  volume={},
  number={},
  pages={281-288},
  abstract={We present WoaH, a virtual reality work-at-height simulator aimed at (i) testing whether future workers are able to manage their stress when high up and thus easily detect susceptibility to vertigo, and (ii) training in a typical work-at-height engineering operation. The simulator is composed of a real ladder synchronized in position with a virtual one placed 11 meters above the ground in a virtual environment. Visualization is done through a head-mounted display (HMD). We conducted a first user study evaluating our simulator in terms of cybersickness, perceived realism and anxiety, through both subjective (questionnaires) and objective (electrodermal activity) measurements, and testing whether vibratory cues could enhance the level of anxiety felt. Results indicate that WoaH generates anxiety as expected and is perceived as realistic. Adding vibrations had significant impact on the perceived realism but not on the electro-dermal activity. These first results bring insights to future developments for a deployment in companies dealing with work at height.},
  keywords={Visualization;Virtual environments;Solid modeling;Navigation;Resists;Companies;Human-centered computing―Human computer interaction (HCI)―Interaction paradigms―Virtual reality;Software and its engineering―Software organization and properties―Virtual worlds software―Virtual worlds training simulations},
  doi={10.1109/VR.2018.8448292},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446242,
  author={Mei, Chao and Zahed, Bushra T. and Mason, Lee and Ouarles, John},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Joint Attention Training for Children with ASD - a VR Game Approach and Eye Gaze Exploration}, 
  year={2018},
  volume={},
  number={},
  pages={289-296},
  abstract={Joint attention is critical to the education and development of a child. Deficits in joint attention are considered by many researchers to be an early predictor of children with Autism Spectrum Disorder (ASD). Training of joint attention have been a significant topic in ASD intervention education research. We propose a novel joint attention training approach using a Customizable Virtual Human (CVH) and a Virtual Reality (VR) game to assist with joint attention training. Previous work has shown that CVHs potentially help the users with ASD to increase their performance in hand-eye coordination, motivate the users to play longer, as well as improve user experience in a training game. Based upon these discovered CVH benefits, we hypothesize that CVHs may also be beneficial in training joint attention for users with ASD. To test our hypothesis, we developed a CVH with customizable facial features in an educational game - Imagination Drums - and conducted a user study on adolescents with high functioning ASD to investigate the effects of CVHs. We collected users' eye-gaze data and task performance during the game to evaluate the users' joint attention with CVHs and the effectiveness of CVHs compared with Non-Customizable Virtual Humans (NCVHs). The study results showed that the CVH make the participants gaze less at the irrelevant area of the game's storyline (i.e. background), but surprisingly, also provided evidence that participants react slower to the CVH's joint attention bids, compared with NCVH. Overall, the study reveals insights of how users with ASD interact with CVHs and how these interactions affect joint attention.},
  keywords={Games;Training;Monitoring;Haptic interfaces;Image color analysis;Autism;Face;Customizable virtual human. Autism Spectrum Disorder. 3D interaction;H.5.2 [Information Interfaces and Presentation]: User Interfaces -Evaluation/methodology},
  doi={10.1109/VR.2018.8446242},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448290,
  author={Lang, Yining and Wei, Liang and Xu, Fang and Zhao, Yibiao and Yu, Lap-Fai},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={297-304},
  abstract={The recent popularity of consumer-grade virtual reality devices, such as Oculus Rift, HTC Vive, and Fove virtual reality headset, has enabled household users to experience highly immersive virtual environments. We take advantage of the commercial availability of these devices to provide a novel virtual reality-based driving training approach designed to help individuals improve their driving habits in common scenarios. Our approach first identifies improper driving habits of a user when he drives in a virtual city. Then it synthesizes a pertinent training program to help improve the users driving skills based on the discovered improper habits of the user. To apply our approach, a user first goes through a pre-evaluation test from which his driving habits are analyzed. The analysis results are used to drive optimization for synthesizing a training program. This training program is a personalized route which includes different traffic events. When the user drives along this route via a driving controller and an eye-tracking virtual reality headset, the traffic events he encounters will help him to improve his driving habits. To validate the effectiveness of our approach, we conducted a user study to compare our virtual reality-based driving training with other training methods. The user study results show that the participants trained by our approach perform better on average than those trained by other methods in terms of evaluation score and response time and their improvement is more persistent.},
  keywords={Training;Solid modeling;Roads;Vehicles;Virtual environments;Safety;Virtual Reality-Modeling and Simulation-Driver Training Simulator},
  doi={10.1109/VR.2018.8448290},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446216,
  author={Kruse, Lucie and Langbehn, Eike and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={I Can See on My Feet While Walking: Sensitivity to Translation Gains with Visible Feet}, 
  year={2018},
  volume={},
  number={},
  pages={305-312},
  abstract={Redirected walking allows users to explore immersive virtual environments by real walking even when the physical tracking space is limited. Redirected walking is usually implemented via translation gains, rotation gains, and curvature gains, while previous research was focused on identifying detection thresholds for such manipulations. To our knowledge, all previous experiments were conducted without a visual self-representation of the user in the virtual environment, in particular, without showing the user's feet. In this paper, we address the question if the virtual self-representation of the user's feet changes the detection thresholds for translation gains. Furthermore, we consider the influence of the holisticness of the visual stimulus, i. e., the type of virtual environment. Therefore, we conducted an experiment to identify detection thresholds for translation gains under three different conditions: (i) without visible virtual feet and (ii) with visible virtual feet both in a high fidelity visually rich virtual environment, and (iii) with visible virtual feet in a low cue virtual environment. The results revealed the range of detection thresholds for translations gains, which cannot be detected by the user when the feet are visible. Furthermore, the results show a significant difference between the two types of environment. Our findings suggest that the virtual environment is more important for manipulation detection than the visual self-representation of the user's feet.},
  keywords={Legged locomotion;Visualization;Foot;Virtual environments;Tracking;Cameras;Avatars;Locomotion;redirected walking;translation gains.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems―Artificial;augmented;and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism―Virtual reality},
  doi={10.1109/VR.2018.8446216},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448288,
  author={Yu, Run and Duer, Zachary and Ogle, Todd and Bowman, Doug A. and Tucker, Thomas and Hicks, David and Choi, Dongsoo and Bush, Zach and Ngo, Huy and Nguyen, Phat and Liu, Xindi},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Experiencing an Invisible World War I Battlefield Through Narrative-Driven Redirected Walking in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={313-319},
  abstract={Redirected walking techniques have the potential to provide natural locomotion while users experience large virtual environments. However, when using redirected walking in small physical workspaces, disruptive overt resets are often required. We describe the design of an educational virtual reality experience in which users physically walk through virtual tunnels representative of the World War I battle of Vauquois. Walking in only a 15- by 5-foot tracked space, users are redirected through subtle, narrative-driven resets to walk through a tunnel nearly 50 feet in length. This work contributes approaches and lessons that can be used to provide a seamless and natural virtual reality walking experience in highly constrained physical spaces.},
  keywords={Legged locomotion;Virtual environments;Three-dimensional displays;Visualization;Foot;History;Redirected walking;narrative;educational VR.: H.5.1 [Information Interfaces and Presentation (e.g., HCI)]: Multimedia Information Systems-Artificial;Augmented and Virtual Realities},
  doi={10.1109/VR.2018.8448288},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447559,
  author={Jackson, Bret and Jelke, Brighten and Brown, Gabriel},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Yea Big, Yea High: A 3D User Interface for Surface Selection by Progressive Refinement in Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={320-326},
  abstract={We present Yea Big, Yea High - a 3D user interface for surface selection in virtual environments. The interface extends previous selection interfaces that support exploratory visualization and 3D modeling. While these systems primarily focus on selecting single objects, Yea Big, Yea High allows users to select part of a surface mesh, a common task for data analysis, model editing, or annotation. The selection can be progressively refined by physically indicating a region of interest between a user's hands. We describe the design of the interface and key challenges we encountered. We present findings from a case study exploring design choices and use of the system.},
  keywords={Three-dimensional displays;Shape;User interfaces;Solid modeling;Virtual environments;Surface reconstruction;Task analysis;Human-centered computing-Human computer interaction-Interaction techniques-Gestural input;Human-centered computing-Human computer interaction-Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8447559},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446317,
  author={Ariza, Oscar and Bruder, Gerd and Katzakis, Nicholas and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analysis of Proximity-Based Multimodal Feedback for 3D Selection in Immersive Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={327-334},
  abstract={Interaction tasks in virtual reality (VR) such as three-dimensional (3D) selection or manipulation of objects often suffer from reduced performance due to missing or different feedback provided by VR systems than during corresponding realworld interactions. Vibrotactile and auditory feedback have been suggested as additional perceptual cues complementing the visual channel to improve interaction in VR. However, it has rarely been shown that multimodal feedback improves performance or reduces errors during 3D object selection. Only little research has been conducted in the area of proximity-based multimodal feedback, in which stimulus intensities depend on spatiotemporal relations between input device and the virtual target object. In this paper, we analyzed the effects of unimodal and bimodal feedback provided through the visual, auditory and tactile modalities, while users perform 3D object selections in VEs, by comparing both binary and continuous proximity-based feedback. We conducted a Fitts' Law experiment and evaluated the different feedback approaches. The results show that the feedback types affect ballistic and correction phases of the selection movement, and significantly influence the user performance.},
  keywords={Three-dimensional displays;Task analysis;Visualization;Haptic interfaces;Feeds;Performance evaluation;Two dimensional displays;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input Devices and Strategies;Evaluation},
  doi={10.1109/VR.2018.8446317},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447552,
  author={Ortega, Michaël and Stuerzlinger, Wolfgang},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Pointing at Wiggle 3D Displays}, 
  year={2018},
  volume={},
  number={},
  pages={335-340},
  abstract={This paper presents two new pointing techniques for wiggle 3D displays, which present the 2D projection of 3D content with automatic (rotatory) motion parallax. Standard pointing at targets in wiggle 3D displays is challenging as the content is constantly in motion. The two pointing techniques presented here take advantage of the cursor's current position or the user's gaze direction for collocating the wiggle rotation center and potential targets. We evaluate the performance of the pointing techniques with a novel methodology that integrates 3D distractors into the ISO-9241-9 standard task. The experimental results indicate that the new techniques are significantly more efficient than standard pointing techniques in wiggle 3D displays. Given that we observed no performance variation for different targets, our new techniques seem to negate any interaction performance penalties of wiggle 3D displays.},
  keywords={Three-dimensional displays;Task analysis;Two dimensional displays;ISO Standards;Shape;Mice;3D Interaction Technique;Pointing;Eye Tracking;H.5.2. Information Interfaces and Presentation: User Interfaces - Interaction styles;Input devices and strategies},
  doi={10.1109/VR.2018.8447552},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448285,
  author={Debarba, Henrique G and Khoury, Jad-Nicolas and Perrin, Sami and Herbelin, Bruno and Boulic, Ronan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perception of Redirected Pointing Precision in Immersive Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={341-346},
  abstract={We investigate the self-attribution of distorted pointing movements in immersive virtual reality. Participants had to complete a multidirectional pointing task in which the visual feedback of the tapping finger could be deviated in order to increase or decrease the motor size of a target relative to its visual appearance. This manipulation effectively makes the task easier or harder than the visual feedback suggests. Participants were asked whether the seen movement was equivalent to the movement they performed, and whether they have been successful in the task. We show that participants are often unaware of the movement manipulation, even when it requires higher pointing precision than suggested by the visual feedback. Moreover, subjects tend to self-attribute movements that have been modified to make the task easier more often than movements that have not been distorted. We discuss the implications and applications of our results.},
  keywords={Task analysis;Distortion;Visualization;Haptic interfaces;Indexes;Electronic mail;Virtual reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology},
  doi={10.1109/VR.2018.8448285},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448289,
  author={Wolf, Dennis and Dudley, John J. and Kristensson, Per Ola},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Performance Envelopes of in-Air Direct and Smartwatch Indirect Control for Head-Mounted Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={347-354},
  abstract={The scarcity of established input methods for augmented reality (AR) head-mounted displays (HMD) motivates us to investigate the performance envelopes of two easily realisable solutions: indirect cursor control via a smartwatch and direct control by in-air touch. Indirect cursor control via a smartwatch has not been previously investigated for AR HMDs. We evaluate these two techniques for carrying out three fundamental user interface actions: target acquisition, goal crossing, and circular steering. We find that in-air is faster than smartwatch (p <; 0.001) for target acquisition and circular steering. We observe, however, that in-air selection can lead to discomfort after extended use and suggest that smartwatch control offers a complementary alternative.},
  keywords={Task analysis;User interfaces;Augmented reality;Mathematical model;Navigation;Trajectory;Resists;Fitts' law;steering law;goal crossing;in-air selection;smartwatch;indirect cursor;AR;augmented reality: Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
  doi={10.1109/VR.2018.8448289},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447553,
  author={Freitag, Sebastian and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interactive Exploration Assistance for Immersive Virtual Environments Based on Object Visibility and Viewpoint Quality}, 
  year={2018},
  volume={},
  number={},
  pages={355-362},
  abstract={During free exploration of an unknown virtual scene, users often miss important parts, leading to incorrect or incomplete environment knowledge and a potential negative impact on performance in later tasks. This is addressed by wayfinding aids such as compasses, maps, or trails, and automated exploration schemes such as guided tours. However, these approaches either do not actually ensure exploration success or take away control from the user. Therefore, we present an interactive assistance interface to support exploration that guides users to interesting and unvisited parts of the scene upon request, supplementing their own, free exploration. It is based on an automated analysis of object visibility and viewpoint quality and is therefore applicable to a wide range of scenes without human supervision or manual input. In a user study, we found that the approach improves users' knowledge of the environment, leads to a more complete exploration of the scene, and is also subjectively helpful and easy to use.},
  keywords={Visualization;Measurement;Task analysis;Navigation;Three-dimensional displays;Cameras;Histograms;Human-centered computing-Human computer interaction-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Rendering- Visibility},
  doi={10.1109/VR.2018.8447553},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447554,
  author={Kulik, Alexander and Kunert, André and Keil, Magdalena and Froehlich, Bernd},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={RST 3D: A Comprehensive Gesture Set for Multitouch 3D Navigation}, 
  year={2018},
  volume={},
  number={},
  pages={363-370},
  abstract={We present a comprehensive multitouch input mapping for 3D navigation of multiscale 3D models. In contrast to prior work, our technique offers explicit control over 3D rotation, 3D translation, and uniform scaling with manipulative gestures that do not require graphical widgets. Our proposed technique is consistent with the established RST mapping (rotation, scaling, translation) for 2D mul-titouch input and follows suggestions from prior work on multitouch 3D interaction. Our implementation includes a rendering technique that can reduce perceptual conflicts of 3D touch input on stereoscopic displays. We also report on two user studies that informed the suggested interaction design and confirmed its usability.},
  keywords={Three-dimensional displays;Two dimensional displays;Navigation;Visualization;Collaboration;Switches;Task analysis;Human-centered computing-Human computer interaction-Interaction techniques-Gestural input},
  doi={10.1109/VR.2018.8447554},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446130,
  author={Jacob Habgood, M. P. and Moore, David and Wilson, David and Alapont, Sergio},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Rapid, Continuous Movement Between Nodes as an Accessible Virtual Reality Locomotion Technique}, 
  year={2018},
  volume={},
  number={},
  pages={371-378},
  abstract={The confounding effect of player locomotion on the vestibulo-ocular reflex is one of the principal causes of motion sickness in immersive virtual reality. Continuous motion is particularly problematic for stationary user configurations, and teleportation has become the prevailing approach for providing accessible locomotion. Unfortunately, teleportation can also increase disorientation and reduce a player's sense of presence within a VR environment. This paper presents an alternative locomotion technique designed to preserve accessibility while maintaining feelings of presence. This is a node-based navigation system which allows the player to move between predefined node positions using a rapid, continuous, linear motion. An evaluation was undertaken to compare this locomotion technique with commonly used, teleportation-based and continuous walking approaches. Thirty-six participants took part in a study which examined motion sickness and presence for each technique, while navigating around a virtual house using PlayStation VR. Contrary to intuition, we show that rapid movement speeds reduce players' feelings of motion sickness as compared to continuous movement at normal walking speeds.},
  keywords={Teleportation;Virtual reality;Games;Headphones;Steel;Navigation;Legged locomotion;PlayStation VR;virtual reality;locomotion;motion-sickness;cultural heritage;Edward Jenner;REVEAL.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446130},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446574,
  author={Hochreiter, Jason and Daher, Salam and Bruder, Gerd and Welch, Greg},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cognitive and Touch Performance Effects of Mismatched 3D Physical and Visual Perceptions}, 
  year={2018},
  volume={},
  number={},
  pages={1-386},
  abstract={While research in the field of augmented reality (AR) has produced many innovative human-computer interaction techniques, some may produce physical and visual perceptions with unforeseen negative impacts on user performance. In a controlled human-subject study we investigated the effects of mismatched physical and visual perception on cognitive load and performance in an AR touching task by varying the physical fidelity (matching vs. non-matching physical shape) and visual mechanism (projector-based vs. HMD-based AR) of the representation. Participants touched visual targets on four corresponding physical-visual representations of a human head. We evaluated their performance in terms of touch accuracy, response time, and a cognitive load task requiring target size estimations during a concurrent (secondary) counting task. After each condition, participants completed questionnaires concerning mental, physical, and temporal demands; stress; frustration; and usability. Results indicated higher performance, lower cognitive load, and increased usability when participants touched a matching physical head-shaped surface and when visuals were provided by a projector from underneath.},
  keywords={Task analysis;Three-dimensional displays;Head;Resists;Visualization;Training;Cameras;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques},
  doi={10.1109/VR.2018.8446574},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447561,
  author={Caluya, Nicko R. and Plopski, Alexander and Ty, Jayzon F. and Sandor, Christian and Taketomi, Takafumi and Kato, Hirokazu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Transferability of Spatial Maps: Augmented Versus Virtual Reality Training}, 
  year={2018},
  volume={},
  number={},
  pages={387-393},
  abstract={Work space simulations help trainees acquire skills necessary to perform their tasks efficiently without disrupting the workflow, forgetting important steps during a procedure, or the location of important information. This training can be conducted in Augmented and Virtual Reality (AR, VR) to enhance its effectiveness and speed. When the skills are transferred to the actual application, it is referred to as positive training transfer. However, thus far, it is unclear which training, AR or VR, achieves better results in terms of positive training transfer. We compare the effectiveness of AR and VR for spatial memory training in a control-room scenario, where users have to memorize the location of buttons and information displays in their surroundings. We conducted a within-subject study with 16 participants and evaluated the impact the training had on short-term and long-term memory. Results of our study show that VR outperformed AR when tested in the same medium after the training. In a memory transfer test conducted two days later AR outperformed VR. Our findings have implications on the design of future training scenarios and applications.},
  keywords={Training;Task analysis;Virtual reality;Resists;Layout;Legged locomotion;Media;H.5.1-Information Interfaces and Presentation: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2-Information Interfaces and Presentation: Multimedia Information Systems-Ergonomics;Evaluation/methodology;Theory and methods},
  doi={10.1109/VR.2018.8447561},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446058,
  author={Cook, Trey and Phillips, Nate and Massey, Kristen and Plopski, Alexander and Sandor, Christian and Edward Swan, J.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={User Preference for SharpView-Enhanced Virtual Text During Non-Fixated Viewing}, 
  year={2018},
  volume={},
  number={},
  pages={1-400},
  abstract={For optical see-through head-mounted displays, the mismatch between a display's focal length and the real world scene inadvertently prevents users from simultaneously focusing on the presented virtual content and the scene. It has been shown that it is possible to ameliorate the out-of-focus blur for images with a known focus distance, by applying an algorithm called Sharp View. However, it remains unclear if Sharp View also improves the readability and clarity of text rendered on the display. In this study, we investigate whether users reported increased text clarity when Sharp View was applied to a text label, and how the focal demand of the display, the focal distance to real world content, and gaze condition affect the result. Our results indicate that, in non-fixated viewing, there is a significant user preference for Sharp View-enhanced text strings.},
  keywords={Observers;Lenses;Switches;Optical imaging;Electronic mail;Visualization;Augmented reality},
  doi={10.1109/VR.2018.8446058},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446391,
  author={Petikam, Lohit and Chalmers, Andrew and Rhee, Taeyhun},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Perception of Real World Depth Map Resolution for Mixed Reality Rendering}, 
  year={2018},
  volume={},
  number={},
  pages={401-408},
  abstract={Compositing virtual objects into photographs with known real world geometry is a common task in mixed reality (MR) applications. This geometry enables rendering of global illumination effects, such as mutual lighting, shadowing, and occlusions between the background photograph and virtual objects. Obtaining high fidelity geometric representations of the real world can be a costly procedure, and is often approximated with depth data. However, it is not clear how much fidelity the depth data should have in order to maintain high visual quality in MR rendering. in this paper, we investigate the relationship between real world depth fidelity and visual quality in MR rendering. We do this by conducting a series of user experiments that measure how seamlessly virtual objects are blended with the background under varying depth resolutions. We independently evaluate the noticeability of multiple composition artifacts that occur with approximate depth. Perceptual thresholds in depth resolution are then obtained for each artifact. The findings can be used to inform trade-off decisions for optimising depth acquisition pipelines in MR applications.},
  keywords={Rendering (computer graphics);Geometry;Lighting;Virtual reality;Visualization;Three-dimensional displays;Image resolution;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception},
  doi={10.1109/VR.2018.8446391},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446300,
  author={Feigl, Tobias and Mutschler, Christopher and Philippsen, Michael},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Human Compensation Strategies for Orientation Drifts}, 
  year={2018},
  volume={},
  number={},
  pages={409-414},
  abstract={No-Pose (NP) tracking systems rely on a single sensor located at the user's head to determine the position of the head. They estimate the head orientation with inertial sensors and analyze the body motion to compensate their drift. However with orientation drift, VR users implicitly lean their heads and bodies sidewards. Hence, to determine the sensor drift and to explicitly adjust the orientation of the VR display there is a need to understand and consider both the user's head and body orientations. This paper studies the effects of head orientation drift around the yaw axis on the user's absolute head and body orientations when walking naturally in the VR. We study how much drift accumulates over time, how a user experiences and tolerates it, and how a user applies strategies to compensate for larger drifts.},
  keywords={Magnetic heads;Head;Resists;Legged locomotion;Tracking;Accelerometers;Magnetometers;VR;orientation drift;head tracking;inertial sensors.: Computing methodologies [Perception] Human-centered computing [Virtual reality]},
  doi={10.1109/VR.2018.8446300},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446383,
  author={Nguyen-Vo, Thinh and Riecke, Bernhard E. and Stuerzlinger, Wolfgang},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulated Reference Frame: A Cost-Effective Solution to Improve Spatial Orientation in VR}, 
  year={2018},
  volume={},
  number={},
  pages={415-422},
  abstract={Virtual Reality (VR) is increasingly used in spatial cognition research, as it offers high experimental control in naturalistic multimodal environments, which is hard to achieve in real-world settings. Although recent technological advances offer a high level of photorealism, locomotion in VR is still restricted because people might not perceive their self-motion as they would in the real world. This might be related to the inability to use embodied spatial orientation processes, which support automatic and obligatory updating of our spatial awareness. Previous research has identified the roles reference frames play in retaining spatial orientation. Here, we propose using visually overlaid rectangular boxes, simulating reference frames in VR, to provide users with a better insight into spatial direction in landmark-free virtual environments. The current mixed-method study investigated how different variations of the visually simulated reference frames might support people in a navigational search task. Performance results showed that the existence of a simulated reference frame yields significant effects on participants completion time and travel distance. Though a simulated CAVE translating with the navigator (one of the simulated reference frames) did not provide significant benefits, the simulated room (another simulated reference frame depicting a rest frame) significantly boosted user performance in the task as well as improved participants preference in the post-experiment evaluation. Results suggest that adding a visually simulated reference frame to VR applications might be a cost-effective solution to the spatial disorientation problem in VR.},
  keywords={Task analysis;Navigation;Visualization;Virtual environments;Resists;Legged locomotion;Cognition;Human-centered computing-Empirical studies in HCI},
  doi={10.1109/VR.2018.8446383},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447560,
  author={Chiu, Han-Pang and Murali, Varun and Villamil, Ryan and Kessler, G. Drew and Samarasekera, Supun and Kumar, Rakesh},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality Driving Using Semantic Geo-Registration}, 
  year={2018},
  volume={},
  number={},
  pages={423-430},
  abstract={We propose a new approach that utilizes semantic information to register 2D monocular video frames to the world using 3D georeferenced data, for augmented reality driving applications. The geo-registration process uses our predicted vehicle pose to generate a rendered depth map for each frame, allowing 3D graphics to be convincingly blended with the real world view. We also estimate absolute depth values for dynamic objects, up to 120 meters, based on the rendered depth map and update the rendered depth map to reflect scene changes over time. This process also creates opportunistic global heading measurements, which are fused with other sensors, to improve estimates of the 6 degrees-of- freedom global pose of the vehicle over state-of-the-art outdoor augmented reality systems [5]-, [19]. We evaluate the navigation accuracy and depth map quality of our system on a driving vehicle within various large-scale environments for producing realistic augmentations.},
  keywords={Three-dimensional displays;Sensors;Augmented reality;Semantics;Navigation;Laser radar;Cameras;augmented reality;autonomous navigation;depth estimation;geo-registration;scene understanding},
  doi={10.1109/VR.2018.8447560},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447556,
  author={Xia, Shihong and Zhang, Zihao and Su, Le},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cascaded 3D Full-Body Pose Regression from Single Depth Image at 100 FPS}, 
  year={2018},
  volume={},
  number={},
  pages={431-438},
  abstract={There are increasing real-time live applications in virtual reality, where it plays an important role in capturing and retargetting 3D human pose. But it is still challenging to estimate accurate 3D pose from consumer imaging devices such as depth camera. This paper presents a novel cascaded 3D full-body pose regression method to estimate accurate pose from a single depth image at 100 fps. The key idea is to train cascaded regressors based on Gradient Boosting algorithm from pre-recorded human motion capture database. By incorporating hierarchical kinematics model of human pose into the learning procedure, we can directly estimate accurate 3D joint angles instead of joint positions. The biggest advantage of this model is that the bone length can be preserved during the whole 3D pose estimation procedure, which leads to more effective features and higher pose estimation accuracy. Our method can be used as an initialization procedure when combining with tracking methods. We demonstrate the power of our method on a wide range of synthesized human motion data from CMU mocap database, Human3.6M dataset and real human movements data captured in real time. In our comparison against previous 3D pose estimation methods and commercial system such as Kinect 2017, we achieve the state-of-the-art accuracy.},
  keywords={Three-dimensional displays;Kinematics;Pose estimation;Forestry;Solid modeling;Training;Databases;Computing methodologies-Computer graphics-Animation-Motion Capture;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR.2018.8447556},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446049,
  author={Li, Dong and Wang, Danli and Weng, Dongdong and Li, Yue and Xun, Hang and Bao, Yihua},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Coded Light Based Extensible Optical Tracking System}, 
  year={2018},
  volume={},
  number={},
  pages={439-445},
  abstract={Optical tracking has become the most commonly used virtual reality (VR) tracking technology because of its high precision and non-contact characteristics. The optical tracking system represented by HTC VIVE has the problem that signals of base stations interfere with each other, and the number of base stations cannot be extended by cascades, thereby limiting the scope of its work. In this paper, an extensible optical tracking system is proposed, which can distinguish the signals from different base stations and support the simultaneous operation of multiple base stations. Furthermore, we designed an encoding scheme to generate independent code for up to 32 base stations and proposed a highspeed decoding method. Experiments demonstrate that the system has high tracking accuracy and low system latency. Users can adjust the number and layout of base stations according to the actual demand, which greatly improves the flexibility of the system, and benefits promoting the development of large scale optical tracking equipment with low cost and high precision.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Optical tracking;virtual reality.: [Computer Vision]: Computer Vision Problems-Tracking;[Human Computer Interaction (HCI)]: Interaction Paradigms-Virtual Reality},
  doi={10.1109/VR.2018.8446049},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448283,
  author={Xu, Feng and Zhao, Tianqi and Luo, Bicheng and Dai, Qionghai},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Generating VR Live Videos with Tripod Panoramic Rig}, 
  year={2018},
  volume={},
  number={},
  pages={446-9},
  abstract={Recent breakthrough in consumer-level virtual reality (VR) devices brings an increasing demand of VR live content. As converting real life content into VR need complex computations, current techniques can not synthesize 360° 3D VR content with high performance, not to mention real time. We propose an end-to-end system that records a scene using a tripod panoramic rig and broadcasts 360° stereo panorama videos in real time. The system performs a panorama stitching technique which pre-compute 3 stitching seam candidates for dynamic seam switching in the live broadcasting. This technique achieves high frame rates (>30fps) with minimum foreground cutoff and temporal jittering artifacts. Stereo vision quality is also better preserved by a proposed weighting-based image alignment scheme. We demonstrate the effectiveness of our approach on a variety of videos delivering live events. And our system has been successfully used in broadcasting live shows to mobile phone users on a professional live broadcasting platform with about 390 million user visits per month.},
  keywords={Stereo vision;Cameras;Switches;Videos;Real-time systems;Broadcasting;Electronic mail;VR live video;360o scene representation;video stitching;image-based rendering: Computing methodologies-Artificial intelligence-Computer vision-Image and video acquisition;Computing methodologies-Computer graphics-Image manipulation-Image-based rendering},
  doi={10.1109/VR.2018.8448283},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446068,
  author={Clements, Jillian M. and Kopper, Regis and Zielinski, David J. and Rao, Hrishikesh and Sommer, Marc A. and Kirsch, Elayna and Mainsah, Boyla O. and Collins, Leslie M. and Appelbaum, Lawrence G.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Neurophysiology of Visual-Motor Learning During a Simulated Marksmanship Task in Immersive Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={451-458},
  abstract={Immersive virtual reality (VR) systems offer flexible control of an interactive environment, along with precise position and orientation tracking of realistic movements. Immersive VR can also be used in conjunction with neurophysiological monitoring techniques, such as electroencephalography (EEG), to record neural activity as users perform complex tasks. As such, the fusion of VR, kinematic tracking, and EEG offers a powerful testbed for naturalistic neuroscience research. In this study, we combine these elements to investigate the cognitive and neural mechanisms that underlie motor skill learning during a multi-day simulated marksmanship training regimen conducted with 20 participants. On each of 3 days, participants performed 8 blocks of 60 trials in which a simulated clay pigeon was launched from behind a trap house. Participants attempted to shoot the moving target with a firearm game controller, receiving immediate positional feedback and running scores after each shot. Over the course of the 3 days that individuals practiced this protocol, shot accuracy and precision improved significantly while reaction times got significantly faster. Furthermore, results demonstrate that more negative EEG amplitudes produced over the visual cortices correlate with better shooting performance measured by accuracy, reaction times, and response times, indicating that early visual system plasticity underlies behavioral learning in this task. These findings point towards a naturalistic neuroscience approach that can be used to identify neural markers of marksmanship performance.},
  keywords={Task analysis;Electroencephalography;Visualization;Electrodes;Tracking;Trajectory;Electronic mail;EEG;virtual reality;neuroscience;mobile brain/body imaging;marksmanship: J.4 [Computer Applications]: Social and Behavioral Sciences=-Psychology},
  doi={10.1109/VR.2018.8446068},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446602,
  author={Wang, Bin and Wang, Guofeng and Sharf, Andrei and Li, Yangyan and Zhong, Fan and Qin, Xueying and Cohenor, Daniel and Chen, Baoquan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Active Assembly Guidance with Online Video Parsing}, 
  year={2018},
  volume={},
  number={},
  pages={459-466},
  abstract={In this paper, we introduce an online video-based system that actively assists users in assembly tasks. The system guides and monitors the assembly process by providing instructions and feedback on possibly erroneous operations, enabling easy and effective guidance in AR/MR applications. The core of our system is an online video-based assembly parsing method that can understand the assembly process, which is known to be extremely hard previously. Our method exploits the availability of the participating parts to significantly alleviate the problem, reducing the recognition task to an identification problem, within a constrained search space. To further constrain the search space, and understand the observed assembly activity, we introduce a tree-based global-inference technique. Our key idea is to incorporate part-interaction rules as powerful constraints which significantly regularize the search space and correctly parse the assembly video at interactive rates. Complex examples demonstrate the effectiveness of our method.},
  keywords={Three-dimensional displays;Solid modeling;Task analysis;Two dimensional displays;Monitoring;Visualization;Shape;Computing methodologies-Computer graphics-Mixed / augmented reality},
  doi={10.1109/VR.2018.8446602},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448286,
  author={Borst, Christoph W. and Lipari, Nicholas G. and Woodworth, Jason W.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Teacher-Guided Educational VR: Assessment of Live and Prerecorded Teachers Guiding Virtual Field Trips}, 
  year={2018},
  volume={},
  number={},
  pages={467-474},
  abstract={We present a VR field trip framework, Kvasir-VR, and assess its two approaches to teacher-guided content. In one approach, networked student groups are guided by a live teacher captured as live-streamed depth camera imagery. The second approach is a standalone (non-networked) version allowing students to individually experience the field trip based on depth camera recordings of the same teacher. Both approaches were tested at two high schools using a VR environment that teaches students about solar energy production via tours of a solar plant. We show that our live networked approach can produce promising test score gains and very high ratings of co-presence, affective attraction, overall opinion, etc. Results show a benefit of live networked VR, as the standalone approach had lower performance in terms of gains and most ratings, although its ratings were still positive. We further consider possible differences of school environment (dedicated vs. integrated classroom), and we conclude with tradeoffs and implications to benefit future design of educational VR.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Collaborative VR;education;avatara;Kinect;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;K.3.0 [Computers and Education]: General},
  doi={10.1109/VR.2018.8448286},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446595,
  author={Eroglu, Sevinc and Gebhardt, Sascha and Schmitz, Patric and Rausch, Dominik and Kuhlen, Torsten Wolfgang},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Fluid Sketching―Immersive Sketching Based on Fluid Flow}, 
  year={2018},
  volume={},
  number={},
  pages={475-482},
  abstract={Fluid artwork refers to works of art based on the aesthetics of fluid motion, such as smoke photography, ink injection into water, and paper marbling. Inspired by such types of art, we created Fluid Sketching as a novel medium for creating 3D fluid artwork in immersive virtual environments. It allows artists to draw 3D fluid-like sketches and manipulate them via six degrees of freedom input devices. Different brush stroke settings are available, varying the characteristics of the fluid. Because of fluids' nature, the diffusion of the drawn fluid sketch is animated, and artists have control over altering the fluid properties and stopping the diffusion process whenever they are satisfied with the current result. Furthermore, they can shape the drawn sketch by directly interacting with it, either with their hand or by blowing into the fluid. We rely on particle advection via curl-noise as a fast procedural method for animating the fluid flow.},
  keywords={Three-dimensional displays;Computational modeling;Fluids;Art;Real-time systems;Mathematical model;Ink;Computing methodologies―Computer graphics―Graphics systems and interfaces―Virtual reality;Human-centered computing―Human computer interaction (HCI)―Interaction devices―Sound-based input / output;Human-centered computing―Human computer interaction (HCI)―HCI design and evaluation methods―User studies},
  doi={10.1109/VR.2018.8446595},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447558,
  author={Wagner Filho, Jorge A. and Rey, Marina F. and Freitas, Carla M. D. S. and Nedel, Luciana},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Visualization of Abstract Information: An Evaluation on Dimensionally-Reduced Data Scatterplots}, 
  year={2018},
  volume={},
  number={},
  pages={483-490},
  abstract={The use of novel displays and interaction resources to support immersive data visualization and improve analytical reasoning is a research trend in the information visualization community. In this work, we evaluate the use of an HMD-based environment for the exploration of multidimensional data, represented in 3D scatterplots as a result of dimensionality reduction (DR). We present a new modeling for this problem, accounting for the two factors whose interplay determine the impact on the overall task performance: the difference in errors introduced by performing dimensionality reduction to 2D or 3D, and the difference in human perception errors under different visualization conditions. This two-step framework offers a simple approach to estimate the benefits of using an immersive 3D setup for a particular dataset. Here, the DR errors for a series of roll call voting datasets when using two or three dimensions are evaluated through an empirical task-based approach. The perception error and overall task performance, on the other hand, are assessed through a comparative user study with 30 participants. Results indicated that perception errors were low and similar in all approaches, resulting in overall performance benefits in both desktop and HMD-based 3D techniques. The immersive condition, however, was found to require less effort to find information and less navigation, besides providing much larger subjective perception of accuracy and engagement.},
  keywords={Data visualization;Three-dimensional displays;Task analysis;Two dimensional displays;Dimensionality reduction;Principal component analysis;Navigation;Immersive visualization;abstract information visualization;dimensionality reduction;3D scatterplots.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8447558},
  ISSN={},
  month={March},}@INPROCEEDINGS{8448291,
  author={Kán, Peter and Kaufmann, Hannes},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Furniture Arrangement Using Greedy Cost Minimization}, 
  year={2018},
  volume={},
  number={},
  pages={491-498},
  abstract={In this paper, we present a novel method for fast generation of furniture arrangements in interior scenes. Our method exploits the benefits of optimization-based approaches for global aesthetic rules and the advantages of procedural approaches for local arrangement of small objects. We generate the furniture arrangements for a given room in two steps: We first optimize the selection and arrangement of furniture objects in a room with respect to aesthetic and functional rules. The infinite trans-dimensional space of furniture layouts is rapidly explored by greedy cost minimization. In the second step, the procedural methods are locally applied in a stochastic fashion to generate important scene details. We demonstrate that our method achieves comparable results to a recent method for automatic interior design in terms of user preferences and that local procedural design enhances the result of optimization-based interior design. Additionally, our method is one order of magnitude faster than the compared method. Finally, the execution times of up to one second show that our method is suitable for generating large-scale indoor virtual environments during runtime.},
  keywords={Layout;Cost function;Minimization;Three-dimensional displays;Space exploration;Stochastic processes;Computing methodologies-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR.2018.8448291},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446481,
  author={Akiyama, Ryo and Yamamoto, Goshiro and Amano, Toshiyuki and Taketomi, Takafumi and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Light Projection-Induced Illusion for Controlling Object Color}, 
  year={2018},
  volume={},
  number={},
  pages={499-500},
  abstract={Using projection mapping, we can control the appearance of realworld objects by projecting colored light onto them. Because a projector can only add illumination to the scene, only a limited color gamut can be presented through projection mapping. In this paper we describe how the controllable color gamut can be extended by accounting for human perception and visual illusions. In particular, we induce color constancy to control what color space observers will perceive. In this paper, we explain the concept of our approach, and show first results of our system.},
  keywords={Color;Image color analysis;Observers;Electronic mail;Visualization;Cameras;Lighting;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception},
  doi={10.1109/VR.2018.8446481},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446560,
  author={Andersen, Daniel and Popescu, Voicu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An AR-Guided System for Fast Image-Based Modeling of Indoor Scenes}, 
  year={2018},
  volume={},
  number={},
  pages={501-502},
  abstract={We present a system that enables a novice user to acquire a large indoor scene in minutes as a collection of images that are sufficient for five degrees-of-freedom virtual navigation by image morphing. The user walks through the scene wearing an augmented reality head-mounted display (AR HMD) enhanced with a panoramic video camera. The AR HMD visualizes a 2D grid partitioning of a dynamically generated floor plan, which guides the user to acquire a panorama from each grid cell. The panoramas are registered offline using both AR HMD tracking data and structure-from - motion tools. Feature correspondences are established between neighboring panoramas. The resulting panoramas and correspondences support interactive rendering via image morphing with any view direction and from any viewpoint on the acquisition plane.},
  keywords={Cameras;Resists;Two dimensional displays;Three-dimensional displays;Visualization;Solid modeling;Navigation;Human-centered computing-Mixed / augmented reality;Computing methodologies-Virtual reality;Computing methodologies-Image-based rendering},
  doi={10.1109/VR.2018.8446560},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446569,
  author={Andreasen, Anastassia and Nilsson, Niels Christian and Serafin, Stefania},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatial Asynchronous Visuo-Tactile Stimuli Influence Ownership of Virtual Wings}, 
  year={2018},
  volume={},
  number={},
  pages={503-504},
  abstract={This poster describes a within-subject study of the virtual body ownership (VBO) illusion using anatomically similar but morphologically different body of a virtual bat. Participants experienced visuo-tactile stimulation of their arms while seeing an object touching the wing of the bat. The mapping between the real and the virtual touch points varied across three conditions: no spatial deviation between visual and tactile input, 50% deviation, and 70% deviation. The results suggest that the degree of experienced VBO varies across the conditions. The illusion was broken in the absence of visuo-tactile stimuli.},
  keywords={Shape;Interviews;Avatars;Virtual environments;Electronic mail;Three-dimensional displays;virtual reality;virtual body ownership;visuo-tactile stimuli;Human-centered computing~Virtual reality;Computing methodologies~ Virtual reality},
  doi={10.1109/VR.2018.8446569},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446448,
  author={Andreasen, Anastassia and Nilsson, Niels Christian and Serafin, Stefania},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Agency Enhances Body Ownership Illusion of Being a Virtual Bat}, 
  year={2018},
  volume={},
  number={},
  pages={505-506},
  abstract={This poster describes a within-subject study of agency's influence on virtual body ownership (VBO) using anatomically similar but morphologically different body of a virtual bat. Paricipants were exposed to flight under four conditions: voluntary movement through virtual environment (VE) with avatar present, voluntary movement through virtual environment (VE) with avatar absent, voluntary limbs movement without movements through VE, and finally involuntary movement of the avatar through VE. The results suggest that agency enhances VBO illusion the most under participants' full control during flight locomotion.},
  keywords={Correlation;Avatars;Virtual environments;Pain;Phantoms;Visualization;Artificial limbs;Agency;Virtual Body Ownership;embodiment.: • Human-centered computing~Virtual reality;• Computing methodologies~ Virtual reality},
  doi={10.1109/VR.2018.8446448},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446165,
  author={Arnskov, Thomas and Elmholdt, Anders and Jensen, Kristian and Kristoffersen, Nicklas and Litvinas, Jonas and Waldhausen, Frederik L. and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Threefold Approach for Precise and Efficient Locomotion in Virtual Environments with Varying Accessibility}, 
  year={2018},
  volume={},
  number={},
  pages={507-508},
  abstract={This poster details the design and evaluation of Locomotion3 - a framework that allows users to freely alternate between real walking, walking-in-place (WIP), and a skateboard metaphor depending on whether navigation requires efficiency, precision, or both. The user study compared the framework to WIP locomotion and the skateboard metaphor and found that Locomotion3 achieves a balance between the efficiency of the skateboard metaphor and the precision of WIP locomotion.},
  keywords={Legged locomotion;Navigation;Analysis of variance;Tracking;Foot;Virtual reality;Resists;I.3.7 [Computer Graphics]: Three-Dimenshional Graphics and Realism-Virtual Reality},
  doi={10.1109/VR.2018.8446165},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446533,
  author={Aschenbrenner, Doris and Li, Meng and Dukalski, Radoslaw and Verlinden, Jouke and Lukosch, Stephan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative Production Line Planning with Augmented Fabrication}, 
  year={2018},
  volume={},
  number={},
  pages={509-510},
  abstract={The project “Factory-in-a-day” aims at reducing the installation time of a new hybrid robot-human production line, from weeks or months that current industrial systems now take, down to one day. The ability to rapidly install (and reconfigure) production lines where robots work alongside humans will strongly reduce operating cost and open a range of new opportunities for industry. In this paper, we explore a method of collaborative fabrication planning with the help of Augmented Reality as part of the concept Augmented Fabrication. In order to plan a new production line, two co-located workers at the factory wear a Microsoft Hololens head-mounted display and thus share a common visual context on the planed position of the robots and the production machines. They are assisted by an external remote expert connected via the Internet who is virtually co-located. We developed three different visualizations of the state of the local collaboration and plan to compare them in a user study.},
  keywords={Planning;Fabrication;Robots;Augmented reality;Three-dimensional displays;Task analysis;Human-centered computing [Mixed / augmented reality];[Social and professional topics]: Computer supported cooperative work;Applied computing [Industry and manufacturing];[Computer systems organization]: Robotics},
  doi={10.1109/VR.2018.8446533},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446583,
  author={Azimi, Ehsan and Winkler, Alexander and Tucker, Emerson and Qian, Long and Sharma, Manyu and Doswell, Jayfus and Navab, Nassir and Kazanzides, Peter},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Optical See-Through Head-Mounted Displays in Training for Critical Care and Trauma}, 
  year={2018},
  volume={},
  number={},
  pages={1-9},
  abstract={One major cause of preventable death is a lack of proper skills for providing critical care. Conventional training for advanced emergency medical procedures is often limited to a verbal block of instructions and/or an instructional video. In this study, we evaluate the benefits of using an optical see-through head-mounted display (OST-HMD) for training of caregivers in an emergency medical environment. A rich user interface was implemented that provides 3D visual aids including images, text and tracked 3D overlays for each task. A user study with 20 participants was conducted for two medical tasks, where each subject received conventional training for one task and HMD training for the other task. Our results indicate that using a mixed reality HMD is more engaging, improves the time-on-task, and increases the confidence level of users.},
  keywords={Task analysis;Training;Resists;Biomedical imaging;Augmented reality;Three-dimensional displays;Human-centered computing-Interaction paradigms-Mixed / augmented reality-;Human-centered computing-Interaction design-Interaction design process and methods-User interface design},
  doi={10.1109/VR.2018.8446583},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446189,
  author={Bhargava, Ayush and Lucaites, Kathryn M. and Hartman, Leah S. and Solini, Hannah and Bertrand, Jeffrey W. and Robb, Andrew C. and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Revisiting Passability Judgments in Real and Immersive Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={513-514},
  abstract={Every task we perform in our day-to-day lives requires us to make judgements about size, distance, depth, etc. The same is true for tasks in an immersive virtual environments (IVE). Increasingly, Virtual Reality (VR) applications are being developed for training and entertainment, many of which require the user to determining whether s/he can pass through an opening. Typically, people determine their ability to pass through an aperture by comparing the width of their shoulders to the width of the opening. Thus, judgments of size and distance in an IVE are necessary for accurate judgments of passability. In this experiment, we empirically evaluate how passability judgments in an IVE, viewed through a Head-Mounted Display (HMD), compare to judgments made in the real world. An exact to scale virtual replica of the room and apparatus was used for the VR condition. Results indicate that the accuracy of passability judgments seem to be comparable to the real world.},
  keywords={Apertures;Virtual environments;Estimation;Psychology;Task analysis;Resists;Human-centered computing-HCI design and evaluation methods-;-Human-centered computing-Empirical studies in HCI},
  doi={10.1109/VR.2018.8446189},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446227,
  author={Brickler, David and Babu, Sabarish V. and Bertrand, Jeffrey and Bhargava, Ayush},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Evaluating the Effects of Stereoscopic Viewing and Haptic Interaction on Perception-Action Coordination}, 
  year={2018},
  volume={},
  number={},
  pages={1-516},
  abstract={This paper details the results of an initial empirical evaluation conducted to examine how stereoscopic viewing and haptic feedback affects fine motor actions in a pick-and-place task, similar to the peg transfer task in an FLS training curriculum for laproscopic sugical training. In a between subjects experiment, we examined the effect of stereoscopic viewing and simulated tactile feedback during the fine motor actions of a participants' actions in the near field on the number of collisions and time to complete the task. We found that stereo and haptic feedback contributed to the effectiveness of task performance in different ways. Specifically, we found that the mean time to complete the trials was significantly higher in the abcense of tactile feedback as compared to when it was present, and the mean number of collisions was significantly higher in the presence of stereo as compared to when it was absent.},
  keywords={Haptic interfaces;Task analysis;Stereo image processing;Three-dimensional displays;Solid modeling;Electronic mail;Performance evaluation;3D Interaction [Haptics]: Human-Computer Interaction-Perception-Action Coordination},
  doi={10.1109/VR.2018.8446227},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446346,
  author={Buhler, Helmut and Misztal, Sebastian and Schild, Jonas},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reducing VR Sickness Through Peripheral Visual Effects}, 
  year={2018},
  volume={},
  number={},
  pages={517-9},
  abstract={This paper proposes and evaluates two novel visual effects that can be applied to Virtual Reality (VR) applications to reduce VR sickness with head-mounted displays (HMD). Unlike other techniques that pursue the same goal, our approach allows a user to move continuously through a virtual environment without reducing the perceived field of view (FOV). A within-design study with 18 users compares reported sickness between the two effects and baseline. The results show lower means of sickness in the two novel effects; however, the difference is not statistically significant across all users, replicating large variety in individual reactions found in previous studies. In summary, reducing optical flow in peripheral vision is a promising approach. Future potential lies in adjusting visual effect parameters to maximize impact for large user groups.},
  keywords={Cameras;Visualization;Visual effects;Virtual reality;Optical flow;Navigation;Resists;VR sickness;peripheral vision;locomotion;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446346},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446598,
  author={Caputo, Fabio M. and Mendes, Daniel and Bonetti, Alessia and Saletti, Giacomo and Giachetti, Andrea},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Smart Choices for Deviceless and Device-Based Manipulation in Immersive Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={The choice of a suitable method for object manipulation is one of the most critical aspects of virtual environment design. It has been shown that different environments or applications might benefit from direct manipulation approaches, while others might be more usable with indirect ones, exploiting, for example, three dimensional virtual widgets. When it comes to mid-air interactions, the success of a manipulation technique is not only defined by the kind of application but also by the hardware setup, especially when specific restrictions exist. In this paper we present an experimental evaluation of different techniques and hardware for mid-air object manipulation in immersive virtual environments (IVE). We compared task performances using both deviceless and device-based tracking solutions, combined with direct and widget-based approaches. We also tested, in the case of freehand manipulation, the effects of different visual feedback, comparing the use of a realistic virtual hand rendering with a simple cursor-like visualization.},
  keywords={Task analysis;Three-dimensional displays;Visualization;Virtual environments;Pins;Tracking;Human-centered computing-Interaction techniques Human-centered computing-User studies Human-centered computing-HCI design and evaluation methods Human-centered computing-Virtual reality},
  doi={10.1109/VR.2018.8446598},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446410,
  author={Chen, Chih-Fan and Rosenberg, Evan Suma},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Content Creation Using Dynamic Omnidirectional Texture Synthesis}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={We present a dynamic omnidirectional texture synthesis (DOTS) approach for generating real-time virtual reality content captured using a consumer-grade RGB-D camera. Compared to a single fixed-viewpoint color map, view-dependent texture mapping (VDTM) techniques can reproduce finer detail and replicate dynamic lighting effects that become especially noticeable with head tracking in virtual reality. However, VDTM is very sensitive to errors such as missing data or inaccurate camera pose estimation, both of which are commonplace for objects captured using consumer-grade RGB-D cameras. To overcome these limitations, our proposed optimization can synthesize a high resolution view-dependent texture map for any virtual camera location. Synthetic textures are generated by uniformly sampling a spherical virtual camera set surrounding the virtual object, thereby enabling efficient real-time rendering for all potential viewing directions.},
  keywords={Cameras;Solid modeling;US Department of Transportation;Image color analysis;Virtual reality;Three-dimensional displays;Real-time systems;virtual reality;view-dependent texture mapping;content creation.: Computing methodologies-Computer Graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computing graphics-Image manipulation-Texturing;Computing methodologies-Computer Graphics-Image manipulation- Image- based rendering},
  doi={10.1109/VR.2018.8446410},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446563,
  author={Chen, Haiwei and Chen, Samantha and Rosenberg, Evan Suma},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirected Walking in Irregularly Shaped Physical Environments with Dynamic Obstacles}, 
  year={2018},
  volume={},
  number={},
  pages={523-524},
  abstract={Redirected walking (RDW) is a virtual reality (VR) locomotion technique that enables the exploration of a large virtual environment (VE) within a small physical space via real walking. Thus far, the physical environment has generally been assumed to be rectangular, static, and free of obstacles. However, it is unlikely that real-world locations that may be used for VR fulfill these constraints. In addition, accounting for dynamic obstacles such as people helps increase user safety when the view of the physical world is occluded by a head-mounted display. In this work, we present the design and initial implementation of a RDW planning algorithm that can redirect the user in an irregularly shaped physical environment with dynamically moving obstacles. This represents an important step towards the use of RDW in more dynamic, real-world environments.},
  keywords={Heuristic algorithms;Legged locomotion;Planning;Prediction algorithms;Safety;Virtual environments;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
  doi={10.1109/VR.2018.8446563},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446494,
  author={Chen, Shu-Yu and Gao, Lin and Lai, Yu-Kun and Rosin, Paul L. and Xia, Shihong},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time 3D Face Reconstruction and Gaze Tracking for Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={525-526},
  abstract={With the rapid development of virtual reality (VR) technology, VR glasses, a.k.a. Head-Mounted Displays (HMDs) are widely available, allowing immersive 3D content to be viewed. A natural need for truly immersive VR is to allow bidirectional communication: the user should be able to interact with the virtual world using facial expressions and eye gaze, in addition to traditional means of interaction. Typical application scenarios include VR virtual conferencing and virtual roaming, where ideally users are able to see other users' expressions and have eye contact with them in the virtual world. Despite significant achievements in recent years for reconstruction of 3D faces from RGB or RGB- D images, it remains a challenge to reliably capture and reconstruct 3D facial expressions including eye gaze when the user is wearing VR glasses, because the majority of the face is occluded, especially those areas around the eyes which are essential for recognizing facial expressions and eye gaze. In this paper, we introduce a novel real-time system that is able to capture and reconstruct 3D faces wearing HMDs and robustly recover eye gaze. We demonstrate the effectiveness of our system using live capture and more results are shown in the accompanying video.},
  keywords={Three-dimensional displays;Cameras;Image reconstruction;Face;Glass;Feature extraction;Real-time systems;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Gestural input},
  doi={10.1109/VR.2018.8446494},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446442,
  author={Cho, Yong-Hun and Lee, Dong-Yong and Lee, In-Kwon},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Path Prediction Using LSTM Network for Redirected Walking}, 
  year={2018},
  volume={},
  number={},
  pages={527-528},
  abstract={Redirected walking enables immersive walking experience in a limited-sized room. To apply redirected walking efficiently and minimize the number of resets, an accurate path prediction algorithm is required. We propose a data-driven path prediction model using Long Short-Term Memory(LSTM) network. User path data was collected via path exploration experiment on a maze-like environment and fed into LSTM network. Our algorithm can predict user's future path based on user's past position and facing direction data. We compare our path prediction result with actual user data and show that our model can accurately predict user's future path.},
  keywords={Legged locomotion;Data models;Predictive models;Solid modeling;Prediction algorithms;Training;Three-dimensional displays;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Machine learning-Machine-learning approaches-Neural networks},
  doi={10.1109/VR.2018.8446442},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446132,
  author={Chowdhury, Tanvir Irfan and Shahnewaz Ferdous, Sharif Mohammad and Peck, Tabitha C. and Quarles, John},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reverse Disability Simulation in a Virtual Environment}, 
  year={2018},
  volume={},
  number={},
  pages={529-530},
  abstract={Disability Simulation (DS) is an approach used to modify attitudes regarding people with disabilities. DS places people without disabilities in situations that are designed for the users to experience a disability. In this research we investigate reverse disability simulation (RDS) in a virtual reality environment. In a RDS people with disabilities perform tasks that are made easier in the virtual environment compared to the real world. We hypothesized that putting people with disabilities in a RDS will increase confidence and enable efficient task completion. To investigate this hypothesis, we conducted a within-subjects experiment in which participants performed a virtual “kicking a ball” task in two different conditions: a normal condition without RDS (i.e., same difficulty as in the real world) and an easy condition with RDS (i.e., physically easier than the real world but visually the same). The results from our study suggest that RDS increased participants' confidence.},
  keywords={Legged locomotion;Solid modeling;Avatars;Games;Task analysis;Foot;Virtual Reality;VR;Virtual Environment;VE;Disability Simulation;DS;Reverser Disability Simulation;Immersion;Rehabilitation;Head-Mounted Display;HMD;Multiple Sclerosis;MS.: [Human computer interaction (HCI)]: Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446132},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446535,
  author={Christou, Chris G. and Michael-Grigoriou, Despina and Sokratous, Dimitris},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Buzzwire: Assessment of a Prototype VR Game for Stroke Rehabilitation}, 
  year={2018},
  volume={},
  number={},
  pages={531-532},
  abstract={We created a VR version of the Buzzwire children's toy as part of a project to develop tools for assessment and rehabilitation of upper-body motor skills for people with dexterity impairment after stroke. In two pilot studies, participants wearing a HMD used a hand-held wand with precision tracking to traverse virtual `wires'. In the first study, we compared able-bodied participant's performance with and without binocular viewing to establish a connection with previous experiments using physical versions of the game. Furthermore, we show that our extended measures were could also discern differences between subjects' dominant versus non-dominant hand. In a second study, we assessed the usability of the system on a small sample of subjects with post-stroke hemiparesis. There was positive acceptance of the technology with no fatigue or nausea and measurements highlighted the differences between the hemiparetic and unaffected hand.},
  keywords={Wires;Games;Task analysis;Stereo vision;Atmospheric measurements;Particle measurements;Usability},
  doi={10.1109/VR.2018.8446535},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446139,
  author={Clifford, Rory M.S. and Khan, Humayun and Hoermann, Simon and Billinghurst, Mark and Lindeman, Robert W.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Immersive Displays on Situation Awareness in Virtual Environments for Aerial Firefighting Air Attack Supervisor Training}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Situation Awareness (SA) is an essential skill in Air Attack Supervision (AAS) for aerial based wildfire firefighting. The display types used for Virtual Reality Training Systems (VRTS) afford different visual SA depending on the Field of View (FoV) as well as the sense of presence users can obtain in the virtual environment. We conducted a study with 36 participants to evaluate SA acquisition in three display types: a high-definition TV (HDTV), an Oculus Rift Head-Mounted Display (HMD) and a 270° cylindrical simulation projection display called the SimPit. We found a significant difference between the HMD and the HDTV, as well as with the SimPit and the HDTV for the three levels of SA.},
  keywords={Resists;HDTV;Training;Task analysis;Virtual environments;Visualization;H.5.2 [User Interfaces]: User Interfaces-Graphical user interfaces (GUI);H.5.m [Information Interfaces and Presentation]},
  doi={10.1109/VR.2018.8446139},
  ISSN={},
  month={March},}
@INPROCEEDINGS{8446545,
  author={Côté, Stéphans and Mercier, Alexandra},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmentation of Road Surfaces with Subsurface Utility Model Projections}, 
  year={2018},
  volume={},
  number={},
  pages={535-536},
  abstract={Subsurface utility work planning would benefit from augmented reality. Unfortunately, the exact pipe location is rarely known, which produces unreliable augmentations. We proposed an augmentation technique that drapes 2D pipe maps onto the road surface and aligns them with corresponding features in the physical world using a pre-captured 3D mesh. Resulting augmentations are more likely to be displayed at the true pipe locations.},
  keywords={Roads;Surface topography;Surface treatment;Augmented reality;Three-dimensional displays;Data visualization;Planning;Augmented Reality;Subsurface Utilities;Engineering;Human-centered computing → Human computer interaction (HCI)→ Interaction paradigms → Mixed / augmented reality;Applied computing → Physical sciences and engineering → Engineering},
  doi={10.1109/VR.2018.8446545},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446368,
  author={Debarba, Henrique Galvan and de Oliveira, Marcelo Elias and Lädermann, Alexandre and Chagué, Sylvain and Charbonnier, Caecilia},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality Visualization of Joint Movements for Physical Examination and Rehabilitation}, 
  year={2018},
  volume={},
  number={},
  pages={537-538},
  abstract={We present a visualization tool for human motion analysis in augmented reality. Our tool builds upon our previous work on joint biomechanical modelling for kinematic analysis, based on optical motion capture and personalized anatomical reconstruction of joint structures from medical imaging. It provides healthcare professionals with the in situ visualization of joint movements, where bones are accurately rendered as a holographic overlay on the subject - like if the user has an “X-ray vision” - and in real-time as the subject performs the movement. Currently, hip and knee joints are supported.},
  keywords={Joints;Visualization;Biological system modeling;Computational modeling;Biomechanics;Solid modeling;Bones;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology},
  doi={10.1109/VR.2018.8446368},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446497,
  author={Debarba, Henrique Galvan and de Oliveira, Marcelo Elias and Lädermann, Alexandre and Chaqué, Sylvain and Charbonnier, Caecilia},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tracking a Consumer HMD with a Third Party Motion Capture System}, 
  year={2018},
  volume={},
  number={},
  pages={539-540},
  abstract={We describe a calibration procedure to track consumer Head Mounted Displays (HMD) using a 3rd party tracking solution. The calibration consists of registering the center of projection of the rendering hardware to a 3rd party tracked object attached to it, and is performed by matching motion datasets from the HMD built-in and 3rd party tracking solutions. We demonstrate this calibration with an augmented reality optical see-through HMD, where the correctness of the alignment is critical to the visual match of a real object by a virtual overlay. We assessed a mean error of 3mm (SD = 1mm) for objects at a distance of 70cm in the projected overlay image.},
  keywords={Calibration;Tracking;Headphones;Resists;Cameras;Adaptive optics;Optical sensors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
  doi={10.1109/VR.2018.8446497},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446213,
  author={Deng, Nianchen and Zhou, Yanqing and Ye, Jiannan and Yang, Xubo},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Calibration Method for On-Vehicle AR-HUD System Using Mixed Reality Glasses}, 
  year={2018},
  volume={},
  number={},
  pages={541-542},
  abstract={Calibration is a key step for on-vehicle AR-HUD systems to ensure the augmented information to be correctly viewed by the driver. State-of-art calibration methods require setting up of spatial tracking devices or attaching markers on vehicles, which is time-consuming and error-prone. In this paper, we present a novel multi-viewpoints calibration method for AR-HUD using only a mixed reality glasses such as HoloLens. The full calibration process can be done in one minute and provides high precise calibration result, while no markers need to be attached on vehicle.},
  keywords={Calibration;Cameras;Optical distortion;Optical imaging;Mathematical model;Virtual reality;Automotive components;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers},
  doi={10.1109/VR.2018.8446213},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446230,
  author={Erkut, Cumhur and Holfelt, Jonas and Serafin, Stefania},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mobile AR In and Out: Towards Delay-Based Modeling of Acoustic Scenes}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={We have previously presented an augmented reality (AR) audio application, where scattering delay networks efficiently generate and organize a reverberator, based on room geometry scanned by an AR device. The application allowed for real-time processing and updating of reflection path geometry and provided a proof-of-concept for plausible audio-spatial registration of a virtual object in real environments. Here we present our ongoing work that aims to extend the simulation to outdoor scenes by using the Waveguide Web, instead of the original formulation with the Scattering Delay Networks. The current implementation is computationally more demanding, but has a potential to provide more accurate second-order reflections, and therefore, better registering of audio-visual AR scenes.},
  keywords={Scattering;Three-dimensional displays;Augmented reality;Computational modeling;Solid modeling;Google;Acoustics;Human-centered computing-Mixed / augmented reality-Human-centered computing-Auditory feedback},
  doi={10.1109/VR.2018.8446230},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446495,
  author={Feigl, Tobias and Mutschler, Christopher and Philippsen, Michael},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Head-to-Body-Pose Classification in No-Pose VR Tracking Systems}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Pose tracking does not yet reliably work in large-scale interactive multi-user VR. Our novel head orientation estimation combines a single inertial sensor located at the user's head with inaccurate positional tracking. We exploit that users tend to walk in their viewing direction and classify head and body motion to estimate heading drift. This enables low-cost long-time stable head orientation. We evaluate our method and show that we sustain immersion.},
  keywords={Magnetic heads;Feature extraction;Head;Tracking;Legged locomotion;Reliability;Estimation;VR;head tracking;inertial sensor fusion;immersion;large-scale;machine learning;motion sickness;Computing methodologies [Supervised learning by classification] Human-centered computing [Virtual reality]},
  doi={10.1109/VR.2018.8446495},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446488,
  author={Mohammad Shahnewaz Ferdous, Sharif and Chowdhury, Tanvir Irfan and Muhammad Arafat, Imtiaz and Quarles, John},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Reason for Increased Postural Instability in Virtual Reality for Persons with Balance Impairments}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={The objective of this study is to investigate how different visual components of Virtual Reality (VR), such as field of view, frame rate, and display resolution affect postural stability in VR. Although previous studies identified these visual components as some of the primary factors that differ significantly in VR from reality, the effect of each component on postural stability in VR is yet unknown. While most people experience postural instability in VR, it is worse for people with balance impairments (BIs). This is likely because they depend more on their visual cues to maintain postural stability. Therefore, we conducted a within-subject study with ten people with balance impairments due to Multiple Sclerosis (MS). In each condition, we varied one component and kept all other components fixed. Each participant explored the virtual environment (VE) in a controlled fashion to make sure that the effect of the visual components was consistent for all participants. Results from our study suggest that decreased field of view and frame rate have significant effects on postural stability, but the effect of display resolution is inconclusive. Therefore, VR systems targeting people with balance impairments should focus on improving field of view and frame rate rather than display resolution.},
  keywords={Visualization;Virtual reality;Thermal stability;Atmospheric measurements;Particle measurements;Stability criteria},
  doi={10.1109/VR.2018.8446488},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446463,
  author={Figueroa, Pablo and Hernández, José Tiberio and Merienne, Frédéric and Chardonnet, Jean-Rémy and Dorado, José and Sebastián Lopez, J.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Heterogeneous, Distributed Mixed Reality Applications. A Concept}, 
  year={2018},
  volume={},
  number={},
  pages={549-550},
  abstract={This poster formulates the concept of heterogeneous distributed mixed reality (HDMR) applications in order to state some interesting research questions in this domain. HDMR applications give synchronous access to shared virtual worlds, from diverse mixed reality (MR) hardware, and at similar levels of functionality. We show the relationship between HDMR and previous concepts, state challenges in their development, and illustrate this concept and its challenges with an example.},
  keywords={Task analysis;Collaboration;Hardware;Virtual environments;Performance evaluation;Gears},
  doi={10.1109/VR.2018.8446463},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446324,
  author={Fritz, Aleksandr and Sun, Bo and Xu, Wei},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Visual Analysis to Explore Mystery at Wildlife Preserve}, 
  year={2018},
  volume={},
  number={},
  pages={551-552},
  abstract={In this paper, we aim to extend our work in VAST Challenge [7] of IEEE visualization conference last year, where we use visual analytics to solve an environment problem on why a local attractive bird, Rose-Crested Blue Pipit, is decreasing. Given the large scale and multi-dimensional datasets on chemical releases, we develop immersive visual analytics to find the connections between manufactures and sensor readings, and eventually, to discover the potential reason of the bird decreasing.},
  keywords={Chemicals;Data visualization;Three-dimensional displays;Production facilities;Conferences;Visual analytics;Data Analytics;VR;Immersive Visualizations},
  doi={10.1109/VR.2018.8446324},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446619,
  author={Georgiou, Orestis and Jeffrey, Craig and Chen, Ziyuan and Xiao Tong, Bao and Hei Chan, Shing and Yang, Boyin and Harwood, Adam and Carter, Tom},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Touchless Haptic Feedback for VR Rhythm Games}, 
  year={2018},
  volume={},
  number={},
  pages={553-554},
  abstract={Haptics is an important part of the VR space as seen by the plethora of haptic controllers available today. Recent advancements have enabled touchless haptic feedback through the use of focused ultrasound thereby removing the need for a controller. Here, we present the world's first mid-air haptic rhythm game in VR and describe the reasoning behind its interface and gameplay, and in particular, how these were enabled by the effective use of state-of-the-art ultrasonic haptic technology.},
  keywords={Haptic interfaces;Games;Rhythm;Ultrasonic imaging;Road transportation;Human computer interaction;haptics;ultrasound;HCI;VR;rhythm games;H.5.1 [Human computer interaction (HCI)]: Interaction devices-Haptic devices;H.5.2 [Human computer interaction (HCI)]: Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446619},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446137,
  author={Gerjets, Peter and Lachmair, Martin and Butz, Martin V. and Lohmann, Johannes},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Knowledge Spaces in VR: Intuitive Interfacing with a Multiperspective Hypermedia Environment}, 
  year={2018},
  volume={},
  number={},
  pages={555-556},
  abstract={Virtual reality technologies, along with motion based input devices allow for the design of innovative interfaces between learners and digital knowledge resources. These interfaces might facilitate knowledge work in educational and scientific contexts. Compared to 2D interfaces, immersive 3D environments provide greater flexibility regarding the interface design, however, so far no general, theory-driven and validated design principles are available. Seeing that complex learning environments can foster the development of various cognitive abilities, like multiperspective reasoning skills (MPRS), such design principles are highly desirable. Using multiperspective hypermedia environments (MHEs) as a testbed, the presented project aims to identify and evaluate design principles, derived from cognitive science. We will create and study interactive, immersive 3D-interface to MHEs using virtual reality technology. To evaluate the developed system, we will contrast the acquisition of MPRS in 2D and 3D learning environments. We expect that the developed design principles will be directly applicable for enhancing the accessibility of other knowledge environments.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;VR;Hypermedia Environment;HCI;Interaction Design;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Theory and Methods},
  doi={10.1109/VR.2018.8446137},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446580,
  author={Ghandorh, Hamza and Eagleson, Roy and de Ribaupierre, Sandrine},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Investigation of Head Motion and Perceptual Motion Cues' Influence on User Depth Perception of Augmented Reality Neurosurgical Simulators}, 
  year={2018},
  volume={},
  number={},
  pages={557-558},
  abstract={Training and planning for neurosurgeries necessitate many requirements from junior neurosurgeons, including perceptual capacities. An effective method of deliberate training is to replicate the required procedures using neurosurgical simulation tools and visualizing a three-dimensional (3D) workspace. However, Augmented Reality (AR) neurosurgical simulators become obsolete for a variety of reasons, including users' distance underestimation. Few investigations have been conducted for improving users' depth perception in AR systems with perceptual motion cues through neurosurgical simulation tools for planning aid purposes. In this poster, we are reporting a user study about whether head motion and perceptual motion cues have any an influence on users' depth perception.},
  keywords={Task analysis;Neurosurgery;IP networks;Training;Tools;Visualization;Planning;Human-centered computing-Human computer interaction (HCI)-Mixed\Augmented Reality-HCI design and evaluation methods},
  doi={10.1109/VR.2018.8446580},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446609,
  author={Giunchi, Daniele and James, Stuart and Steed, Anthony},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Model Retrieval by 3D Sketching in Immersive Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={559-560},
  abstract={We describe a novel method for searching 3D model collections using free-form sketches within a virtual environment as queries. As opposed to traditional Sketch Retrieval, our queries are drawn directly onto an example model. Using immersive virtual reality the user can express their query through a sketch that demonstrates the desired structure, color and texture. Unlike previous sketch-based retrieval methods, users remain immersed within the environment without relying on textual queries or 2D projections which can disconnect the user from the environment. We show how a convolutional neural network (CNN) can create multi-view representations of colored 3D sketches. Using such a descriptor representation, our system is able to rapidly retrieve models and in this way, we provide the user with an interactive method of navigating large object datasets. Through a preliminary user study we demonstrate that by using our VR 3D model retrieval system, users can perform quick and intuitive search. Using our system users can rapidly populate a virtual environment with specific models from a very large database, and thus the technique has the potential to be broadly applicable in immersive editing systems.},
  keywords={Solid modeling;Three-dimensional displays;Virtual environments;Image retrieval;Two dimensional displays},
  doi={10.1109/VR.2018.8446609},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446511,
  author={Gonzaga, Luiz and RobertoVeronez, Mauricio and Lanzer Kannenberg, Gabriel and Nunes Alves, Demetrius and Lessio Cazarin, Caroline and Gomes Santana, Leonardo and Luca de Fraga, Jean and Inocencio, Leonardo C. and Vieira de Souza, Lais and Marson, Fernando and Bordin, Fabiane and Tognoli, Francisco M.W.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Virtual Fieldwork: Advances for the Petroleum Industry}, 
  year={2018},
  volume={},
  number={},
  pages={561-562},
  abstract={Laser scanning and photogrammetry techniques have been broadly adopted by Oil&Gas industry for modeling petroleum reservoir analogues. Beyond the benefits of digital data itself, computer systems employed by geoscientists for interpretation and modeling tasks provide high quality rendering, point clouds surface meshes and photo-realistic textured models. But these systems, commonly, have used 2-D display, the 3-D models and information are projected on the screen, providing a limited visualization and restrictive toolset for interpretation. This work proposes to break this paradigm by developing a fully immersive system capable to virtually teleport the geoscientists to the fieldwork and provide a complete toolset for the outcrop's interpretation. Besides, the system has been evaluated and validated by geologists with different skills and it has emerged as an useful and attractive toolset for Oil&Gas industry.},
  keywords={Geology;Tools;Geologic measurements;Computational modeling;Three-dimensional displays;Petroleum industry;Laser modes;Virtual Reality-Visualization-Visualization techniques-Geology},
  doi={10.1109/VR.2018.8446511},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446215,
  author={Grogorick, Steve and Albuquerque, Georgia and Maqnor, Marcus},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Gaze Guidance in Immersive Environments}, 
  year={2018},
  volume={},
  number={},
  pages={563-564},
  abstract={We investigate the efficiency of five different gaze guidance techniques for immersive environments, probing our peripheral vision's sensitivity to different stimuli embedded in complex, real-world panorama still images. We conducted extensive user studies for a commercially available headset as well as in a custom-built dome projection environment. The dome enables us to create true 360° visual immersion at high-resolution, akin to what may be expected of future-generation VR headsets. Evaluation with high-quality eye tracking shows that local luminance modulation as proposed by Bailey et al. is the most effective technique, eliciting saccades to the target region with up to 40 % success rate within the first second.},
  keywords={Headphones;Visualization;Electronic mail;Shape;Gaze tracking;Conferences;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception},
  doi={10.1109/VR.2018.8446215},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446125,
  author={Hamzeheinejad, Negin and Straka, Samantha and Gall, Dominik and Weilbach, Franz and Erich Latoschik, Marc},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Robot-Assisted Virtual Reality Therapy for Neurologically-Caused Gait Impairments}, 
  year={2018},
  volume={},
  number={},
  pages={565-566},
  abstract={This paper presents an immersive Virtual Reality (VR) therapy system for gait rehabilitation after neurological impairments, e.g., caused by accidents or strokes: The system targets increase of patients' motivation to perform the repeated exercise by providing stimulating virtual exercise environments with the final goal to increase therapy efficiency and effectiveness. Instead of simply working out on immobile stationary devices, the system allows them to walk through and explore a stimulating virtual world. Patients are immersed in the virtual environments using a Head-Mounted Display (HMD). Walking patterns are captured by motion sensors attached to the patients' feet to synchronize locomotion speed between the real and the virtual world. A user-centered design process evaluated usability, user experience, and feasibility to confirm the overall goals of the system before any sensitive clinical trials with impaired patients can start. Overall, the results demonstrated an encouraging user experience and acceptance while it did not induce any unwanted side-effects, e.g., nausea or cyber-sickness.},
  keywords={Legged locomotion;Medical treatment;Training;Virtual environments;Robot sensing systems;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446125},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446048,
  author={Hanasaki, Atsutoshi and Uchiyama, Hideaki and Shlmada, Atsushi and Taniquch, Rin-Ichiro},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Deep Localization on Panoramic Images}, 
  year={2018},
  volume={},
  number={},
  pages={567-568},
  abstract={Sensor pose estimation is an essential technology for various applications. For instance, it can be used not only to display immersive contents according user movements in Virtual Reality (VR) and but also to superimpose computer-generated objects onto images from a camera in Augmented Reality (AR). As a technical term definition, camera localization with respect to a pre-created map database is specifically referred to as image based localization, memory based localization, or camera relocalization.},
  keywords={Cameras;Training;Robustness;Databases;Pose estimation;Standards;Human-centered computing;Human computer interaction(HCI);Interaction paradigm;Mixed/augmented reality},
  doi={10.1109/VR.2018.8446048},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446458,
  author={Handosa, Mohamed and Schulze, Hendrik and Gracanin, Denis and Tucker, Matthew and Manuel, Mark},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Approach to Embodiment and Interactions with Digital Entities in Mixed-Reality Environments}, 
  year={2018},
  volume={},
  number={},
  pages={569-570},
  abstract={The advances in mixed reality (MR) technologies provide an opportunity to support the deployment and use of MR for training and education. We describe an approach that extends the functionality of the Microsoft HoloLens device to support a wider range of embodied interactions by making use of the Microsoft Kinect V2 device. The embodied interactions can support novel interaction scenarios, especially within the context of training and skills development, thereby removing or reducing the need for training equipment.},
  keywords={Virtual reality;Training;Tracking;Cognition;Electronic mail;Skeleton;Hospitals;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Gestural input},
  doi={10.1109/VR.2018.8446458},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446345,
  author={Hashemian, Abraham M. and Kitson, Alexandra and Nquyen-Vo, Thinh and Benko, Hrvoje and Stuerzlinger, Wolfgang and Riecke, Bernhard E.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating a Sparse Peripheral Display in a Head-Mounted Display for VR Locomotion}, 
  year={2018},
  volume={},
  number={},
  pages={571-572},
  abstract={Head-Mounted Displays (HMDs) provide immersive experiences for virtual reality. However, their field of view (FOV) is still relatively small compared to the human eye, which adding sparse peripheral displays (SPDs) could address. We designed a new SPD, SparseLightVR2, which increases the HMD's FOV to 180° horizontally. We evaluated SparseLightVR2 with a study (N=29) by comparing three conditions: 1) no SPD, where the peripheral display (PD) was inactive; 2) extended SPD, where the PD provided visual cues consistent with and extending the HMD's main screen; and 3) counter-vection SPD, where the PD's visuals were flipped horizontally during VR travel to provide optic flow in the opposite direction of the travel. The participants experienced passive motion on a linear path and reported introspective measures such as sensation of self-motion. Results showed, compared to no SPD, both extended and counter-vection SPDs provided a more natural experience of motion, while extended SPD also enhanced vection intensity and believability of movement. Yet, visually induced motion sickness (VIMS) was not affected by display condition. To investigate the reason behind these non-significant results, we conducted a follow-up study and had users increase peripheral counter-vection visuals on the central HMD screen until they nulled out vection. Our results suggest extending HMDs through SPDs enhanced vection, naturalness, and believability of movement without enhancing VIMS, but reversed SPD motion cues might not be strong enough to reduce vection and VIMS.},
  keywords={Visualization;Resists;Virtual reality;Mirrors;Head-mounted displays;Atmospheric measurements;Particle measurements;Virtual reality;wide field-of-view;sparse peripheral display;peripheral vision;vection;navigational search task;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems -Artificial, augmented and virtual realities},
  doi={10.1109/VR.2018.8446345},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446262,
  author={Hirt, Christian and Zank, Markus and Kunz, Andreas},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Preliminary Environment Mapping for Redirected Walking}, 
  year={2018},
  volume={},
  number={},
  pages={573-574},
  abstract={Redirected walking applications allow a user to explore large virtual environments in a smaller physical space by employing so-called redirection techniques. To further improve the immersion of a virtual experience, path planner algorithms were developed which choose redirection techniques based on the current position and orientation of the user. In order to ensure a reliable performance, planning algorithms depend on accurate position tracking using an external tracking system. However, the disadvantage of such a tracking method is the time-consuming preparation of the physical environment which renders the system immobile. A possible solution to eliminate this dependency is to replace the external tracking system with a state-of-the-art inside-out tracker based on the concept of Simultaneous Localization and Mapping (SLAM). In this paper, we present an approach in which we attach a commercially available SLAM device to a head-mounted display to track the head motion of a user. From sensor recordings of the device, we construct a map of the surrounding environment for future processing in an existing path planner for redirected walking.},
  keywords={Legged locomotion;Simultaneous localization and mapping;Three-dimensional displays;Planning;Cloud computing;Virtual environments;Human-centered computing-Virtual reality;Computing methodologies-Tracking},
  doi={10.1109/VR.2018.8446262},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446240,
  author={Hoesch, Anne and Poeschl, Sandra and Weidner, Florian and Walter, Roberto and Doering, Nicola},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Relationship Between Visual Attention and Simulator Sickness: A Driving Simulation Study}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Although visual attention cues are of particular importance for driving simulation tasks, research on the relationship of visual attention and simulator sickness is scarce. This exploratory study is aimed at investigating this relation with a laboratory study in a fixed-based driving simulator (N = 36). No correlation between visual attention and simulator sickness was shown, but the direction of the relation shows a negative tendency.},
  keywords={Visualization;Solid modeling;Task analysis;Correlation;Roads;Electronic mail;Pressing;Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human computer interaction (HCI)-HCI design and evaluation methods-mixed/augmented reality},
  doi={10.1109/VR.2018.8446240},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446311,
  author={Hoppe, Adrian H. and van de Camp, Florian and Stiefelhagen, Rainer},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Personal Perspective: Using Modified World Views to Overcome Real-Life Limitations in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={577-578},
  abstract={Virtual Reality opens up new possibilities as it allows to overcome real-life limitations and create novel experiences. While interacting with other people, it is beneficial to share a common view point. We modify the virtual world to allow face-to-face interaction with another person, while still retaining an optimal point of view on presented data. This is done by adapting the virtual environment independently for each user, using translation, rotation and scaling. The presented modification of the world gives a natural solution to the problems of collaborative analysis of content. It is therefore beneficial for usage in human-human interaction scenarios that support cooperative work.},
  keywords={Collaboration;Task analysis;Avatars;Virtual environments;Electronic mail},
  doi={10.1109/VR.2018.8446311},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446382,
  author={Hutton, Courtney and Ziccardi, Shelby and Medina, Julio and Rosenbarg, Evan Suma},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Please Don't Puke: Early Detection of Severe Motion Sickness in VR}, 
  year={2018},
  volume={},
  number={},
  pages={579-580},
  abstract={Motion sickness is a potentially debilitating side effect experienced by certain users of virtual reality systems. Unexpected results from a user study on redirected walking suggest that there is a need to quickly identify participants who have an extremely low tolerance for virtual motion manipulations and remove them from the experience. In this poster, we investigate the use of a previously introduced “fast motion sickness” measure to identify potential outliers with heightened levels of sensitivity. This work demonstrates a promising experimental methodology and suggests possible shared characteristics among users in this group.},
  keywords={Frequency modulation;Calibration;Electronic mail;Virtual environments;Tutorials;Headphones;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/VR.2018.8446382},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446622,
  author={Ichiyama, Tomohiro and Matsubayashi, Atsushi and Makino, Yasutoshi and Shinoda, Hiroyuki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Control Operation Support of Unstable System by Visual Feedback}, 
  year={2018},
  volume={},
  number={},
  pages={581-582},
  abstract={In this paper, we show that an inverted pendulum can be stabilized manually even when a user does not know the physical characteristics and the current state of the pendulum. We display two markers: one indicates current position of the base of the pendulum and the other indicates the target position where the base should be located 0.3 seconds later. Subjects can stabilize the pendulum for a significantly longer time than seeing the real pendulum directly, just by chasing the target marker.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;User interface;Motion induction;Visual feedback;Inverted pendulum},
  doi={10.1109/VR.2018.8446622},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446076,
  author={Lhemedu-Steinke, Quinatei and Meixner, Gerrit and Weber, Michael},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing VR Display with Conventional Displays for User Evaluation Experiences}, 
  year={2018},
  volume={},
  number={},
  pages={583-584},
  abstract={The adoption of virtual reality in various industrial sectors other than gaming is spreading by the day. Many people are still sceptical regarding the potentials and advantages of virtual reality because this has not been made obvious enough to convince them. We investigated how virtual reality affects the concentration, involvement and enjoyment of users during evaluation sessions. Eighty four participants drove on a virtual automated driving simulator with and without the Oculus rift CV1. The experiment showed a statistically significant result for all variables that virtual reality enables better concentration, better involvement and enjoyment when compared with conventional displays.},
  keywords={Virtual reality;Resists;Three-dimensional displays;Solid modeling;Conferences;Headphones;Navigation;Virtual reality;User evaluation;Sense of presence},
  doi={10.1109/VR.2018.8446076},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446502,
  author={Inks, Zachariah J. and Volonte, Matias and Beadle, Sarah and Horing, Bjoern and Robb, Andrew C. and Babu, Sabarish V.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Standardization of Medical Trials Using Virtual Experimenters}, 
  year={2018},
  volume={},
  number={},
  pages={585-586},
  abstract={We describe a system for experiment standardization and distributing medication in medical trials using a virtual human. In our system, we employed a virtual experimenter that explains the experiment, medication, procedure and risks involved through a large screen display. The participant is able to interact with and ask questions of the virtual experimenter through a touch screen interface on an additional monitor display. During the interaction, the virtual experimenter will present the participant with a pill. The pill is physically distributed by a custom-made Arduino based dispenser. We conducted an initial user evaluation of the system using a placebo response protocol and a perceived pain scale. In the study, participants submerged their hand in a hot water bath before and after interacting with the system and reported their perceived pain response. The system was either a virtual human or text interface that either disseminated a pill stating that it was an analgesic or did not provide a medication. Through this system, we propose the potential use of virtual humans as a method to provide a consistent and standardized interaction between a participant and experimenter, while maintaining the benefits of social interaction in medication trials.},
  keywords={Pain;Biomedical monitoring;Monitoring;Electronic mail;Temperature measurement;Temperature sensors;Atmospheric measurements;Human-centered computing;Virtual Human;Placebo;Virtual Experimenter},
  doi={10.1109/VR.2018.8446502},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446073,
  author={Isogawa, Mariko and Mikami, Dan and Fukuda, Takehiro and Saijo, Naoki and Takahashi, Kosuke and Kimata, Hideaki and Kashino, Makio},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={What Can VR Systems Tell Sports Players? Reaction-Based Analysis of Baseball Batters in Virtual and Real Worlds}, 
  year={2018},
  volume={},
  number={},
  pages={587-588},
  abstract={This study aims at ascertaining the applicability of a virtual reality environment (VRE) to sports training. Hitting an incoming object is one of the most common actions in various ball games, in which players are required to move to a suitable position and hit the object in a split second; this is a complicated task requiring spatio-temporal reaction to the object. Due to this complexity, how a VRE can serve as a training environment still remains an open question. In the work reported in this paper, we investigated the idea of substituting a VRE for an actual environment for training on the task of hitting a baseball. By focusing on the batter's temporal behavior with real and virtual environments, we clarified factors that contribute to the batter's reaction. This helped us understand how training VREs can be effectively utilized and the VRE requirements needed for sports training.},
  keywords={Training;Videos;Timing;Task analysis;Virtual reality;Laboratories;Computing methodologies Virtual reality;Human-centered computing Virtual reality},
  doi={10.1109/VR.2018.8446073},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446517,
  author={Jung, Thomas and Bauer, Patrick},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3D Touch-and-Drag: Gesture-Free 3D Manipulation with Finger Tracking}, 
  year={2018},
  volume={},
  number={},
  pages={589-590},
  abstract={In this study, we define a new modeling technique called 3D touch-and-drag, wherein users select vertices by simply approaching them with a 3D cursor such as a forefinger. Operations are finished by removing the 3D cursor from a line or plane in 3D space. These lines or planes constrain the modeling operations, as is the case when using 3D widgets. User tests demonstrated that there was no significant difference in movement time between moving a sphere along a line using a pinch gesture and using the proposed technique. Since it is easier to select a vertex by just approaching it, compared with performing a pinch gesture, we believe that 3D touch-and-drag is more efficient than current techniques, while being just as precise.},
  keywords={Three-dimensional displays;Solid modeling;Task analysis;Computational modeling;Tools;Thumb;Mice;Human-centered computing-Gestural Input},
  doi={10.1109/VR.2018.8446517},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446370,
  author={Kaluschke, Maximilian and Weller, René and Zachmann, Gabriel and Pelliccia, Luigi and Lorenz, Mario and Klimant, Philipp and Knopp, Sebastian and Atze, Johannes P. G. and Móckel, Falk},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HIPS - A Virtual Reality Hip Prosthesis Implantation Simulator}, 
  year={2018},
  volume={},
  number={},
  pages={591-592},
  abstract={We present the first VR training simulator for hip replacement surgeries. We solved the main challenges of this task - high and stable forces during the milling process while simultaneously a very sensitive feedback is required - by using an industrial robot for the force output and the development of a novel massively parallel haptic rendering algorithm with support for material removal.},
  keywords={Haptic interfaces;Hip;Surgery;Rendering (computer graphics);Training;Robots;Milling;Human-centered computing;Human computer interaction;Interaction devices;Haptic devices},
  doi={10.1109/VR.2018.8446370},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446143,
  author={Kanno, Keynes Masayoshi and Lamounier, Edgard Afonso and Cardoso, Alexandre and Lopes, Ederaldo José and Mendes de Lima, Gerson Flávio},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality System for Aiding Mild Alzheimer Patients and Caregivers}, 
  year={2018},
  volume={},
  number={},
  pages={593-594},
  abstract={Alzheimer's Disease (AD) has become ever more prominent within the area of healthcare. Studies show that it is the most common cause of dementia in older adults. In addition, a large percentage of patients and their caregivers still face enormous challenges concerning treatment and the performing of everyday tasks. Forgetfulness and location awareness are recurrent symptoms. On the other hand, smartphones have become a more common feature of their daily lives. Therefore, this paper presents a mobile application to help individuals diagnosed in the early stages of Alzheimer's Disease to identify objects and people. In addition, it can track the location of an AD individual, since they frequently become disoriented and lost. The application presented herein proposes an accessible interface, based on Augmented Reality techniques that uses speech commands for different features: time reminders for taking medicine, identification of which medicine to be taken, people recognition from photos, among others. In addition tests revealed a promising interface using voice recognition and a feasibility of locating individuals using the caregiver's mobile application.},
  keywords={Augmented reality;Alzheimer's disease;Speech recognition;Assistive technology;Task analysis;Alzheimer's Disease;Mobile Technology;Augmented Reality Interface;Assistive Technology;K.6.1 [Management of Computing and Information Systems]: Project and People Management-Life Cycle;K.7.m [The Computing Profession]: Miscellaneous-Ethics},
  doi={10.1109/VR.2018.8446143},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446212,
  author={Kaplan, Oral and Yamamoto, Goshiro and Taketomi, Takafumi and Yoshltake, Yasuhide and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Situated Knee Trajectory Visualization for Self Analysis in Cycling}, 
  year={2018},
  volume={},
  number={},
  pages={595-596},
  abstract={Inflammation, stiffness, and swelling are frequently reported symptoms of patellar tendinitis among cyclists; making knee pain a consistently observed overuse injury in cycling. In this paper, we investigate the applicability of a knee trajectory visualization to self-analysis for increasing awareness of movement patterns leading to injuries. We briefly explain overuse injuries and patellar instability, describe the experiments we did with cyclists for gathering requirements, and finally illustrate an augmented reality concept. We also show two different types of visualizations with participant opinions; one being conventional and other being a video-based one and discuss how situated visualizations can be utilized for improving self awareness to injury causes.},
  keywords={Visualization;Trajectory;Knee;Injuries;Training;Electronic mail;Augmented reality;Human−centered computing;Mixed / augmented reality;Human-centered computing;Information visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2018.8446212},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446484,
  author={Kato, Shingo and Nakamoto, Takamichi},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Olfactory Display Based on Sniffing Action}, 
  year={2018},
  volume={},
  number={},
  pages={597-598},
  abstract={An olfactory display is a device which provides various scents to a user. Such devices are expected to be applied to VR since olfactory stimulus influences human emotion and enhances user experience. One of the main problems in the conventional olfactory display is that the odorants emitted from the device not only reach the nose but spread into the ambient air, so that the user experience may be changed by the remaining odor. To solve this problem, we have developed a newly structured olfactory display which utilizes human respiratory action. In the method, DC fan is driven to create an odor stream in front of the nostril. Thus, odor goes through the nostril only when the user sniffs it. The odor plume generation is based on the combination of SAW atomizer with micro dispensing valve. We have fabricated a prototype, and then evaluated the waste odor emitted into the air using a commercially available gas detector. It was demonstrated that the new structure makes it possible to reduce the waste odor emission into the air compared to the conventional method.},
  keywords={Olfactory;Fans;Valves;Surface acoustic wave devices;Surface acoustic waves;Gas detectors;Olfactory Display;Aspiration;SAW;Solenoid Valve;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented and virtual realities},
  doi={10.1109/VR.2018.8446484},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446090,
  author={Kawai, Hideki and Hara, Hiroki and Yanagida, Yasuyuki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Reclining Angle on the Perception of Horizontal Plane for HMD Users}, 
  year={2018},
  volume={},
  number={},
  pages={599-600},
  abstract={In recent years, head mounted displays (HMDs) have become widely used among general users thanks to the release of low-cost and high-performance versions. It is expected that virtual reality (VR) experiences will often be performed in a relaxed posture, while sitting on a reclining seat at home or in the supine position on a bed in rehabilitation institutions. However, there is no clear knowledge on the subject of how humans perceive the horizontal plane of a virtual world while using HMDs in reclining postures. In this study, we investigated how the angle of the subjective horizontal plane changes in relation to the angle of the upper body in a reclining seat. As a result, we found that the angle of the perceived horizontal plane changes depending on the angle of the upper body, and the physical direction of gravity has little effect on this perception.},
  keywords={Visualization;Resists;Head;Virtual reality;Legged locomotion;Electronic mail;Three-dimensional displays;HMD;subjective visual horizontal;Reclining angle.: H.5.1 [Information Interface and Presentation]: Multimedia Information System - Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446090},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446046,
  author={Kim, Aelee and Chang, Minha and Choi, Yeseul and Jeon, Sohyeon and Lee, Kyoungmin},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Immersion on Emotional Responses to Film Viewing in a Virtual Environment}, 
  year={2018},
  volume={},
  number={},
  pages={601-602},
  abstract={In this study, we explore how immersion affects people's sense of emotions in a virtual environment. The primary goals of this study are to analyze the possible use of virtual reality (VR) as an affective medium and research the relationship between immersion and emotion. To investigate these objectives, we compared two viewing conditions (HMD vs. No-HMD) and applied two types of emotional content (horror and empathy) to examine whether the level of immersion could influence emotional responses. The results showed that viewers who watched the horror movie using HMD felt more scared than those in the No-HMD condition. However, there were no significant emotional differences between the HMD and No-HMD conditions in the movie groups exposed to empathy. Regarding these results, we may assume that the effect of an immersive viewing experience on emotional responses in VR is deeply related to the degree of arousal and strong perceptual cues. The horror movie used in this study included intense visual and audio stimuli found in the typical horror film format. In contrast, viewers experienced less stimulating perceptual input when they are watching the empathetic movie. In conclusion, VR undoubtedly elicits a more immersive experience and greater emotional responses to the horror film. This study has confirmed the efficacy of VR as an emotional amplifier and successfully demonstrated the important association between immersion and emotion in VR.},
  keywords={Resists;Motion pictures;Psychology;Virtual environments;Correlation;Atmospheric measurements;Immersion;Emotion;Virtual Reality;H.1.2 [Models and Principles]: User/Machine Systems-Human Factors;I.3.7 [Computing Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
  doi={10.1109/VR.2018.8446046},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446175,
  author={Klose, Elisa Maria and Schmidt, Ludger},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A User-Based Comparison of Two Augmented Reality Glasses}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={We present a scenario-based laboratory study with 40 participants comparing two augmented reality (AR) glasses in a travelling scenario. In a within-subject design, the binocular Epson Moverio BT-200 and the monocular Vuzix M100 were compared with regard to performance, acceptance, workload and preference. While performance was equal with both glasses, the Epson Moverio BT-200 glasses got higher acceptance and lower workload ratings and were preferred by the majority of the participants. The findings provide knowledge on human factors in AR glasses usage.},
  keywords={Glass;Augmented reality;Human factors;Videos;Cameras;Google;Feeds;Data glasses;user study;performance;acceptance;workload.: Human-centered computing-Mixed / augmented reality;Human-centered computing-User studies},
  doi={10.1109/VR.2018.8446175},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446275,
  author={Klotzsche, F. and Mariola, A. and Hofmann, S. and Nikulin, V. V. and Villringer, A. and Gaebler, M.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using EEG to Decode Subjective Levels of Emotional Arousal During an Immersive VR Roller Coaster Ride}, 
  year={2018},
  volume={},
  number={},
  pages={605-606},
  abstract={Emotional arousal is a key component of a user's experience in immersive virtual reality (VR). Subjective and highly dynamic in nature, emotional arousal involves the whole body and particularly the brain. However, it has been difficult to relate subjective emotional arousal to an objective, neurophysiological marker - especially in naturalistic settings. We tested the association between continuously changing states of emotional arousal and oscillatory power in the brain during a VR roller coaster experience. We used novel spatial filtering approaches to predict self-reported emotional arousal from the electroencephalogram (EEG) signal of 38 participants. Periods of high vs. low emotional arousal could be classified with accuracies significantly above chance level. Our results are consistent with prior findings regarding emotional arousal in less naturalistic settings. We demonstrate a new approach to decode states of subjective emotional arousal from continuous EEG data in an immersive VR experience.},
  keywords={Electroencephalography;Feature extraction;Brain modeling;Virtual reality;Oscillators;Resists;Neuroscience;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-Laboratory experiments;Applied computing-Life and medical sciences-Consumer health},
  doi={10.1109/VR.2018.8446275},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446614,
  author={Knopp, Sebastian and Lorenz, Mario and Pelliccia, Luigi and Klimant, Philipp},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Industrial Robots as Haptic Devices for VR-Training}, 
  year={2018},
  volume={},
  number={},
  pages={607-608},
  abstract={Many VR-training application require the integration of haptics, i.e. for surgical training. However, surgical VR-training is still limited to minimal invasive surgeries. For surgeries where high forces occur, like hip replacement, no VR-training applications have been developed. One cause for this is the lack of appropriate haptic devices which can deliver high forces. Novel industrial collaborative robots can provide high forces. Although, they lack control interfaces allowing to use them as haptic devices. We present 4 approaches for using these robots as general, multipurpose haptic input and output devices. The implemented approach was integrated into a VR hip replacement training application. An initial assessment demonstrates the general feasibility of our solution.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Haptics;Industrial robot;Virtual reality;Surgery;Medicine;Training;Applied computing → Life and medical sciences;Human-centered computing → Virtual reality;Human-centered computing → Haptic devices},
  doi={10.1109/VR.2018.8446614},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446582,
  author={Kon, Yuki and Nakamura, Takuto and Yem, Vibol and Kajimoto, Hiroyuki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HangerOVER: Mechanism of Controlling the Hanger Reflex Using Air Balloon for HMD Embedded Haptic Display}, 
  year={2018},
  volume={},
  number={},
  pages={609-610},
  abstract={The Hanger Reflex is a phenomenon in which the head rotates unintentionally when it is sandwiched by a wire hanger. The reflex is effectively generated by pressing on specific points, and can be reproduced by pressing with an actuator. We propose the HangerOVER, an HMD-embedded haptic display that can provide both force and motion senses using the Hanger Reflex. In this paper, we designed a mechanism of controlling the Hanger Reflex using air balloons for HMD embedded haptic display, and confirmed the occurrence of the Hanger Reflex.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;balloon haptic head-mounted display;force display;Hanger Reflex;haptic display;haptic interaction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2018.8446582},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446282,
  author={Kondo, Ryota and Sugimoto, Maki and Minamizawa, Kouta and Inami, Masahiko and Kitazaki, Michiteru and Tani, Yamato},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Illusory Body Ownership Between Different Body Parts: Synchronization of Right Thumb and Right Arm}, 
  year={2018},
  volume={},
  number={},
  pages={611-612},
  abstract={Illusory body ownership can be induced by visual-tactile stimulation or visual-motor synchronicity. We aimed to test whether a right thumb could be remapped to a virtual right arm and illusory body ownership of the virtual arm induced through synchronous movements of the right thumb and the virtual right arm. We presented the virtual right arm in synchronization with movements of a participant's right thumb on a head-mounted display (HMD). We found that the participants felt as though their right thumb became the right arm, and that the right arm belonged to their own body.},
  keywords={Thumb;Synchronization;Rubber;Legged locomotion;Resists;Observers;Manipulators;Body ownership;rubber hand illusion;human augmentation;virtual reality;I.3.7 [Computer graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.l.2 [Models and Principles]: User/Machine Systems-human factors},
  doi={10.1109/VR.2018.8446282},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446528,
  author={Koumaditis , Konstantinos and Chinello, Francesco and Venckute, Sarune},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design of a Virtual Reality and Haptic Setup Linking Arousals to Training Scenarios: A Preliminary Stage}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Using Virtual Reality (VR) to realise immersive training environments is not a new concept. However, investigating arousal in immersive environments is. By arousal, we denote a general physical and psychological activity that in the form of anxiety and stress for example, can affect trainees' performance. In this work, we describe the setup design for a two-phase explorative experiment linking arousal and performance, during training in a Virtual Reality (VR) environment. To do so we are using an appraised well-crafted VR puzzle game, questionnaires (i.e. NASA Task Load Index [3]), and sensors (skin conductance response / pulse). The experiment will involve participants from the public that will be trained in two predefined processes of variant difficulty.},
  keywords={Training;Task analysis;Sensors;Haptic interfaces;Games;Virtual reality;Skin;Virtual Reality;Haptics;Arousal;Stress;Training},
  doi={10.1109/VR.2018.8446528},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446055,
  author={Lages, Wallace S. and Li, Yuan and Bowman, Doug A.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Environment-Independent Techniques for 3D Position Marking in Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={615-616},
  abstract={Specifying 3D positions in the real world is an essential step to create augmented reality content. However, this task can be challenging when information about the depth or geometry of the target location is not available. In this paper, we evaluate alternative techniques for 3D pointing at a distance without knowledge about the environment. We present the results of two studies evaluating the accuracy of techniques based on geometry and human depth perception. We find that geometric methods provide higher accuracy but may suffer from low precision due to pointing errors. We propose a solution that increases precision by combining multiple samples to obtain a better estimate of the target position.},
  keywords={Three-dimensional displays;Standards;Meters;Augmented reality;Task analysis;Geometry;Augmented reality;interaction;marker placement;H5.1 [Information interfaces and presentation]: Multimedia Information Systems. - Artificial augmented and virtual realities;H.5.2: User Interfaces-Pointing},
  doi={10.1109/VR.2018.8446055},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446184,
  author={Leber, Isabel and Merk, Matthias and Tullius, Gabriela and Hertkorn, Peter},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Pico Projectors with Spatial Contextual Awareness to Create Augmented Knowledge Spaces for Interdisciplinary Engineering Teams}, 
  year={2018},
  volume={},
  number={},
  pages={617-618},
  abstract={Engineers of the research project “Digital Product Life-Cycle” are using a graph-based design language to model all aspects of the product they are working on. This abstract model is the base for all further investigations, developments and implementations. In particular at early stages of development, collaborative decision making is very important. We propose a semantic augmented knowledge space by means of mixed reality technology, to support engineering teams. Therefore we present an interaction prototype consisting of a pico projector and a camera. In our usage scenario engineers are augmenting different artefacts in a virtual working environment. The concept of our prototype contains both an interaction and a technical concept. To realise implicit and natural interactions, we conducted two prototype tests: (1) A test with a low-fidelity prototype and (2) a test by using the method Wizard of Oz. As a result, we present a prototype with interaction selection using augmentation spotlighting and an interaction zoom as a semantic zoom.},
  keywords={Prototypes;Knowledge engineering;Collaboration;Semantics;Cameras;Testing;Creativity;Human-centered computing―Ubiquitous computing―;Human-centered computing―Mixed/augmented reality},
  doi={10.1109/VR.2018.8446184},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446271,
  author={Linder, Rhema and Stacy, Alexandria M. and Lupfer, Nic and Kerne, Andruid and Ragan, Eric D.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Pop the Feed Filter Bubble: Making Reddit Social Media a VR Cityscape}, 
  year={2018},
  volume={},
  number={},
  pages={619-620},
  abstract={On Reddit, users from tens of thousands of communities create and promote internet content, including pictures, videos, news, memes, and creative writing. However, like most social media feeds, subscribing to a very small subset of available content creates filter bubbles. These bubbles, while created unintentionally, skew perceptions of reality. This phenomena provides an impetus for researchers to design techniques breaking out of filter bubbles. Virtual reality provides opportunities for new environments that contextualize social media among multiple perspectives. We present one solution to the filter bubble problem: Blue Link City, which enables contextualized exploration of Reddit.},
  keywords={Urban areas;Feeds;Social network services;Buildings;Virtual reality;Three-dimensional displays;Visualization},
  doi={10.1109/VR.2018.8446271},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446123,
  author={Liu, Shaohua and Song, Xiyuan and Jiang, Hao and Shi, Min and Mao, Tianlu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Behavioral Simulation of Passengers in a Waiting Hall}, 
  year={2018},
  volume={},
  number={},
  pages={621-622},
  abstract={In this paper, we introduced a behavioral decision and execution method to simulate crowded passengers in a waiting hall. The method, as well as its simulation framework, is designed under the special purpose of passenger safety investigation. It supports the simulation of both regular crowded passenger behaviors and emergency passenger behavior. Situations under different time tables and density control measure could easily be conducted and simulated for safety purposes.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Behavioral simulation;passenger behavior;virtual waiting hall;safety investigation;K.7.m [Simulation;Modeling;and Visualization]: Types of Simulation-Animation},
  doi={10.1109/VR.2018.8446123},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446222,
  author={Liu, Juan and Li, Hanchao and Zhao, Lu and Zhao, Siwei and Qi, Guowen and Bian, Yulong and Meng, Xiangxu and Yang, Chenglei},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Method of View-Dependent Stereoscopic Projection on Curved Screen}, 
  year={2018},
  volume={},
  number={},
  pages={623-624},
  abstract={In this paper, we present a method of view-dependent stereoscopic projection on curved screen. It allows the user to walk around with the correct perspective view of the virtual scene consistent with his/her location. To solve the problem of distortion and drift of virtual objects when projecting the view-dependent scene images on curved screen, we operate a dynamic parallax adjustment of the stereoscopic images according to viewpoints. User evaluation shows that our proposed approaches are effective on improving visual experience.},
  keywords={Stereo image processing;Visualization;Three-dimensional displays;Head;Cameras;Rendering (computer graphics);view-dependent;virtual reality;screen flickers;I.3.7 Graphics: 3D Graphics and Realism-Virtual Reality},
  doi={10.1109/VR.2018.8446222},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446312,
  author={Lugrin, Jean-Luc and Oberdorfer, Sebastian and Latoschik, Marc Erich and Wittmann, Alice and Seufert, Christian and Grafe, Silke},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-Assisted vs Video-Assisted Teacher Training}, 
  year={2018},
  volume={},
  number={},
  pages={625-626},
  abstract={This paper compares teacher training in Virtual Reality (VR) to traditional approaches based on videos analysis and reflections. Our VR-assisted teacher training targets classroom management (CM) skills, using a low cost collaborative immersive VR platform. First results reveal a significant improvement using the VR approach.},
  keywords={Seminars;Training;Virtual reality;Videos;Collaboration;Graphical user interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446312},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446052,
  author={Manabe, Shinnosuke and Ikeda, Sei and Kimura, Asako and Shibata, Fumihisa},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Casting Virtual Shadows Based on Brightness Induction for Optical See-Through Displays}, 
  year={2018},
  volume={},
  number={},
  pages={627-628},
  abstract={This paper proposes a novel method for casting virtual shadows on real surfaces on an optical see-through head-mounted display without any extra physical filter devices. Instead, the method presents shadows as results of brightness induction. To produce brightness induction, we place a texture of the real scene with a certain transparency around the shadow area to amplify the luminance of the surrounding area. To make this amplification unnoticeable, the transparency of the surrounding region is gradually increased as the distance from the shadow region. In the experiment with 23 participants, we confirmed that users tend to perceive the shadow region is darker than a non-shadow area under the conditions where a circular virtual shadow is placed on a flat surface.},
  keywords={Brightness;Optical imaging;Electronic mail;Casting;Cameras;Visualization;Optical modulation;Human-centered computing-Human-computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Human-computer interaction (HCI)-Interaction devices-Displays and imagers},
  doi={10.1109/VR.2018.8446052},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446522,
  author={Martinez, Jonatan and Griffiths, Daniel and Biscione, Valerio and Georgiou, Orestis and Carter, Tom},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Touchless Haptic Feedback for Supernatural VR Experiences}, 
  year={2018},
  volume={},
  number={},
  pages={629-630},
  abstract={Haptics is an important part of the VR space as seen by the plethora of haptic controllers available today. By using a novel ultrasonic haptic device, we developed and integrated mid-air haptic sensations without the need to wear or hold any equipment in a VR game experience. The compelling experience combines visual, audio and haptic stimulation in a supernatural narrative in which the user takes on the role of a wizard apprentice. By using different haptified patterns we could generate a wide range of sensations which mimic supernatural interactions (wizard spells). We detail our methodology and briefly discuss our findings and future work.},
  keywords={Haptic interfaces;Fires;Lightning;Games;Ultrasonic imaging;Human computer interaction;Acoustics;haptics;HCI;VR;ultrasound;supernatural;H.5.1 [Human computer interaction (HCI)]: Interaction devices - Haptic devices;H.5.2 [Human computer interaction (HCI)]: Interaction paradigms- Virtual reality},
  doi={10.1109/VR.2018.8446522},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446062,
  author={Matsumoto, Keigo and Yamada, Ayaka and Nakamura, Anna and Uchmura, Yasushi and Kawai, Keitaro and Tanikawa, Tomohiro},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Biomechanical Parameters Under Curvature Gains and Bending Gains in Redirected Walking}, 
  year={2018},
  volume={},
  number={},
  pages={631-632},
  abstract={In this study, we examined the effect of walking biomechanics, which occurs when the curvature of the walking path in virtual space is changed, while the actual walking path remains constant. Curvature gains and bending gains were used to change the virtual walking path. We found a significant difference in most biomechanical parameters when curvature manipulation and bending manipulation are applied compared with the case in which they are not applied. Some parameters were also suggested to depend on the visual sense or disagreement between the visual and other senses.},
  keywords={Legged locomotion;Biomechanics;Visualization;Electronic mail;Education;Virtual environments;Cameras;Human-centered computing-Virtual reality;Computing methodologies-Virtual reality},
  doi={10.1109/VR.2018.8446062},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446504,
  author={Medina-Papaqayo, Sergio and Perez-Gutierrez, Byron and Vega-Medina, Lizeth and Leon-Printrlquez, Hernando and Jairnes, Norman and Alarcon, Claudia and Uribe-Ouevedo, Alvaro},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Intraousseous Access Simulator in Newborns VR System}, 
  year={2018},
  volume={},
  number={},
  pages={633-634},
  abstract={Simulators in medical procedures are important tools for learning, teaching and training allowing practices in a wide range of scenarios of life-like situations. In the field of vascular access, the use of virtual reality has been emerging as a complement to address the current problems due to the low availability of simulation manikins for training and its high cost. Intraosseous access in newborns is an invasive, fast and effective medical procedure of high complexity employed in critically ill newborns as an option to access veins after peripheral access has failed. Due to vein vasoconstriction present in neonatal shock and cardiorespiratory arrest among other life-threatening conditions it is difficult to perform any other forms of venous access. Intraosseous access requires the proper handling of the related equipment and knowledge, this is only possible with continuous training that cannot be done in real patients. Mastering this technique is required to preserve patient's life, achieve a good recovery and reduce the risk of infection or even death. This paper presents the development of a newborn's intraosseous access simulator for training covering the required steps involved in the procedure. To increase the immersion of the simulation, a force feedback haptic device is integrated to simulate the needle insertion beneath leg tissues to the bone with a biomechanical tissue model, which is the more important skill to be developed in this procedure, and a consumer head mounted display to provide a stereo view of the operation room to give depth to the user when approaching to the patient leg. Our preliminary results were evaluated by a medical expert in terms of usability of the prototype.},
  keywords={Prototypes;Pediatrics;Solid modeling;Bones;Virtual reality;Haptic interfaces;Training;J.3 [Computer Applications]: Life and Medical Sciences-Health;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptic I/O},
  doi={10.1109/VR.2018.8446504},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446510,
  author={Miller, Noah and Willemsen, Pete and Feyen, Robert},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Interface Affordances for Controlling a Push Broom in VR}, 
  year={2018},
  volume={},
  number={},
  pages={635-636},
  abstract={This study explores how VR controller interfaces affect how participants hold a virtual push broom in VR. We aim to understand how the affordances available with current VR controllers and a custom broom VR controller impact user hand placement in a visual VR broom task. We compare hand placement in two VR conditions against hand placement holding a real push broom. Our goal is to understand the roles that controllers have on recreating physically accurate actions in VR training scenarios. The results from this initial pilot show an effect of the broom controller condition but also that the order in which some of the conditions were presented to subjects affected the way subjects held the VR and real push brooms in subsequent actions. Future work will continue to explore how controller affordance may impact the role of training in VR.},
  keywords={Training;Task analysis;Human computer interaction;Aerospace electronics;Virtual reality;Visualization;Electronic mail;Human-centered computing [Human computer interaction (HCI)]: Interaction devices Human-centered computing [Human computer interaction (HCI)]: Empirical studies in HCI},
  doi={10.1109/VR.2018.8446510},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446057,
  author={Misiak, Martin and Seider, Doreen and Zur, Sascha and Fuhrmann, Arnulph and Schreiber, Andreas},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Exploration of OSGi-Based Software Systems in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={We present an approach for exploring OSGi-based software systems in virtual reality. We employ an island metaphor, which represents every module as a distinct island. The resulting island system is displayed in the confines of a virtual table, where users can explore the software visualization on multiple levels of granularity by performing intuitive navigational tasks. Our approach allows users to get a first overview about the complexity of an OSGi-based software system by interactively exploring its modules as well as the dependencies between them.},
  keywords={Visualization;Virtual reality;Buildings;Software systems;Conferences;Urban areas;Software visualization;OSGi;real-world metaphor;virtual reality},
  doi={10.1109/VR.2018.8446057},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446439,
  author={Miyamoto, Ken and Kashima, Takahiro and Tsukahara, Osamu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={AR in a Large Area Through Instance Recognition with Hybrid Sensors}, 
  year={2018},
  volume={},
  number={},
  pages={639-640},
  abstract={This paper presents the concept of instance recognition with access points and vision for realizing augmented reality in a large indoor environment. The proposed study aims at contributing to the reduction of computational cost and mismatches that occur when the database size for instance recognition is large and includes similar textures. The proposed method consists of database construction and instance recognition through the database. The database construction process involves structuring pairs of images and access points for managing images through the access points. The objective of the instance recognition process is to find the best fit by incorporating access points and vision. The evaluation results show that the proposed method consumes 68% lesser computational time and has 10% greater recognition accuracy than our previous work using only vision.},
  keywords={Conferences;Three-dimensional displays;Virtual reality;User interfaces;Handheld computers;Wireless communication;Mobile computing;Access point;affinity propagation;image retrieval;VLAD;I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Tracking},
  doi={10.1109/VR.2018.8446439},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446429,
  author={Moser, Kenneth R. and Arefin, Mohammed Safayet and Edward Swan, J.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Impact of Alignment Point Distance Distribution on SPAAM Calibration of Optical See-Through Head-Mounted Displays}, 
  year={2018},
  volume={},
  number={},
  pages={641-642},
  abstract={The use of Optical See-Through Head-Mounted Displays (OST-HMDs) for presenting Augmented Reality experiences has become more common, due to the increasing availability of lower cost head-worn device options. Despite this growth, commercially available OST hardware remains devoid of the integrated eye-tracking cameras necessary for automatically calibrating user-specific view parameters, leaving manual calibration methods as the most consistently viable option across display types. The Single Point Active Alignment Method (SPAAM) is currently the most-cited manual calibration technique, due to the relaxation of user constraints with respect to allowable motion during the calibration process. This work presents the first formal study directly investigating the effects that alignment point distribution imposes on SPAAM calibration accuracy and precision. A user experiment, employing a single expert user, is presented, in which SPAAM calibrations are performed under each of five conditions. Four of the conditions cross alignment distance (arm length, room scale) with user pose (sitting, standing). The fifth condition is a control condition, in which the user is replaced with a rigidly mounted camera; the control condition removes the effect of noise from uncontrollable postural sway. The final experimental results show no significant impact on calibration due to user pose (sitting, standing). The control condition also did not differ from the user produced calibration results, suggesting that posture sway was not a significant factor. However, both the user and control conditions show significant improvement using arm's length alignment points over room scale alignments, with an order of magnitude difference in eye location estimate error between conditions.},
  keywords={Calibration;Cameras;Optical sensors;Head-mounted displays;Three-dimensional displays;Augmented reality;Manuals;Augmented reality;Optical see-through head-mounted display;Single point active alignment method (SPAAM)},
  doi={10.1109/VR.2018.8446429},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446269,
  author={Ng, Adrian K. T. and Chan, Leith K. Y. and Lau, Henry Y. K.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Study of Cybersickness and Sensory Conflict Theory Using a Motion-Coupled Virtual Reality System}, 
  year={2018},
  volume={},
  number={},
  pages={643-644},
  abstract={Sensory conflict theory attempts to provide the framework of cyber-sickness in virtual reality (VR) systems by the mismatch between visual and vestibular senses. This study examined whether coupling motion sensations to the visual stimulus in a VR setting could reduce the discomfort. A motion-coupled VR system was used. Motion platform provides motion that supplements visual stimulus from the head-mounted display. Participants experience programmed visual and motion yaw rotations while viewing a virtual apartment. Three conditions were tested on how motion and visual stimuli synchronise which each other: purely visual, motion synchronised with visual, and visually-levelled frame of reference. Results showed that providing matching visual-motion stimuli decreased the miserable score (MISC) of cybersickness and increased the joyfulness score (JOSC) of their subjective feeling.},
  keywords={Visualization;Virtual reality;Resists;Atmospheric measurements;Particle measurements;Electronic mail;Robot sensing systems;Visual-vestibular conflict;motion platform;HMD.: Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception},
  doi={10.1109/VR.2018.8446269},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446225,
  author={Nguyen, Anh and Rothacher, Yannick and Kunz, Andreas and Brugger, Peter and Lenggenhager, Bigna},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Environment Size on Curvature Redirected Walking Thresholds}, 
  year={2018},
  volume={},
  number={},
  pages={645-646},
  abstract={Redirected walking (RDW) refers to a number of techniques that enable users to explore a virtual environment larger than the real physical space. These techniques are based on the introduction of a mismatch in rotation, translation and curvature between the virtual and real trajectories, quantified as rotational, translational and curvature gains. When these gains are applied within certain thresholds, the manipulation is unnoticeable and immersion is maintained. Existing studies on RDW thresholds reported a wide range of threshold values. These differences could be attributed to many factors such as individual differences, walking speed, or environment settings. In this paper, we propose a study to investigate one of the environment settings that could potentially influence curvature RDW thresholds: the environment size. The detailed description of the study is also provided, where the adaptive, 2-alternative forced choice method is used to identify the detection thresholds.},
  keywords={Legged locomotion;Visualization;Optical sensors;Integrated optics;Virtual environments;Adaptive optics},
  doi={10.1109/VR.2018.8446225},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446318,
  author={Ogawa, Nami and Narumi, Takuji and Hirose, Michitaka},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Object Size Perception in Immersive Virtual Reality: Avatar Realism Affects the Way We Perceive}, 
  year={2018},
  volume={},
  number={},
  pages={647-648},
  abstract={How does the representation of an embodied avatar influence the way in which a human perceives the scale of a virtual environment? It has been shown that the scale of the external environment is perceived relative to the size of one's body. However, the influence of avatar realism on the perceived scale has not been investigated, despite the fact that it is common to embody avatars of various representations, from iconic to realistic. This study examined how avatar realism would affect perceived graspable object sizes as the size of the avatar hand changes. In the experiment, we manipulated the realism (high, medium, and low) and size (veridical and enlarged) of the avatar hand, and measured the perceived size of a cube. The results showed that the size of the cube was perceived to be smaller when the avatar hand was enlarged for all degrees of realism of the hand. However, the enlargement of the avatar hand had a greater influence on the perceived cube size for the highly realistic avatar than for the medium-level and low-level realism conditions. This study shed new light on the importance of the avatar representation in a three-dimensional user interface field, in how it can affect the manner in which we perceive the scale of a virtual environment.},
  keywords={Avatars;Estimation;Hysteresis;Skin;Virtual environments;Human-centered computing-Virtual reality},
  doi={10.1109/VR.2018.8446318},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446445,
  author={Park, Jungsik and Park, Jong-II},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Framework for Virtual 3D Manipulation of Face in Video}, 
  year={2018},
  volume={},
  number={},
  pages={649-650},
  abstract={This paper presents a framework that enables a user to manipulate his/her face shape three-dimensionally in video. Existing face manipulation applications and methods have some limitations: single photo, manipulation on image domain, or limited deformation. In the proposed framework, face is tracked from video by using landmark tracking and fitting 3d morphable face model to image, and the face model is further deformed according to the user input with mesh deformation method and rendered with texture from the frame image onto the camera preview. Therefore, unlike conventional applications and researches for face manipulation, the proposed framework allows the user to perform free-form 3d deformation of face in video and to view the deformed face at various viewpoints.},
  keywords={Face;Three-dimensional displays;Shape;Solid modeling;Deformable models;Strain;Cameras;Computing methodologies-Computer graphics-Graphics systems and interface-Mixed / augmented reality},
  doi={10.1109/VR.2018.8446445},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446173,
  author={Pavllo, Dario and Porssut, Thibault and Herbelin, Bruno and Boulic, Ronan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Marker-Based Finger Tracking with Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={651-652},
  abstract={Hands in virtual reality applications represent our primary means for interacting with the environment. Although marker-based motion capture with inverse kinematics (IK) works for body tracking, it is less reliable for fingers often occluded when captured with cameras. Many computer vision and virtual reality applications circumvent the problem by using an additional system (e.g. inertial trackers). We explore an alternative solution that tracks hands and fingers using solely a motion capture system based on cameras and active markers with machine learning techniques. Our animation of fingers is performed by a predictive model based on neural networks, which is trained on a movements dataset acquired from several subjects with a complementary capture system (inertial). The system is as efficient as a traditional IK algorithm, provides a natural reconstruction of postures, and handles occlusions.},
  keywords={Real-time systems;Machine learning;Prediction algorithms;Virtual reality;Kinematics;Tracking;Predictive models;Human-centered computing―Virtual reality;Computing methodologies―Neural networks},
  doi={10.1109/VR.2018.8446173},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446435,
  author={Peer, Alex and Ullich, Peter and Ponto, Kevin},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Vive Tracking Alignment and Correction Made Easy}, 
  year={2018},
  volume={},
  number={},
  pages={653-654},
  abstract={The alignment of virtual and real coordinate spaces is a general problem in virtual reality research, as misalignments may influence experiments that depend on correct representation or registration of objects in space. This work proposes an automated alignment and correction for the HTC Vive tracking system by using three Vive Trackers arranged to describe the desired axis of origin in the real space. The proposed technique should facilitate the alignment of real and virtual scenes, and automatic correction of a source of error in the Vive tracking system shown to cause misalignments on the order of tens of centimeters. An initial proof-of-concept simulation on recorded data demonstrates a significant reduction of error.},
  keywords={Tracking;Position measurement;Virtual reality;Headphones;Solid modeling;Measurement uncertainty;Electronic mail;I.3.7 [Computing Methodologies]: Graphics Utilities-Virtual reality},
  doi={10.1109/VR.2018.8446435},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446523,
  author={Pei, Qikai and Guo, Juan and LU, Haiwen and Ma, Guilona and Li, Wensong and Zhang, Xinyu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={COP: A New Continuous Packing Layout for 360 VR Videos}, 
  year={2018},
  volume={},
  number={},
  pages={655-656},
  abstract={We present a new projection format and packing layout for 360 VR videos using octahedron mapping. A spherical video is projected onto an octahedron, where the upper and lower hemispheres correspond to the upper and lower half of the octahedron, respectively. Four regular triangles in the half of the octahedron, are transformed into isosceles right-angled triangles and packed into a square by remaining adjacent edges. Two equal squares resulted respectively from the upper and lower half of the octahedron, are placed side by side and adjoined by a common edge. This generates a 2:1 aspect ratio rectangle. We demonstrate that our new projection format and layout have advantages in uniformity of pixel density, internal continuity and rectangle aspect ratio while encoding 360 VR videos.},
  keywords={Videos;Layout;Two dimensional displays;Three-dimensional displays;Encoding;Google;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR.2018.8446523},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446529,
  author={Pham, Duc-Minh},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Human Identification Using Neural Network-Based Classification of Periodic Behaviors in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={657-658},
  abstract={There are a lot of techniques that help computer systems or devices identify their users in order to not only protect privacy, personal information, and sensitive data but also provide appropriate treatments, advertisements, or benefits. With passcode, password, fingerprint, or iris, people need to explicitly do some required activities such as typing their codes, showing their eyes, and putting their fingers on the scanners. Those solutions should be used in high-secure scenarios such as executing banking transactions and unlocking personal phones. In other systems such as gaming machines and collaborative frameworks, which aim to prioritize user experience and convenience, it would be better if user profile can be collected and built implicitly. Among those systems, virtual reality (VR) is a new trend, a new platform supporting not only fully immersive experience for gamers but also a collaborative environment for students, researchers, and other people. Currently, VR systems can track user physical activities via trackable devices such as HMD and VR controllers. Therefore, we aim to use virtual reality as our identification equipment. In virtual reality, we can easily simulate an invariant condition at any time so that people have larger probability to replicate their behaviors without any external affections. Therefore, we want to investigate if we could classify VR users based on their periodic interaction with virtual objects. We collect the position and direction of user's head or hands when doing a task and build a classification model based on those data using convolutional neural network approach. We have done an experiment to explore the capability of our proposed technique. The result was motivated with the highest accuracy of 90.92%. Identification in VR hence is potentially applicable. In the future, we plan to do a large-scale experiment with a larger group of participants to examine the strength of our method.},
  keywords={Task analysis;Virtual reality;Solid modeling;Collaboration;Magnetic heads;Games;Headphones;Human identification;virtual reality;neural network;classification;periodic task;Human-centered computing~Virtual reality;Computing methodologies~Neural networks},
  doi={10.1109/VR.2018.8446529},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446542,
  author={Phan, Thai and Hönig, Wolfgang and Ayanian, Nora},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality Collaboration Between Human-Agent Teams}, 
  year={2018},
  volume={},
  number={},
  pages={659-660},
  abstract={Collaboration between two or more geographically dispersed teams has applications in research and training. In many cases specialized devices, such as robots, may need to be combined between the collaborating groups. However, it would be expensive or even impossible to collocate them at a single physical location. We describe the design of a mixed reality test bed which allows dispersed humans and physically embodied agents to collaborate within a single virtual environment. We demonstrate our approach using Unity's networking architecture as well as open source robot software and hardware. In our scenario, a total of 3 humans and 6 drones must move through a narrow doorway while avoiding collisions in the physical spaces as well as virtual space.},
  keywords={Drones;Servers;Virtual reality;Tracking;Collaboration;Wireless communication;Robots;Human-centered computing-Virtual reality;Computer systems organization-Robotic autonomy},
  doi={10.1109/VR.2018.8446542},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446192,
  author={Plouzeau, Jérémy and Chardonnet, Jean-Rémy and Merienne, Frédéric},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Cybersickness Indicators to Adapt Navigation in Virtual Reality: A Pre-Study}, 
  year={2018},
  volume={},
  number={},
  pages={661-662},
  abstract={We propose an innovative method to navigate in a virtual environment by adapting the acceleration parameters to users in real time, in order to reduce cybersickness. Indeed, navigation parameters for most navigation interfaces are still determined by rate-control devices. Inappropriate parameter settings may lead to strong sickness, making the application unusable. Past research found that especially accelerations should not be set too high. Here, we define the accelerations as a function of a cybersickness indicator: the Electro-Dermal Activity (EDA). A pre-study was conducted to test the effectiveness of our approach and showed promising results where cybersickness tends to decrease with our adaptive navigation method.},
  keywords={Navigation;Acceleration;Virtual environments;Real-time systems;User interfaces;Three-dimensional displays;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Interaction design-Interaction design process and methods-User centered design},
  doi={10.1109/VR.2018.8446192},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446247,
  author={Pohl, Daniel and Choudhury, Nural and Achtelik, Markus},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Concept for Rendering Optimizations for Full Human Field of View HMDs}, 
  year={2018},
  volume={},
  number={},
  pages={663-664},
  abstract={To enable high immersion for virtual reality head-mounted displays (HMDs), a wide field of view of the display is required. Today's consumer solutions are mostly around 90 to 110 degrees field of view. The full human field of view for both eyes together has been measured to be between 200 and 220 degrees. Prototypes of HMDs with such properties have been shown. As the rendering workload increases with more pixels to fill the field of view, we propose a novel rendering method optimized for HMDs that cover the full human field of view. We target lower end HMDs where the cost of eye tracking would increase the price too much. Our method works without eye tracking, making use of certain human vision properties that appear once the full human field of view is covered. We achieve almost twice the rendering performance using our method.},
  keywords={Rendering (computer graphics);Gaze tracking;Resists;Optimization;Head-mounted displays;Virtual reality;Head;Computing methodologies-Rendering},
  doi={10.1109/VR.2018.8446247},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446078,
  author={Pouke, Matti and Tiiro, Arttu and LaValle, Steven M. and Ojala, Timo},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Visual Realism and Moving Detail on Cybersickness}, 
  year={2018},
  volume={},
  number={},
  pages={665-666},
  abstract={In this study we compare two conditions of visual detail - modern graphics and detail reduction through cel-shading - in experiencing Cybersickness during virtual movement along a preprogrammed path within a scene depicting a real-world outdoor museum viewed with Oculus CV1. The Cybersickness experience was quantified with the Fast-Motion-Sickness (FMS) scale and the Simulator Sickness Questionnaire (SSQ). We found weak evidence for realistic graphics being more sickness-inducing. Also, FMS scores peaked whenever a participant was entering a building.},
  keywords={Frequency modulation;Visualization;Buildings;Virtual reality;Dynamics;I.3 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
  doi={10.1109/VR.2018.8446078},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446288,
  author={Raimbaud, Pierre and Merienne, Frédéric and Danglade, Florence and Lou, Ruding and Hernández, José Tiberio and Figueroa, Pablo},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Smart Adaptation of BIM for Virtual Reality, Depending on Building Project Actors' Needs: The Nursery Case}, 
  year={2018},
  volume={},
  number={},
  pages={667-668},
  abstract={Nowadays, virtual reality (VR) is widely used in the AEC (architecture, engineering and construction) industry. One crucial issue is how to reuse Building Information Modeling (BIM) models in VR applications. This paper presents an approach for a smart adaptation of BIM models for using in VR scene, by following the needs expressed by building projects actors. The main adaptation consists in filtering BIM data to keep the necessary ones for VR, according to the user objectives. Moreover, VR system should be chosen by taking into account the purpose of usage of the VR model. This approach is applied to a study case of a nursery building project.},
  keywords={Adaptation models;Buildings;Solid modeling;Computational modeling;Three-dimensional displays;Prototypes;Virtual reality;BIM;virtual reality;model adaptation;user needs},
  doi={10.1109/VR.2018.8446288},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446075,
  author={Rajeswaran, Pavithra and Hung, Na-Teng and Kesavadas, Thenkurussi and Vozenilek, John and Kumar, Praveen},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={AirwayVR: Learning Endotracheal Intubation in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={669-670},
  abstract={Endotracheal intubation is a procedure in which a tube is passed through the mouth into the trachea (windpipe) to maintain an airway and provide artificial respiration. This lifesaving procedure, utilized in many clinical situations, requires complex psychomotor skills. Healthcare providers need significant training and experience to acquire skills necessary for a quick and atraumatic endotracheal intubation to prevent complications. However, medical professionals have limited training platforms and opportunities to be trained on this procedure. In this poster, we present a virtual reality-based simulation trainer for intubation training. This VR based intubation trainer provides an environment for healthcare professionals to assimilate these complex psychomotor skills while also allowing a safe place to practice swift and atraumatic intubation. User survey results are presented demonstrating that VR is a promising platform to train medical professionals effectively for this procedure.},
  keywords={Training;Solid modeling;Virtual reality;Medical services;Atmospheric modeling;Animation;Electron tubes;Endotracheal Intubation;Virtual reality;Medical simulation.: Application of Virtual reality for medical simulation;Using Virtual reality for Intubation training},
  doi={10.1109/VR.2018.8446075},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446127,
  author={Renner, Patrick and Blattgerste, Jonas and Pfeiffer, Thies},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Path-Based Attention Guiding Technique for Assembly Environments with Target Occlusions}, 
  year={2018},
  volume={},
  number={},
  pages={671-672},
  abstract={An important use-case of augmented reality-based assistance systems is supporting users in search tasks by guiding their attention towards the relevant targets. This has been shown to reduce search time and errors, such as wrongly picked items or false placements. The optimization of attention guiding techniques is thus one area of research in augmented assistance. In this paper, we address the problem of attention guiding in domains with occluded targets. We propose and evaluate a variant of a line-based approach and show that it improves upon two existing approaches in a newly designed evaluation scenario.},
  keywords={Three-dimensional displays;Task analysis;Splines (mathematics);Augmented reality;Navigation;Visualization;Electronic mail;Attention guiding;simulated augmented reality;evaluation.: H.5.2 [Information Interfaces and Presentation (e.g. HCI)]: User Interfaces-Miscellaneous},
  doi={10.1109/VR.2018.8446127},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446285,
  author={Rubo, Marius and Gamer, Matthias},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Vertex Displacements to Distort Virtual Bodies and Objects While Preserving Visuo-Tactile Congruency During Touch}, 
  year={2018},
  volume={},
  number={},
  pages={673-674},
  abstract={Distorting virtual bodies during virtual body illusions is a key element in several fields of basic research and clinical applications. This technique typically results in a loss of visuo-tactile congruency during touch, which is concealed by restrictions in participant movement. We present a method based on vertex displacement which allows to preserve visuo-tactile congruency during touch while more freely moving a visually distorted virtual body and/or object.},
  keywords={Distortion;Virtual reality;Hip;Three-dimensional displays;Visualization;Strain;Graphics processing units;H.5.1 [Multimedia Information Systems]: Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446285},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446206,
  author={Ryge, Andreas N. and Vollmers, Casper and Hvass, Jonatan S. and Andersen, Lars K. and Berthelsen, Theis and Bruun-Pedersen, Jon R. and Nilsson, Niels C. and Nordahl, Rolf},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Preliminary Investigation of the Effects of Discrete Virtual Rotation on Cybersickness}, 
  year={2018},
  volume={},
  number={},
  pages={675-676},
  abstract={Most virtual reality (VR) applications require the user to travel through the virtual environment (VE). However, some users are susceptible to cybersickness, and this issue is particularly prominent if the user is physically stationary while virtually moving. One approach to minimizing cybersickness is to rotate the user in discrete steps. This poster presents a between-subjects study (n=42) comparing this approach to smooth virtual rotation. The results revealed a statistically significant increase in self-reported sickness after exposure to the VE in case of both conditions. No statistically significant differences between the two conditions were found.},
  keywords={Visualization;Three-dimensional displays;User interfaces;Virtual environments;Task analysis;Cloud computing;Human-centered computing- Visualization- Visualization techniques-Treemaps;Human-centered computing-Visualization-Visualization design and evaluation methods},
  doi={10.1109/VR.2018.8446206},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446126,
  author={Sato, Kunihiko and Rekimoto, Jun},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Voice Conversion System Based on Deep Neural Network Capable of Parallel Computation}, 
  year={2018},
  volume={},
  number={},
  pages={677-678},
  abstract={Voice conversion (VC) algorithms modify the speech of a particular speaker to resemble that of another speaker. Many existing virtual reality (VR) and augmented reality (AR) systems make it possible to change the appearance of users, and if VC is added, then users can also change their voice. State-of-the-art VC methods employ recurrent neural networks (RNNs), including long short-term memory (LSTM) networks, for generating converted speech. However, it is difficult for RNNs to perform parallel computations because the computations at each timestep depend on the results of a previous timestep, which prevents them from operating in real-time. In contrast, we propose a novel VC approach based on a dilated convolutional neural network (Dilated CNN), which is a deep neural network model that allows for parallel computation. We adapted the Dilated CNN model to perform convolutions in both the forward and reverse directions to ensure the learning is successful. In addition, to ensure the model can be parallelized during both the training and inference phases, we developed a model architecture that predicts all output values from the value of the input speech, and does not rely on predicted values for the next input. The results demonstrate that the proposed VC approach has a faster conversion rate relative to that of state-of-the-art methods, while improving speech quality a little and maintaining speaker similarity.},
  keywords={Computational modeling;Solid modeling;Adaptation models;Training;Real-time systems;Neural networks;Predictive models;Voice conversion;Voice avatars;Deep learning;H.5.5 [Information Systems] Sound and Music Computing-Artificial;augmented and virtual realities},
  doi={10.1109/VR.2018.8446126},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446412,
  author={Schiavi, Barbara and Gechter, Franck and Gechter, Céline and Rizzo, Albert},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Teach Me a Story: an Augmented Reality Application for Teaching History in Middle School}, 
  year={2018},
  volume={},
  number={},
  pages={679-680},
  abstract={Augmented Reality (AR) is now more and more widespread in many application fields. Thanks to the new progresses in computer power, AR can now be used widely in education without any expensive additional devices. In this paper is presented the feedback of an experimental protocol using an AR application as an additional support for a History lesson in secondary schools. Even if the technical part has a lead role in the student experience, the most challenging issue is related to the choice of the teaching lesson as itself which must fit several, sometimes contradictory, requirements.},
  keywords={Education;Augmented reality;Three-dimensional displays;History;Protocols;Urban areas;Electronic mail;Augmented Reality;Pedagogical application;History;Human-centered computing [Human computer interaction (HCI)];HCI design and evaluation methods-User studies},
  doi={10.1109/VR.2018.8446412},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446398,
  author={Shaikh, Omar and Sun, Yilu and Stevenson Won, Andrea},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Movement Visualizer for Networked Virtual Reality Platforms}, 
  year={2018},
  volume={},
  number={},
  pages={681-682},
  abstract={We describe the design, deployment and testing of a module to track and graphically represent user movement in a collaborative virtual environment. This module allows for the comparison of ground-truth user/observer ratings of the affective qualities of an interaction with automatically generated representations of the participants' movements in real time. In this example, we generate three charts visible both to participants and external researchers. Two display the sum of the tracked movements of each participant, and a third displays a “synchrony visualizer”, or a correlation coefficient based on the relationship between the two participants' movements. Users and observers thus see a visual representation of “nonverbal synchrony” as it evolves over the course of the interaction. We discuss this module in the context of other applications beyond synchrony.},
  keywords={Tracking;Virtual environments;Avatars;Real-time systems;Correlation;Visualization;Virtual Reality;Avatars;Nonverbal Behavior;Synchrony;Social Interaction;ACM Classification Keywords;I.3.7 [Computer Graphics] Three-Dimensional Graphics and Realism;Virtual Reality;K.8 [Personal Computing] Games},
  doi={10.1109/VR.2018.8446398},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446450,
  author={Si, Weixin and Liao, Xianavun and Wang, Qiong and Heng, Phena-Ann},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality-Based Personalized Virtual Operative Anatomy for Neurosurgical Guidance and Training}, 
  year={2018},
  volume={},
  number={},
  pages={683-684},
  abstract={This paper presents a novel augmented reality (AR) interactive environment for neurosurgical training. Comparing with traditional virtual reality based neurosurgical simulator, our system provides a more natural and intuitive fashion for surgeons. To achieve holographic visualization of virtual brain on 3D-printed skull (workspace), the first step is to reconstruct the personalized anatomy structure from segmented MR imaging. Then, tailored to the computational power of HoloLens, we employ the mass-spring method to model the mechanical response of brain. After that, a precise registration method is employed to map the virtual-real spatial information, which can overlay the virtual operative brain on workspace. In addition, bimanual haptic interface is also integrated into our simulator, which is more similar with real neurosurgery. In experiments, we conduct accuracy validation on our registration method, as well as the validity test on the developed simulators. The results demonstrate that our simulator can provide high-accuracy augmented visualization effects and deep immersion for novice surgeons.},
  keywords={Neurosurgery;Training;Augmented reality;Brain modeling;Solid modeling;Three-dimensional displays;Human-centered computing-Visualization-Visualization techniques-Augmented reality;Human-centered computing-Visualization-Surgical simulation},
  doi={10.1109/VR.2018.8446450},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446486,
  author={Slavova, Yoana and Mu, Mu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparative Study of the Learning Outcomes and Experience of VR in Education}, 
  year={2018},
  volume={},
  number={},
  pages={685-686},
  abstract={Virtual Reality (VR) is believed to have a pivotal role in transforming teaching and learning in higher education. The novelty factor and full immersion in a virtual environment can undoubtedly improve students' attention. However, it is still unclear how the use of VR would impact the learning experience and outcomes with respect to knowledge acquisition. We conducted a comparative study on students' performance in a standardised assessment when course content is delivered using VR and conventional lecture slides. Results show that improvement in social interaction and productivity tools in VR are essential for its greater impact in higher education.},
  keywords={Education;Virtual reality;Visualization;Tools;Headphones;Interviews;Knowledge acquisition;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented and virtual realities},
  doi={10.1109/VR.2018.8446486},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446420,
  author={Smith, Mason and McNamara, Ann},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Gaze Direction in a Virtual Environment Via a Dynamic Full-Image Color Effect}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={For developers of immersive 360-degree virtual environments, directing the viewer's gaze towards Points of Interest (POIs) is a challenge. Limited research exists testing the effectiveness of various gaze direction techniques. However, there is a lack of empirical research evaluating real-time color effects designed to direct the viewer's gaze. We developed a novel VR gaze-directing stimulus using a dynamic real-time color effect and tested its effectiveness in a user study. The stimulus was influenced by color psychology research and chosen by an informal pilot study. Results suggest that the stimulus encouraged participants to direct their gaze back towards POIs. In the majority of subjects who encountered the stimulus, their gaze was successfully directed back to POIs within a few seconds. While the task of holding viewer gaze in VR remains a challenge, this experiment has uncovered new information about the potential of color effect-based VR gaze direction.},
  keywords={Image color analysis;Psychology;Automobiles;Color;Virtual environments;Real-time systems;HMD;gaze direction;color psychology;H.5.1 [HCI]: Multimedia Information Systems - Artificial;augmented;virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality},
  doi={10.1109/VR.2018.8446420},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446471,
  author={Soomro, Shoaib R. and Eldes, Osman and Urey, Hakan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Mobile 3D Telepresence Using Head-Worn Devices and Dual-Purpose Screens}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Head-mounted displays and augmented reality headsets are emerging as the future of human-computer interaction. Such devices can display high resolution 3D images and use on-board cameras to capture the surroundings of the user. However, capturing the user who is wearing the device to facilitate 3D telepresence is not possible with such headsets. Here we propose and demonstrate a new integrated platform to provide mobile 3D telepresence experience using a head-worn device and a dual-purpose passive screen. At the core of this telepresence architecture, we use a portable multi-layered passive screen which facilitates the stereoscopic 3D display using a pair of head-worn projectors and at the same time, captures the multi-perspective views of the user on a head-worn camera through reflections of the screen. The screen contains retroreflective material for stereo image display and an array of convex mirrors for 3D capture. The 3D telepresence is demonstrated using an experimental setup where a local-user wearing the developed head-worn device perceives the 3D images on the dual-purpose screen, while the captured perspective views of user-1 are rendered as stereo viewpoints and showed to the user-2 on a virtual reality headset.},
  keywords={Three-dimensional displays;Telepresence;Cameras;Headphones;Mirrors;Virtual reality;3D Telepresence;3D Display;3D Imaging;Wearable devices.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446471},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446128,
  author={Spagnoletti, Giovanni and Meli, Leonardo and Baldi, Tommaso Lisini and Gioioso, Guido and Pacchierotti, Claudio and Prattichizzo, Domenico},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Rendering of Pressure and Textures Using Wearable Haptics in Immersive VR Environments}, 
  year={2018},
  volume={},
  number={},
  pages={691-692},
  abstract={Haptic systems have only recently started to be designed with wearability in mind. Compact, unobtrusive, inexpensive, easy-to-wear, and lightweight haptic devices enable researchers to provide compelling touch sensations to multiple parts of the body, significantly increasing the applicability of haptics in many fields, such as robotics, rehabilitation, gaming, and immersive systems. In this respect, wearable haptics has a great potential in the fields of virtual and augmented reality. Being able to touch virtual objects in a wearable and unobtrusive way may indeed open new exciting avenues for the fields of haptics and VR. This work presents a novel wearable haptic system for immersive virtual reality experiences. It conveys the sensation of touching objects made of different materials, rendering pressure and texture stimuli through a moving platform and a vibrotactile abbrv-doi-hyperref-narrowmotor. The device is composed of two platforms: one placed on the nail side of the finger and one in contact with the finger pad, connected by three cables. One small servomotor controls the length of the cables, moving the platform towards or away from the fingertip. One voice coil actuator, embedded in the platform, provides vibrotactile stimuli to the user.},
  keywords={Haptic interfaces;Rendering (computer graphics);Solid modeling;Robots;Actuators;Virtual environments;Hardware;Tactile and hand-based interfaces;Haptic devices Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446128},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446226,
  author={Sportillo, Daniele and Paljic, Alexis and Ojeda, Luciano and Fuchs, Philippe and Roussarie, Vincent},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Light Virtual Reality Systems for the Training of Conditionally Automated Vehicle Drivers}, 
  year={2018},
  volume={},
  number={},
  pages={693-694},
  abstract={In conditionally automated vehicles, drivers can engage in secondary activities while traveling to their destination. However, drivers are required to appropriately respond, in a limited amount of time, to a take-over request when the system reaches its functional boundaries. In this context, Virtual Reality systems represent a promising training and learning tool to properly familiarize drivers with the automated vehicle and allow them to interact with the novel equipment involved. In this study, the effectiveness of an Head-Mounted display (HMD)-based training program for acquiring interaction skills in automated cars was compared to a user manual and a fixed-base simulator. Results show that the training system affects the take-over performances evaluated in a test drive in a high-end driving simulator. Moreover, self-reported measures indicate that the HMD-based training is preferred with respect to the other systems.},
  keywords={Training;Automobiles;Virtual reality;Manuals;Wheels;Electronic mail},
  doi={10.1109/VR.2018.8446226},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446144,
  author={Stebbins, Travis and Ragan, Eric D.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirected Scene Rotation for Immersive Movie Experiences}, 
  year={2018},
  volume={},
  number={},
  pages={695-696},
  abstract={Virtual reality (VR) allows for immersive and natural viewing experiences; however, these often expect users to be standing and able to physically turn and move easily. Seated VR applications, specifically immersive 360 degree movies, must be appropriately designed to facilitate user comfort and prevent sickness. Our research explores a scene rotation-based method for redirecting a viewer's gaze and its effectiveness given three parameter adjustments: rotation delay, rotation speed, and angle threshold. The technique may be useful in the development of future immersive movie or VR experiences. From the research, we expect to discover which parameters prove most effective at redirecting a viewer's gaze in an immersive movie experience. We present preliminary developments and an informal usability evaluation to collect participant feedback about preference, comfort, and sickness.},
  keywords={Motion pictures;Virtual reality;Three-dimensional displays;Delays;Legged locomotion;Rain;Visualization;Human-centered computing-Visualization- Visualization techniques-Virtual reality},
  doi={10.1109/VR.2018.8446144},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446199,
  author={Sun, Junwei and Stuerzlinger, Wolfgang},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Selecting Invisible Objects}, 
  year={2018},
  volume={},
  number={},
  pages={697-698},
  abstract={We augment 3D user interfaces with a new technique that enables users to select objects that are invisible from the current viewpoint. We present a layer-based method for selecting invisible objects, which works for arbitrary objects and scenes. The user study shows that with our new techniques users can easily select hidden objects.},
  keywords={Three-dimensional displays;Mice;Task analysis;Wheels;User interfaces;Cameras;Two dimensional displays;3D selection;transparency.: H.5.2 [Information Interfaces and Presentation]: User Interfaces- Graphical User Interfaces (GUI)},
  doi={10.1109/VR.2018.8446199},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446431,
  author={Tahsiri, Mina and Lawson, Glyn and Abdullah, Che and Roper, Tessa},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Multisensory Virtual Environment for OSH Training}, 
  year={2018},
  volume={},
  number={},
  pages={699-700},
  abstract={This paper presents a multisensory and low-cost virtual training simulator developed in Unity 3D, with the aim of improving the effectiveness of Occupational Safety and Health (OSH) training. The prototype system facilitates heat and smell feedback functions operated by an Arduino microprocessor and triggered based on the proximity of the avatar to receptive 4 within the Virtual Environment (VE). The prototype enables the creation of bespoke virtual representations using the 3D scanning function of the Google Tango device making multisensory VE OSH training a feasible and versatile approach in the short-term future.},
  keywords={Training;Three-dimensional displays;Avatars;Virtual environments;Prototypes;Infrared heating;Multisensory;Virtual Environment;training;safety},
  doi={10.1109/VR.2018.8446431},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446350,
  author={Tanaka, Yoshiyuki and Shiokawa, Tadayoshi and Shiokawa, Mitsuhisa},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Scope of Manipulability Sharing: A Case Study for Sports Training}, 
  year={2018},
  volume={},
  number={},
  pages={701-702},
  abstract={Recently, advanced information communication technology and robotic technology have been used for developing a sports-like game application and low-cost interface devices, such as wii-sports, to encourage performing exercises in a room. Such an application can provide an easy-to-understand visual feedback for players by using a virtual reality head-mount display. However, those do not provide quantitative information about the body forms required for training during sports to improve the performance and skill of players. The purpose of this study is to develop an intelligent scope of manipulability sharing considering the dynamic change in the human body form (structure) in sports motion, thus providing an evaluation result based on the manipulability theory for both the player and the instructor in real time. Evaluation tests using a prototype system using a smart glasses and a Kinect sensor are conducted to verify the effectiveness of the proposed scope of manipulability sharing in pitching and batting motions.},
  keywords={Handheld computers;Xenon;Conferences;Virtual reality;Three-dimensional displays;User interfaces;Computer science;Human-centered computing-Visualization-Visualization techniques;Human-centered computing- Visualization-Visualization theory;concepts and paradigms},
  doi={10.1109/VR.2018.8446350},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446424,
  author={Tarng, Stanley and Erfanian, Aida and Hu, Yaoping and Merienne, Frédéric},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Exploration on the Integration of Vibrotactile and Force Cues for 3D Interactive Tasks}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Vibrotactile and force cues of the haptic modality is increasing used to facilitate interactive tasks in three-dimensional (3D) virtual environments (VE). While maximum likelihood estimation (MLE) explains the integration of multi-sensory cues in many studies, an existing work yielded mean and amplitude mismatches when using MLE to interpret the integration of vibrotactile and force cues. To investigate these mismatches, we proposed mean-shifted MLE and conducted a study of comparing MLE and mean-shift MLE. Mean-shifted MLE shared the same additive assumption of the cues as MLE, but took account mean differences of both cues. In a VE, the study replicated the visual scene, the 3D interactive task, and the cues from the existing work. All human participants in the study were biased to rely on the vibrotactile cue for their task, departing from unbiased reliance towards both cues in the existing work. After validating the replications, we applied MLE and mean-shifted MLE to interpret the integration of the vibrotactile and force cues. Similar to the existing work, MLE failed to explain the mean mismatch. Mean-shifted MLE remedied this mismatch, but maintained the amplitude mismatch. Further examinations revealed that the integration of the vibrotactile and force cues might violate the additive assumption of MLE and mean-shifted MLE. This sheds a light for modeling the integration of vibrotactile and force cues to aid 3D interactive tasks within VEs.},
  keywords={Maximum likelihood estimation;Force;Task analysis;Three-dimensional displays;Visualization;Testing;Haptic interfaces;H.5.1 [Multimedia Information Systems]: Artificial;augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2018.8446424},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446538,
  author={Tian, Hao and Wana, Chanabo and Zhana, Xinvu},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Realtime Virtual Grasping System for Manipulating Complex Objects}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={With the introduction of new VR/AR devices, realistic and fast interaction within virtual environments becomes more and more appealing. However, the challenge is to make interactions with virtual objects accurately reflect interactions with physical objects in realtime. In this paper, we present a virtual grasping system for multi-fingered hands when manipulating complex objects. Humanlike grasping postures and realistic grasping motions guarantee a physically plausible appearance for hand grasping. Our system does not require any pre-captured motion data. Our system is fast enough to allow realtime interaction during virtual grasping for complex objects.},
  keywords={Grasping;Virtual environments;Indexes;Task analysis;Three-dimensional displays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2018.8446538},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446310,
  author={Trejo, Fernando and Hu, Yaoping},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={User Performance of VR-Based Tissue Dissection Under the Effects of Force Models and Tracing Speeds}, 
  year={2018},
  volume={},
  number={},
  pages={707-708},
  abstract={Significant research efforts have been devoted to the development of force models that estimate soft-tissue biomechanical responses, finding an application on virtual reality (VR) based surgery training simulation. Nonetheless, the effects of force models on user performance of surgical tasks at different translation speeds are yet unclear. Thus, this work evaluated the effects of simple Weibull and realistic Analytic force models on 10 naïve human subjects for performing 1 degree-of-freedom (DOF) brain-tissue dissection tasks on a VR simulator at speeds of 0.10, 1.27, and 2.54 cm/s. Relying on 4 objective and 5 subjective performance metrics, two-way and one-way ANOVA analyses showed that a realistic force model such as the Analytic model is required to lessen the workload perceived by users only at a low dissection speed of 0.10 cm/s like that observed in neurosurgery. It was also found that dissections performed at the speed of 0.10 cm/s demand more refined manual skills than those at higher speeds. This finding complies with the lengthy surgery training curricula required to master surgical skills.},
  keywords={Force;Analytical models;Task analysis;Solid modeling;Brain modeling;Measurement;Computational modeling;H.5.1 [Multimedia Information Systems]: Artificial;augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2018.8446310},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446279,
  author={Tuanquin, Nikita Mae B. and Hoermann, Simon and Petersen, Carl James and Lindeman, Robert W.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effects of Olfactory Stimulation and Active Participation on Food Cravings in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={709-710},
  abstract={Previous work on combining Virtual Reality (VR) and visual cue-exposure therapy has shown promising results that suggest its potential as a tool to support treatments for eating disorders. Visual food cues can elicit cravings in people no matter where they get their exposure from (e.g., photograph, real world, or virtual world). However, there is little work on the influence of olfactory stimuli in VR. Consequently, we investigated the effects of olfactory stimuli and VR on food cravings. In particular, we examined the hypothesis that olfactory and interaction with food in VR can further increase food cravings. The results of this study show that VR can elicit similar effects as exposure to traditional cues when compared to a neutral baseline. Furthermore, the added olfactory cues increased food cravings and the urge to eat the presented food, which also increased when participants were allowed to interact with the virtual food. In conclusion, VR was shown to have considerable potential to be a valid alternative to traditional food-exposure interventions.},
  keywords={Olfactory;Virtual reality;Task analysis;Medical treatment;Cotton;Visualization;Radio frequency;Virtual Reality;food cravings;food cues;olfactory cues;food interaction.: Human-centered computing - Interaction paradigms - Virtual Reality},
  doi={10.1109/VR.2018.8446279},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446142,
  author={Turner, Eric and Jiang, Haomiao and Saint-Macary, Damien and Bastani, Behnam},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Phase-Aligned Foveated Rendering for Virtual Reality Headsets}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={We propose a novel method of foveated rendering for virtual reality, targeting head-mounted displays with large fields of view or high pixel densities. Our foveation method removes motion-induced flicker in the periphery by aligning the rendered pixel grid to the virtual scene content during rasterization and upsampling. This method dramatically reduces detectability of motion artifacts in the periphery without complex interpolation or anti-aliasing algorithms.},
  keywords={Rendering (computer graphics);Headphones;Graphics processing units;Three-dimensional displays;Virtual reality;Head-mounted displays;Head;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446142},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446141,
  author={Uhr, Manuela and Nitschke, Joachim and Zhang, Jingxin and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hybrid Decision Support System for Traffic Engineers}, 
  year={2018},
  volume={},
  number={},
  pages={713-714},
  abstract={We present a hybrid system combining immersive and non-immersive technology for traffic engineering experts in the cooperative process of construction site planning and decision-making. We exploit a four-sided CAVE setup, which allows a 360° view of the actual real-world location, while an interactive tabletop positioned in the center of the CAVE provides a map-based 2D planning view. By selecting different locations on the digital map, the 360° environment changes with respect to the selected spot. The tabletop can display more detailed data, e. g., traffic flow and traffic light programs. In the described setup group decisions can be made more effectively compared to current workplace situations. In preliminary focus group discussions, we received positive feedback and plan to expand the system's features in the future.},
  keywords={Planning;Urban areas;Three-dimensional displays;Decision support systems;Human computer interaction;Two dimensional displays;Employment;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input Devices and Strategies;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces-Computer-supported cooperative work},
  doi={10.1109/VR.2018.8446141},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446207,
  author={Veronez, Mauricio R. and Gonzaga, Luiz and Bordin, Fabiane and Kupssinsku, Lucas and Kannenberg, Gabriel Lanzer and Duarte, Tiago and Santana, Leonardo G. and de Fraga, Lean Luca and Alves, Demétrius Nunes and Marson, Fernando Pinho},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={RIDERS: Road Inspection & Driver Simulation}, 
  year={2018},
  volume={},
  number={},
  pages={715-716},
  abstract={The main goal of this paper was to evaluate the use of a low cost immersive driving simulator to improve the teaching learning process of the Transport Infrastructure undergraduate course. The driving simulator that was developed in a virtual reality environment to assist both the teaching of engineering and the research on road safety. An experiment was conducted in Transport Infrastructure 1 course for Civil Engineering students in a Brazilian university. The students developed a geometric design of a road that was posteriorly modeled in 3D and provided in simulator. Students piloted a vehicle in the immersive simulator in the same road that they designed. Subsequently the usability of the system was assessed by the SUS metric (System Usability Scale). We performed an evaluation with 52 users and the SUS metric that we found was of 73% assuring a degree of usability above average and demonstrating that the immersive system is good to be used as a complementary tool in the learning of transport infrastructure.},
  keywords={Roads;Solid modeling;Vehicles;Usability;Virtual reality;Safety;Visualization;Road;virtual reality;transport simulations;driving simulator},
  doi={10.1109/VR.2018.8446207},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446587,
  author={Waldow, Kristoffer and Fuhrmann, Arnulph and Grünvogel, Stefan M.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Do Textures and Global Illumination Influence the Perception of Redirected Walking Based on Translational Gain?}, 
  year={2018},
  volume={},
  number={},
  pages={717-718},
  abstract={For locomotion in virtual environments (VE) the method of redirected walking (RDW) enables users to explore large virtual areas within a restricted physical space by (almost) natural walking. The trick behind this method is to manipulate the virtual camera in an user-undetectable manner that leads to a change of his movements. If the virtual camera is manipulated too strong then the user recognizes this manipulation and reacts accordingly. We studied the effect of human perception of RDW under the influence of the level of realism in rendering the virtual scene.},
  keywords={Legged locomotion;Lighting;Rendering (computer graphics);Virtual environments;Cameras;Tracking;Virtual Reality;Locomotion;Human Perception},
  doi={10.1109/VR.2018.8446587},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446544,
  author={Wang, Kuocheng and Adimulam, Kishore and Kesavadas, Thenkurussi},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tetrahedral Mesh Visualization in a Game Engine}, 
  year={2018},
  volume={},
  number={},
  pages={719-720},
  abstract={A tetrahedral mesh is used to perform finite element analysis (FEA) of surgical cuts in a medical simulator. Visualization of a tetrahedral mesh is an important challenge in the process of constructing a realistic simulator. As game engines become increasingly popular, many companies are beginning to develop their own engines, or using existing engines for software development. However, game engines such as Unity and Unreal do not accept volume mesh. What is more, during the cutting process, the topology of tetrahedral mesh will change, resulting in progressively difficult visualization. In this paper, we present a procedure to prepare a tetrahedral mesh for a surgery simulator. We will also show an example of cutting a sphere by deleting tetrahedral elements.},
  keywords={Engines;Games;Visualization;Surgery;Computational modeling;Three-dimensional displays;Solid modeling;Tetrahedral mesh;Unity;surgical simulation},
  doi={10.1109/VR.2018.8446544},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446341,
  author={Wang, Chao and Lianq, Shuanq and Jia, Jinyuan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersing Web3D Furniture into Real Interior Images}, 
  year={2018},
  volume={},
  number={},
  pages={721-722},
  abstract={Platforms for interior DIY(Do It Yourself) design should hold sufficient realistic sense and light manual operation in interior modeling, besides more flexibility and adaptation of online editing are also essential. But current pure Web3D or pure image based platforms are hardly meet those goals. Therefore this paper presents a lightweight and immersive solutions by editing virtual 3D furniture into captured 2D interior pictures interactively, placing them at the optimal location automatically and rendering them in real time to consistently harmonize them with real interior pictures in terms of geometric layout and lighting visual effects. Our contributions consist in: (1)cuboid modeling from camera captured interior pictures interactively without loss of realistic sense and with light manual operation; (2)lightweight automatic furniture arrangement method with enough flexibility and adaptation for online editing; (3)lightweight IBL(Image Based Lighting) and PBR(Physic Based Rendering) to make virtual furniture immerse into real interior image more visually authentic. Compared with those existing online systems, this solution can provide low-cost, convenient, pervasive online services for interior DIY design over mobile Internet.},
  keywords={Solid modeling;Two dimensional displays;Lighting;Three-dimensional displays;Rendering (computer graphics);Layout;Human-centered computing;Visualization techniques;Treemaps;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2018.8446341},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446457,
  author={Willemsen, Pete and Jaros, William and McGregor, Charles and Downs, Edward and Berndt, Maranda and Passofaro, Alexander},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Memory Task Performance Across Augmented and Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={723-724},
  abstract={As commodity virtual reality and augmented reality hardware becomes more accessible, the opportunity to use these systems for learning and training will increase. This study provides an exploratory look at performance differences for a simple memory matching task across four different technologies that could easily be used for learning and training. We compare time and number of attempts to successfully complete a memory matching game across virtual reality, augmented reality, a large touchscreen table-top display and a real environment. The results indicate that participants took more time to complete the task in both the augmented reality and real conditions. Augmented reality and real environments were statistically different than the fastest two conditions, which occurred in the virtual reality and table-top touch display conditions.},
  keywords={Task analysis;Touch sensitive screens;Augmented reality;Face;Training;Human computer interaction;Human-centered computing [Human computer interaction (HCI)]: Empirical studies in HCI- [Applied computing]: Education-Interactive learning environments},
  doi={10.1109/VR.2018.8446457},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446433,
  author={Xie, Chun and Shishido, Hidehiko and Kameda, Yoshinari and Suzuki, Kenji and Kitahara, Itaru},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Calibration Method for Large-Scale Projection Based Floor Display System}, 
  year={2018},
  volume={},
  number={},
  pages={725-726},
  abstract={We propose a calibration method for deploying a large-scale projection-based floor display system. In our system, multiple projectors are installed on the ceiling of a large indoor space like a gymnasium to achieve a large projection area on the floor. The projection results suffer from both perspective distortion and lens distortion. In this paper, we use projector-camera systems, in which a camera is mounted on each projector, with the “straight lines have to be straight” methodology, to calibrate our projection system. Different from conventional approaches, our method does not use any calibration board and makes no requirement on the overlapping among the projections and the cameras' fields of view.},
  keywords={Distortion;Cameras;Calibration;Lenses;Floors;Image color analysis;Optimization;Large-scale projection;projector-camera system;lens distortion;calibration;augmented reality.: -Human-centered computing~Displays and imagers - Human-centered computing~Mixed / augmented reality - Computing methodologies~Camera calibration},
  doi={10.1109/VR.2018.8446433},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446287,
  author={Yamada, Shohei and Chandrasiri, Naiwala P.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Hand Gesture Annotation in Remote Collaboration Using Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={727-728},
  abstract={In this research we have devised a system which can tell works to the worker by using the hand gestures of the helper. In this system, by using augmented reality, it is possible to display as if the helper's hand model actually exist in front of the worker. In order to evaluate the usefulness of the proposed system, we conducted comparative experiments on remote work support by instruction annotations using conventional method of drawn lines, and the proposed method of by using hand gesture instructions. As a result, no significant difference was found between two methods in terms of ease of understanding in the instructions. However, regarding working time, the hand gesture instructions were shorter by 20 seconds (shortened by 19%) on average than the other.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Hand gesture;augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446287},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446479,
  author={Yamamoto, Tatsuki and Matsumoto, Keigo and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Adopting the Roll Manipulation for Redirected Walking}, 
  year={2018},
  volume={},
  number={},
  pages={729-730},
  abstract={The contribution of this paper is to propose a novel Redirected Walking (RDW) technique that adopts manipulation gain in the roll direction. RDW is a technique that enables users to explore a large Virtual Environment (VE) while walking within a physically limited space by manipulating their virtual vision. Thus far, studies have determined the detection thresholds for translation, rotation, and curvature gains, but the thresholds are limited in the yaw direction. In contrast, in other research areas, movements in the yaw and roll directions have sometimes been addressed at the same time. In the present study, we investigated the detection threshold of inclination gain in the roll direction. We applied an inclination gain that was increased gradually while the participants walked 3 meters straight ahead. The results showed that users can detect an inclination gain of 1.93° on the left hand and 1.39° on the right hand.},
  keywords={Legged locomotion;Electronic mail;Virtual environments;Visualization;Foot;Meters;Human-centered computing-Virtual reality},
  doi={10.1109/VR.2018.8446479},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446577,
  author={Yoshida, Shunsuke},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={On-The-Fly Simulator of Tabletop Light-Field 3-D Displays Powered by a Game Engine}, 
  year={2018},
  volume={},
  number={},
  pages={731-732},
  abstract={This paper proposes a method for visual simulation of glass-free tabletop light-field 3-D displays based on a popular game-engine. Light-field 3-D displays generally entail several trade-offs among the screen optics and projection parameters. These parameters are linked to hardware configurations. Therefore, physical cut-and-try prototyping and well-controlled fabrication are practically difficult. To improve the final 3-D visuals of 3-D displays, a simulator based on a popular game engine was developed by our proposed approach. Utilizing built-in functions efficiently, the simulator could be developed in a more agile and intuitive manner compared to full-scratch coding. Various parameters were examined in real time.},
  keywords={Three-dimensional displays;Games;Engines;Solid modeling;Image color analysis;Visualization;Cameras;Tabletop 3-D display;light-field 3-D display;vertex and fragment shader;ray casting;game engine;simulator},
  doi={10.1109/VR.2018.8446577},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446453,
  author={Yu, Run and Bowman, Doug A.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Force Push: Exploring Expressive Gesture-to-Force Mappings for Indirect 3D Object Manipulation}, 
  year={2018},
  volume={},
  number={},
  pages={733-734},
  abstract={We present Force Push, a hyper-natural gesture-to-action mapping for object manipulation in virtual reality (VR). It maps hand gestures derived from human-human interaction to physics-driven movement of an object and uses expressive features of gestures to enhance controllability. An initial user study shows both the performance and broader user experience qualities of Force Push as compared to a traditional direct control mapping.},
  keywords={Force;Three-dimensional displays;Task analysis;Drag;Human computer interaction;Controllability;Gesture;object manipulation;indirect mapping;H.5.2 [Information Interfaces and Presentation];User Interfaces;Interaction styles},
  doi={10.1109/VR.2018.8446453},
  ISSN={},
  month={March},}
@INPROCEEDINGS{8446260,
  author={Zhang, Zhenliang and Cao, Benyang and Guo, Jie and Weng, Dongdong and Liu, Yue and Wang, Yongtian},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Inverse Virtual Reality: Intelligence-Driven Mutually Mirrored World}, 
  year={2018},
  volume={},
  number={},
  pages={735-736},
  abstract={Since artificial intelligence has been integrated into virtual reality, a new branch of virtual reality, which is called inverse virtual reality (IVR), is created. A typical IVR system contains both the intelligence-driven virtual reality and the physical reality, thus constructing an intelligence-driven mutually mirrored world. We propose the concept of IVR, and describe the details about the definition, structure and implementation of a typical IVR system. The parallel living environment is proposed as a typical application of IVR, which reveals that IVR has a significant potential to extend the human living environment.},
  keywords={Virtual environments;Robots;Entertainment industry;Mirrors;Control systems;Visualization;Human-centered computing-Human computer interaction-Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446260},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446291,
  author={Zhang, Zhenliang and Li, Yue and Weng, Dongdong and Liu, Vue and Wang, Yongtian},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Physics-Inspired Input Method for Near-Field Mixed Reality Applications Using Latent Active Correction}, 
  year={2018},
  volume={},
  number={},
  pages={737-738},
  abstract={Calibration accuracy is one of the most important factors to affect the user experience in mixed reality applications. For a typical mixed reality system built with the optical see-through head-mounted display (OST-HMD), a key problem is how to guarantee the accuracy of hand-eye coordination by decreasing the instability of the eye and the HMD in long-term use. In this paper, we propose a real-time latent active correction (LAC) algorithm to decrease hand-eye calibration errors accumulated over time. Experimental results show that we can successfully use the LAC algorithm to physics-inspired virtual input methods.},
  keywords={Calibration;Task analysis;Virtual reality;Real-time systems;Keyboards;Adaptive optics;Three-dimensional displays;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Graphical user interfaces},
  doi={10.1109/VR.2018.8446291},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446129,
  author={Zhang, Zhenliang and Cao, Benyang and Weng, Dongdong and Liu, Yue and Wang, Yongtian and Huang, Hua},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Hand-Based Interaction for Near-Field Mixed Reality with Optical See-Through Head-Mounted Displays}, 
  year={2018},
  volume={},
  number={},
  pages={739-740},
  abstract={Hand-based interaction is one of the most widely-used interaction modes in the applications based on optical see-through head-mounted displays (OST-HMDs). In this paper, such interaction modes as gesture-based interaction (GBI) and physics-based interaction (PBI) are developed to construct a mixed reality system to evaluate the advantages and disadvantages of different interaction modes for near-field mixed reality. The experimental results show that PBI leads to a better performance of users regarding their work efficiency in the proposed tasks. The statistical analysis of T-test has been adopted to prove that the difference of efficiency between different interaction modes is significant.},
  keywords={Task analysis;Virtual reality;Three-dimensional displays;Optical sensors;Calibration;Optical imaging;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Graphical user interfaces},
  doi={10.1109/VR.2018.8446129},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446581,
  author={Zhao, Yuchen and Forte, Maurizio and Kopper, Regis},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Touch Museum}, 
  year={2018},
  volume={},
  number={},
  pages={741-742},
  abstract={In recent years, digital technology has become ubiquitous in the museum. They have changed the ways museums document, preserve and present cultural heritage [3]. Now, we are exploring if there are some ways that could provide more historical context to a displayed object and make an exhibition more immersive. Therefore, we did a project called “The Virtual Reality Touch Museum” and used an experiment to test if such museum performs better on “Presence” and learning achievements. As the results show, our VR Touch Museum was outstanding in “presence” but more research is necessary to verify how effective it is for learning.},
  keywords={Virtual reality;Monitoring;Art;Visualization;Resists;Electronic mail;History;virtual reality;touch;museum;presence;learning achievements: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;virtual realities},
  doi={10.1109/VR.2018.8446581},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446221,
  author={Ziegler, Peter and Roth, Daniel and Knots, Andreas and Kreuzer, Michael and von Mammen, Sebastian},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulator Sick but Still Immersed: A Comparison of Head-Object Collision Handling and Their Impact on Fun, Immersion, and Simulator Sickness}, 
  year={2018},
  volume={},
  number={},
  pages={743-744},
  abstract={We compared three techniques for handling head-object collisions in room-scale virtual reality (VR). We developed a game whose mechanics induce such collisions which we either addressed (1) not at all, (2) by fading the screen information to black, or (3) by restricting translation, i.e. correcting the virtual offset in such a way that no penetration occurred. We measured these conditions' impact on simulator sickness, fun, and immersion perception. We found that the translation-restricted method yielded the greatest immersion value but also contributed the most to simulator sickness.},
  keywords={Games;Visualization;Time measurement;Atmospheric measurements;Particle measurements;Virtual reality;Collision avoidance;Human-centered computing-Virtual Reality;Human-centered computing-Empirical studies in visualization},
  doi={10.1109/VR.2018.8446221},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446151,
  author={Zimmerer, Chris and Fischbach, Martin and Latoschik, Marc Erich},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Space Tentacles - Integrating Multimodal Input into a VR Adventure Game}, 
  year={2018},
  volume={},
  number={},
  pages={745-746},
  abstract={Multimodal interfaces for Virtual Reality (VR), e.g., based on speech and gesture input/output (I/O), often exhibit complex system architectures. Tight couplings between the required I/O processing stages and the underlying scene representation and the simulator system's flow-of-control tend to result in high development and maintainability costs. This paper presents a maintainable solution for realizing such interfaces by means of a cherry-picking approach. A reusable multimodal I/O processing platform is combined with the simulation and rendering capabilities of the Unity game engine, allowing to exploit the game engine's superior API usability and tool support. The approach is illustrated based on the development of a multimodal VR adventure game called Space Tentacles.},
  keywords={Games;Semantics;Generators;Engines;Software;Grounding;Virtual reality;Human-centered computing-Interaction paradigms;Software and its engineering-Software architectures;Computing methodologies-Virtual reality},
  doi={10.1109/VR.2018.8446151},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446395,
  author={Chardonnet, Jean-Rerny and Di Loreto, Cédric and Ryard, Julien and Housseau, Alain},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Virtual Reality Simulator to Detect Acrophobia in Work-at-Height Situations}, 
  year={2018},
  volume={},
  number={},
  pages={747-748},
  abstract={We propose to demonstrate a novel immersive virtual reality simulator aimed at detecting whether potential workers at height are able to climb high up for dangerous operations. Our simulator consists in a dynamic platform that simulates the vibrations of an aerial device during elevation, a real ladder synchronized in position with a virtual one placed on top of a virtual electric pole, a harness that allows users safely climbing the ladder and a head-mounted display (HMD) for visualization. Our demonstration invites users to experience a high fidelity work-at-height situation triggering fear of heights.},
  keywords={Virtual environments;Visualization;Solid modeling;Tracking;Vibrations;Electronic mail;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Software and its engineering-Software organization and properties-Virtual worlds software-Virtual worlds training simulations},
  doi={10.1109/VR.2018.8446395},
  ISSN={},
  month={March},}@INPROCEEDINGS{8447551,
  author={Firdose, Saeik and Lunqaro†, Pietro and Tollmar, Konrad},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Demonstration of Gaze-Aware Video Streaming Solutions for Mobile VR}, 
  year={2018},
  volume={},
  number={},
  pages={749-750},
  abstract={This demo features an embodiment of Smart Eye-tracking Enabled Networking (SEEN), a novel content delivery method for optimizing the provision of 360° video streaming. SEEN relies on eye-gaze information from connected eye trackers to provide high quality, in real time, in the proximity of users' fixations points, while lowering the quality at the periphery of the users' fields of view. The goal is to exploit the characteristics of the human vision to reduce the bandwidth required for the mobile provision of future data intensive services in Virtual Reality (VR). This demo provides a tangible experience of the tradeoffs among bandwidth consumption, network performances (RTT) and Quality of Experience (QoE) associated with SEEN's novel content provision mechanisms.},
  keywords={Streaming media;Bandwidth;Quality of experience;Visualization;Resists;Virtual reality;Computer architecture;Human-centered computing;-user interface management systems;-Eye-tracking;-Video streaming;-QoE},
  doi={10.1109/VR.2018.8447551},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446259,
  author={Grinshpoon, Alon and Sadri, Shirin and Loeb, Gabrielle J. and Elvezio, Carmine and Feiner, Steven K.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hands-Free Interaction for Augmented Reality in Vascular Interventions}, 
  year={2018},
  volume={},
  number={},
  pages={751-752},
  abstract={Vascular interventions are minimally invasive surgical procedures in which a physician navigates a catheter through a patient's vasculature to a desired destination in the patient's body. Since perception of relevant patient anatomy is limited in procedures of this sort, virtual reality and augmented reality systems have been developed to assist in 3D navigation. These systems often require user interaction, yet both of the physician's hands may already be busy performing the procedure. To address this need, we demonstrate hands-free interaction techniques that use voice and head tracking to allow the physician to interact with 3D virtual content on a head-worn display while making both hands available intraoperatively. Our approach supports rotation and scaling of 3D anatomical models that appear to reside in the surrounding environment through small head rotations using first-order control, and rigid body transformation of those models using zero-order control. This allows the physician to easily manipulate a model while it stays close to the center of their field of view.},
  keywords={Head;Three-dimensional displays;Solid modeling;Surgery;Cameras;Augmented reality;Hands-free interaction;augmented reality;vascular interventions;head tracking;head-worn display},
  doi={10.1109/VR.2018.8446259},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446319,
  author={Gugenheimer, Jan and Stemasov, Evgeny and Sareen, Harpreet and Rukzio, Enrico},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Demonstration of FaceDisplay: Asymetric Multi-User Interaction for Mobile VR}, 
  year={2018},
  volume={},
  number={},
  pages={753-754},
  abstract={Mobile VR HMDs enable users to experience virtual reality content in a variety of nomadic scenarios, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. This leads to a scenario where the HMD User experiences a sense of isolation and the Non-HMD Users a sense of exclusion. To battle these phenomena we present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. This allows Non-HMD User to see inside the immersed users virtual world and enable them to interact via touch and gestures. We built a VR HMD prototype consisting of three additional screens and present interaction techniques and an example application that leverage the FaceDisplay design space.},
  keywords={Resists;Virtual reality;Prototypes;Visualization;Electronic mail;Hardware;Cameras;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/VR.2018.8446319},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446551,
  author={Gugenheimer, Jan and Stemasov, Evgeny and Frommel, Julian and Rukzio, Enrico},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Demonstration of ShareVR: Co-Located Experiences for Virtual Reality Between HMD and Non-HMD Users}, 
  year={2018},
  volume={},
  number={},
  pages={755-756},
  abstract={Most current virtual reality (VR) head-mounted displays (HMD) create a highly immersive experience and are currently becoming part of the living room entertainment (e.g. PSVR). However, current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user). This results in all the bystanders (Non-HMD users) in the living room being excluded from the experience and degraded to mainly observing the HMD user. In this demonstration we show ShareVR, a VR system using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. Additionally, we implemented several experiences for the asymmetric nature of ShareVR, exploring its design space.},
  keywords={Resists;Virtual reality;Visualization;Games;Space exploration;Electronic mail;Aerospace electronics;Human-centered computing- Visualization- Visualization techniques- Treemaps;Human-centered computing-Visualization-Visualization design and evaluation methods},
  doi={10.1109/VR.2018.8446551},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446461,
  author={Haeling, Jonas and Winkler, Christian and Leenders, Stephan and Keßelheim, Daniel and Hildebrand, Axel and Necker, Marc},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={In-Car 6-DoF Mixed Reality for Rear-Seat and Co-Driver Entertainment}, 
  year={2018},
  volume={},
  number={},
  pages={757-758},
  abstract={Conventional VR tracking methods inside a driving vehicle fail out-of-the-box. We present two novel tracking solutions for experiencing a virtual scene on a head-mounted display in a moving vehicle. We leverage advanced vehicle positioning via sensor fusion of existing car sensors to extract an accurate vehicle pose, which we use to mirror car movements virtually. For 6-DoF head-tracking, we exploit IMU-only tracking for two separate approaches - with and without additional hardware. The virtual car, in which the demo user is situated, moves in a virtual urban scene with diverse and appealing VR scenery generated from real world map data.},
  keywords={Automobiles;Tracking;Virtual reality;Sensor fusion;Headphones;Entertainment industry;Resists;Human-centered computing-Mixed / augmented reality;Human-centered computing-Virtual reality},
  doi={10.1109/VR.2018.8446461},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446462,
  author={Kaluschke, Maximilian and Weller, René and Zachmann, Gabriel and Pelliccia, Luigi and Lorenz, Mario and Klimant, Philipp and Knopp, Sebastian and Atze, Johannes P. G. and Möckel, Falk},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Virtual Hip Replacement Surgery Simulator with Realistic Haptic Feedback}, 
  year={2018},
  volume={},
  number={},
  pages={759-760},
  abstract={We present the first VR training simulator for hip replacement surgeries. We solved the main challenges of this task - high and stable forces during the milling process while simultaneously a very sensitive feedback is required - by using an industrial robot for the force output and the development of a novel massively parallel haptic rendering algorithm with support for material removal.},
  keywords={Haptic interfaces;Surgery;Hip;Rendering (computer graphics);Training;Milling;Robots;Human-centered computing-Human computer interaction-Interaction devices-Haptic devices},
  doi={10.1109/VR.2018.8446462},
  ISSN={},
  month={March},}@INPROCEEDINGS{8445841,
  author={Kato, Shingo and Lseki, Masaaki and Nakamoto, Takamichi},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Demonstration of Olfactory Display Based on Sniffing Action}, 
  year={2018},
  volume={},
  number={},
  pages={761-762},
  abstract={An olfactory display is a device which provides various scents to a user. One of the main problems in the conventional olfactory display is the residual smell in the ambient air. To suppress the unnecessary odor emission and let an user receive comfortable amount of odor, we have developed a newly structured olfactory display based on the combination of SAW atomizer with micro dispensing valves. The odor delivery process is designed so the odor can go into the nostril only when the user sniffs it. We also developed a VR game using the device combined with head mounted display(HMD). The user can experience the clear change of scent through the developed VR game in this research demonstration.},
  keywords={Olfactory;Surface acoustic waves;Valves;Resists;Games;Virtual reality;Olfactory Display;Aspiration;SAW;Solenoid Valve;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems - Artificial;augmented and virtual realities},
  doi={10.1109/VR.2018.8445841},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446385,
  author={Keller, Marilyn and Exposito, Frédéric},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Game Room Map Integration in Virtual Environments for Free Walking}, 
  year={2018},
  volume={},
  number={},
  pages={763-764},
  abstract={Current tracking systems now enable real walking in a virtual scene with a Head Mounted Display (HMD). However, the play area usually remains limited to a few square meters because of tracking limits and the lack of free space in game rooms. This paper describes our demonstration showing how we can use an RGB-D sensor to increase the real game surface by dynamically acquiring a map of the actual game room and integrating it into the virtual environment. Our system was designed to be integrable to any virtual environment and aims to enable free walking with a HMD by showing the position of the real obstacles to the user.},
  keywords={Virtual environments;Games;Legged locomotion;Three-dimensional displays;Cameras;Navigation;Simultaneous localization and mapping;Human-centered computing-Visualization- Visualization techniques;Human-centered computing-Visualization-Interaction Paradigm-Virtual Reality Computing methodologies-Computer Graphics-Image Manipulation},
  doi={10.1109/VR.2018.8446385},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446524,
  author={Kon, Yuki and Nakamura, Takuto and Sakuraqi, Rei and Shlonolrl, Hirotaka and Yem, Vibol and Kajirnoto, Hiroyuki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HangerOVER: Development of HMO-Embedded Haptic Display Using the Hanger Reflex and VR Application}, 
  year={2018},
  volume={},
  number={},
  pages={765-766},
  abstract={The Hanger Reflex is a phenomenon in which the head rotates unintentionally when it is sandwiched by a wire hanger. The reflex is effectively generated by pressing on specific points, and can be reproduced by pressing with an actuator. We propose the HangerOVER, an HMD-embedded haptic display that can provide both force and motion senses using the Hanger Reflex. In this paper, we designed HangerOVER that HMD-embedded force and motion display using the Hanger Reflex, and developed four VR applications for demonstration.},
  keywords={Force;Haptic interfaces;Resists;Virtual reality;Vibrations;Three-dimensional displays;Actuators;balloon haptic head-mounted display;force display;Hanger Reflex;haptic display;haptic interaction;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [User Interfaces];Haptic I/O},
  doi={10.1109/VR.2018.8446524},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446167,
  author={Langbehn, Eike and Lubos, Paul and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirected Spaces: Going Beyond Borders}, 
  year={2018},
  volume={},
  number={},
  pages={767-768},
  abstract={Real walking in virtual reality (VR) is a promising locomotion technique since it offers multi-modal feedback to the user. Unfortunately, the virtual environment (VE) is limited by the available space in the physical world. So far, several techniques were developed to overcome this problem, e. g. redirected walking (RDW) and the use of impossible spaces. RDW subtly manipulates the viewpoint of the user to reorient her walking direction. Impossible spaces are based on subtle changes of the VE to reuse the same physical space for different virtual spaces. In this research demonstration, we show how these two approaches of redirected walking and impossible spaces can be combined. In particular, for our implementation we focus on the use of curved corridors that benefits both methods.},
  keywords={Legged locomotion;Virtual environments;Visualization;Electronic mail;Three-dimensional displays;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques},
  doi={10.1109/VR.2018.8446167},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446553,
  author={Marquardt, Alexander and Trepkowski, Christina and Maiero, Jens and Kruijff, Ernst and Hinkeniann, Andre},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Multisensory Virtual Reality Exposure Therapy}, 
  year={2018},
  volume={},
  number={},
  pages={769-770},
  abstract={In this research demo, we show different examples for the integration of multisensory cues to enhance user engagement and trigger emotional responses in immersive environments. Our primary focus is to use this modular-designed system for the supportive treatment of different anxiety disorders.},
  keywords={Medical treatment;Virtual environments;Stress;Psychology;Electronic mail;Olfactory;Human-centered Computing-Multisensory Cues-Virtual Reality-Therapy;Treatment-3D User Interfaces},
  doi={10.1109/VR.2018.8446553},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446396,
  author={Renner, Patrick and Pfeiffer, Thies},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Attention Guiding Using Augmented Reality in Complex Environments}, 
  year={2018},
  volume={},
  number={},
  pages={771-772},
  abstract={The localization of objects or locations in an environment is an essential task relevant for many work processes. An augmented reality (AR)-based assistance system may support users by guiding their attention towards the relevant targets. This will reduce the time needed for visual search and reduce errors, such as wrongly picked items or false placements. The design of proper attention guiding techniques is thus one area of research in augmented assistance. We developed a number of new attention guiding techniques and evaluated them in several experiments together with recent or classic existing techniques in varying picking scenarios. In our current work, we adapted a standardized assembly scenario to a more complex environment including occlusions to provide a scenario supporting reproducibility of our results. In the research demo, a number of visual and acoustic guiding techniques can be tested and combined. The demonstration is implemented on the Microsoft HoloLens.},
  keywords={Three-dimensional displays;Augmented reality;Task analysis;Visualization;Glass;Maintenance engineering;User interfaces;Attention guiding;augmented reality assistance;evaluation: H.5.2 [Information Interfaces and Presentation (e.g. HCI)]: User Interfaces-Miscellaneous},
  doi={10.1109/VR.2018.8446396},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446136,
  author={Rietzler, Michael and Geiselhart, Florian and Brich, Julia and Rukzio, Enrico},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Demo of the Matrix Has You: Realizing Slow Motion in Full-Body Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={773-774},
  abstract={We perceive the flow of time as a constant factor in the real world, but there are examples in media, like films or games, where time is being manipulated and slowed down. Manipulating temporal cues is simple in linear media by slowing down video and audio. Interactive media like VR however poses additional challenges, because user interaction speed is independent from media speed. While the speed of the environment can still be manipulated easily, interaction is a new aspect to consider. We implemented such manipulation by slowing down visual feedback of user movements. In prior experiments we slowed down the virtual representation of a user by applying a velocity based low pass filter and by visually redirecting the motion. We found such a manipulation to be even contributing to realism, enjoyment or presence as long as it is consistent with the experience.},
  keywords={Media;Visualization;Games;Tracking;Virtual environments;Hardware;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446136},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446160,
  author={Schild, Jonas and Misztal, Sebastian and Roth, Beniamin and Flock, Leonard and Luiz, Thomas and Lerner, Dieter and Herkersdorf, Markus and Weaner, Konstantin and Neuberaer, Markus and Franke, Andreas and Kemp, Claus and Pranqhofer, Johannes and Seele, Sven and Buhler, Helmut and Herpers, Rainer},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Applying Multi-User Virtual Reality to Collaborative Medical Training}, 
  year={2018},
  volume={},
  number={},
  pages={775-776},
  abstract={We present a multi-user virtual reality (VR) setup that aims at providing novel training tools for paramedics that enhances current learning methods. The hardware setup consists of a two-user full-scale VR environment with head-mounted displays for two interactive trainees and one additional desktop pc for one trainer participant. The software provides a connected multi-user environment, showcasing a paramedic emergency simulation with focus on anaphylactic shock, a representative scenario for critical medical cases that happen too rare to eventually occur within a regular curricular term of vocational training. The prototype offers hands-on experience on multi-user VR in an applied scenario, generating discussion around current state and future development concerning three important research areas: (a) user navigation, (b) interaction, (c) level of visual abstraction, and (d) level of task abstraction.},
  keywords={Solid modeling;Virtual reality;Games;Task analysis;Vocational training;Software;Virtual reality;serious games;medical training;multiuser VR},
  doi={10.1109/VR.2018.8446160},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446399,
  author={Sinclair, Mike and Ofek, Eyal and Holz, Christian and Choi, Inrak and Whitmire, Eric and Strasnick, Evan and Benko, Hrvoje},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Three Haptic Shape-Feedback Controllers for Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={777-778},
  abstract={We present three new novel haptic controllers that render shape force feedback during interaction. 1) CLAW is a multi-purpose controller that renders tactile forces for common hand interactions, such as grasping, touching, and triggering grasped objects. 2) Haptic Revolver is a general-purpose handheld VR controller that renders touch contact with virtual surfaces, motion shear along a surface, textures, and shapes using interchangeable wheels. 3) Haptic Links haptic render shape feedback between two controllers using variable-stiffness locking mechanisms to provide force feedback for grasping and interacting with two-handed objects such as wind instruments, steering wheels, handle bars, or bow and arrow.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Haptics;Virtual Reality;Controller;#K.6.1 [Management of Computing and Information Systems];Project and People Management;Life Cycle;K.7.m [The Computing Profession];Miscellaneous-Ethics},
  doi={10.1109/VR.2018.8446399},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446506,
  author={Tiator, Marcel and Fischer, Ben and Gerhardt, Laurin and Nowottnik, David and Preu, Hendrik and Geiger, Christian},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cliffhanger-VR}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={The performance and security in outdoor climbing sports can depend on anxieties. These appear in frightening situations where, e.g. a deep fall is risked. Deep falls can lead to serious injuries or even to fatal accidents. Such situations can be trained in order to be mentally resistant to them and thereby to make climbing safer. However, drawbacks have to be taken into account. The trainee has to bring himself in a possible hazardous situation and nature is not directly reachable for every person. Thus, we present a system, where a user can climb at low heights in reality and simultaneously on a high cliff in VR. In this contribution, we describe the system architecture and future possibilities to safely train stressful outdoor climbing situations indoors.},
  keywords={Poles and towers;Three-dimensional displays;Rocks;Training;Resists;Electronic mail;Sensors;Virtual Reality-Climbing-Mental Training-Body-Tracking-Finger-Tracking;},
  doi={10.1109/VR.2018.8446506},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446332,
  author={Vogel, Daniel and Lubos, Paul and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={AnimationVR - Interactive Controller-Based Animating in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={781-782},
  abstract={Animating with keyframes gives animators a lot of control but they can be tedious and complicated to work with. Currently, different solutions try to simplify animation creation by recording the natural hand movements of the user. However, most of the solutions are bound to 2D animations [1] or suffer from a low workflow speed [4]. The proposed Unity plugin AnimationVR uses the HTC Vive system to enable the puppeteering animation technique in VR while still allowing for a fast workflow speed by utilizing the 6DOF controllers. During storyboarding or rapid prototyping, the user can immediately see the results of their actions. Also, Animation Vr is written for easy integration in already existing Unity projects. The plug in was evaluated by four animation experts who agreed that Animation Vr accelerates the workflow while decreasing the animation precision. This trade-off makes it useful for storyboarding in professional environments. The experts also noted the ease of use of the puppeteering technique which could enable beginners to create complex animations without any experience with animating.},
  keywords={Animation;Cameras;Interviews;Virtual reality;Two dimensional displays;Tools;Electronic mail;Human-centered computing-Human computer interaction (HCI)-Interactive systems and tools-User interface programming},
  doi={10.1109/VR.2018.8446332},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446263,
  author={Yamamoto, Tatsuki and Shimatani, Jumpei and Ohashi, Isamu and Matsumoto, Keigo and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mobius Walker: Pitch and Roll Redirected Walking}, 
  year={2018},
  volume={},
  number={},
  pages={783-784},
  abstract={A redirected walking (RDW) techniques enable users to walk around infinite virtual environments (VEs) in a finite physical space. In previous studies on RDW, many researchers have discussed manipulations in the yaw direction, but few have tackled with redirection in pitch and roll directions. We propose a novel VR system, which realizes pitch and roll redirections and allows users to experience walking on the 3D model of Mobius Strip in the VE.},
  keywords={Legged locomotion;Strips;Three-dimensional displays;Electronic mail;Solid modeling;Aerospace electronics;Acceleration;Human-centered computing-Virtual reality},
  doi={10.1109/VR.2018.8446263},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446490,
  author={Yamashita, Shogo and Suwa, Shunichi and Miyaki, Takashi and Rekimoto, Jun},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Water Flow Measurement Technology Assessing Spatial User Interaction in an Underwater Immersive Virtual Reality Environment}, 
  year={2018},
  volume={},
  number={},
  pages={785-786},
  abstract={Underwater immersive virtual reality (VR) environments can reproduce unique VR experiences such as swimming in the sea with beautiful coral reefs and a cage surrounded by sharks. Underwater VR poses new technical challenges to creating user interactions because water and the surround-screen make existing methods for realizing user interaction irrelevant. In this research, we present a potential water flow measurement technology aimed at accessing human-computer interaction in underwater VR. Flow measurement can be realized by using tracer particles that are scattered in fluids. However, existing tracer particles are not suitable for underwater immersive VR because the particles stop users from viewing the content on the screen. Therefore, we propose transparent tracer particles that become invisible in water and polarization-based technologies that enable cameras to track the movement of particles. This technology enables virtual objects in VR to react with the actual movement of water and haptic feedback by creating water flow in the swimming pool. These additions would enhance the illusion of immersion in underwater VR.},
  keywords={Atmospheric measurements;Particle measurements;Cameras;Tracking;Virtual reality;Sea measurements;Human-centered computing-Interface design prototyping;Human-centered computing-Virtual reality},
  doi={10.1109/VR.2018.8446490},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446516,
  author={Yem, Vibol and Vu, Kevin and Kon, Yuki and Kajimoto, Hiroyuki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Softness-Hardness and Stickiness Feedback Using Electrical Stimulation While Touching a Virtual Object}, 
  year={2018},
  volume={},
  number={},
  pages={787-788},
  abstract={With the advantages of small size and light weight, electrical stimulation devices have been investigated for providing haptic feedback in relation to virtual objects. Electrical stimulation devices can directly activate sensory receptors to produce reaction force or touch sensations. In the current study, we tested a new method for inducing electrical force sensation in the fingertip, presenting haptic feedback designed to alter softness, hardness and stickiness perception. We developed a 3D virtual reality system combined with finger-motion capture and electrical stimulation devices. The system can provide visual feedback and the sensation of illusory force that moved the index finger by forward-flexion or backward-extension using tendon or cathodic stimulation. In the demo, participants can experience the sensation of softness, hardness and stickiness of a virtual object.},
  keywords={Electrical stimulation;Force;Electrodes;Thumb;Haptic interfaces;Strain;Softness-hardness perception;stickiness perception;electrical stimulation;virtual touch},
  doi={10.1109/VR.2018.8446516},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446549,
  author={Boem, Alberto and Iwata, Hiroo},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Encounter-Type Haptic Interfaces for Virtual Reality Musical Instruments}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={This paper summarizes the author's interest in haptic interfaces for Virtual Reality Musical Instruments. The current research focuses on finding interfaces that can improve physical interaction and presence with such instruments. Musical expression is a topic rarely addressed in the field of Virtual Reality. During the years, the author has explored different systems and concepts while finding the thesis topic for the Ph.D. research. They include the development and evaluation of deformable input surfaces and Shape-Changing interfaces. The results from these implementations led us to investigate Encounter-type haptics, a method that has never received a proper consideration in the design of virtual musical instruments. This represents the current stage of our research. However, the exact direction towards the Ph.D. thesis topic is still in search. Through this paper, we will describe the background and motivations behind this research together with the research hypothesis developed until now.},
  keywords={Haptic interfaces;Instruments;Music;Virtual reality;Shape;Three-dimensional displays;User interfaces;Virtual reality;Music;Haptic feedback;Deformable interfaces;shape-change;Expressivity;H.5.1 [Information Interfaces and Presentation Multimedia: Information Systems - Artificial-augmented and virtual realities];H.5.2 [User Interfaces]: Haptic I/O;H.5.5 [Infor-mation Interfaces and Presentation]: Sound and Music Computing},
  doi={10.1109/VR.2018.8446549},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446617,
  author={Bönsch, Andrea},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Locomotion with Virtual Agents in the Realm of Social Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={791-793},
  abstract={My research focuses on social locomotion of computer-controlled, human-like, virtual agents in virtual reality applications. Two main areas are covered in the literature: a) user-agent-dynamics in, e.g., pedestrian scenarios and b) pure inter-agent-dynamics. However, joint locomotion of a social group consisting of a user and one to several virtual agents has not been investigated yet. I intend to close this gap by contributing an algorithmic model of an agent's behavior during social locomotion. In addition, I plan to evaluate the effects of the resulting agent's locomotion patterns on a user's perceived degree of immersion, comfort, as well as social presence.},
  keywords={Solid modeling;Social groups;Virtual reality;Medical services;Legged locomotion;Cultural differences;H.1.2 [Models and Principles]: User/Machine Systems-Human Factors H.5.2 [Information Interfaces and Presentation]: User Interfaces-EvaluationlMethodology I.3.7 [Computer Graphics]: Three- Dimensional Graphics and Realism-Virtual Reality;J.4 [Computer Applications]: Social And Behavioral Sciences-Psychology},
  doi={10.1109/VR.2018.8446617},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446400,
  author={Vonach, Emanuel},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Robot Supported Virtual and Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={In this dissertation different aspects from research in the fields of Tangible User Interfaces, encounter-type devices and Passive Haptics are combined to investigate the benefits that robots offer for providing haptic feedback in Virtual and Augmented Reality. Robotic elements like micro drives and robotic arms are employed for the actuation of passive or active physical objects. In that way physical props can be collocated with virtual counterparts to allow high fidelity, natural interaction.},
  keywords={Haptic interfaces;Manipulators;User interfaces;Virtual reality;Monitoring;Robot sensing systems;Immersive virtual reality;encounter-type/prop-based/passive haptic feedback;actuation;tangible user interface.: H.5.2 [Information interfaces and presentation]: User Interfaces-Input devices and strategies;Haptic I/O;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446400},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446053,
  author={Whitmire, Eric},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={High-Fidelity Interaction for Virtual and Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={796-798},
  abstract={Expressive interaction with wearable head-mounted displays for virtual (VR) and augmented reality (AR) systems is essential for practical adoption. These systems pose new challenges and have higher performance standards compared to other computing paradigms. In this position paper, I argue that interactive devices for VR and AR systems can leverage high-precision tracking and haptics to achieve a robust set of interaction techniques and a rich sense of presence. I describe my past and proposed future research in designing interactive devices that innovate in the domains of eye tracking, wearable finger input, and handheld controllers.},
  keywords={Haptic interfaces;Gaze tracking;Rendering (computer graphics);Virtual reality;Input devices;Thumb;Sensors;Human-computer interaction;input devices},
  doi={10.1109/VR.2018.8446053},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446138,
  author={Gerstweiler, Georg},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Guiding People in Complex Indoor Environments Using Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={801-802},
  abstract={Complex public buildings like airports use various systems to guide people to a certain destination. Such approaches are usually implemented by showing a floor plan, having guiding signs or color coded lines on the floor. With a technology that supports 6DOF tracking in indoor environments it is possible to guide people individually by using augmented reality guiding visualizations. The proposed research concentrates on three topics which are the main reason, why such a guiding system is still not available in real world situations. At first a tracking solution HyMoTrack is presented, based on a visual hybrid tracking approach for smart phones and tested in a real world airport scenario. The tracking and the guiding part of a reliable indoor navigation requests a 3D model of the environment. For that reason a 3D model generation algorithm was implemented, which automatically creates a 3D mesh out of a vectorized 2D floor plan. Finally the human aspect of an AR guiding system is researched and a novel AR path concept is presented for guiding people with AR devices. This FOVPath is designed to react not only to the position of the user and the target, but is also dependent on the view direction and the field of view (FOV) capabilities of the used device. This ensures that the user always gets reasonable information within the current FOV. To evaluate the concept technical evaluations and user studies were and will be performed.},
  keywords={Solid modeling;Three-dimensional displays;Floors;Augmented reality;Indoor environments;Airports;indoor tracking;navigation;augmented reality;Path Planning;Path Visualization;3D Model Generation;CAD;Human-centered computing-Mixed / augmented reality;Computing methodologies~ Tracking},
  doi={10.1109/VR.2018.8446138},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446146,
  author={Chowdhury, Tanvir Irfan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Reverse Disability Simulation in a Virtual Environment}, 
  year={2018},
  volume={},
  number={},
  pages={803-804},
  abstract={Disability Simulation (DS) is an approach used to modify attitudes regarding people with disabilities (PwD). DS places people without disabilities (PwoD) in situations that are designed for the users to experience a disability. The focus of my PhD dissertation is to transform the concept of disability simulation into reverse disability simulation (RDS) in a virtual reality (VR) environment. In a RDS people with disabilities perform tasks that are made easier in the virtual environment compared to the real world. In a sense, RDS is the “ability” simulation for people with disability. Despite the fact that DS is being used to raise awareness, it also endured some criticism for not being effective. Our first and second study put the criticism into the test, and found out that DS, in fact, can be used as a tools to teach PwoD about facts regarding disability. This result encouraged us to think deeper and invent ideas how to use same concept of DS for PwD. In my third user study, we hypothesized that putting Pw D in a RDS will increase confidence and enable efficient task completion. To investigate this hypothesis, we conducted a within-subjects experiment in which participants performed a virtual “kicking a ball” task in two different conditions: a normal condition without RDS (i.e., same difficulty as in the real world) and an easy condition with RDS (i.e., physically easier than the real world but visually the same). The results from our study suggest that RDS increased participants' confidence. The outcome of our finding has the potential to be used in VR rehabilitation for PwD.},
  keywords={Solid modeling;Task analysis;Wheelchairs;Games;Legged locomotion;Virtual environments;Virtual Reality;Virtual Environment;Disability Simulation;Reverser Disability Simulation;Immersion;Presence;Head-Mounted Display;[Human computer interaction (HCI)]: Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446146},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446552,
  author={Hochreiter, Jason},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optical Touch Sensing on Non-Parametric Rear-Projection Surfaces}, 
  year={2018},
  volume={},
  number={},
  pages={805-806},
  abstract={The field of augmented reality (AR) has introduced many novel input and output approaches for human-computer interaction. As touching physical objects with the fingers or hands is both natural and intuitive, touch-based graphical interfaces are ubiquitous, but many such interfaces are limited to flat screens or simple objects. We propose an optical method for multi-touch detection and response on non-parametric surfaces with dynamic rear-projected imagery, which we demonstrate on two head-shaped surfaces. We are interested in exploring the advantages of this approach over two-dimensional touch input displays, particularly in healthcare training scenarios.},
  keywords={Cameras;Sensors;Medical services;Three-dimensional displays;Training;Task analysis;Head;H.5.l [Information Interfaces and Presentation]: Multimedia Information Systems-Animations;Artificial;Augmented’ and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;I.3.8 [Computer Graphics]: Applications},
  doi={10.1109/VR.2018.8446552},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446579,
  author={Thomas, Jerald},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Leveraging Configuration Spaces and Navigation Functions for Redirected Walking}, 
  year={2018},
  volume={},
  number={},
  pages={807-809},
  abstract={Redirected walking has been shown to be an effective technique for allowing natural locomotion in a virtual environment that is larger than the physical environment. In this position paper, I identify two large limitations of redirected walking and provide brief descriptions of solutions. I continue to introduce the conceptual design for an algorithm, inspired by techniques in the field of coordinated multi-robotics, that improves upon the current state of redirected walking by addressing these limitations. I then explain how it will become the foundation for my thesis and provide future research vectors.},
  keywords={Navigation;Legged locomotion;Heuristic algorithms;Virtual environments;Transforms;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI theory;concepts and models},
  doi={10.1109/VR.2018.8446579},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446466,
  author={Clements, Jillian},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Predicting Performance During a Dynamic Target Acquisition Task in Immersive Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={810-811},
  abstract={Visual-motor skill is the ability integrate visual perception and motor control. These skills allow the eyes and hands to move in a coordinated way to optimally achieve the goal of the task at hand, which is crucial for success in tasks such as athletics or surgery. Immersive virtual reality (VR) provides a controllable experimental environment to study visual-motor skill learning. In this work, we present a novel experimental framework that combines immersive VR and electroencephalography (EEG) to investigate the kinematic and neurophysiological mechanisms that underlie motor skill performance during a multi-day simulated marksmanship training regimen. We propose two approaches for modeling the biological elements associated with visual-motor skill to predict shot success based on kinematic data and neurophysiological biomarkers.},
  keywords={Trajectory;Task analysis;Solid modeling;Predictive models;Virtual reality;Electroencephalography;Conferences;virtual reality;EEG;visual-motor skill;marksmanship.: 1.5 [Pattern Recognition]: Applications J.4 [Computer Applications]: Social and Behavioral Sciences-Psychology},
  doi={10.1109/VR.2018.8446466},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446521,
  author={Zhang, Jingxin},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Extended Abstract: Natural Human-Robot Interaction in Virtual Reality Telepresence Systems}, 
  year={2018},
  volume={},
  number={},
  pages={812-813},
  abstract={Telepresence systems have the potential to overcome limits and distance constraints of the real-world by enabling people to remotely visit and interact with each other. However, current telepresence systems usually lack natural ways of supporting interaction and exploration of remote environments (REs). In particular, single we-bcams for capturing the RE provide only a limited illusion of spatial presence and movement control of mobile platforms in today's telepresence systems are often restricted to simple interaction devices. One of the main challenges of telepresence systems is to allow users to explore a RE in an immersive, intuitive and natural way, e. g. real walking in the user's local environment (LE), and thus controlling motions of the robot platform in the RE. The goal of the presented research project is to meet these challenges, and contribute to the development and evaluation of novel telep-resence system and interactive behaviours in 360° virtual environments with a focus on full-view telepresence, spatial perception, locomotion, usability and motion sickness.},
  keywords={Telepresence;Legged locomotion;Cameras;Robot vision systems;Task analysis;Virtual reality;Virtual reality;telepresence;interaction;locomotion},
  doi={10.1109/VR.2018.8446521},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446133,
  author={Rasmussen, Loki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Doctorate Consortium Proposal}, 
  year={2018},
  volume={},
  number={},
  pages={814-816},
  abstract={Start of the above-titled section of the conference proceedings record.},
  keywords={},
  doi={10.1109/VR.2018.8446133},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446187,
  author={Speicher, Marco},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shopping in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={In contrast to traditional retail stores, online shopping offers many advantages, such as unlimited opening hours and a stronger focus on functionality. But this is accompanied by a complex categorization, limited product visualization and immersion. Virtual Reality (VR) has the potential to create new shopping experiences that combine the advantages of e-commerce sites and conventional brick-and-mortar shops. We examined the main features of online and offline shops in terms of buying behavior and customer frequency. Furthermore, we designed and implemented an immersive WebVr online purchasing environment and aimed to retain the benefits of online shops, such as search functionality and availability, while focusing on the shopping experience and immersion. This VR shop prototype was evaluated in a case study with respect to the Virtual Reality Shopping Experience (VRSE) model. The next step is to classify, investigate and evaluate the next generation of VR shops, including product interaction and navigation techniques, as well as store and product representations.},
  keywords={Three-dimensional displays;Task analysis;Virtual reality;User interfaces;Visualization;Solid modeling;Navigation;Human-centered computing-User Studies;Human-centered computing-Virtual Reality},
  doi={10.1109/VR.2018.8446187},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446159,
  author={Lee, Myungho},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mediated Physicality: Inducing Illusory Physicality of a Virtual Human via Environmental Objects}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={A physical embodiment of a virtual human has shown benefits in applications that involve social interaction with virtual humans. However, it often incorporates cumbersome haptic devices or robotic bodies. In this position paper, we first discuss our motivation for utilizing a surrounding environment in human-virtual human interaction and present our preliminary studies and results. Considering the previous studies and related literature, we define the concept of Mediated Physicality for virtual humans, which utilizes environmental objects to increase perceived physicality of the virtual humans, and discuss fundamental aspects of the Mediated Physicality as well as future research plans.},
  keywords={Virtual environments;Vibrations;Haptic interfaces;Synchronization;Games;Visualization;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;and Virtual Realities;1.4 [Computer Applications]: Social and Behavioral Sciences-Psychology},
  doi={10.1109/VR.2018.8446159},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446292,
  author={Renner, Patrick},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Prompting Techniques for Guidance and Action Assistance Using Augmented-Reality Smart-Glasses}, 
  year={2018},
  volume={},
  number={},
  pages={820-822},
  abstract={In the context of picking and assembly tasks, assistance systems based on Augmented Reality (AR) can help finding target objects and conducting correct actions. The aim is to develop guiding and action assistance techniques for smart glasses, which are easily understandable not only for workers, but also for impaired and elderly people.},
  keywords={Task analysis;Three-dimensional displays;Visualization;Augmented reality;Manuals;Gaze tracking},
  doi={10.1109/VR.2018.8446292},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446261,
  author={Aseeri, Sahar A. and Interrante, Victoria},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Influence of Avatar Representation and Behavior on Communication in Social Immersive Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={823-824},
  abstract={Virtual reality applications have begun to offer great potential for communication in recent years. Creating an immersive virtual social environment that simulates a real social environment requires providing users with communication cues such as visual, verbal, and nonverbal cues to increase their sense of inhabiting the virtual world. In this work, we will investigate the influence of avatar representation and behavior on communication in an immersive, multiuser, same-place virtual environment by comparing three conditions of avatar representation: video see-through, scanned realistic avatar, and no-avatar representations. Subjective and objective measurements will be used to describe participants' observations and track their movement behavior to ascertain the effect of avatar representations on communication, based on personal presence, social presence, and trustworthiness.},
  keywords={Avatars;Three-dimensional displays;Virtual environments;Cameras;Real-time systems;Current measurement;Avatar;communication;personal presence;social presence;trustworthiness;virtual reality},
  doi={10.1109/VR.2018.8446261},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446426,
  author={Lages, Wallace S.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Walk-Centric User Interfaces}, 
  year={2018},
  volume={},
  number={},
  pages={825-826},
  abstract={Walking is bound to become a common activity in wearable augmented reality. Compared to walking in virtual reality, walking in augmented reality is very simple and uncomplicated. The ability to use AR in different places and even while walking is likely to deeply impact the way users will experience this technology. The research on walk-centric interfaces has the goal to explore this design space, and bring about a better understanding of how walking affects the design of augmented reality applications. This paper outlines the design space, preliminary, and future work on this topic.},
  keywords={Legged locomotion;Task analysis;Augmented reality;Visualization;User interfaces;Mobile handsets;Augmented reality;interaction;walking.: H5.1 [Information interfaces and presentation]: Multimedia Information Systems. - Artificial;augmented;and virtual realities. H.5.2: User Interfaces},
  doi={10.1109/VR.2018.8446426},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446508,
  author={Huang, Wen},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Effectiveness of Head-Mounted Display Virtual Reality (HMD VR) Environment on Students' Learning for a Virtual Collaborative Engineering Assembly Task}, 
  year={2018},
  volume={},
  number={},
  pages={827-829},
  abstract={The emerging VR social networks (e.g., Facebook Spaces, Rec Room) provide opportunities for engineering faculties to design collaborative virtual engineering tasks in their classroom instruction with HMD VR system. However, we do not how this capacity will affect students' learning and their professional skills (e.g., communication and collaboration). The proposed study is expected to fill this research gap and will use a mixed-methods design to explore students' performance and learning outcomes in a virtual collaborative automotive assembly task. The quantitative data will be collected from the pre-and-post task survey and the task itself. This data will be used to analyze the differences among experiment and control groups. Students' responses to the open questions in the post-task survey will serve as triangulation and provide deep insight for the quantitative results. The study is expected to not only contribute to the research field but also benefit different stakeholders in the engineering education systems.},
  keywords={Task analysis;Collaboration;Resists;Virtual environments;Collaborative work;Three-dimensional displays;head-mounted display;virtual reality;engineering education;collaborative learning},
  doi={10.1109/VR.2018.8446508},
  ISSN={},
  month={March},}@INPROCEEDINGS{8445842,
  author={Dudczig, Manuel},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={360° Video - Light Design Experience}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={The 360° video was created for a lighting design company (www.lichtliebe.de) by (www.vrendex.de) illustrating lamp prototypes in an architectural setting to experience their influence and ambience. Based on a virtual reality scene the video was rendered as camera flight through the building to give an ambient impression during daylight and night. Being able to show customers and architects what the lamps will look like - even before first prototypes are available is speeding up the feedback process of designing new customer influenced products. Virtual technologies are also capable of giving house builders an impression of how variants and positions of lights will influence their living space.},
  keywords={Virtual reality;Rendering (computer graphics);Lighting;Cameras;Prototypes;Companies;Buildings;Lights;Design;360°;Rendering;Virtual Reality},
  doi={10.1109/VR.2018.8445842},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446561,
  author={Karakottas, Antonis and Papachristou, Alexandros and Doumanoqlou, Alexandros and Zioulis, Nikolaos and Zarpalas, Dimitrios and Daras, Petros},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented VR}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={Traditional VR is mostly about headset experiences either in completely virtual environments or 360° videos. On the other hand AR has been mixing realities by inserting the virtual within the real. In this work we present the Augmented VR concept that lies at the middle right of the virtuality continuum, typically referred to as augmented virtuality. We offer another perspective by blending the real within the virtual focusing on capturing actual human performances in three dimensions and emplacing them within virtual environments [1]-[3]. By compressing and transmitting this new type of 3D media we can also achieve real-time interaction, communication and collaboration between users. Being in full 3D our media are compatible with a variety of applications be it either VR, AR, MR and open up new exciting opportunities like free viewpoint spectating while also increasing the feeling of immersion of all participating users. We demonstrate our technology via a prototype two player game that can support spectating in various devices like head mounted displays (VR) or tablet laptops (AR). Our system is easy to setup, requiring minimal non-technical human intervention, and relatively low cost taking one step ahead in making this technology available to the consumer public.},
  keywords={Three-dimensional displays;Real-time systems;Media;Virtual environments;Videos;Human-centered computing - Human computer interaction (HCI) - Interaction paradigms - Virtual reality;Human-centered computing - Human computer interaction (HCI) - Interaction paradigms - Mixed/augmented reality;Computing methodologies - Computer graphics - Graphics systems and interfaces - Mixed/augmented reality;Computing methodologies - Computer vision - Image and video acquisition - Camera calibration;Computing methodologies-Computer vision - Computer vision representations - Appearance and texture representations;Computing methodologies - Computer vision - Computer vision problems - Reconstruction;Multi-user Virtual Reality;Teleimmersion},
  doi={10.1109/VR.2018.8446561},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446124,
  author={Kuchelmeister, Volker},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Immersion: Simulating Immersive Experiences in VR}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={This is an investigation in how VR can simulate experiences designed for large scale immersive environments. Immersive display and interaction environments and systems have been utilised in simulation, visualisation, entertainment, the arts and museological context for a long time before VR made its resurgence only a few years back. These systems include amongst others 360 degree cylindrical projection environments [3], curved screens, hemispherical projection systems [1] and multi-perspective installations [2]. In comparison to traditional screen based media, immersive environments provide a unique delivery platform for ultra-high resolution digital content at a real-world scale and for multiple simultaneous viewers. This makes them the ideal stage for impactful experiences in public museums, festivals and exhibitions. Applications and experiences created for a specific platform rely on the complex and costly technical infrastructure they were originally designed for. Descriptions and video documentation only go so far in illustrating an immersive experience. The embodied aspect, the emotional engagement and the dimensional extend, central to immersion, is mostly lost in translation. This project offers a prototypical implementation of a large scale virtual exhibition incorporating various immersive environments and applications situated within a fictional 3D scene (Figure 1). The focus is on simulation and conservation of existing applications and to create test bed for future projects.},
  keywords={Art;Three-dimensional displays;Solid modeling;Media;Visualization;Virtual reality;Context modeling;Immersive Environments;Virtual Reality;UI;Simulation},
  doi={10.1109/VR.2018.8446124},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446576,
  author={Lempitsky, Victor and Vakhitov, Alexander and Starostin, Andrew},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={CarpetVR: The Magic Carpet Meets the Magic Mirror}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={We present CarpetVR - a new system for marker-based positional tracking suitable for mobile VR (Figure 1). The system utilizes all sensors present on a modern smartphone (a camera, a gyroscope, and an accelerometer) and does not require any additional sensors. CarpetVR uses a single floor marker that we call the magic carpet. CarpetVR augments a standard mobile VR setup with a slanted mirror that can be attached either to the smartphone or to the head mount in front of the smartphone camera. As the person walks over the marker, the smartphone camera is able to see the marker thanks to the reflection in the mirror. Our tracking engine then uses a computer vision module to detect the marker and to estimate the smartphone position with respect to the marker at 40 frames per second. This estimate is integrated with high framerate signals from the gyroscope and the accelerometer. The resulting estimates of the position and the orientation are then used to render the virtual world. Our sensor fusion algorithm ensures minimal-latency tracking with very little jitter.},
  keywords={Mirrors;Cameras;Sensor fusion;Sensor systems;Gyroscopes;Accelerometers},
  doi={10.1109/VR.2018.8446576},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446366,
  author={McMillian, McKennon and Finney, Hunter and Hopper, Jonathan and Jones, J. Adam},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Depth Light}, 
  year={2018},
  volume={},
  number={},
  pages={834-835},
  abstract={The Depth Light solves the problem of not being able to view the real world, without having to remove the Head Mounted Display, accurately and easily. The Depth Light is activated by a button or trigger press on an HTC Vive controller and consists of a Vive controller, an ultrasonic depth finder, a microcontroller (to send measured distances over serial), a web camera, and a mount for the microcontroller and camera. The device works by finding the distance between the device and the nearest real-world object, taking a sum of these distances, and sending this over serial to a computer as an average. In Unity3D, an object is rendered at the distance sent from the micro controller. This object is then textured with the video feed from the web camera. This object's distance changes in the virtual environment in real time as the Depth Lights micro controller sends new information. As the distance changes the scale of the object also changes, this is to keep the object the same size in the field of vision. The data from the Depth Light is handled by a Unity3D plugin. This plug-in handles all the rendering commands and all of the scaling.},
  keywords={Cameras;Microcontrollers;Presses;Acoustics;Ultrasonic variables measurement;Streaming media;Feeds},
  doi={10.1109/VR.2018.8446366},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446298,
  author={Picinali, Lorenzo},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3D Tune-In: 3D-Games for Tuning and Learning About Hearing Aids}, 
  year={2018},
  volume={},
  number={},
  pages={836-836},
  abstract={3D Tune-In is an EU-funded project which brings together the relevant stakeholders from the videogame industry, academic institutions, a large hearing aid manufacturer, and hearing communities, to produce digital games in the field of hearing aid technologies and hearing loss [2] [3]. The project has now completed the development of the 3D Tune-In Toolkit [1], a flexible, cross-platform library of code and guidelines that gives traditional game and software developers access to high-quality sound spatialisation (both for headphones and loudspeakers), hearing loss and hearing aid simulations. The test application for the Toolkit is currently available for free through the 3D Tune-In project website (http://3d-tune-in.eu/). The C++ code will be released open-source through GitHub in Spring 2018. In addition to the Toolkit, 3D Tune-In has produced 5 different applications aimed at different groups of the hearing impaired and non-hearing impaired communities. The video briefly describes the project context, goals and main outcomes.},
  keywords={Three-dimensional displays;Hearing aids;Auditory system;Games;Tuning;Solid modeling;Open source software;3D sound;binaural;Ambisonic;serious games;hearing aids;hearing loss.: [Human Centred Computing] Audio Feedback;[Social and Professional Topics] Assistive Technologies},
  doi={10.1109/VR.2018.8446298},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446157,
  author={Rastegar, Ali},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Music}, 
  year={2018},
  volume={},
  number={},
  pages={837-837},
  abstract={Virtual reality headsets and sound alongside other elements enables it to achieve its ultimate goal which is to simulate user's physical presence in a virtual environment. User's input to alter and interact with the virtual world is another important factor that has been the subject of extensive researches many of which in the field of art. But even user's ability to look around and move toward a sound source can also be considered as user's input. Therefore, user's input can be regarded as one the main elements of the virtual reality. User's input is a relatively new concept in arts but it has been the basic elements of video games from the beginning and therefore there is no surprise that gaming was the starting point of the virtual reality. As virtual reality is becoming more widespread, it is expected that this technology will be adapted to other fields. But it is also realistic to think that due to user's ability to make different decisions, gamifications will also be adapted alongside this technology. The focus of this project is user-centered art where user's input is the fundamental element of the artwork.},
  keywords={Art;Music;Headphones;Virtual environments;Games},
  doi={10.1109/VR.2018.8446157},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446390,
  author={Reinhuber, Elke and Seide, Benjamin and Williams, Ross},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Secret Detours: A Garden in Singapore}, 
  year={2018},
  volume={},
  number={},
  pages={838-838},
  abstract={Visions of East Asian mythology materialise in a decidedly modern metropolis, a place without a past-two worlds collide. Secret Detours engages the audience with over-whelming vistas in a full spherical presentation, encompassing the viewers from all angles. The movie short is set within a lush Chinese garden, adapted from the great traditions of imperial landscaping (cf. [3]) - in the Yunnan Garden in the West of Singapore. Four dancers, dressed in the colours of the cardinal directions, examine the spaces, the paths and the detours of the green scenery (cf. [1]). The spherical video relates to the experience of being surrounded by mythological creatures and their traces inside the garden. As the beautiful layout of the grounds is composed from a range of intersections with multiple meandering paths to choose from, the omnidirectional video invites similarly to explore the atmosphere between an exquisite selection of trees, shrubs, bushes and pieces of architecture. In 360° environments, the camera is almost objective and the viewer becomes the editor of the piece (cf. [2]), different to the directed camera and edit of `traditional' movies. The question arises how the author can direct the eyes of the audience with different camera settings, perspective, focus, direction of actors, transitions and in this way, prompt emotions?},
  keywords={Art;Media;Cameras;Motion pictures;Cultural differences;History;Publishing;media art;dance;cultural heritage;cardinal directions;spherical video.: K.6.1 [Management of Computing and Information Systems]: Project and People Management-Life Cycle;K.7.m [The Computing Profession]: Miscellaneous-Ethics},
  doi={10.1109/VR.2018.8446390},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446328,
  author={Ross, Miriam},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Until Jesse 360}, 
  year={2018},
  volume={},
  number={},
  pages={839-839},
  abstract={At the dead end of a party, Jesse looks around to see if she can make a connection. This cinematic virtual reality (CVR), 360-degree, film explores what it means to be amongst those brief, late night, moments when strangers come into contact. Until Jesse 360 takes into account the potential for CVR to create the `empathy machine' [2] by situating the viewer amongst, rather than at a distance from, intimate conversations. At the same time, it questions some of the unwritten rules that have emerged in the first few years of CVR, mainly that dynamic editing and short shot length should not be used in case they disorient the viewer. By exploring the possibility of switching between perspectives and providing the viewer with `impossible' viewpoints, Until Jesse 360 challenges our conception of VR space as well as how we can be positioned within it. In this way, it takes into account John Mateer's point that “existing methods for film can be adapted to immersive presentation so long as they also take into consideration unique aspects of the CVR platform” [1] by playing with editing style whilst making the most of 360-degree space.},
  keywords={Virtual reality;Switches;Three-dimensional displays;User interfaces;Media;Dairy products},
  doi={10.1109/VR.2018.8446328},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446527,
  author={Sagardia, Mikel and Turrillas, Alexander Martin and Hulin, Thomas},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Realtime Collision Avoidance for Mechanisms with Complex Geometries}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={This video presents a collision avoidance framework for mechanisms with complex geometries. The performance of the framework is showcased with the haptic interface HUG [3]. We are able to avoid contacts with the robot links and with moving objects in the environment in 1 kHz. The main contribution of our approach is its generic and extensible nature; it can be applied to any mechanism consisting of arbitrarily complex rigid bodies, in contrast to common solutions that use simplified models [2], [7]. In the preprocessing phase, first, the kinematic chain of the mechanism is described [1]. Second, we generate voxelized distance fields and point-sphere hierarchies for the geometry of each mechanism link and each object in the environment [6]. After that, our system requires only the joint angles and information of the environment state (e.g., object poses tracked by optical sensors) to compute collision avoidance forces. At runtime, each link is artificially dilated by a safety isosurface. If a point of an object goes through this surface, a normal force scaled by its penetration depth is computed and applied to the corresponding link. If humans are generically modeled as mechanisms and properly tracked, our system can also prevent collisions with them, ensuring save human-machine collaboration. Figure 1 illustrates the framework and its basic components. The multi-body collision computation architecture was first developed for virtual maintenance simulations with haptic feedback [5], [4], and thereafter extended to collision avoidance of mechanisms. A first prototype was previously published in [8].},
  keywords={Collision avoidance;Geometry;Robots;Haptic interfaces;Virtual reality;Force;Solid modeling;Computing methodologies-Artificial intelligence-Planning and scheduling-Robotic planning;Computing methodologies-Computer graphics-Animation-Collision detection;Human-centered computing-Human computer interaction-Interaction paradigms-Collaborative interaction;Human-centered computing-Human computer interaction-},
  doi={10.1109/VR.2018.8446527},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446477,
  author={Takala, Tuukka M. and Heiskanen, Heikki},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Auto-Scaled Full Body Avatars for Virtual Reality: Facilitating Interactive Virtual Body Modification}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={Virtual reality avatars and the illusion of virtual body ownership are increasingly attracting attention from researchers [1] [2]. As a continuation to our previous work with avatars [3], we updated our existing RUIS for Unity toolkit [4] with new capabilities that facilitate the creation of virtual reality applications with adaptive and customizable avatars.},
  keywords={Avatars;Tracking;Three-dimensional displays;Torso;Computer science;Mirrors},
  doi={10.1109/VR.2018.8446477},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446469,
  author={Todsen, Tobias and Melchiors, Jacob and Wennerwaldt, Kasper},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Use of Virtual Reality to Teach Teamwork and Patient Safety in Surgical Education}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={The use 360VR Videos may increase the Engagement and Attentiveness of Students compared to Traditional 2D videos used in Medical Education (1, 2). We therefore Developed a Stereoscopic 360VR Video to Demonstrate how to Use the WHO's Surgical Safety Checklist in the Operating Room (see Figure 1). With use of VR technology we aimed to give the Medical Students a Realistic Experience of the Operating Room where they can observe the Teamwork performed to Ensure Patient Safety during Surgery. The video is recorded with a Vuze 3D 360 Spherical VR Camera and edited in Final Cut Pro with use of Dashwoods 360VR Toolbox Workflow Plugins.},
  keywords={Surgery;Education;Videos;Safety;Virtual reality;Head;Neck;Medical Education, Surgery, Patient Safety, Stereoscopic 360VR},
  doi={10.1109/VR.2018.8446469},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446550,
  author={Vogel, Daniel and Lubos, Paul and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={AnimationVR - Interactive Controller-Based Animating in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={Animating with keyframes gives animators a lot of control but they can be tedious and complicated to work with. Currently, different solutions try to simplify animation creation by recording the natural hand movements of the user. However, most of the solutions are bound to 2D animations [1] or suffer from a low workflow speed [2]. The proposed Unity plugin Animation Vr uses the HTC Vive system to enable the puppeteering animation technique in VR while still allowing for a fast workflow speed by utilizing the controllers of the VR system. Also, Animation Vr is written for easy integration in already existing Unity projects. The plugin was evaluated with four animation experts. The consensus was that Animation Vr increases the workflow speed while decreasing the animation precision. This tradeoff makes it useful for storyboarding in professional environments. Additionally, the plugin could improve the understanding of VR storytelling as the animators would create and instantly review the animations in the correct medium. The experts also noted the ease of use of the puppeteering technique which could enable beginners to create complex animations with little to no experience with Animation Vr. Additionally, the accessibility for animation beginners could improve the communication in animation teams between animators and directors.},
  keywords={Animation;Virtual reality;Interviews;Robots;Two dimensional displays;Indexes;Programming;Human-centered computing-Human computer interaction (HCI)-Interactive systems and tools-User interface programming},
  doi={10.1109/VR.2018.8446550},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446239,
  author={Woods, Andrew and Bourke, Paul and Oliver, Nick},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Beacon Virtua}, 
  year={2018},
  volume={},
  number={},
  pages={844-844},
  abstract={In Beacon Virtua [1] you can explore the legacy of the shipwrecked VOC ship Batavia by visiting a simulation of Beacon Island (see Figure 1). Beacon Virtua will take you on a tour of the island including its jetties, fishing shacks and several grave sites of Batavia voyagers who were buried on the island after the ship was wrecked and following the uprising. The graves have been reconstructed through a technique called photogrammetric 3D reconstruction, a process which uses multiple photographs of an object to build an accurate and detailed 3D model of it. Beacon Virtua presents the island as it was in 2013, using audio and photography captured during multiple expeditions to the island to preserve this period in its history. [2] In 2013 there were around 15 shacks located across Beacon Island, originally used by the fishing community. These shacks have been recreated as 3D models, which can be explored inside and out. Around the island are photographic panorama bubbles offering 360° views of the island. These bubbles have been captured using a special panoramic photography process - stepping inside a bubble allows you to see the island from that point exactly as it was in 2013.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Virtual environments;virtual reality;heritage;Unity3D;• Computing methodologtes-Vlrtual reality;• Human-centered computing- Virtual reality;• Human-centered computing-Geographic visualization},
  doi={10.1109/VR.2018.8446239},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446389,
  author={Guo, Rongkai and McMahan, Ryan P. and Weyers, Benjamin},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3DUI-League: 9th Annual 3DUI Contest}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={The 9th annual IEEE 3DUI Contest focuses on the development of 3D User Interfaces (3DUIs) for three different tasks in fully immersive Virtual Environments (VEs): (1) Ladder Climbing, (2) First-Person View Flying, and (3) Tower Stacking. The 3DUI Contest is part of the 2018 IEEE Conference on Virtual Reality and 3D User Interfaces held in Reutlingen, Germany. The contest is open to anyone interested in 3DUIs, from researchers to students, enthusiasts, and professionals. The purpose of the contest is to stimulate innovative and creative solutions to challenging 3DUI problems.},
  keywords={Task analysis;Poles and towers;Stacking;Hardware;Three-dimensional displays;User interfaces;Electronic mail;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques},
  doi={10.1109/VR.2018.8446389},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446437,
  author={Barrera Machuca, Mayra Donaji and Sun, Junwei and Pham, Due-Minh and Stuerzlinger, Wolfgang},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Fluid VR: Extended Object Associations for Automatic Mode Switching in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={846-847},
  abstract={Constrained interaction and navigation methods for virtual reality reduce the complexity of the interaction. Yet, with previously presented solutions, users need to learn new interaction tools or remember different actions for changing between different interaction methods. In this paper, we propose Fluid VR, a new 3D user interface for interactive virtual environments that lets users seamlessly transition between navigation and selection. Based on the selected object's properties, Fluid VR applies specific constraints to the interaction or navigation associated with the object. This way users have a better control of their actions, without having to change tools or activate different modes of interaction.},
  keywords={Navigation;Three-dimensional displays;Fluids;Orbits;User interfaces;Switches;Virtual reality;Interaction techniques. 3D selection. 3D navigation. 3D interfaces;Virtual reality.: H.5.2. Information interfaces and presentation (e.g;HCI): Interaction styles},
  doi={10.1109/VR.2018.8446437},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446051,
  author={Hirt, Christian and Nguyen, Anh and Zank, Markus},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3DUI Contest 2018 - Team NaN}, 
  year={2018},
  volume={},
  number={},
  pages={848-849},
  abstract={For the contest held at IEEE VR 2018 3DUIs for three tasks have to be designed. The tasks include climbing a ladder, fly a drone in first-person perspective and stacking objects. The goal of our design is to find an easy yet intuitive solution to the given tasks. In that way, for the ladder task, a 3DUI is implemented closely mimicking a real ladder ascent or descent using only the controllers to interact with the environment. For the drone flying, we propose a solution which uses the head orientation to control the drone. The third task concluded with a no gravity, temporary storage space which is used to prepare the final stack that can be released with a trigger press.},
  keywords={Drones;Task analysis;Aerospace electronics;Virtual reality;Stacking;Gravity;Resists},
  doi={10.1109/VR.2018.8446051},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446176,
  author={Sun, Bo and Fritz, Aleksandr and Perry, Vincent and Havig, Paul and Su, Simon},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3DUI Contest 2018: 3D Interaction}, 
  year={2018},
  volume={},
  number={},
  pages={850-851},
  abstract={We developed novel interactions to enable user interaction with the 3D virtual environment addressing the three tasks (Ladder Climbing, First-Person View Flying, and Tower Stacking) of the 3DUI contest 2018. Using Unity3D as our development environment, we implemented and deployed our application on HTC Vive HMD system.},
  keywords={Drones;Task analysis;Three-dimensional displays;Stacking;Data visualization;Virtual reality;Electronic mail;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual Reality},
  doi={10.1109/VR.2018.8446176},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446131,
  author={Li, Yuan and Yu, Run and Zhang, Lei and Lages, Wallace S. and Bowman, Doug A.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Climb, Direct, Stack: Smart Interfaces for ELeague Contest}, 
  year={2018},
  volume={},
  number={},
  pages={852-853},
  abstract={In this paper, we present multiple 3D interaction techniques to address the three tasks in the 2018 3DUI contest: ladder climbing, drone flying and tower stacking. The proposed techniques provide smart interfaces to aid efficiency and error prevention. We discuss the principles and rationale used in the design.},
  keywords={Drones;Three-dimensional displays;Task analysis;Poles and towers;Stacking;User interfaces;Virtual reality;3D interaction;usability;error prevention: H.5.2 [Information interfaces and presentation]: User Interfaces. - Interaction techniques},
  doi={10.1109/VR.2018.8446131},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446277,
  author={Montes Rodrigues, André and Nagamura, Mario and Freire da Costa, Luis Gustavo and Zuffo, Marcelo Knorich},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Batmen Forever: Unified Virtual Hand Metaphor for Consumer VR Setups}, 
  year={2018},
  volume={},
  number={},
  pages={854-855},
  abstract={In this work, we present a hand-based natural interaction that allows performing fundamental actions such as moving or controlling objects and climbing ladders. The setup was restricted to available consumer VR technology, aiming to advance towards a practical unified framework for 3D interaction. The strategy was syncing the closest natural movement allowed by the device with primary task actions, either directly or indirectly, creating hypernatural UIs. The prototype allowed successful completion of the three challenges proposed by the 2018 3DUI Contest, as validated by a preliminary user study with participants from the target audience and also from the general public.},
  keywords={Three-dimensional displays;User interfaces;Conferences;Virtual reality;Industrial engineering;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques},
  doi={10.1109/VR.2018.8446277},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446244,
  author={Audinot, Alexandre and Goga, Emeric and Goupil, Vincent and Jorqensen, Carl-Johan and Reuzeau, Adrien and Argelaguet, Ferran},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Climb, Fly, Stack: Design of Tangible and Gesture-Based Interfaces for Natural and Efficient Interaction}, 
  year={2018},
  volume={},
  number={},
  pages={856-857},
  abstract={This paper describes three novel 3D interaction metaphors conceived to fulfill the three tasks proposed in the current edition of the IEEE VR 3DUI Contest. We propose the VladdeR, a tangible interface for Virtual laddeR climbing, the FPDrone, a First Person Drone control flying interface, and the Dice Cup, a tangible interface for virtual object stacking. All three interactions take advantage of body proprioception and previous knowledge of real life interactions without the need of complex interaction mechanics: climbing a tangible ladder through arm and leg motions, control a drone like a child flies an imaginary plane by extending your arms or stacking objects as you will grab and stack dice with a dice cup.},
  keywords={Drones;Task analysis;Stacking;Target tracking;Legged locomotion;User interfaces;H.5.2 [User Interfaces]: User Interfaces-Graphical user interfaces (GUI)},
  doi={10.1109/VR.2018.8446244},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446286,
  author={Grandi, Jerônimo G. and Debarba, Henrique G. and Franz, Juliano and Oliveira, Victor and Ticona, Abel and Souza, Gabrielle A. and Berti, Izadora and Villa, Steeven and Nedel, Luciana and Maciel, Anderson},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3DAthlon: 3D Gestural Interfaces to Support a 3-Stage Contest in VR}, 
  year={2018},
  volume={},
  number={},
  pages={858-859},
  abstract={In the context of the 3DUI Contest promoted by the IEEE VR 2018, we propose 3D interaction techniques that address three distinct tasks in a virtual environment setup: climbing a ladder, controlling a quadcopter in a first-person view flight, and building a tower by stacking a series of objects. The interaction techniques were developed so the player, our 3D-athlete, has control over the events in each task, following metaphors that facilitate the use of the interface, and having status and spatial awareness supported by clear feedback cues. Thus, the player should be able to execute the tasks with precision and agility.},
  keywords={Three-dimensional displays;Task analysis;Stacking;Avatars;Virtual environments;Poles and towers;Trajectory;H.5.2. [Information Interfaces and Presentation]: User Interfaces-Input devices and strategies},
  doi={10.1109/VR.2018.8446286},
  ISSN={},
  month={March},}@INPROCEEDINGS{8446047,
  author={Bernardin, Antonin and Cortes, Guillaume and Fribourg, Rebecca and Luong, Tiffany and Nouviale, Florian and Si-Mohammed, Hakim},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward Intuitive 3D User Interfaces for Climbing, Flying and Stacking}, 
  year={2018},
  volume={},
  number={},
  pages={860-861},
  abstract={In this paper, we propose 3D user interfaces (3DUI) that are adapted to specific Virtual Reality (VR) tasks: climbing a ladder using a puppet metaphor, piloting a drone thanks to a 3D virtual compass and stacking 3D objects with physics-based manipulation and time control. These metaphors have been designed to provide the user with an intuitive, playful and efficient way to perform each task.},
  keywords={Three-dimensional displays;Drones;Compass;Task analysis;User interfaces;Stacking;Navigation;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR.2018.8446047},
  ISSN={},
  month={March},}
