@INPROCEEDINGS{10494108,
  author={Matsumoto, Takashi and Wu, Erwin and Liao, Chen-Chieh and Koike, Hideki},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={ARpenSki: Augmenting Ski Training with Direct and Indirect Postural Visualization}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Alpine skiing is a popular winter sport, and several systems have been proposed to enhance training and improve efficiency. However, many existing systems rely on simulation-based environments, which suffer from drawbacks such as a gap between real skiing and the lack of body ownership. To address these limitations, we present ARpenSki, a novel augmented reality (AR) ski training system that employs a see-through head mounted display (HMD) to deliver augmented visual training cues that may be applied on real slopes. The proposed AR system provides a transparent view of the lower half of the field of vision, where we implemented three different AR-based direct and indirect postural visualization methods. We conducted an user study to investigate the influence of different visual cues in the AR environment. Our results indicate that a simple AR visualization of the user’s spine (Figure 1.2) yields the most favorable training performance, surpassing conventional visualizations by 7% improvement in the user’s posture. Building upon these promising findings, we further tested our system on real slopes and showed the potential of a real AR skiing application.},
  keywords={Training;Visualization;Three-dimensional displays;Buildings;Resists;User interfaces;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR58804.2024.00024},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494166,
  author={Kim, Sunbum and Lee, Geehyuk},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating an In-Hand Ball-Shaped Controller for Object Manipulation in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={10-19},
  abstract={The ball-shaped device is considered an intuitive means for 3D interaction, but its effectiveness in object manipulation has not been fully explored yet. In this study, we explored the use of an in-hand ball-shaped controller for object manipulation in virtual reality. We developed a ball-shaped controller with pressure-sensing capabilities featuring specifically designed interactions for object manipulation, including selection, translation, rotation, and scaling. Evaluations are conducted on 6-DOF docking tasks involving translation and rotation, and on 7-DOF tasks that additionally include scaling. These tasks involved manipulating objects at both close and distant ranges. The results indicated that, while the performance of the ball-shaped controller for direct object manipulation was comparable to other methods, it significantly improved completion times and reduced task load for distant object manipulation. Furthermore, the ball-shaped controller effectively reduced wrist and arm movements and is the most preferred method.},
  keywords={Wrist;Performance evaluation;Three-dimensional displays;Focusing;Virtual reality;User interfaces;6-DOF;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494107,
  author={Jin, Tao and Dasa, Mallesham and Smith, Connor and Apicharttrisorn, Kittipat and Seshan, Srinivasan and Rowe, Anthony},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={MeshReduce: Scalable and Bandwidth Efficient 3D Scene Capture}, 
  year={2024},
  volume={},
  number={},
  pages={20-30},
  abstract={3D video enables a remote viewer to observe a 3D scene from any angle or location. However, current 3D capture solutions incur high latency, consume significant bandwidth, and scale poorly with the number of depth sensors and size of scenes. These problems are largely caused by the current monolithic approach to 3D capture and the use of inefficient data representations for streaming. This paper introduces MeshReduce, a distributed scene capture, stream, and render system that advocates for the use of textured mesh data representation early in the 3D video capture and transmission process. Textured meshes are compact and can provide lower bitrates for the same quality compared to other 3D data representations. However, streaming textured meshes creates compute and memory challenges to achieve bandwidth efficiency. MeshReduce addresses these issues by using a pipeline that creates independent mesh reconstructions and incrementally merges them, rather than creating a single mesh directly from all sensor streams. While this enables a more efficient implementation, this approach requires optimal exchange of textured meshes across the network. MeshReduce also incorporates a novel approach for network rate control that divides bandwidth between texture and mesh for efficient, adaptive 3D video streaming. We demonstrate a real-time integrated embedded compute implementation of MeshReduce that can operate with commercial Azure Kinect depth cameras as well as a custom sensor front-end that uses LiDAR and 360° camera inputs to dramatically increase coverage.},
  keywords={Three-dimensional displays;Spectral efficiency;Pipelines;Bandwidth;Virtual reality;Streaming media;User interfaces;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Information systems;Information systems applications;Multimedia information systems;Multimedia content creation},
  doi={10.1109/VR58804.2024.00026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494087,
  author={Arefin, Mohammed Safayet and Montalto, Carlos and Plopski, Alexander and Swan, J. Edward},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A SharpView Font with Enhanced Out-of-Focus Text Legibility for Augmented Reality Systems}, 
  year={2024},
  volume={},
  number={},
  pages={31-41},
  abstract={In an optical see-through augmented reality system, virtual and real information can be viewed at different distances. This requires users to frequently change their eye focus from one distance to another, and only one piece of information is in sharp focus while the other is out of focus. Previous studies have found that due to out-of-focus virtual information, when integrating information between the distances, users suffer fatigue and miss important information. Therefore, this paper introduces a novel font, termed a SharpView font, which looks sharper and more legible than standard fonts when seen out of focus. Our method models out-of-focus blur with Zernike polynomials and coefficients, develops a focus correction algorithm based on constrained total variation optimization, and proposes a novel gradient-based algorithm to quantify the sharpness of textual information. We have evaluated the SharpView font through simulation and optically viewed camera-based measurement. When seen out of focus, our proposed font are significantly sharper than standard fonts, as assessed both visually and quantitatively through simulation (40%-44%), as well as the optics of an augmented reality display (24%-32%).},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Optical variables measurement;User interfaces;Optical imaging;Polynomials;Augmented Reality;Focal Distance Switching;Out-of-focus Problem;Short AR Text;Depth Based Visual Aberration;Zernike Polynomials & coefficients;Out-of-focus correction;SharpView Font;Total Variation Image Deconvolution},
  doi={10.1109/VR58804.2024.00027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494130,
  author={Ota, Hiroki and Hagimori, Daiki and Perusquía-Hernández, Monica and Isoyama, Naoya and Hirao, Yutaro and Uchiyama, Hideaki and Kiyokawa, Kiyoshi},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hap’n’Roll: A Scroll-inspired Device for Delivering Diverse Haptic Feedback with a Single Actuator}, 
  year={2024},
  volume={},
  number={},
  pages={42-50},
  abstract={Hap’n’Roll is a wearable device that leverages the concept of a scroll to present, with a single motor, tactile sensations of various sizes, shapes, and textures. Hap’n’Roll is composed of two axes, a sheet, and one motor. By changing the number of sheet wraps, the thickness within the user’s hand can be adjusted. Additionally, using holes on the sheet to secure the fingertips, it can present a wide range of sizes and shapes. Unlike typical existing handheld shape-changing devices, Hap’n’Roll is not limited to cylindrical forms. Furthermore, by moving different materials attached on the sheet to the fingertips, it can also express different textures. A user study showed that Hap’n’Roll can convey at least three sizes (small, medium, and large) and four types of shapes (a cylinder, a rectangle, a cone, and a cup), with a shape and size identification accuracy of approx. 76.1%. The identification accuracy for shape alone was approx. 98.5%. Moreover, several applications were developed to showcase the effectiveness of Hap’n’Roll’s mechanism for various haptic feedback.},
  keywords={Actuators;Three-dimensional displays;Shape;Virtual reality;User interfaces;Motors;Haptic interfaces;haptic device;shape presentation;material presentation},
  doi={10.1109/VR58804.2024.00028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494084,
  author={Zhang, Junjie and Lee, Lik-Hang and Wang, Yuyang and Jin, Shan and Fei, Dan-Lu and Hui, Pan},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Jump Cut Effects in Cinematic Virtual Reality: Editing with the 30-degree Rule and 180-degree Rule}, 
  year={2024},
  volume={},
  number={},
  pages={51-60},
  abstract={Virtual reality (VR) is an immersive medium that offers users a unique opportunity to experience a digital environment realistically. As the demand for VR content continues to grow, the importance of effective VR editing techniques becomes increasingly apparent. This paper is a pioneering work investigating the effects of jump cuts on the viewer’s sense of presence, viewing experience, and edit quality in cinematic VR. Specifically, this work focuses on using the 30-degree and 180-degree rules in VR editing to minimize the adverse effects of jump cuts. We conducted a user study with thirteen participants, who watched nine different VR edits and completed a survey for each edited video. Our results indicate that employing the 30-degree and 180-degree rules in VR editing can significantly improve the sense of presence, viewing experience, and edit quality while mitigating the negative effects of jump cuts. We provide valuable insights for VR content creators and editors to achieve more effective and immersive VR experiences.},
  keywords={Surveys;Visualization;Three-dimensional displays;Design methodology;Focusing;Virtual reality;User interfaces;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/VR58804.2024.00029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494184,
  author={Mostajeran, Fariba and Schneider, Sebastian and Bruder, Gerd and Kühn, Simone and Steinicke, Frank},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analyzing Cognitive Demands and Detection Thresholds for Redirected Walking in Immersive Forest and Urban Environments}, 
  year={2024},
  volume={},
  number={},
  pages={61-71},
  abstract={Redirected walking is a locomotion technique that allows users to naturally walk through large immersive virtual environments (IVEs) by guiding them on paths that might vary from the paths they walk in the real world. While this technique enables the exploration of larger spaces in the IVE via natural walking, previous work has shown that it induces extra cognitive load for the users. On the other hand, previous research has shown that exposure to virtual nature environments can restore users’ diminished attentional capacities and lead to enhanced cognitive performances. Therefore, the aim of this paper is to investigate if the environment in which the user is redirected has the potential to reduce its cognitive demands. For this purpose, we conducted an experiment with 28 participants, who performed a spatial working memory task (i.e., 2-back test) while walking and being redirected with different gains in two different IVEs (i.e., (i) forest and (ii) urban). The results of frequentist and Bayesian analysis are consistent and provide evidence against an effect of the type of IVE on detection thresholds as well as cognitive and locomotion performances. Therefore, redirected walking is robust to the variation of the IVEs tested in this experiment. The results partly challenge previous research findings and, therefore, require future work in this direction.},
  keywords={Legged locomotion;Three-dimensional displays;Navigation;Urban areas;Virtual environments;Forestry;User interfaces;virtual reality;redirected walking;cognitive performance;virtual nature;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494138,
  author={Schott, Ephraim and Zoeppig, Tony Jan and Lammert, Anton Benjamin and Froehlich, Bernd},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Excuse Me: Large Groups in Small Rooms}, 
  year={2024},
  volume={},
  number={},
  pages={72-82},
  abstract={Standing in a large crowd can be uncomfortable and usually results in other users obstructing the view of the virtual environment. In this paper, we present four techniques designed to improve the user’s view in crowded environments. Inspired by related work on various transparency and clipping techniques, as well as observed user behavior in crowded scenarios, our paper addresses the visibility problem by locally manipulating the appearance of other users. Three of our techniques define a region of interest using a handheld flashlight metaphor. Depending on the technique, occluding users are either pushed to the side, scaled, or made partially transparent. The fourth technique allows users to vertically adjust their position. A user study with 24 participants found that the transparency technique was advantageous for quick search tasks. However, in a realistic museum setting, no clear favorite could be determined because the techniques make different trade-offs and users weighted these aspects differently. In a final ranking, the vertical position adjustment and transparency techniques were the most popular, but the scaling technique and vertical position adjustment were found to be the most natural.},
  keywords={Social computing;Three-dimensional displays;Shape;Avatars;Virtual environments;Switches;User interfaces;Human-centered computing;Interaction paradigms;Virtual reality;Human computer interaction (HCI);Interaction techniques;Collaborative and social computing;Collaborative and social computing theory, concepts and paradigms;Computer supported cooperative work},
  doi={10.1109/VR58804.2024.00031},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494128,
  author={Mikhailova, Veronika and Gerhardt, Christoph and Kunert, Christian and Schwandt, Tobias and Weidner, Florian and Broll, Wolfgang and Döring, Nicola},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Age and Realism of Avatars in Simulated Augmented Reality: Experimental Evaluation of Anticipated User Experience}, 
  year={2024},
  volume={},
  number={},
  pages={83-93},
  abstract={Augmented reality (AR) presents vivid opportunities for interpersonal communication. With the growing diversity of social AR users, understanding their unique needs and perceptions becomes crucial. This study delves into how younger, middle-aged, and older adults perceive avatars with different aging attributes and degrees of realism, focusing on their anticipated user experience within a social AR system. We conducted an online within-subjects experiment involving $N = 2086$ age-diverse participants from Germany who assessed a set of nine gender-matched avatars for their perceived social attractiveness (research question 1 = RQ1) and the likelihood of selecting these avatars for self-representation in social AR (RQ2). The evaluated avatars represented different age groups (younger, middle-aged, and older) and levels of realism (low, medium, and high). We validated both the created avatars and our experimental setup and employed a linear mixed-effects modeling approach to analyze the data. Our findings unveiled a strong preference for younger high-realism avatars as communication partners (RQ1), which was consistent across all participant age groups. Similarly, participants favored younger high-realism avatars for self-representation in social AR (RQ2). However, older adults were more inclined to opt for avatars resembling their actual age. The study highlights the prevalence of age-related stereotypes in avatar-based communication. Similar to face-to-face social interactions, these stereotypes tend to render older avatars less socially attractive than their younger counterparts, irrespective of the avatar’s degree of realism. Our results invite considerations on how to combat these stereotypes through a more thoughtful and inclusive avatar design process that encompasses a broader spectrum of aging attributes.},
  keywords={Hair;Three-dimensional displays;Avatars;Focusing;Aging;User interfaces;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;HCI design and evaluation methods;User studies},
  doi={10.1109/VR58804.2024.00032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494075,
  author={Wang, Na and Zhou, Jin and Li, Jie and Han, Bo and Li, Fei and Chen, Songqing},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={HardenVR: Harassment Detection in Social Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={94-104},
  abstract={Social Virtual Reality (VR) is regarded as one of the most popular VR applications since it transcends geographical barriers, allowing users to interact in simulated environments for various purposes. Despite its promising prospects, there is a growing concern about the harassment issue due to the immersive nature of social VR compared to other online social environments. Existing protections against harassment in social VR are highly limited in terms of practical effectiveness. The deficiency of studies toward understanding and preventing harassment in social VR further complicates the regulation and intervention efforts of social VR platforms in such situations. To address these challenges, we, in this paper, quantitatively investigate human interaction behaviors in social VR. More specifically, we first build a customized platform based on Mozilla Hubs, a popular social VR platform, to collect data about users’ social interaction behaviors involving harassment instances. A subsequent analysis of the collected dataset SAHARA (Social interAction beHAviors in vR with hArassment) reveals that the task of online harassment detection in social VR is complicated since it depends on not only users’ actions but also their spatial and temporal relationships. To accurately discern harassment, we propose a novel framework HardenVR (HA-Rassment DEtectioN framework for social VR). As a context-aware harassment detection framework, HardenVR employs a transformer-based model to capture relative poses and learn users’ hand actions in 6-DOF (Degree-of-Freedom). Meanwhile, multiple mechanisms, including the extra attention mechanism, distance-aware clustering method, and the sliding window, have been introduced into the model to handle challenges of data imbalance, over-fitting, and continuous detection. The design of HardenVR aims to achieve the balance between accuracy, efficiency, and cost-effectiveness for the task of harassment detection. As a starting point, HardenVR successfully learns pose information as the context to identify harassment and the experiment results show its detection accuracy as high as 98.26%.},
  keywords={Solid modeling;Three-dimensional displays;Design methodology;Virtual reality;User interfaces;Transformers;Regulation;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494179,
  author={Zhang, Yu and Gao, Peizhong and Kang, Fangzhou and Li, Jiaxiang and Liu, Jiacheng and Lu, Qi and Xu, Yingqing},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={OdorAgent: Generate Odor Sequences for Movies Based on Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={105-114},
  abstract={Numerous studies have shown that integrating scents into movies enhances viewer engagement and immersion. However, creating such olfactory experiences often requires professional perfumers to match scents, limiting their widespread use. To address this, we propose OdorAgent which combines a LLM with a text-image model to automate video-odor matching. The generation framework is in four dimensions: subject matter, emotion, space, and time. We applied it to a specific movie and conducted user studies to evaluate and compare the effectiveness of different system elements. The results indicate that OdorAgent possesses significant scene adaptability and enables inexperienced individuals to design odor experiences for video and images.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Films;Olfactory;Virtual reality;User interfaces;Human-centered computing;Interaction design;Systems and tools for interaction design},
  doi={10.1109/VR58804.2024.00034},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494079,
  author={Hatira, Amal and Gelmez, Zeynep Ecem and Batmaz, Anil Ufuk and Sarac, Mine},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Hand and Object Visibility in Navigational Tasks Based on Rotational and Translational Movements in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={115-125},
  abstract={During object manipulation in Virtual Reality (VR) systems, realistically visualizing avatars and objects can hinder user performance and experience by complicating the task or distracting the user from the environment due to possible occlusions. Users might feel the urge to go through biomechanical changes, such as re-positioning the head to visualize the interaction area. In this paper, we investigate the effect of hand avatar and object visibility in navigational tasks using a VR headset. We performed two user studies where participants grasped a small, cylindrical object and navigated it through the virtual obstacles performing rotational or translational movements. We used three different visibility conditions for the hand avatar (opaque, transparent, and invisible) and two conditions for the object (opaque and transparent). Our results indicate that participants performed faster and with fewer collisions using the invisible and transparent hands compared to the opaque hand and fewer collisions with the opaque object compared to the transparent one. Furthermore, participants preferred to use the combination of the transparent hand avatar with the opaque object. The findings of this study might be useful to researchers and developers in deciding the visibility/transparency conditions of hand avatars and virtual objects for tasks that require precise navigational activities.},
  keywords={Visualization;Three-dimensional displays;Navigation;Avatars;User interfaces;Market research;User experience;Human-centered computing;Human Computer Interaction (HCI);Virtual Reality},
  doi={10.1109/VR58804.2024.00035},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494165,
  author={Zhang, Chenkai and Cao, Ruochen and Cunningham, Andrew and Walsh, James A.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Whatever could be, could be: Visualizing Future Movement Predictions}, 
  year={2024},
  volume={},
  number={},
  pages={126-136},
  abstract={As technology grants us superhuman powers, looking into what the future may hold is no longer science fiction. Artificial Intelligence and Mixed Reality technologies can allow users to see what the future may hold. In this paper, we present our work evaluating visualizations of future predictions in the Football domain. We explore the problem space, examining what a future may be. Three visualizations—2 Arrow Lines, 5 Arrow Lines, and Heatmap—are introduced as representations that show both individual predictions of movement (2 Arrow Lines and 5 Arrow Lines) and more generalized predictions (Heatmap). Whilst football is used as an example domain in this work, the visualizations and findings aim to generalize to other scenarios that contain trajectory information. Two VR studies $(2 \times \mathrm{n}=24)$ examined the visualizations in both simple/complex, timed/non-timed, and short/long-range viewing situations. Results show Heatmap as the most effective and preferred by the vast majority of participants. Findings offer insights into future visualization, serving as visual heuristics beyond the realm of sports and into the real world.},
  keywords={Heating systems;Visualization;Three-dimensional displays;Mixed reality;Virtual reality;Games;User interfaces;Human-centered computing;Visualization;Visualization techniques;Future visualizations;Visualization design and evaluation},
  doi={10.1109/VR58804.2024.00036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494114,
  author={Wen, Jiqing and Gold, Lauren and Ma, Qianyu and LiKamWa, Robert},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Coach: Volumetric Motion Annotation and Visualization for Immersive Sports Coaching}, 
  year={2024},
  volume={},
  number={},
  pages={137-146},
  abstract={Remote sports coaching connects athletes to interactive training sessions, despite busy schedules and/or lack of access to local trainers. In current formats, athletes record videos of themselves, which they send to coaches for feedback or use in self-coaching. Additionally, some coaches turn to video conferencing platforms such as FaceTime or Zoom for live coaching. A significant challenge with these methods of remote sports coaching is the absence of spatial analysis capabilities, which hinders in-depth assessment of athletic performance.This paper introduces Augmented Coach, an immersive and interactive sports coaching system. Augmented Coach utilizes volumetric data to reconstruct the 3D representations of the athletes. As a result, coaches can not only view the resulting point cloud videos of the athletes performing athletic movements, but also employ the system’s spatial annotation and visualization tools to gain insights into movement patterns and communicate with remote athletes. Unlike existing tools tailored to specific sports, Augmented Coach explores spatial kinesthetic values shared across various sports through a pilot study and designs adaptable features applicable to diverse sports coaching scenarios. To assess the system’s usability, we conducted a user study involving ten users, spanning certified coaches and experienced athletes from various sports, illustrating how they can utilize the system’s features to enhance coaching in their respective disciplines.},
  keywords={Three-dimensional displays;Annotations;Design methodology;Data visualization;Virtual reality;Trajectory;Usability;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Visualization;Visualization systems and tools;Interaction design;Interaction design process and methods;User centered design},
  doi={10.1109/VR58804.2024.00037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494086,
  author={Bozgeyikli, Lal “Lila”},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Virtual Objects: Exploring Bidirectional Embodied Tangible Interaction with a Virtual Human in World-Fixed Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={147-156},
  abstract={In everyday interactions with others, we often affect each other’s space through shared physical objects. Despite the commonality of such bidirectional interactions in real life, few have explored this form of interaction in virtual reality. This paper explores bidirectional embodied tangible interaction between a human and a virtual human through shared objects that span the real-virtual boundary in world-fixed virtual reality. The shared objects extend from the real world into the virtual world (and vice versa). We discuss the novel interaction concept and implementation details and present the results of a between-subjects user study with 40 participants where we compared the developed novel real-virtual shared object interaction with a control version where the shared objects were separated as completely physical and virtual. In summary, the results showed that presence and co-presence were increased with the real-virtual object interaction, along with affective attraction to the virtual human and enjoyment of interaction.},
  keywords={Three-dimensional displays;Virtual reality;User interfaces;World-fixed virtual reality;bidirectional interaction;tangible interaction;embodied interaction;real-virtual objects;real and virtual boundary;presence;co-presence;• Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494124,
  author={Shen, Yuxin and Xu, Manjie and Liang, Wei},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Context-Aware Head-and-Eye Motion Generation with Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={157-167},
  abstract={In humanity’s ongoing quest to craft natural and realistic avatars within virtual environments, the generation of authentic eye gaze behaviors stands paramount. Eye gaze not only serves as a primary non-verbal communication cue, but it also reflects cognitive processes, intent, and attentiveness, making it a crucial element in ensuring immersive interactions. However, automatically generating these intricate gaze behaviors presents significant challenges. Traditional methods can be both time-consuming and lack the precision to align gaze behaviors with the intricate nuances of the environment in which the avatar resides. To overcome these challenges, we introduce a novel two-stage approach to generate context-aware head-and-eye motions across diverse scenes. By harnessing the capabilities of advanced diffusion models, our approach adeptly produces contextually appropriate eye gaze points, further leading to the generation of natural head-and-eye movements. Utilizing Head-Mounted Display (HMD) eye-tracking technology, we also present a comprehensive dataset, which captures human eye gaze behaviors in tandem with associated scene features. We show that our approach consistently delivers intuitive and lifelike head-and-eye motions and demonstrates superior performance in terms of motion fluidity, alignment with contextual cues, and overall user satisfaction.},
  keywords={Solid modeling;Visualization;Head;Motion segmentation;Computational modeling;Avatars;Virtual environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00039},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494131,
  author={Taheri, Atieh and Caetano, Arthur and Sra, Misha},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Steps: The Experience of Walking for a Lifelong Wheelchair User in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={168-178},
  abstract={Many people often take walking for granted, but for individuals with mobility disabilities, this seemingly simple act can feel out of reach. This reality can foster a sense of disconnect from the world since walking is a fundamental way in which people interact with each other and the environment. Advances in virtual reality and its immersive capabilities have made it possible to enable those who have never walked in their life to “virtually” experience walking. We co-designed a VR walking experience with a person with Spinal Muscular Atrophy who has been a lifelong wheelchair user. Over 9 days, we collected data on this person’s experience through a diary study and analyzed this data to better understand the design elements required. Given that they had only ever seen others walking and had not experienced it first-hand, determining which design parameters must be considered in order to match the virtual experience to their idea of walking was challenging. Generally, we found the experience of walking to be quite positive, providing a perspective from a higher vantage point than what was available in a wheelchair. Our findings provide insights into the emotional complexities and evolving sense of agency accompanying virtual walking. These findings have implications for designing more inclusive and emotionally engaging virtual reality experiences.},
  keywords={Legged locomotion;Visualization;Three-dimensional displays;Wheelchairs;Humanoid robots;User interfaces;User experience;Walking Simulation;Accessibility;Inclusive VR Design;Mobility Impairments;Participatory Design;Diary Study;Human-centered computing;Empirical studies in accessibility;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494073,
  author={Li, Yi and Liu, Zhitao and Yuan, Li and Tang, Haolan and Fan, Youteng and Xie, Ning},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dynamic Scene Adjustment Mechanism for Manipulating User Experience in VR}, 
  year={2024},
  volume={},
  number={},
  pages={179-188},
  abstract={With the progression of VR tech, virtual interactive environments are becoming increasingly realistic and controllable. Research has substantiated the influence of VR environmental variables on user experience and engagement. Concurrently, real-time user status monitoring advancements have unlocked dynamic adjustments to VR environments through user interaction with real-time status and feedback, increasing researchers’ focus on enhancing user experience and engagement by adjusting VR environmental variables. This paper introduces an interactive paradigm for VR environments called the Dynamic Scene Adjustment (DSA) mechanism, which seeks to modify the VR environmental variables in real-time according to the user’s status and performance to enhance user engagement and experience. We selected the perspective of the impact of visual environment variables on player status, embedding the DSA mechanism into a music VR game with brain-computer interaction for specific VR tasks. Experimental findings affirm that incorporating the DSA mechanism into the VR game enhances the user’s engagement and performance, thereby strongly validating the rationality of the proposed DSA approach. This work can assist researchers think about dynamic regulation in VR environments from a new perspective and will shed light on the design of VR healing, VR education, VR games, and other fields.},
  keywords={Visualization;Three-dimensional displays;Games;Virtual reality;User experience;Real-time systems;Regulation;Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494145,
  author={Cheng, Shiwei and Sheng, Danyi and Gao, Yuefan and Dong, Zhanxun and Han, Ting},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enhancing Positive Emotions through Interactive Virtual Reality Experiences: An EEG-Based Investigation}, 
  year={2024},
  volume={},
  number={},
  pages={189-199},
  abstract={Virtual reality (VR), as an immersive interactive technology, holds the potential to promote feelings of well-being by evoking positive emotions. However, the underlying causes and extent of emotional responses elicited by VR remain underexplored. Accordingly, we aimed to investigate the types of interaction behaviors in VR that effectively enhance positive emotions, using electroencephalogram (EEG) signals as measurements of emotional expressions. In an exploratory study conducted on a virtual museum $(N =22)$, we designed four interactive tasks with varying user autonomy and interaction functions. An individual emotion model based on EEG was employed to predict the promotion of positive emotions and its extent. The results indicated that simply roaming the virtual museum had no obvious impact on positive emotions. However, incorporating specific interaction functions such as doodles, emojis, and comments increased positive emotions, with the extent of the increase closely linked to the degree of user autonomy.},
  keywords={Emotion recognition;Solid modeling;Three-dimensional displays;Virtual environments;Virtual museums;Electroencephalography;Emotional responses;Human-centered computing;Ubiquitous and mobile computing;Affective Computing;Human-computer interaction(HCI);Interaction paradigms;Virtual reality;Brain-computer interaction},
  doi={10.1109/VR58804.2024.00042},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494139,
  author={Aliza, Aliza and Zaugg, Irene and Çelik, Elif and Stuerzlinger, Wolfgang and Ortega, Francisco Raul and Batmaz, Anil Ufuk and Sarac, Mine},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Eye-Hand Coordination Training: A Systematic Comparison of 2D, VR, and AR Display Technologies and Task Instructions}, 
  year={2024},
  volume={},
  number={},
  pages={200-210},
  abstract={Previous studies on Eye-Hand Coordination Training (EHCT) focused on the comparison of user motor performance across different hardware with cross-sectional studies. In this paper, we compare user motor performance with an EHCT setup in Augmented Reality (AR), Virtual Reality (VR), and on a 2D touchscreen display in a longitudinal study. Through a ten-day user study, we thoroughly analyzed the motor performance of twenty participants with five task instructions focusing on speed, error rate, accuracy, precision, and none. As a novel evaluation criterion, we also analyzed the participants’ performance in terms of effective throughput. The results showed that each task instruction has a different effect on one or more psychomotor characteristics of the trainee, which highlights the importance of personalized training programs. Regarding different display technologies, the majority of participants could see more improvement in VR than in 2D or AR. We also identified that effective throughput is a good candidate for monitoring overall motor performance progress in EHCT systems.},
  keywords={Training;Three-dimensional displays;Error analysis;Two-dimensional displays;Focusing;User interfaces;Touch sensitive screens;Human-centered computing;Human Computer Interaction (HCI);Virtual Reality;Pointing;Touch screens},
  doi={10.1109/VR58804.2024.00043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494071,
  author={Wang, Xuanyu and Zhang, Weizhan and Fu, Hongbo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A3RT: Attention-Aware AR Teleconferencing with Life-Size 2.5D Video Avatars}, 
  year={2024},
  volume={},
  number={},
  pages={211-221},
  abstract={Augmented Reality (AR) teleconferencing aims to enable remotely separated users to meet with each other in their own physical spaces as if they are face-to-face. Among all solutions, the video-avatar-based approach has the advantage of balancing fidelity and the sense of co-presence using easy-to-setup devices, including only a camera and an AR Head-Mounted Display (HMD). However, non-verbal cues indicating “who is looking at whom” are always lost or misdelivered in multiparty teleconferencing experiences. To make users aware of such non-verbal cues, existing solutions explore screen-based visualizations, incorporate additional hardware, or alter to use a virtual avatar representation. However, they lack immersion, are less feasible for everyday usage due to complex installations, or lose the fidelity of remote users’ authentic appearances. In this paper, we decompose such attention awareness into the awareness of being looked at and the awareness of attention between other users and address them in a decoupled process. Specifically, through a user study, we first find an unobtrusive and reasonable layout “Attention Circle” to retarget a looker’s head gaze to the one being looked at. We then conduct the second user study to find an effective and intuitive “rotatable 2.5D video avatar with attention thumbnail” visualization to aid users in being aware of other users’ attention. With the design choice distilled from the studies, we implement A3RT, a proof-of-concept prototype system that empowers attention-aware 2.5D-video-avatar-based multiparty AR teleconferencing in an easy, everyday setup. Ablation and usability studies on the prototype verify the effectiveness of our proposed components and the full system.},
  keywords={Teleconferencing;Visualization;Three-dimensional displays;Head-mounted displays;Avatars;Layout;Prototypes;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI)},
  doi={10.1109/VR58804.2024.00044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494089,
  author={Wu, Jian and Wang, Ziteng and Wang, Lili and Duan, Yuhan and Li, Jiaheng},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={FanPad: A Fan Layout Touchpad Keyboard for Text Entry in VR}, 
  year={2024},
  volume={},
  number={},
  pages={222-232},
  abstract={Text entry poses a significant challenge in the realm of virtual reality (VR). This paper introduces FanPad, a novel solution designed to facilitate dual-hand text input within head-mounted displays (HMDs). FanPad accomplishes this by ingeniously mapping and curving the 26 typing keys (T26) QWERTY keyboard onto the touchpads of both controllers. The curved key layout of FanPad is derived from the natural movement of the thumb when interacting with the touchpad, resembling an arc with a thumb-length fixed radius.To optimize the experience, we introduce a customization process for the FanPad curve to better cope with individual hand shapes and thumb movements. We also provide a version with more overlap area named FanPad-Ov for different users with different typing habits.Our first user study examined the effects of curving and different overlap areas by comparing four potential layouts. The results clearly favor the FanPad and FanPad-Ov layout compared to the nocurving version, SKPad(-Ov). Subsequently, the second user study was conducted to assess long-term performance and improvement on customized FanPads. Notably, novices achieved a typing speed of 19.73 words per minute (WPM), demonstrating a remarkable increase of 58.47% after a 60-phrase training in six days. The highest typing speed reached an impressive 24.19 WPM.},
  keywords={Training;Fans;Three-dimensional displays;Head-mounted displays;Shape;Layout;Thumb;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Text input;HCI design and evaluation methods;User studies},
  doi={10.1109/VR58804.2024.00045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494099,
  author={Sermarini, John and Michlowitz, Robert A. and LaViola, Joseph J. and Walters, Lori C. and Azevedo, Roger and Kider, Joseph T.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Augmented Reality’s Role in Enhancing Spatial Perception for Building Facade Retrofit Design for Non-experts}, 
  year={2024},
  volume={},
  number={},
  pages={233-243},
  abstract={Augmented Reality (AR) tools have demonstrated considerable promise to enhance creative architectural design and support the retrofitting problem-solving processes through on-site daylighting visualization. AR’s capacity to integrate embodied motion enhances the non-expert’s understanding of the spatial characteristics and design ramifications within the built environment for complex facade design. Motion provides insights and increases the accessibility of retrofitting, encouraging more energy-efficient rework as opposed to complete building reconstruction. This study investigates the decision-making outcomes and cognitive-physical load implications of integrating a Building Information Modeling-driven AR system into the retrofitting design process and how movement is best leveraged to understand daylighting impacts. We conducted a study with 128 non-expert participants, who were asked to choose a window facade retrofit to improve an interior space. We analyze the effects of head movement, head rotations, and eye movements to understand how embodied motion improves overall objective performance across several daylighting and energy design metrics. We found no significant difference in the overall decision-making outcome between those who used an AR tool or a conventional desktop approach and that greater eye movement in AR was related to non-experts better balancing the complicated impacts facades have on daylight, aesthetics, and energy. This study indicates future expansion of AR retrofitting tools should encourage more eye movement.},
  keywords={Visualization;Three-dimensional displays;Daylighting;Decision making;Buildings;User interfaces;Hardware;Augmented Reality;Building Information Modeling;Daylighting;Human Computer Interaction;Applied computing;Arts and humanities;Architecture (buildings);Computer-aided design;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality Human-centered computing;Visualization;Visualization application domains;Information visualization},
  doi={10.1109/VR58804.2024.00046},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494188,
  author={Song, Jianbin and Shi, Rongkai and Li, Yue and Gao, BoYu and Liang, Hai-Ning},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Controller-based Techniques for Precise and Rapid Text Selection in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={244-253},
  abstract={Text selection is a common task in interactive systems. Often, it can be difficult because the letters and words are too small and clustered together to allow precise selection. Compared to traditional 2D interfaces, text selection is more challenging in virtual reality (VR) head-mounted displays (HMDs) because users interact with the immersive 3D space via mid-air interaction, which has higher degrees of freedom but becomes more imprecise and involves a higher workload due to the lack of support from a fixed structure like a desk. There has been limited exploration of techniques that support precise and rapid text selection at the character, word, sentence, or paragraph levels in VR HMDs. To fill this gap, we propose three controller-based text selection methods: Joystick Movement, Depth Movement, and Wrist Orientation. They are evaluated against a baseline selection method via a user study with 24 participants. Results show that the three proposed techniques significantly improved the performance and user experience over the baseline, especially for the selection beyond the character level.},
  keywords={Wrist;Three-dimensional displays;Head-mounted displays;Text recognition;Virtual reality;User interfaces;User experience;Text Selection;Virtual Reality;Head-mounted Display;User Study;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;interaction techniques;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR58804.2024.00047},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494156,
  author={Ratnarajah, Anton and Manocha, Dinesh},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Listen2Scene: Interactive material-aware binaural sound propagation for reconstructed 3D scenes}, 
  year={2024},
  volume={},
  number={},
  pages={254-264},
  abstract={We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for indoor 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network can handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function for the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acoustic effect in 0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU. We have evaluated the accuracy of our approach with binaural acoustic effects generated using an interactive geometric sound propagation algorithm and captured real acoustic effects /real-world recordings. We also performed a perceptual evaluation and observed that the audio rendered by our approach is more plausible than audio rendered using prior learning-based and geometric-based sound propagation algorithms. We quantitatively evaluated the accuracy of our approach using statistical acoustic parameters, and energy decay curves. The demo videos, code and dataset are available online1.},
  keywords={Solid modeling;Three-dimensional displays;Spatial audio;User interfaces;Rendering (computer graphics);Acoustics;Vectors;Computing methodologies;Machine learning;Machine learning approaches;Learning latent representations},
  doi={10.1109/VR58804.2024.00048},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494192,
  author={Quere, Clément and Menin, Aline and Julien, Raphaël and Wu, Hui-Yin and Winckler, Marco},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={HandyNotes: using the hands to create semantic representations of contextually aware real-world objects}, 
  year={2024},
  volume={},
  number={},
  pages={265-275},
  abstract={This paper uses Mixed Reality (MR) technologies to provide a seamless integration of digital information in physical environments through human-made annotations. Creating digital annotations of physical objects evokes many challenges for performing (simple) tasks such as adding digital notes and connecting them to real-world objects. For that, we have developed an MR system using the Microsoft HoloLens2 to create semantic representations of contextually-aware real-world objects while interacting with holographic virtual objects. User interaction is enhanced with use of fingers as placeholders for menu items. We demonstrate our approach through two real-world scenarios. We also discuss the challenges for using MR technologies.},
  keywords={Three-dimensional displays;Annotations;Semantics;Mixed reality;Virtual reality;User interfaces;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction techniques;Gestural input;Graphical user interfaces},
  doi={10.1109/VR58804.2024.00049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494182,
  author={Liu, Jingjing and Lou, Jianwen and Zheng, Youyi and Zhou, Kun},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Indoor Lighting Generation Driven by Human Activity Learned from Virtual Experience}, 
  year={2024},
  volume={},
  number={},
  pages={276-285},
  abstract={A good indoor lighting solution should fit with people’s habitual activity and have a low energy cost. However, it’s challenging to capture and model human activity in reality due to its high complexity, let alone incorporating it into lighting planning. As a result, indoor lighting designing still relies on professional’s hands, which is laborious and inefficient. To solve this problem, we propose a novel framework for automatic indoor lighting generation driven by human activity learned from virtual experience. We first harnesses Virtual Reality to simulate and model the user’s daily activities within an indoor scene, and then devises a robust objective function which encompasses multiple activity-driven cost terms for lighting layout optimization. With the objective function and the collected user behavioral data, such as trajectory and head pose, an optimization algorithm is applied to search for the optimal solution. Experiments under different indoor scenes demonstrate that the proposed method can generate lighting solutions that meet personalized behavioral needs in an energy-economic way, which are competitive against those designed by professionals.},
  keywords={Solid modeling;Costs;Three-dimensional displays;Lighting;Virtual reality;User interfaces;Linear programming;Human-centered Design;Virtual Reality;Interior Design;Remodeling},
  doi={10.1109/VR58804.2024.00050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494109,
  author={Chen, Mengyu and Peljhan, Marko and Sra, Misha},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={ConnectVR: A Trigger-Action Interface for Creating Agent-based Interactive VR Stories}, 
  year={2024},
  volume={},
  number={},
  pages={286-297},
  abstract={The demand for interactive narratives is growing with increasing popularity of VR and video gaming. This presents an opportunity to create interactive storytelling experiences that allow players to engage with a narrative from a first person perspective, both, immersively in VR and in 3D on a computer. However, for artists and storytellers without programming experience, authoring such experiences is a particularly complex task as it involves coding a series of story events (character animation, movements, time control, dialogues, etc.) to be connected and triggered by a variety of player behaviors. In this work, we present ConnectVR, a trigger-action interface to enable non-technical creators design agent-based narrative experiences. Our no-code authoring method specifically focuses on the design of narratives driven by a series of cause-effect relationships triggered by the player’s actions. We asked 15 participants to use ConnectVR in a preliminary workshop study as well as two artists to extensively use our system to create VR narrative projects in a three-week in-depth study. Our findings shed light on the creative opportunities facilitated by ConnectVR’s trigger-action approach, particularly its capability to establish chained behavioral effects between virtual characters and objects. The results of both studies underscore the positive feedback from participants regarding our system’s capacity to not only support creativity but also to simplify the creation of interactive narrative experiences. Results indicate compatibility with non-technical narrative creator’s workflows, showcasing its potential to enhance the overall creative process in the realm of VR narrative design.},
  keywords={Visualization;Three-dimensional displays;Conferences;Multimedia systems;Virtual reality;User interfaces;Programming;Virtual Reality;Interactive Storytelling;3D Authoring Tools;Immersive Experience Design;Virtual Agents;H.5.1 [INFORMATION INTERFACES AND PRESENTATION (e.g., HCI)]: Multimedia Information Systems—Artificial;augmented;virtual realities},
  doi={10.1109/VR58804.2024.00051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494174,
  author={Bimberg, Pauline and Feldmann, Michael and Weyers, Benjamin and Zielasko, Daniel},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Influence of Environmental Context on the Creation of Cartoon-like Avatars in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={298-308},
  abstract={The user study presented in this paper explores the effects that being immersed in different virtual scenes has on a user’s avatar-design behavior. For this purpose, we have developed a character creation tool that lets users configure their appearance in Virtual Reality. This tool has then been employed in a user study involving 33 participants, who were asked to configure a virtual avatar in a beach and a hospital environment. Our results show that the environment that participants were immersed in influenced their design behavior, with the beach environment leading to a more extensive use of accessories than the hospital scene. Against our expectations, most participants stated to have had little intention to represent either their real or ideal selves, instead opting to explore different characters that they found funny, likable, or interesting. In addition, we found indications for a possible connection between the participants’ feelings of presence and how they incorporate features of themselves into their avatar designs.},
  keywords={Three-dimensional displays;Hospitals;Avatars;Buildings;Virtual environments;User interfaces;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;Empirical studies in HCI},
  doi={10.1109/VR58804.2024.00052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494151,
  author={Lee, Geonsun and Healey, Jennifer and Manocha, Dinesh},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={DocuBits: VR Document Decomposition for Procedural Task Completion}, 
  year={2024},
  volume={},
  number={},
  pages={309-319},
  abstract={Reading monolithic instructional documents in VR is often challenging, especially when tasks are collaborative. Here we present DocuBits, a novel method for transforming monolithic documents into small, interactive instructional elements. Our approach allows users to:(i) create instructional elements (ii) position them within VR and (iii) use them to monitor and share progress in a multi-user VR learning environment. We describe our design methodology as well as two user studies evaluating how both individual users and pairs of users interact with DocuBits compared to monolithic documents while performing a chemistry lab task. Our analysis shows that, for both studies, DocuBits had substantially higher usability, while decreasing perceived workload $(p \lt 0.001)$. Our collaborative study showed that participants perceived higher social presence, collaborator awareness as well as immersion and presence $(p \lt 0.001)$. We discuss our insights for using text-based instructions to support enhanced collaboration in VR environments.},
  keywords={Training;Performance evaluation;Three-dimensional displays;Design methodology;Collaboration;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques;Collaborative interaction},
  doi={10.1109/VR58804.2024.00053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494076,
  author={Zhang, He and Li, Xinyang and Sun, Yuanxi and Fu, Xinyi and Qiu, Christine and Carroll, John M.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games}, 
  year={2024},
  volume={},
  number={},
  pages={320-330},
  abstract={Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.},
  keywords={Human computer interaction;Solid modeling;Three-dimensional displays;Metaverse;Games;Virtual reality;Predictive models;Human-centered computing;Human computer interaction (HCI);Virtual reality;HCI design and evaluation methods;Computing methodologies;Artificial intelligence;Activity recognition and understanding;Database},
  doi={10.1109/VR58804.2024.00054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494101,
  author={Lai, Yunfei and Sun, Minghui and Li, Zhuofeng},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={GazePuffer: Hands-Free Input Method Leveraging Puff Cheeks for VR}, 
  year={2024},
  volume={},
  number={},
  pages={331-341},
  abstract={Gaze input is a popular hands-free input method that allows for intuitive and rapid pointing but lacks a confirmation mechanism. This study introduces GazePuffer, an interaction method that combines puffing cheeks with gaze. We explored the design space of mouth gestures, proposed a set of candidate gestures, filtered them through user subjective evaluation, and selected five basic gestures and four variations. We determined the corresponding virtual reality (VR) actions for these gestures through brainstorming. We achieved an accuracy of 93.8% in recognizing the five basic mouth gestures using the built-in sensors of the head-mounted display devices. We compared GazePuffer with two baseline methods in target selection tasks, demonstrating that GazePuffer is on par with Gaze&Pinch in throughput and speed, slightly outperforming Gaze&Dwell. Finally, we showcased the applicability of GazePuffer in real VR interaction tasks, with users generally finding it usable and effortless.},
  keywords={Heuristic algorithms;Mouth;Virtual reality;User interfaces;Fatigue;Throughput;Space exploration;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input;Interaction design;Interaction design process and methods;User centered design;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR58804.2024.00055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494134,
  author={Mazursky, Alex and Brooks, Jas and Desta, Beza and Lopes, Pedro},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={ThermalGrasp: Enabling Thermal Feedback even while Grasping and Walking}, 
  year={2024},
  volume={},
  number={},
  pages={342-353},
  abstract={Most thermal interfaces attach Peltier elements and their required cooling systems (heatsinks and fans) directly to the palm or sole, preventing users from grasping or walking. To solve this problem, we present ThermalGrasp, an engineering approach for wearable thermal interfaces that enables users to grab and walk on real objects with minimal obstruction. Our approach moves the therma l device and cooling unit to areas not used in grasping or walking (e.g., dorsal hand/foot). We then use thin, compliant materials to conduct heat to/from the palm or sole. Unlike traditional Peltiers with heatsinks, our thin materials enable grasping and walking on real objects while enjoying thermal feedback. Using our approach, a user can, for example, grasp a passive prop (e.g., a stick that acts as a torch in VR), yet feel its thermal state (e.g., hot due to its flame). In our user studies, ThermalGrasp struck a useful balance between thermal and haptic realism. We believe that ThermalGrasp is a first step towards not forcing users to choose between either feeling thermal feedback or being able to engage with grasping/walking in interactive experiences.},
  keywords={Legged locomotion;Heating systems;Three-dimensional displays;Thermal engineering;Grasping;Virtual reality;User interfaces;thermal feedback;haptic feedback;virtual reality;wearables},
  doi={10.1109/VR58804.2024.00056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494190,
  author={Povinelli, Kassie C. and Zhao, Yuhang},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Springboard, Roadblock or “Crutch”?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={354-364},
  abstract={Social virtual reality (VR) serves as a vital platform for transgender individuals to explore their identities through avatars and foster personal connections within online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. Prior research acknowledges this issue but overlooks the potential solution of voice changers. We interviewed 13 transgender and gender-nonconforming users of social VR platforms, focusing on their experiences with and without voice changers. We found that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through both hearing their modified voice and the reactions of others to their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified the technical barriers to current voice changer technology and potential improvements to alleviate the problems that transgender and gender-nonconforming users face.},
  keywords={Training;Three-dimensional displays;Avatars;Focusing;Auditory system;User interfaces;Transgender issues;Social virtual reality;interview study;voice communication;transgender;gender-nonconforming},
  doi={10.1109/VR58804.2024.00057},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494078,
  author={Xu, Sen-Zhe and Huang, Kui and Fan, Cheng-Wei and Zhang, Song-Hai},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={SafeRDW: Keep VR Users Safe When Jumping Using Redirected Walking}, 
  year={2024},
  volume={},
  number={},
  pages={365-375},
  abstract={Redirected Walking (RDW) is an important intermediary layer in virtual reality (VR) interaction systems. It addresses the issues of spatial restrictions in VR exploration by imperceptibly remapping the virtual environment’s movement to the physical environment, enhancing the user’s immersive experience. In VR, jumping is also a noteworthy motion besides walking. However, existing redirected walking (RDW) algorithms typically focus on reducing collisions between users and obstacles during walking but overlook the safety when users perform significant actions such as jumping. This oversight can pose serious risks to users during VR exploration, especially when there are physical obstacles or boundaries near the virtual locations that require user jumping. We propose SafeRDW, the first RDW algorithm that takes the user’s jumping safety into consideration. The proposed method considers both walking and jumping actions in the virtual environment, reducing physical resets and redirecting users to safer locations when a jump is required in the virtual space, ensuring user safety. Simulation experiments and user study results both show that our method not only reduces the number of resets, but also significantly ensures user safety when they reach the jumping points in the virtual scene.},
  keywords={Legged locomotion;Three-dimensional displays;Virtual environments;Immersive experience;User interfaces;Safety;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR58804.2024.00058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494191,
  author={Tan, Haoyu and Nie, Tongyu and Rosenberg, Evan Suma},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Invisible Mesh: Effects of X-Ray Vision Metaphors on Depth Perception in Optical-See-Through Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={376-386},
  abstract={This paper investigates the influence of X-ray vision metaphors on distance estimation in optical-see-through augmented reality (AR) in action space. A within-subjects study $(\mathrm{N}=30)$ was conducted to evaluate depth judgments across five conditions, including a novel “invisible mesh” technique. Participants performed a series of blind walking tasks that required estimating the depth of AR objects displayed at multiple distance ranges in front or behind a physical occluding surface. Although quantitative results regarding the impact of different X-ray vision metaphors on distance perception were inconclusive, participant feedback revealed a diversity of strategies and preferences. Overall, the findings suggest that no single metaphor was considered universally superior, and multiple X-ray vision metaphors may be suitable for different users and situations. This research contributes to understanding of X-ray vision techniques and informs the design considerations for AR systems aiming to enhance depth perception and user experience.},
  keywords={Legged locomotion;Visualization;Three-dimensional displays;Optical feedback;Estimation;User interfaces;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces},
  doi={10.1109/VR58804.2024.00059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494180,
  author={Liu, Chang and Lin, Qunfen and Zeng, Zijiao and Pan, Ye},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={EmoFace: Audio-driven Emotional 3D Face Animation}, 
  year={2024},
  volume={},
  number={},
  pages={387-397},
  abstract={Audio-driven emotional 3D face animation aims to generate emotionally expressive talking heads with synchronized lip movements. However, previous research has often overlooked the influence of diverse emotions on facial expressions or proved unsuitable for driving MetaHuman models. In response to this deficiency, we introduce EmoFace, a novel audio-driven methodology for creating facial animations with vivid emotional dynamics. Our approach can generate facial expressions with multiple emotions, and has the ability to generate random yet natural blinks and eye movements, while maintaining accurate lip synchronization. We propose independent speech encoders and emotion encoders to learn the relationship between audio, emotion and corresponding facial controller rigs, and finally map into the sequence of controller values. Additionally, we introduce two post-processing techniques dedicated to enhancing the authenticity of the animation, particularly in blinks and eye movements. Furthermore, recognizing the scarcity of emotional audio-visual data suitable for MetaHuman model manipulation, we contribute an emotional audio-visual dataset and derive control parameters for each frames. Our proposed methodology can be applied in producing dialogues animations of non-playable characters (NPCs) in video games, and driving avatars in virtual reality environments. Our further quantitative and qualitative experiments, as well as an user study comparing with existing researches show that our approach demonstrates superior results in driving 3D facial models. The code and sample data are available at https://github.com/SJTU-Lucy/EmoFace},
  keywords={Solid modeling;Video games;Three-dimensional displays;Lips;Face recognition;Feature extraction;Encoding;Human-centered computing;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR58804.2024.00060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494132,
  author={Woodworth, Jason W. and Borst, Christoph W.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Validation of a Library of Active Affective Tasks for Emotion Elicitation in VR}, 
  year={2024},
  volume={},
  number={},
  pages={398-407},
  abstract={Emotion recognition models require datasets of physiological responses to stimuli designed to elicit targeted emotions, preferably stimuli similar to the experience during which the models will be used. Many libraries of such stimuli have been created to ease this data collection process, most of which involve passive media such as images or videos. Virtual Reality, however, offers an opportunity to investigate uniquely active emotion elicitation stimuli that directly center the user in the experience with an increased feeling of presence and potential to elicit stronger emotions. We leverage this to introduce a set of four active affective tasks in VR designed to quickly elicit targeted emotions without need for narrative understanding common to passive stimuli. We compare our tasks with selections from an existing affective library of passive 360° videos and validate our approach by comparing self-reported emotional responses to the stimuli. Results indicate that these types of active task stimuli can reliably elicit strong emotions comparable to other passive media and provide the basis for building a larger library of training- and education-relevant tasks.},
  keywords={Solid modeling;Emotion recognition;Virtual reality;Media;Libraries;Data models;Physiology;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Applied computing;Law, social and behavioral sciences;Psychology},
  doi={10.1109/VR58804.2024.00061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494146,
  author={Babu, Sabarish V. and Hsieh, Wei-An and Chuang, Jung-Hong},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Benefits of Near-field Manipulation and Viewing to Distant Object Manipulation in VR}, 
  year={2024},
  volume={},
  number={},
  pages={408-417},
  abstract={In this contribution, we propose to enhance two distant object manipulation techniques, BMSR (Bimanual Near-Field Metaphor with Scaled Replica) and the classic Scaled HOMER (Scaled Hand-Centered Object Manipulation Extending Ray Casting), via nearfield scaled replica manipulation and viewing. In the proposed Direct BMSR, context replicas are displayed so that the target replica can be manipulated relative to its context, allowing the user to directly manipulate the target replica in their arm’s reach space. Some additional features were implemented to make Direct BMSR an effective interface for manipulating objects from a distance. We proposed Scaled HOMER+NFSRV, which augments Scaled HOMER with a near-field scaled replica view (NFSRV) of the target object and its context, enabling the user to observe how the target replica is manipulated in relation to its context in their arm’s reach space while manipulating it from a distance. We conducted a between-subjects empirical evaluation of BMSR, Direct BMSR, Scaled HOMER, and Scaled HOMER+NFSRV. Our findings revealed that Direct BMSR and Scaled HOMER+NFSRV significantly outperformed BMSR and Scaled HOMER, respectively, in terms of accuracy. This finding highlights the advantages of adding near-field scaled replica viewing and manipulation with respect to distant object manipulation.},
  keywords={Casting;Three-dimensional displays;Design methodology;Virtual reality;User interfaces;Near-field scaled replica manipulation;Near-field scaled replica viewing;Distant object manipulation;Virtual object manipulation;Human-centered computing [Interaction design]: Interaction design process and methods;User interface design},
  doi={10.1109/VR58804.2024.00062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494158,
  author={Canales, Ryan and Roble, Doug and Neff, Michael},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Avatar Stylization on Trust}, 
  year={2024},
  volume={},
  number={},
  pages={418-428},
  abstract={Virtual Reality (VR) affords great freedom in how one represents themselves in virtual interactions through the selection of different avatars. However, it remains unclear which avatar should be chosen for a given social scenario. Social interaction often relies on the establishment of trust. Are people more likely to trust you if you select a highly realistic avatar or is there flexibility in representation? This work presents a study exploring this question using a high stakes medical scenario. Participants meet three different doctors with three different style levels: realistic, caricatured, and an in-between “Mid” level. Trust ratings are largely consistent across the style levels, but participants were more likely to select doctors with the “Mid” level of stylization for a second opinion. There is a clear preference against one of the three doctor identities, with evidence that this may be related to movement features.},
  keywords={Surveys;Visualization;Three-dimensional displays;Electric breakdown;Avatars;Medical services;Oral communication;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR58804.2024.00063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494161,
  author={Bai, Zechen and Chen, Peng and Peng, Xiaolan and Liu, Lu and Yao, Naiming and Chen, Hui},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters}, 
  year={2024},
  volume={},
  number={},
  pages={429-438},
  abstract={Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications. https://github.com/showlab/BYOC},
  keywords={Deep learning;Solid modeling;Three-dimensional displays;Codes;Virtual reality;User interfaces;Human in the loop;Virtual Human;Facial Animation;Blendshape;Human-in-the-loop},
  doi={10.1109/VR58804.2024.00064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494160,
  author={Hu, Jinghui and Dudley, John J. and Kristensson, Per Ola},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={SkiMR: Dwell-free Eye Typing in Mixed Reality}, 
  year={2024},
  volume={},
  number={},
  pages={439-449},
  abstract={We present SkiMR: a dwell-free eye typing system that enables fast and accurate hands-free text entry on mixed reality headsets. SkiMR uses a statistical decoder to infer users’ intended text based on users’ eye movements on a virtual keyboard. It does not rely on dwell timeouts for key selections, which enables it to be faster than traditional eye typing. We study this dwell-free eye typing system, deployed on a HoloLens 2, in two studies. In the first study (n = 12) we show that dwell-free eye typing results in a significantly faster text entry rate compared to traditional dwell-based eye typing with word prediction support, and a hybrid dwell-free method that uses dwell timeouts to delimit word entry. Based on the insights from the first study we evaluate the feasibility of a refined system in a more realistic composition task and use an interaction mechanism that provides real-time predictions during dwell-free eye typing. The second study (n = 16) demonstrates that this final system allow users to compose original text at 12 words per minute with a corrected character error rate of 1.1%. Overall, this work demonstrates the high potential for fast and accurate hands-free text entry using dwell-free eye typing for mixed reality headsets.},
  keywords={Headphones;Three-dimensional displays;Error analysis;Mixed reality;Keyboards;Virtual reality;User interfaces;Text input;Mixed / augmented reality;Keyboards},
  doi={10.1109/VR58804.2024.00065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494163,
  author={Aoyama, Shutaro and Liu, Jen-Shuo and Wang, Portia and Jain, Shreeya and Wang, Xuezhen and Xu, Jingxi and Song, Shuran and Tversky, Barbara and Feiner, Steven},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Asynchronously Assigning, Monitoring, and Managing Assembly Goals in Virtual Reality for High-Level Robot Teleoperation}, 
  year={2024},
  volume={},
  number={},
  pages={450-460},
  abstract={We present a prototype virtual reality user interface for robot teleoperation that supports high-level specification of 3D object positions and orientations in remote assembly tasks. Users interact with virtual replicas of task objects. They asynchronously assign multiple goals in the form of 6DoF destination poses without needing to be familiar with specific robots and their capabilities, and manage and monitor the execution of these goals. The user interface employs two different spatiotemporal visualizations for assigned goals: one represents all goals within the user’s workspace (Aggregated View), while the other depicts each goal within a separate world in miniature (Timeline View). We conducted a user study of the interface without the robot system to compare how these visualizations affect user efficiency and task load. The results show that while the Aggregated View helped the participants finish the task faster, the participants preferred the Timeline View.},
  keywords={Visualization;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;Spatiotemporal phenomena;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction design;Interaction design process and methods;User interface design;Computer systems organization;Embedded and cyber-physical systems Robotics;External interfaces for robotics},
  doi={10.1109/VR58804.2024.00066},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494095,
  author={Lee, Sung-Ha and Joo, Ho-Taek and Chung, Insik and Park, Donghyeok and Choi, Yunho and Kim, Kyung-Joong},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Novel Approach for Virtual Locomotion Gesture Classification: Self-Teaching Vision Transformer for a Carpet-Type Tactile Sensor}, 
  year={2024},
  volume={},
  number={},
  pages={461-471},
  abstract={Locomotion gesture classification in virtual reality (VR) is the process of analyzing and identifying specific user movements in the real world to navigate virtual environments. However, existing methods often necessitate the use of wearable sensors, which present limitations. To address this, we utilize a high-resolution carpet-type tactile sensor as a foot action recognition interface, which was previously unexplored in the context of locomotion gesture classification. This interface can capture the user’s foot pressure data in detail to distinguish similar actions. In this paper, to efficiently process captured user’s foot tactile data and classify nuanced actions, we utilize a Vision Transformer (ViT) architecture and propose a novel Self-Teaching Vision Transformer (STViT) model integrating elements of the Shifted window Vision Transformer (SwinViT) and Data-efficient image Transformer (DeiT). However, unlike DeiT, our model uses itself from $N -$steps prior as the teacher model, which is continuously updated. Therefore, improving the ability to classify actions by referencing its own knowledge from previous training stages progressively refines its understanding of similar action gestures. Also, we used the base architecture of SwinViT to utilize patch merging, which improves the ability to differentiate between variations in similar actions by capturing information at different scales. We evaluated seven vision-based methods, demonstrating promising results. Not only did our model outperform ResNet by 19.6%, but it also outperformed each Deit and SwinViT by 3.3% and 2.9%, achieving 92.7% accuracy. To validate our model’s real-world applicability, we conducted user preference tests and in-game performance evaluations with 18 participants. As a result, the participants preferred our model to SwinViT and DeiT, backing up the computational results. The video demonstrating the VR testing for STViT can be found in https://youtu.be/NJslvanRn18},
  keywords={Training;Solid modeling;Three-dimensional displays;Computational modeling;Tactile sensors;Virtual environments;Computer architecture;Human-centered computing;Human;computer interaction (HCI);Interaction techniques;Gestural input;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494178,
  author={Yun, Haoran and Ponton, Jose Luis and Beacco, Alejandro and Andujar, Carlos and Pelechano, Nuria},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Role of Expected Collision Feedback in Crowded Virtual Environments}, 
  year={2024},
  volume={},
  number={},
  pages={472-481},
  abstract={An increasing number of virtual reality applications require environments that emulate real-world conditions. These environments often involve dynamic virtual humans showing realistic behaviors. Understanding user perception and navigation among these virtual agents is key for designing realistic and effective environments featuring groups of virtual humans. While collision risk significantly influences human locomotion in the real world, this risk is largely absent in virtual settings. This paper studies the impact of the expected collision feedback on user perception and interaction with virtual crowds. We examine the effectiveness of commonly used collision feedback techniques (auditory cues and tactile vibrations) as well as inducing participants to expect that a physical bump with a real person might occur, as if some virtual humans actually correspond to real persons embodied into them and sharing the same physical space. Our results indicate that the expected collision feedback significantly influences both participant behavior—encompassing global navigation and local movements—and subjective perceptions of presence and copresence. Specifically, the introduction of a perceived risk of actual collision was found to significantly impact global navigation strategies and increase the sense of presence. Auditory cues had a similar effect on global navigation and additionally enhanced the sense of copresence. In contrast, vibrotactile feedback was primarily effective in influencing local movements.},
  keywords={Vibrations;Three-dimensional displays;Navigation;Design methodology;Virtual environments;User interfaces;Virtual Reality;Crowd Simulation;User Studies;Collision Avoidance;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Perception;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/VR58804.2024.00068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494194,
  author={Tseng, Wen-Jie and Kontrazis, Petros Dimitrios and Lecolinet, Eric and Huron, Samuel and Gugenheimer, Jan},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Understanding Interaction and Breakouts of Safety Boundaries in Virtual Reality Through Mixed-Method Studies}, 
  year={2024},
  volume={},
  number={},
  pages={482-492},
  abstract={Virtual Reality (VR) technologies become ubiquitous, allowing people to employ immersive experiences in their homes. Since VR participants are visually disconnected from their real-world environment, commercial products propose safety boundaries to prevent colliding with their surroundings. However, there is a lack of empirical knowledge on how people perceive and interact with safety boundaries in everyday VR usage. This paper investigates this research gap with two mixed-method empirical studies. Study 1 reports an online survey (n=48) collecting data about attitudes towards safety boundaries, behavior while interacting with them, and reasons for breakout. Our analysis with open coding reveals that some VR participants ignored safety boundaries intentionally, even breaking out of them and continuing their actions. Study 2 investigates how and why VR participants intentionally break out when interacting close to safety boundaries and obstacles by replicating breakouts in a lab study (n=12). Our interview and breakout data discover three strategies, revealing VR participants sometimes break out of boundaries based on their real-world spatial information. Finally, we discuss improving future VR safety mechanisms by supporting participants’ real-world spatial mental models using landmarks.},
  keywords={Surveys;Three-dimensional displays;Virtual reality;User interfaces;Aerospace electronics;Encoding;Safety;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Empirical studies in HCI},
  doi={10.1109/VR58804.2024.00069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494141,
  author={Dong, Zhenxing and Jia, Jidong and Li, Yan and Ling, Yuye},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic Displays}, 
  year={2024},
  volume={},
  number={},
  pages={493-501},
  abstract={Recently, deep learning-based computer-generated holography (CGH) has demonstrated tremendous potential in three-dimensional (3D) displays and yielded impressive display quality. However, most existing deep learning-based CGH techniques can only generate holograms of 1080p resolution, which is far from the ultra-high resolution (16K+) required for practical virtual reality (VR) and augmented reality (AR) applications to support a wide field of view and large eye box. One of the major obstacles in current CGH frameworks lies in the limited memory available on consumer-grade GPUs which could not facilitate the generation of higher-definition holograms. To overcome the aforementioned challenge, we proposed a divide-conquer-and-merge strategy to address the memory and computational capacity scarcity in ultra-high-definition CGH generation. This algorithm empowers existing CGH frameworks to synthesize higher-definition holograms at a faster speed while maintaining high-fidelity image display quality. Both simulations and experiments were conducted to demonstrate the capabilities of the proposed framework. By integrating our strategy into HoloNet and CCNNs, we achieved significant reductions in GPU memory usage during the training period by 64.3% and 12.9%, respectively. Furthermore, we observed substantial speed improvements in hologram generation, with an acceleration of up to 3× and 2×, respectively. Particularly, we successfully trained and inferred 8K definition holograms on an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore, we conducted full-color optical experiments to verify the effectiveness of our method. We believe our strategy can provide a novel approach for memory- and time-efficient holographic displays.},
  keywords={Training;Image quality;Solid modeling;Three-dimensional displays;Computational modeling;Neural networks;Graphics processing units;Holography;User interfaces;Optical imaging;VR/AR;Holographic Displays;Computer-generated Hologram},
  doi={10.1109/VR58804.2024.00070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494121,
  author={Arboleda, Stephanie Arévalo and Kunert, Christian and Hartbrich, Jakob and Schneiderwind, Christian and Diao, Chenyao and Gerhardt, Christoph and Surdu, Tatiana and Weidner, Florian and Broll, Wolfgang and Werner, Stephan and Raake, Alexander},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Beyond Looks: A Study on Agent Movement and Audiovisual Spatial Coherence in Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={502-512},
  abstract={The appearance of virtual humans (avatars and agents) has been widely explored in immersive environments. However, virtual humans’ movements and associated sounds in real-world interactions, particularly in Augmented Reality (AR), are yet to be explored. In this paper, we investigate the influence of three distinct movement patterns (circle, side-to-side, and standing), two rendering styles (realistic and cartoon), and two types of audio (spatial audio and non-spatial audio) on emotional responses, social presence, appearance and behavior plausibility, audiovisual coherence, and auditory plausibility. To enable that, we conducted a study (N=36) where participants observed an agent reciting a short fictional story. Our results indicate an effect of the rendering style and the type of movement on the subjective perception of the agents behaving in an AR environment. Participants reported higher levels of excitement when they observed the realistic agent moving in a circle compared to the cartoon agent or the other two movement patterns. Moreover, we found an influence of agent’s movement pattern on social presence and higher appearance and behavior plausibility for the realistic rendering style. Regarding audiovisual spatial coherence, we found an influence of rendering style and type of audio only for the cartoon agent. Additionally, the spatial audio was perceived as more plausible than non-spatial audio. Our findings suggest that aligning realistic rendering styles with realistic auditory experiences may not be necessary for 1-1 listening experiences with moving sources. However, movement patterns of agents influence excitement and social presence in passive unidirectional communication scenarios.},
  keywords={Visualization;Spatial audio;Spatial coherence;Coherence;Tutorials;User interfaces;Rendering (computer graphics);virtual humans;audiovisual spatial coherence;agents;augmented reality;movement patterns},
  doi={10.1109/VR58804.2024.00071},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494181,
  author={Li, Shiyu and Schieber, Hannah and Corell, Niklas and Egger, Bernhard and Kreimeier, Julian and Roth, Daniel},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance}, 
  year={2024},
  volume={},
  number={},
  pages={513-523},
  abstract={Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user’s hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.****https://github.com/roth-hex-lab/gbot},
  keywords={Target tracking;Three-dimensional displays;Pose estimation;Kinematics;Real-time systems;Object tracking;Task analysis;Computing methodologies;Artificial intelligence;Computer vision Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality},
  doi={10.1109/VR58804.2024.00072},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494176,
  author={Chen, Liuqing and Cai, Yu and Wang, Ruyue and Ding, Shixian and Tang, Yilin and Hansen, Preben and Sun, Lingyun},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Supporting Text Entry in Virtual Reality with Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={524-534},
  abstract={Text entry in virtual reality (VR) often faces challenges in terms of efficiency and task loads. Prior research has explored various solutions, including specialized keyboard layouts, tracked physical devices, and hands-free interaction. Yet, these efforts often fall short of replicating the efficiency of real-world text entry, or introduce additional spatial and device constraints. This study leverages the extensive capabilities of large language models (LLMs) in context perception and text prediction to enhance text entry efficiency by reducing users’ manual keystrokes. Three LLM-assisted text entry methods - Simplified Spelling, Content Prediction, and Keyword-to-Sentence Generation - are introduced, aligning with user cognition and the contextual predictability of English text at word, grammatical structure, and sentence levels. Through user experiments encompassing various text entry tasks on an Oculus-based VR prototype, these methods demonstrate a 16.4%, 49.9%, 43.7% reduction in manual keystrokes, translating to efficiency gains of 21.4%,74.0%, 76.3%, respectively. Importantly, these methods do not increase manual corrections compared to manual typing, while significantly reducing physical, mental, and temporal loads and enhancing overall usability. Long-term observations further reveal users’ strategies for using these LLM-assisted methods, showing that users’ proficiency with the methods can reinforce their positive effects on text entry efficiency.},
  keywords={Three-dimensional displays;Layout;Prototypes;Collaboration;Manuals;Virtual reality;Organizations;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;Interaction techniques;Text input},
  doi={10.1109/VR58804.2024.00073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494125,
  author={Wang, Yue and Zhang, Yan and Yang, Xuanhui and Wang, Hui and Liu, Dongxu and Yang, Xubo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Foveated Fluid Animation in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={535-545},
  abstract={Large-scale fluid simulation is widely useful in various Virtual Reality (VR) applications. While physics-based fluid animation holds the promise of generating highly realistic fluid details, it often imposes significant computational demands, particularly when simulating high-resolution fluid for VR. In this paper, we propose a novel foveated fluid simulation method that enhances both the visual quality and computational efficiency of physics-based fluid simulation in VR. To leverage the natural foveation feature of human vision, we divide the visible domain of the fluid simulation into foveal, peripheral, and boundary regions. Our foveated fluid system dynamically allocates computational resources, striking a balance between simulation accuracy and computational efficiency. We implement this approach using a multi-scale method. To evaluate the effectiveness of our approach, we have conducted subjective studies. Our findings show a significant reduction in computational resource requirements, resulting in a speedup of up to 2.27 times. It is crucial to note that our method preserves the visual quality of fluid animations at a level that is perceptually identical to full-resolution outcomes. Additionally, we investigate the impact of various metrics, including particle radius and viewing distance, on the visual effects of fluid animations. Our work provides new techniques and evaluations tailored to facilitate real-time foveated fluid simulation in VR, which can enhance the efficiency and realism of fluids in VR applications.},
  keywords={Measurement;Fluids;Three-dimensional displays;Virtual reality;User interfaces;Animation;Visual effects;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Animation;Physical simulation},
  doi={10.1109/VR58804.2024.00074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494198,
  author={Marquardt, Alexander and Steininger, Melissa and Trepkowski, Christina and Weier, Martin and Kruijff, Ernst},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Selection Performance and Reliability of Eye and Head Gaze Tracking Under Varying Light Conditions}, 
  year={2024},
  volume={},
  number={},
  pages={546-556},
  abstract={Augmented Reality (AR) applications increasingly rely on eye and head gaze tracking for user interaction, with their efficacy influenced by environmental factors such as spatial arrangements and lighting conditions. This paper presents two studies that examine how these variables affect the performance of eye and head gaze tracking in AR environments. While eye tracking partially delivered faster results, its performance exhibited greater variability, especially under dynamic lighting conditions. Conversely, head gaze tracking, while providing more consistent results, showed a notable reduction in accuracy in environments with fluctuating light levels. Furthermore, the spatial properties of the environment had notable implications on both tracking methods. Our research demonstrates that both spatial properties and lighting conditions are key determinants in the choice of a tracking method, underscoring the need for AR systems that can dynamically adapt to these environmental variables.},
  keywords={Three-dimensional displays;Lighting;Gaze tracking;User interfaces;Environmental factors;Reliability;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR58804.2024.00075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494098,
  author={Keller, Judith K. and Kusari, Agon and Czok, Sophie and Simgen, Birgit and Steinicke, Frank and Diekhof, Esther K.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={ACHOO - Bless you! Sense of Presence can provoke Proactive Mucosal Immune Responses in Immersive Human-Agent Interactions}, 
  year={2024},
  volume={},
  number={},
  pages={557-567},
  abstract={Previous work suggests that the mere visual perception of disease cues displayed in 2D videos or photos can proactively enhance mucosal immune responses even without actual pathogen exposure. In this paper, we present the first immersive immunological experiment, which investigates if social interactions with virtual agents in virtual reality (VR) can lead to a mucosal immune response, in particular, a proactive release of secretory immunoglobin A(sIgA) in saliva. Therefore, we simulated a virtual bus stop scenario of enhanced airborne contagion risk in which participants were required to closely approach and establish eye contact with ten agents in two conditions. In the first (i.e., contagion) condition, seven of the ten agents sneezed directly before smiling or at predefined intervals. The second (i.e., control) condition used the same agents but without sneezes. We tested 70 healthy participants in a between-subjects design, measured changes in salivary sIgA, as well as subjectively perceived disgust and contagion risk, and assessed their sense of presence and cybersickness in the VE. We found that sIgA secretion increased in both scenarios, while in the control scenario, this increase also correlated with the perceived involvement and sense of presence in the VE. This suggests that the intimate social interactions with virtual agents were sufficient to trigger increased sIgA secretion regardless of sneezing. Hence, VR can be used to provoke proactive immune responses in laboratory experiments.},
  keywords={Pathogens;Three-dimensional displays;Pandemics;Design methodology;User interfaces;Data collection;Particle measurements;Human-centered computing;Human-computer interaction (HCI);Interaction paradigms;Virtual Reality;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR58804.2024.00076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494110,
  author={Qu, Jing and Zhu, Shantong and Shen, Yiran and Zhang, Yanjie and Bu, Lingguo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Understanding the Impact of Longitudinal VR Training on Users with Mild Cognitive Impairment Using fNIRS and Behavioral Data}, 
  year={2024},
  volume={},
  number={},
  pages={568-578},
  abstract={With the growing needs on rehabilitation of the mild cognitive impairment (MCI) users group and the advantages of virtual reality (VR) technologies in cognitive training, the development of VR-based rehabilitation training methods has become a hot spot recently. However, the challenges in accurately measuring users’ needs and quantifying training system efficacy are still not well resolved, especially for longitudinal tracking. In this study, a VR-based cognitive training and evaluation system is designed and implemented, targeting at fulfilling the rehabilitation needs of MCI users. It evaluates the impact of longitudinal VR-based training on MCI users with a number of feedback methodologies including brain activation indicators, brain network connectivity indicators, behavioral indicators and the Montreal Cognitive Assessment (MoCA) scale scores, extracted from multi-modal data collected while training. A two-month longitudinal tracking ergonomics experiment was conducted to validate the usability of the feedback methodologies and to explore the influence of the training duration on the rehabilitation efficacy. The results showed that our proposed VR-based cognitive training and evaluation system had a positively significant impact on the rehabilitation of the MCI group. Meanwhile, the multi-source feedbacks can also help the updates and iterations of VR-based rehabilitation training systems. Finally, this study provides guidance for the selection of rehabilitation cycles and emphasizes the importance of quantitative studies with longitudinal follow-up in assessing rehabilitation efficacy.},
  keywords={Training;Technological innovation;Three-dimensional displays;Target tracking;Sociology;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Applied computing;Life and medical sciences;Health care information systems;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR58804.2024.00077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494096,
  author={Giunchi, Daniele and Numan, Nels and Gatti, Elia and Steed, Anthony},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={DreamCodeVR: Towards Democratizing Behavior Design in Virtual Reality with Speech-Driven Programming}, 
  year={2024},
  volume={},
  number={},
  pages={579-589},
  abstract={Virtual Reality (VR) has revolutionized how we interact with digital worlds. However, programming for VR remains a complex and challenging task, requiring specialized skills and knowledge. Powered by large language models (LLMs), DreamCodeVR is designed to assist users, irrespective of their coding skills, in crafting basic object behavior in VR environments by translating spoken language into code within an active application. This approach seeks to simplify the process of defining behaviors visual changes through speech. Our preliminary user study indicated that the system’s speech interface supports elementary programming tasks, highlighting its potential to improve accessibility for users with varying technical skills. However, it also uncovered a wide range of challenges and opportunities. In an extensive discussion, we detail the system’s strengths, weaknesses, and areas for future research.},
  keywords={Visualization;Codes;Three-dimensional displays;Buildings;Virtual reality;Programming;Writing;large language model;VR;rapid prototyping;low code programming;speech programming;Human-centered computing;Virtual reality Human-centered computing;Natural language interfaces},
  doi={10.1109/VR58804.2024.00078},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494104,
  author={Suga, Yui and Mizoguchi, Izumi and Kajimoto, Hiroyuki},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Presentation of Finger-size Shapes by Combining Force Feedback and Electro-tactile Stimulation}, 
  year={2024},
  volume={},
  number={},
  pages={590-597},
  abstract={In virtual environments, rapid and accurate perception of object shapes is critical for achieving skillful interactions and enhancing immersion. Typically, conventional force feedback devices that provide force direction and magnitude across the entire finger faces challenges in accurately representing fine geometric details of object shapes, such as edges. To address this issue, we augmented force feedback devices with a compact and high-density cutaneous electrical stimulation mechanism, aiming to improve the representation of subtle shapes. In the evaluation experiments, reactive force and edge cues for virtual objects were simultaneously presented, and shape discrimination tests were conducted on four types of column shapes with a thickness of 7.5 mm. Comparing force feedback-only, cutaneous feedback-only, and their combination conditions, our results demonstrated that the combined approach improved shape discrimination accuracy, confirming a more precise perception of fine geometric details.},
  keywords={Three-dimensional displays;Shape;Force feedback;Force;Virtual environments;Focusing;Electrical stimulation;3D shapes;electro-tactile display;haptics;virtual reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces;Haptic I/O},
  doi={10.1109/VR58804.2024.00079},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494173,
  author={Cheng, Ruizhi and Murat, Erdem and Yu, Lap-Fai and Chen, Songqing and Han, Bo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Understanding Online Education in Metaverse: Systems and User Experience Perspectives}, 
  year={2024},
  volume={},
  number={},
  pages={598-608},
  abstract={Thanks to recent advances in immersive technologies, virtual reality (VR) is becoming increasingly popular in online education, particularly in light of the rise of the Metaverse. However, there is currently no in-depth investigation of the user experience of VR-based online education and the comparison of it with video-conferencing-based counterparts. To fill these critical gaps, we conduct multiple sessions of two courses in a university with 10 and 37 participants on Mozilla Hubs (Hubs for short), a social VR platform that is deemed as one of the early prototypes of the Metaverse, and let them compare the classroom experience on Hubs with Zoom, a popular video-conferencing application. In addition to employing traditional analytical methods to understand user experience, we benefit from an end-to-end measurement study of Hubs to corroborate our findings and systematically detect its performance bottlenecks. Our study leads to the following key observations. First, the scalability issue of Hubs makes it inadequate for accommodating large courses. Second, compared to Zoom, Hubs can offer a better sense of place presence and social presence to students, thanks to its avatar-based interactions and the hand and head tracking enabled by headsets. Third, even though VR headsets help students concentrate in class, effectively utilizing learning tools through them remains a challenge.},
  keywords={Headphones;Three-dimensional displays;Metaverse;Statistical analysis;Tracking;Education;Virtual reality},
  doi={10.1109/VR58804.2024.00080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494143,
  author={Huang, Siyu and Popescu, Voicu},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={HyperXRC: Hybrid In-Person + Remote Extended Reality Classroom - A Design Study}, 
  year={2024},
  volume={},
  number={},
  pages={609-618},
  abstract={This paper investigates HyperXRC, a hybrid classroom design that accommodates both local and remote students. The instructor wears an extended reality (XR) headset that shows the local classroom and the local students, as well as remote students modeled with video sprites. The remote students are displayed either on virtual banners hanging off the classroom ceiling, or on virtual billboards placed in empty classroom seats. Thereby, the remote students are integrated into the field of view of the instructor, who remains aware of the remote students while teaching. A controlled user study with two experiments evaluated the HyperXRC design from the instructor and from the local students perspective. In the first experiment (N = 15) participants served as instructors to a hybrid classroom of 14 local and 15 remote students. Participants were more likely to detect hand-raising and head-on-desk remote student actions in the HyperXRC conditions (59%) than in a conventional videoconferencing condition (36%). This advantage did not come at the cost of decreasing the detection rate of local student actions. Furthermore, instructor participants preferred the HyperXRC to the videoconferencing approach. In the second experiment (N = 16) participants served as local students. The participants preferred the lecture when the instructor used videoconferencing to the one when the instructor used HyperXRC, wearing the XR headset.},
  keywords={Headphones;Measurement;Three-dimensional displays;Portable computers;Extended reality;Education;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augm. reality},
  doi={10.1109/VR58804.2024.00081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494077,
  author={Benjamin, Juanita and Erickson, Austin and Gottsacker, Matthew and Bruder, Gerd and Welch, Greg},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Transitive Perceptual Effects Between Virtual Entities in Outdoor Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={619-629},
  abstract={Augmented reality (AR) head-mounted displays (HMDs) provide users with a view in which digital content is blended spatially with the outside world. However, one critical issue faced with such display technologies is misperception, i.e., perceptions of computer-generated content that differs from our human perception of other real-world objects or entities. Misperception can lead to mistrust in these systems and negative impacts in a variety of application fields. Although there is a considerable amount of research investigating either size, distance, or speed misperception in AR, far less is known about the relationships between these aspects. In this paper, we present an outdoor AR experiment (N = 20) using a HoloLens 2 HMD. Participants estimated size, distance, and speed of Familiar and Unfamiliar outdoor animals at three distances (30, 60, 90 meters). To investigate whether providing information about one aspect may influence another, we divided our experiment into three phases. In Phase I, participants estimated the three aspects without any provided information. In Phase II, participants were given accurate size information, then asked to estimate distance and speed. In Phase III, participants were given accurate distance and size information, then asked to estimate speed. Our results show that estimates of speed in particular of the Unfamiliar animals benefited from provided size information, while speed estimates of all animals benefited from provided distance information. We found no support for the assumption that distance estimates benefited from provided size information.},
  keywords={Meters;Dinosaurs;Three-dimensional displays;Head-mounted displays;Animals;Affordances;Estimation;Human-centered computing;Human computer interaction;Human computer interaction (HCI) Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR58804.2024.00082},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494113,
  author={Chalmers, Andrew and Zaman, Faisal and Rhee, Taehyun},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Avatar360: Emulating 6-DoF Perception in 360°Panoramas through Avatar-Assisted Navigation}, 
  year={2024},
  volume={},
  number={},
  pages={630-638},
  abstract={360° images offer panoramic views of captured environments, placing users within an egocentric perspective. While users can freely rotate their viewpoint, they don’t experience 6-DoF navigation with translational movement. In this research, we introduce Avatar360, a novel method to elicit 6-DoF perception in 360° panoramas, using avatar-assisted navigation combined with an exocentric view of the 360° panorama. We seamlessly integrate a 3D avatar into 360° panoramas, allowing users to navigate a 3D virtual landscape congruent with the 360° background. By aligning the exocentric perspective of the 360° panorama with the avatar’s movements, we replicate a sensation of 6-DoF navigation in 360° panoramas. We explore mechanisms for simultaneous avatar and viewpoint controls, as well as procedures for transitions between spatially connected 360° panoramas. A user study was conducted to assess the perception of 6-DoF navigation in 360° panoramas via a 3D avatar, evaluating users’ sense of movement, disorientation, and presence. We also gained insight into perspective view controls and transition techniques between panoramas. Statistical analysis shows avatar-assisted navigation elicits a user’s sense of movement within 360° panoramas. Our results also provide guidelines for effective view control and transition strategies in avatar-assisted 360° navigation.},
  keywords={Three-dimensional displays;Navigation;Statistical analysis;Avatars;Prototypes;User interfaces;6-DOF;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR58804.2024.00083},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494111,
  author={Zhang, Zhenliang and Zhang, Zeyu and Jiao, Ziyuan and Su, Yao and Liu, Hangxin and Wang, Wei and Zhu, Song-Chun},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={On the Emergence of Symmetrical Reality}, 
  year={2024},
  volume={},
  number={},
  pages={639-649},
  abstract={Artificial intelligence (AI) has revolutionized human cognitive abilities and facilitated the development of new AI entities capable of interacting with humans in both physical and virtual environments. Despite the existence of virtual reality, mixed reality, and augmented reality for many years, integrating these technical fields remains a formidable challenge due to their disparate application directions. The advent of AI agents, capable of autonomous perception and action, further compounds this issue by exposing the limitations of traditional human-centered research approaches. It is imperative to establish a comprehensive framework that accommodates the dual perceptual centers of humans and AI agents in both physical and virtual worlds. In this paper, we introduce the symmetrical reality framework, which offers a unified representation encompassing various forms of physical-virtual amalgamations. This framework enables researchers to better comprehend how AI agents can collaborate with humans and how distinct technical pathways of physical-virtual integration can be consolidated from a broader perspective. We then delve into the coexistence of humans and AI, demonstrating a prototype system that exemplifies the operation of symmetrical reality systems for specific tasks, such as pouring water. Finally, we propose an instance of an AI-driven active assistance service that illustrates the potential applications of symmetrical reality. This paper aims to offer beneficial perspectives and guidance for researchers and practitioners in different fields, thus contributing to the ongoing research about human-AI coexistence in both physical and virtual environments.},
  keywords={Human computer interaction;Solid modeling;Three-dimensional displays;Neuroscience;Virtual environments;Psychology;Prototypes;Human-centered computing — Human computer interaction (HCI) — Interaction paradigms — Mixed / augmented reality;Human-centered computing — Human computer interaction (HCI) — HCI theory, concepts and models},
  doi={10.1109/VR58804.2024.00084},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494081,
  author={Yang, Bangbang and Dong, Wenqi and Ma, Lin and Hu, Wenbo and Liu, Xiao and Cui, Zhaopeng and Ma, Yuewen},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation}, 
  year={2024},
  volume={},
  number={},
  pages={650-660},
  abstract={Diffusion-based methods have achieved prominent success in generating 2D media. However, accomplishing similar proficiencies for scene-level mesh texturing in 3D spatial applications, e.g., XR/VR, remains constrained, primarily due to the intricate nature of 3D geometry and the necessity for immersive free-viewpoint rendering. In this paper, we propose a novel indoor scene texturing framework, which delivers text-driven texture generation with enchanting details and authentic spatial coherence. The key insight is to first imagine a stylized 360° panoramic texture from the central viewpoint of the scene, and then propagate it to the rest areas with inpainting and imitating techniques. To ensure meaningful and aligned textures to the scene, we develop a novel coarse-to-fine panoramic texture generation approach with dual texture alignment, which both considers the geometry and texture cues of the captured scenes. To survive cluttered geometries during texture propagation, we design a separated strategy, which conducts texture inpainting in visible regions and then learns an implicit imitating network to synthesize textures in occluded and tiny structural areas. Extensive experiments and the immersive VR application on real-world indoor scenes demonstrate the high quality of the generated textures and the engaging experience on VR headsets. Project webpage: https://ybbbbt.com/publication/dreamspace.},
  keywords={Geometry;Visualization;Three-dimensional displays;Pipelines;Spatial coherence;Lighting;Resists;Computing methodologies;Computer graphics;Image manipulation;Texturing;Artificial intelligence;Computer vision},
  doi={10.1109/VR58804.2024.00085},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494196,
  author={Luo, Yusheng and Zhu, Lifeng and Song, Aiguo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Force-regulated Elastic Linear Objects Tracking for Virtual and Augmented Reality}, 
  year={2024},
  volume={},
  number={},
  pages={661-669},
  abstract={Elastic rods are commonly seen in our daily life. Although humans are sensitive to the shape change of rods, it is not intuitive to estimate the external forces applied to generate the deformation. We propose a method to interactively track the elastic linear objects by using the Cosserat rod model to regulate the captured noisy points. We develop a framework based on particle filters to work with the physics-based model, turning the inverse physics problem into a forward simulation and search problem. We show that with the proposed method, we can simultaneously digitalize the shape as well as the external forces on real-world elastic rods. With these capabilities, we demonstrate virtual and augmented reality applications to facilitate the interaction with elastic linear objects. The tracking performance is also validated with experiments.},
  keywords={Solid modeling;Three-dimensional displays;Shape;Computational modeling;Force;User interfaces;Turning},
  doi={10.1109/VR58804.2024.00086},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494100,
  author={Simon, Cassandre and Boukli-Hacene, Manel and Lebrun, Flavien and Otmane, Samir and Chellali, Amine},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Impact of Multimodal Instructions for Tool Manipulation Skills on Performance and User Experience in an Immersive Environment}, 
  year={2024},
  volume={},
  number={},
  pages={670-680},
  abstract={With the mentoring model, a mentee can learn technical skills under the supervision of more experienced peers who demonstrate their knowledge through several communication modalities. Supporting the mentoring model within shared immersive training simulators holds promise in enhancing mentor-mentee interactions and learning outcomes in a safe environment. However, efficient communication within these spaces remains an open issue. This work presents a user study that explores the combination of communication modalities (verbal-visual, verbal-haptic, visual-haptic, and verbal-visual-haptic) to convey instructions to learners on the amplitude of movements to perform during a tool-handling task in an immersive environment. The study aims to examine the impact of the four modality combinations on performance (speed and accuracy of movement replication), mental workload, and participants’ user experience. The results show that participants achieved higher accuracy with the visual-haptic and verbal-visual-haptic conditions. Moreover, they performed the movements faster, and their movement trajectories were closer to the reference trajectories in the visual-haptic condition. Finally, the most preferred verbal-visual-haptic combination enhanced the users’ sense of presence, co-presence, social presence, and learning experience. No impact on the mental workload was observed. These results suggest that combining haptic and visual modalities is the best suited for enhancing learners’ performance. Adding the verbal modality can also improve the user experience in the immersive learning environment. These findings contribute to improving the design of immersive collaborative systems and pave the way for exploring novel avenues of research into the efficacy of multimodal communication for enhancing the mentoring-based acquisition of technical skills in VR. These tools hold promise for diverse applications, including medical simulation.},
  keywords={Training;Visualization;Three-dimensional displays;Collaboration;User interfaces;Motors;User experience;Multimodal interactions;Mentorship;Remote collaboration;Immersive learning;Human-centered computing Virtual reality;Human-centered computing User studies;Human-centered computing Collaborative interaction},
  doi={10.1109/VR58804.2024.00087},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494120,
  author={Orlosky, Jason and Liu, Chang and Sakamoto, Kenya and Sidenmark, Ludwig and Mansour, Adam},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={EyeShadows: Peripheral Virtual Copies for Rapid Gaze Selection and Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={681-689},
  abstract={In eye-tracked augmented and virtual reality (AR/VR), instantaneous and accurate hands-free selection of virtual elements is still a significant challenge. Though other methods that involve gaze-coupled head movements or hovering can improve selection times in comparison to methods like gaze-dwell, they are either not instantaneous or have difficulty ensuring that the user’s selection is deliberate. In this paper, we present EyeShadows, an eye gaze-based selection system that takes advantage of peripheral copies (shadows) of items that allow for quick selection and manipulation of an object or corresponding menus. This method is compatible with a variety of different selection tasks and controllable items, avoids the Midas touch problem, does not clutter the virtual environment, and is context sensitive. We have implemented and refined this selection tool for VR and AR, including testing with optical and video see-through (OST/VST) displays. Moreover, we demonstrate that this method can be used for a wide range of AR and VR applications, including manipulation of sliders or analog elements. We test its performance in VR against three other selection techniques, including dwell (baseline), an inertial reticle, and head-coupled selection. Results showed that selection with EyeShadows was significantly faster than dwell (baseline), outperforming in the select and search and select tasks by 29.8% and 15.7%, respectively, though error rates varied between tasks.},
  keywords={Three-dimensional displays;Target tracking;Error analysis;Virtual environments;User interfaces;Rendering (computer graphics);Adaptive optics;Eye Tracking;Virtual Reality;Augmented Reality;Selection;Hands-free;Manipulation},
  doi={10.1109/VR58804.2024.00088},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494080,
  author={Lin, Wan-Yi and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Babu, Sabarish V. and Pagano, Christopher and Lin, Wen-Chieh},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Empirical Evaluation of the Calibration of Auditory Distance Perception under Different Levels of Virtual Environment Visibilities}, 
  year={2024},
  volume={},
  number={},
  pages={690-700},
  abstract={The perception of distance is a complex process that often involves sensory information beyond that of just vision. In this work, we investigated if depth perception based on auditory information can be calibrated, a process by which perceptual accuracy of depth judgments can be improved by providing feedback and then performing corrective actions. We further investigated if perceptual learning through carryover effects of calibration occurs in different levels of a virtual environment’s visibility based on different levels of virtual lighting. Users performed an auditory depth judgment task over several trials in which they walked where they perceived an aural sound to be, yielding absolute estimates of perceived distance. This task was performed in three sequential phases: pretest, calibration, posttest. Feedback on the perceptual accuracy of distance estimates was only provided in the calibration phase, allowing to study the calibration of auditory depth perception. We employed a 2 (Visibility of virtual environment) $\times 3$ (Phase) $\times 5$ (Target Distance) multi-factorial design, manipulating the phase and target distance as within-subjects factors, and the visibility of the virtual environment as a between-subjects factor. Our results revealed that users generally tend to underestimate aurally perceived distances in VR similar to the distance compression effects that commonly occur in visual distance perception in VR. We found that auditory depth estimates, obtained using an absolute measure, can be calibrated to become more accurate through feedback and corrective action. In terms of environment visibility, we find that environments visible enough to reveal their extent may contain visual information that users attune to in scaling aurally perceived depth.},
  keywords={Visualization;Three-dimensional displays;Virtual environments;Lighting;User interfaces;Calibration;Task analysis;Auditory Distance Perception;Perceptual Learning and Calibration;Virtual Reality},
  doi={10.1109/VR58804.2024.00089},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494137,
  author={Yin, Zhizhuo and Wang, Yuyang and Papatheodorou, Theodoros and Hui, Pan},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text2VRScene: Exploring the Framework of Automated Text-driven Generation System for VR Experience}, 
  year={2024},
  volume={},
  number={},
  pages={701-711},
  abstract={With the recent development of the Virtual Reality (VR) industry, the increasing number of VR users pushes the demand for the massive production of immersive and expressive VR scenes in related industries. However, creating expressive VR scenes involves the reasonable organization of various digital content to express a coherent and logical theme, which is time-consuming and labor-intensive. In recent years, Large Language Models (LLMs) such as ChatGPT 3.5 and generative models such as stable diffusion have emerged as powerful tools for comprehending natural language and generating digital contents such as text, code, images, and 3D objects. In this paper, we have explored how we can generate VR scenes from text by incorporating LLMs and various generative models into an automated system. To achieve this, we first identify the possible limitations of LLMs for an automated system and propose a systematic framework to mitigate them. Subsequently, we developed Text2VRScene, a VR scene generation system, based on our proposed framework with well-designed prompts. To validate the effectiveness of our proposed framework and the designed prompts, we carry out a series of test cases. The results show that the proposed framework contributes to improving the reliability of the system and the quality of the generated VR scenes. The results also illustrate the promising performance of the Text2VRScene in generating satisfying VR scenes with a clear theme regularized by our well-designed prompts. This paper ends with a discussion about the limitations of the current system and the potential of developing similar generation systems based on our framework.},
  keywords={Industries;Solid modeling;Systematics;Three-dimensional displays;Natural languages;Virtual reality;Production;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques;Text input},
  doi={10.1109/VR58804.2024.00090},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494105,
  author={Kaluschke, Maximilian and Weller, Rene and Yin, Myat Su and Hosp, Benedikt W. and Kulapichitr, Farin and Suebnukarn, Siriwan and Haddawy, Peter and Zachmann, Gabriel},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reflecting on Excellence: VR Simulation for Learning Indirect Vision in Complex Bi-Manual Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={712-721},
  abstract={Indirect vision through a mirror, while bi-manually manipulating both the mirror and another tool is a relatively common way to perform operations in various types of surgery. However, learning such psychomotor skills requires extensive training; they are difficult to teach; and they can be quite costly, for instance, for dentistry schools. In order to study the effectiveness of VR simulators for learning these kinds of skills, we developed a simulator for training dental surgery procedures, which supports tracking of eye gaze and tool trajectories (mirror and drill), as well as automated outcome scoring. We carried out a pre-/post-test study in which 30 fifth-year dental students received six training sessions in the access opening stage of the root canal procedure using the simulator. In addition, six experts performed three trials using the simulator. The outcomes of drilling performed on realistic plastic teeth showed a significant learning effect due to the training sessions. Also, students with larger improvements in the simulator tended to improve more in the real-world tests. Analysis of the tracking data revealed novel relationships between several metrics w.r.t. eye gaze and mirror use, and performance and learning effectiveness: high rates of correct mirror placement during active drilling and high continuity of fixation on the tooth are associated with increased skills and increased learning effectiveness. Larger time allocation for tooth inspections using the mirror, i.e., indirect vision, and frequency of inspection are associated with increased learning effectiveness. Our findings suggest that eye tracking can provide valuable insights into student learning gains of bi-manual psychomotor skills, particularly in indirect vision environments.},
  keywords={Training;Drilling;Teeth;Gaze tracking;Virtual reality;Inspection;Dentistry;K.3.1 [COMPUTERS AND EDUCATION];Computer Uses in Education-Computer-assisted instruction (CAI);J.3 [LIFE AND MEDICAL SCIENCES];Medical information systems;I.6.3 [SIMULATION AND MODELING];Applications},
  doi={10.1109/VR58804.2024.00091},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494093,
  author={Robert, Florent and Wu, Hui-Yin and Sassatelli, Lucile and Winckler, Marco},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Task-based methodology to characterise immersive user experience with multivariate data}, 
  year={2024},
  volume={},
  number={},
  pages={722-731},
  abstract={Virtual Reality (VR) technologies enable strong emotions compared to traditional media, stimulating the brain in ways comparable to real-life interactions. This makes VR systems promising for research and applications in training or rehabilitation, to imitate realistic situations. Nonetheless, the evaluation of the user experience in immersive environments is daunting, the richness of the media presents challenges to synchronise context with behavioural metrics in order to provide fine-grained personalised feedback or performance evaluation. The variety of scenarios and interaction modalities multiplies this difficulty of user understanding in face of lifelike training scenarios, complex interactions, and rich context.We propose a task-based methodology that provides fine-grained descriptions and analyses of the experiential user experience (UX) in VR that (1) aligns low-level tasks (i.e. take an object, go somewhere) with multivariate behaviour metrics: gaze, motion, skin conductance, (2) defines performance components (i.e., attention, decision, and efficiency) with baseline values to evaluate task performance, and (3) characterises task performance with multivariate user behaviour data. To illustrate our approach, we apply the task-based methodology to an existing dataset from a road crossing study in VR. We find that the task-based methodology allows us to better observe the experiential UX by highlighting fine-grained relations between behaviour profiles and task performance, opening pathways to personalised feedback and experiences in future VR applications.},
  keywords={Training;Solid modeling;Three-dimensional displays;Computational modeling;Virtual reality;Media;User interfaces;virtual reality;user experience;multivariate data;task modeling;behaviour characterisation;Human-centered computing;HCI theory;concepts and models;User models;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR58804.2024.00092},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494175,
  author={Yuan, Lin-Ping and Li, Boyu and Wang, Jindong and Qu, Huamin and Zeng, Wei},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Generating Virtual Reality Stroke Gesture Data from Out-of-Distribution Desktop Stroke Gesture Data}, 
  year={2024},
  volume={},
  number={},
  pages={732-742},
  abstract={This paper exploits ubiquitous desktop interaction data as an input source for generating virtual reality (VR) interaction data, which can benefit tasks like user behavior analysis and experience enhancement. Time-varying stroke gestures are selected as the primary focus because of their prevalence across various applications and their diverse patterns. The commonalities (e.g., features like velocity and curvature) between desktop and VR strokes allow the generation of additional dimensions (e.g., z vectors) in VR strokes. However, distribution shifts exist between different interaction environments (i.e., desktop vs. VR), and within the same interaction environment for different strokes by various users, making it challenging to build models capable of generalizing to unseen distributions. To address the challenges, we formulate the problem of generating VR strokes from desktop strokes as a conditional time series generation problem, aiming to learn representations that are capable of handling out-of-distribution data. We propose a novel architecture based on conditional generative adversarial networks, with the generator encompassing three steps: discretizing the output space, characterizing latent distributions, and learning conditional domain-invariant representations. We evaluate the effectiveness of our methods by comparing them with state-of-the-art time series generation models and conducting ablation studies. We further illustrate the applicability of the enriched VR datasets through two applications: VR stroke classification and stroke prediction.},
  keywords={Solid modeling;Three-dimensional displays;Time series analysis;Virtual reality;Computer architecture;User interfaces;Generative adversarial networks;Human-centered computing;Virtual reality},
  doi={10.1109/VR58804.2024.00093},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494127,
  author={Lougiakis, Christos and González, Jorge Juan and Ganias, Giorgos and Katifori, Akrivi and Ioannis-Panagiotis and Roussou, Maria},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Physics-based Hand Interaction in Virtual Reality: Custom Soft Body Simulation vs. Off-the-Shelf Integrated Solution}, 
  year={2024},
  volume={},
  number={},
  pages={743-753},
  abstract={Physics-based hand interaction in VR has been extensively explored, but almost none of the solutions are usable. The only exception is CLAP, a custom soft body simulation offering realistic and smooth hand interaction in VR. Even CLAP, however, imposes constraints on virtual hand and object behavior. We introduce HPTK+, a software solution that utilizes the physics engine NVIDIA PhysX. Benefiting from the engine’s maturity and integration with game engines, we aim to enable more general and free-hand interactions in virtual environments. We conducted a user study with 27 participants comparing the interactions supported by both libraries. Results indicate an overall preference for CLAP, but no significant differences in other measures or performance, except variance. These findings provide insights into the libraries’ suitability for specific tasks. Additionally, we highlight HPTK+’s exclusive support for diverse interactions, positioning it as an ideal candidate for further research in physics-based VR interactions.},
  keywords={Three-dimensional displays;Scalability;Virtual environments;User interfaces;Particle measurements;User experience;Software;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Computing methodologies;Modeling and simulation;Simulation support systems;Simulation environments;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00094},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494103,
  author={Sindhupathiraja, Siddhanth Raja and Ullah, A K M Amanat and Delamare, William and Hasan, Khalad},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Bi-Manual Teleportation in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={754-764},
  abstract={Teleportation, a widely-used locomotion technique in Virtual Reality (VR), allows instantaneous movement within VR environments. Enhanced hand tracking in modern VR headsets has popularized hands-only teleportation methods, which eliminate the need for physical controllers. However, these techniques have not fully explored the potential of bi-manual input, where each hand plays a distinct role in teleportation: one controls the teleportation point and the other confirms selections. Additionally, the influence of users’ posture, whether sitting or standing, on these techniques remains unexplored. Furthermore, previous teleportation evaluations lacked assessments based on established human motor models such as Fitts’ Law. To address these gaps, we conducted a user study $( \mathrm{N}=20)$ to evaluate bi-manual pointing performance in VR teleportation tasks, considering both sitting and standing postures. We proposed a variation of the Fitts’ Law model to accurately assess users’ teleportation performance. We designed and evaluated various bi-manual teleportation techniques, comparing them to uni-manual and dwell-based techniques. Results showed that bi-manual techniques, particularly when the dominant hand is used for pointing and the non-dominant hand for selection, enable faster teleportation compared to other methods. Furthermore, bi-manual and dwell techniques proved significantly more accurate than uni-manual teleportation. Moreover, our proposed Fitts’ Law variation more accurately predicted users’ teleportation performance compared to existing models. Finally, we developed a set of guidelines for designers to enhance VR teleportation experiences and optimize user interactions.},
  keywords={Solid modeling;Three-dimensional displays;Tracking;Computational modeling;Virtual reality;Teleportation;User interfaces;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality;Humancentered computing;Interaction techniques;Gestural input},
  doi={10.1109/VR58804.2024.00095},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494102,
  author={Sasikumar, Prasanth and Hajika, Ryo and Gupta, Kunal and Gunasekaran, Tamil Selvan and Pai, Yun Suen and Bai, Huidong and Nanayakkara, Suranga and Billinghurst, Mark},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A User Study on Sharing Physiological Cues in VR Assembly Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={765-773},
  abstract={In collaborative settings where multiple individuals are tasked with completing a shared goal, understanding one’s partner’s emotional state could be crucial for achieving a successful outcome. This is particularly relevant in remote collaboration contexts, where physical distance can impede understanding, empathy, and mutual comprehension between partners. In this paper, we demonstrate representing emotional patterns from physiological data in a shared Virtual Reality (VR) environment, and explore how it impacted communication styles. A user study investigated the potential effects of this emotional representation in fostering empathetic communication during remote collaboration. The study’s findings revealed that although there was minimal variance in the workload associated with observing physiological cues, participants generally preferred monitoring their partner’s attentional state. However, with the assembly task chosen, most participants only directed a minimal proportion of their attention toward the physiological cues displayed by their partner, and were frequently uncertain of how to interpret and use the information obtained. We also discuss limitations of the research and opportunities for future work.},
  keywords={Measurement;Three-dimensional displays;Collaboration;Virtual reality;User interfaces;Physiology;Biomedical monitoring;AR Assembly;Physiological sensing;Collaborative VR;Adaptive VR;Emotion Adaptive VR},
  doi={10.1109/VR58804.2024.00096},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494090,
  author={Gao, Haolin and Yue, Kang and Yang, Songyue and Liu, Yu and Guo, Mei and Liu, Yue},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Depth-based Perception Conflicts in Virtual Reality through Error-Related Potentials}, 
  year={2024},
  volume={},
  number={},
  pages={774-784},
  abstract={Virtual Reality (VR) offers a valuable platform for real-life skills training. However, previous research has indicated that human’s perception of depth in VR differs from that of the real world. Such perceptual conflicts can impact immersion and the learning of skills, thus attracting widespread attention. Various methods have been proposed to enhance users’ depth perception, yet the underlying mechanisms of depth perception conflicts still require further research. In this paper, we used Error-Related Potentials (ErrPs) from electroencephalography (EEG) data to investigate the differences in participants’ perceptions at varying depths within the near-field. We designed a within-subjects experiment to successfully introduce depth perception conflicts. From participants exposed to three distinct depths, we collected questionnaire results, performance data, and EEG data. Our findings showed that EEG can effectively detect depth perception conflicts and, following each conflict, participants’ behavioral patterns showed significant changes. In situations with shallower depths, participants exhibited stronger responses to the designed conflicts. This increased sensitivity correlates with their accuracy in depth estimation. This study represents a novel approach to depth perception in VR using ErrPs, setting the stage for further use of physiological signals to measure the granularity of depth perception in VR/AR environments.},
  keywords={Training;Estimation error;Electric potential;Sensitivity;Three-dimensional displays;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00097},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494083,
  author={Pathmanathan, Nelusa and Rau, Tobias and Yang, Xiliu and Calepso, Aimée Sousa and Amtsberg, Felix and Menges, Achim and Sedlmair, Michael and Kurzhals, Kuno},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={785-795},
  abstract={The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.},
  keywords={Visualization;Three-dimensional displays;Collaboration;Gaze tracking;User interfaces;User experience;Task analysis;Human-centered computing;Visualization;Visualization techniques;Visualization design and evaluation methods},
  doi={10.1109/VR58804.2024.00098},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494135,
  author={Kim, Seonji and Kim, Dooyoung and Shin, Jae-Eun and Woo, Woontack},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Object Cluster Registration of Dissimilar Rooms Using Geometric Spatial Affordance Graph to Generate Shared Virtual Spaces}, 
  year={2024},
  volume={},
  number={},
  pages={796-805},
  abstract={We propose Object Cluster Registration (OCR) using Geometric Spatial Affordance Graph (GSAG) to support user interaction with multiple objects in a shared space generated from two dissimilar rooms. Previous research on generating a shared virtual space from dissimilar real spaces has only reflected the information of individual objects and aimed at maximizing the area of the shared space. This led to limited interactions relying on the singular affordances of objects, neglecting to consider the usability and effectiveness of the generated shared spaces. The proposed OCR with GSAG, which considers the relationship between objects based on facing formation, extracts optimal object cluster pairs to align dissimilar rooms in generating shared virtual spaces. In an evaluation study involving 100 multi-cluster space pairs, applying OCR using GSAG showed greater effectiveness in preserving object correlations compared to cases where OCR was not used. Furthermore, the size of the shared space did not significantly differ between the two methods. This suggests that factoring in the relationship between objects does not compromise the objective of maximizing the shared virtual space. The proposed method is expected to serve as a foundation for generating shared virtual spaces that are more user-oriented and efficient by facilitating a wider range of collaborative activities for remote users in dissimilar real spaces with varied configurations.},
  keywords={Three-dimensional displays;Correlation;Affordances;Optical character recognition;Collaboration;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Virtual reality},
  doi={10.1109/VR58804.2024.00099},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494112,
  author={Soni, Nikita and Obajemu, Oluwatomisin and Jurczyk, Katarina and Peddireddy, Chaitra and Vallee, Maeson and Tierney, Ailish and Saririan, Niloufar and Zuck, Cameron John and Stofer, Kathryn A. and Anthony, Lisa},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparative Usability Study of Physical Multi-touch versus Virtual Desktop-Based Spherical Interfaces}, 
  year={2024},
  volume={},
  number={},
  pages={806-816},
  abstract={Physical multi-touch spherical displays can provide a direct, hands-on, embodied interaction experience with global visualization data like ocean temperatures and currents. However, current commercially available displays may be cost-prohibitive for educational institutions and/or non-profits to acquire. Virtual globe-based visualizations like Google Earth are a potential alternative, but it is not clear how well the interactive affordances of physical spheres may transfer to the virtual. We conducted a within-subjects comparative study with 21 participants who completed similar tasks on a physical and a virtual spherical interface platform, which were designed to be as similar as possible, in order to allow us to compare the interaction experiences. Our results overall showed no significant difference be-tween usability or task time on the two platforms. In their qualitative feedback, participants noticed the differences between the physical sphere and virtual sphere in terms of effort and motor demand. Our research implies that, in resource-constrained environments, a virtual globe can be a sufficient substitute for a physical sphere from a usability perspective.},
  keywords={Earth;Three-dimensional displays;Costs;Data visualization;Virtual reality;Motors;Internet;Human-centered computing;Spherical display interfaces;Flatscreen displays;Touchscreens},
  doi={10.1109/VR58804.2024.00100},
  ISSN={2642-5254},
  month={March},}
@INPROCEEDINGS{10494091,
  author={Torres, Ángel and Molina, José P. and García, Arturo S. and González, Pascual},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Prototyping of Augmented Reality interfaces for air traffic alert and their evaluation using a Virtual Reality aircraft-proximity simulator}, 
  year={2024},
  volume={},
  number={},
  pages={817-826},
  abstract={Despite pilots’ training and technology aids, mid-air collisions can occur and do occur, especially near airfields and in non-controlled airspaces where different kinds of aircraft fly. Technology typically helps pilots in the form of a flat display in the cockpit that shows nearby air traffic. New airliners are now fitted with see-through displays (HUD) to present information right in front of the pilot, and modern military fighters mount that kind of displays in the pilot’s helmet to help them no matter what direction they are looking in. This technology, however, could reach light and sport aviation in the next years thanks to new light and affordable Augmented Reality (AR) glasses, such as the ones targeting applications in urban mobility. Looking ahead to that moment, in this work we rely on Virtual Reality (VR) -in particular, Cardboard VR- to prototype and test different AR interfaces for air traffic alert. Firstly, we proposed and tested four different head-mounted display (HMD) AR interfaces with four pilots in our own VR aircraft-proximity simulator. Then, the two best-scored interfaces were selected for a second evaluation, and they were compared against another proposal (Circular HUD by Alce et al.) and a fixed-mounted (FM) conventional HUD radar (FMHUD Radar), tested by four additional pilots. Overall, pilots showed preference for our AR proposals. Interestingly, pilots with more experience preferred the more conventional, radar-like designs, while those with less flight hours were more open to a different, novel design (HMD 3D Arrow).},
  keywords={Training;Three-dimensional displays;Resists;Radar;User interfaces;Military aircraft;Safety;Air traffic alert;guidance techniques;augmented reality;virtual prototyping;H.5.2 [Information Interfaces and Presentation];User Interfaces},
  doi={10.1109/VR58804.2024.00101},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494148,
  author={Wagner, Jorge and Silva, Claudio T. and Stuerzlinger, Wolfgang and Nedel, Luciana},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and reflecting on potential benefits of Immersive Analytics for urban data exploration}, 
  year={2024},
  volume={},
  number={},
  pages={827-838},
  abstract={Current visualization research has identified the potential of more immersive settings for data exploration, leveraging VR and AR technologies. To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City. Here, we reimagine how TaxiVis’ functionalities could be implemented and extended in a 3D immersive environment. Among the unique features we identify as being enabled by the Immersive TaxiVis prototype are alternative uses of the additional visual dimension, a fully visual 3D spatio-temporal query framework, and the opportunity to explore the data at different scales and frames of reference. By revisiting the case studies from the original paper, we demonstrate workflows that can benefit from this immersive perspective. Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and on how the exploration of urban datasets can be facilitated in the coming years.},
  keywords={Visualization;Three-dimensional displays;Urban areas;Data visualization;Prototypes;Virtual reality;Reflection;Human-centered computing;Visualization;Visualization systems and tools;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00102},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494157,
  author={Park, Seunghoon and Son, Seungwoo and Kim, Jungha and Kim, Gerard J.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Directional Airflow toward Vection and Cybersickness}, 
  year={2024},
  volume={},
  number={},
  pages={839-848},
  abstract={Employing airflow feedback is an effective means of enriching the virtual reality experience multimodally. Most interestingly, there have been conflicting reports of its effect on cybersickness. For example, some research has found that the airflow can be felt as refreshment and ease cybersickness, while others seen it as promoting the sense of vection and worsening the sickness symptoms. We present a comprehensive experimental study to clarify and investigate the deeper mechanism of the effect of directional airflow on cybersickness and user experience in virtual navigation. Our findings indicated that the airflow consistent with the navigation direction can strengthen the sense of vection, while no effect was observed for the inconsistent. On the other hand, regardless of the direction, the refreshing effect of the airflow was significant, resulting in the overall decreased level of cybersickness. Such a result can help us design the virtual navigation experience with higher dynamism and lesser extent of sickness symptoms.},
  keywords={Three-dimensional displays;Cybersickness;Navigation;Design methodology;User interfaces;User experience;Airflow;Cybersickness;Vection;Presence;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00103},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494183,
  author={Immohr, Felix and Rendle, Gareth and Lammert, Anton and Neidhardt, Annika and Heyde, Victoria Meyer Zur and Froehlich, Bernd and Raake, Alexander},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Effect of Binaural Auralization on Audiovisual Plausibility and Communication Behavior in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={849-858},
  abstract={Spatial audio representations have been shown to positively impact user experience in traditional, non-immersive communication media. While spatial audio also contributes to presence in single-user immersive VR, its impact in virtual communication scenarios has not yet been fully understood. This work aims to further investigate which communication scenarios benefit from spatial audio representations. We present a study in which pairs of interlocutors undertake a collaborative task in an audiovisual Virtual Environment (VE) under different auralization and scene arrangement conditions. The novel task is designed to encourage simultaneous conversation and movement, with the aim of increasing the relevance of spatial hearing. Results are obtained through questionnaires measuring social presence and plausibility, as well as through conversational and behavioral analysis. Although participants are shown to favor binaural auralization over diotic audio in a direct active-listening comparison, no significant differences in social presence, plausibility, or communication behavior could be found. Our results suggest that spatial audio may not affect user experience in dyadic communication scenarios where spatial auditory information is not directly relevant to the considered task.},
  keywords={Three-dimensional displays;Spatial audio;Virtual environments;Collaboration;Oral communication;User interfaces;Media;Human-centered computing;Human computer interaction (HCI);HCI and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00104},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494119,
  author={Merz, Christian and Göttfert, Christopher and Wienrich, Carolin and Latoschik, Marc Erich},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Universal Access for Social XR Across Devices: The Impact of Immersion on the Experience in Asymmetric Virtual Collaboration}, 
  year={2024},
  volume={},
  number={},
  pages={859-869},
  abstract={This article investigates the influence of input/output device characteristics and degrees of immersion on the User Experience (UX) of specific eXtended Reality (XR) effects, i.e., presence, self-perception, other-perception, and task perception. It targets universal access to social XR, where dedicated XR hardware is unavailable or can not be used, but participation is desirable or even necessary. We compare three different device configurations: (i) desktop screen with mouse, (ii) desktop screen with tracked controllers, and (iii) Head-Mounted Display (HMD) with tracked controllers. 87 participants took part in collaborative dyadic interaction (a sorting task) with asymmetric device configurations in a specifically developed social XR. In line with prior research, the sense of presence and embodiment were significantly lower for the desktop setups. However, we only found minor differences in task load and no differences in usability and enjoyment of the task between the conditions. Additionally, the perceived humanness and virtual human plausibility of the other were not affected, no matter the device used. Finally, there was no impact regarding co-presence and social presence independent of the level of immersion of oneself or the other. We conclude that the device in social XR is important for self-perception and presence. However, our results indicate that the devices do not affect important UX and usability aspects, specifically, the qualities of social interaction in collaborative scenarios, paving the way for universal access to social XR encounters and significantly promoting participation.},
  keywords={Target tracking;Three-dimensional displays;Collaboration;Hardware;User experience;X reality;Task analysis;VR;XR;social VR;immersion;co-presence;social presence;asymmetric collaboration;dyadic;Human-centered computing;Collaborative and social computing;Empirical studies in collaborative and social computing},
  doi={10.1109/VR58804.2024.00105},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494187,
  author={Mandl, David and Mori, Shohei and Mohr, Peter and Peng, Yifan and Langlotz, Tobias and Schmalstieg, Dieter and Kalkofen, Denis},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Neural Bokeh: Learning Lens Blur for Computational Videography and Out-of-Focus Mixed Reality}, 
  year={2024},
  volume={},
  number={},
  pages={870-880},
  abstract={We present Neural Bokeh, a deep learning approach for synthesizing convincing out-of-focus effects with applications in Mixed Reality (MR) image and video compositing. Unlike existing approaches that solely learn the amount of blur for out-of-focus areas, our approach captures the overall characteristic of the bokeh to enable the seamless integration of rendered scene content into real images, ensuring a consistent lens blur over the resulting MR composition. Our method learns spatially varying blur shapes, i.e., bokeh, from a dataset of real images acquired using the physical camera that is used to capture the photograph or video of the MR composition. Accordingly, those learned blur shapes mimic the characteristics of the physical lens. As the run-time and the resulting quality of Neural Bokeh increase with the resolution of input images, we employ low-resolution images for the MR view finding at runtime and high-resolution renderings for compositing with high-resolution photographs or videos in an offline process. We envision a variety of applications, including visual enhancement of image and video compositing containing creative utilization of out-of-focus effects.},
  keywords={Visualization;Three-dimensional displays;Runtime;Image resolution;Shape;Mixed reality;Virtual reality;Mixed Reality;Computational Videography;Deep Learning;Bokeh;Depth of Field},
  doi={10.1109/VR58804.2024.00106},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494092,
  author={Wang, Xueqi and Li, Yue and Liang, Hai-Ning},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={MagicMap: Enhancing Indoor Navigation Experience in VR Museums}, 
  year={2024},
  volume={},
  number={},
  pages={881-891},
  abstract={Museum visitors are typically advised to follow trajectories planned by curators. Nevertheless, the diverse locomotion techniques available in Virtual Reality (VR) offer various navigation methods that are unattainable within physical museum spaces. Interestingly, these techniques have rarely been explored within museum settings. Our study aims to investigate appropriate navigation methods in VR museums. We first conducted a study in a virtual reconstruction of a local museum with the following navigation methods: a 2D minimap, a World-in-Miniature (WiM) system, and a WiM map. Our results showed that the WiM map with a point-and-select interaction technique outperformed the other two regarding ease of learning, reduced workload, lessened motion sickness, and greater user preferences. Based on the findings, we improved the WiM map and introduced MagicMap. It builds upon the WiM map and translates the curatorial principles of museum visiting into a hierarchical menu layout. Our further evaluation showed that MagicMap supported prolonged engagement in VR museums, enhanced system usability and overall user experience, and reduced users’ perceived workload. Our findings have implications for the future design of navigation systems in VR museums and complex indoor environments.},
  keywords={Navigation;Design methodology;Virtual reality;Motion sickness;User interfaces;Museums;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;HCI design and evaluation methods;User studies},
  doi={10.1109/VR58804.2024.00107},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494177,
  author={Yang, Jackie Junrui and Qiu, Leping and Corona-Moreno, Emmanuel Angel and Shi, Louisa and Bui, Hung and Lam, Monica S. and Landay, James A.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={AMMA: Adaptive Multimodal Assistants Through Automated State Tracking and User Model-Directed Guidance Planning}, 
  year={2024},
  volume={},
  number={},
  pages={892-902},
  abstract={Novel technologies such as augmented reality and computer perception lay the foundation for smart assistants that can guide us through real-world tasks, such as cooking or home repair. However, the nature of real-world interaction requires assistants that adapt to users’ mistakes, environments, and communication preferences. We propose Adaptive Multimodal Assistants (AMMA), a software architecture for task guidance with generated adaptive interfaces from step-by-step instructions. This is achieved through 1) an automatically generated user action state tracker and 2) a guidance planner that leverages a continuously trained user model. The assistant also adjusts its guidance and communication delivery methods based on observed user performance as well as implicit and explicit user feedback. We demonstrated the viability of AMMA by building an adaptive cooking assistant running in a high-fidelity virtual reality-based simulator. A user study of the cooking assistant showed that AMMA can reduce the task completion time and the number of manual communication methods changes.},
  keywords={Solid modeling;Adaptation models;Adaptive systems;Three-dimensional displays;Navigation;Software architecture;Virtual assistants;Augmented reality;interface generation;smart assistant;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interactive systems and tools;User interface toolkits},
  doi={10.1109/VR58804.2024.00108},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494106,
  author={Zhang, Yan and You, Keyao and Hu, Xiaodan and Zhou, Hangyu and Kiyokawa, Kiyoshi and Yang, Xubo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Retinotopic Foveated Rendering}, 
  year={2024},
  volume={},
  number={},
  pages={903-912},
  abstract={Foveated rendering (FR) improves the rendering performance of virtual reality (VR) by allocating fewer computational loads in the peripheral field of view (FOV). Existing FR techniques are built based on the radially symmetric regression model of human visual acuity. However, horizontal-vertical asymmetry (HVA) and vertical meridian asymmetry (VMA) in the cortical magnification factor (CMF) of the human visual system have been evidenced by retinotopy research of neuroscience, suggesting the radially asymmetric regression of visual acuity. In this paper, we begin with functional magnetic resonance imaging (fMRI) data, construct an anisotropic CMF model of the human visual system, and then introduce the first radially asymmetric regression model of the rendering precision for FR applications. We conducted a pilot experiment to adapt the proposed model to VR head-mounted displays (HMDs). A user study demonstrates that retinotopic foveated rendering (RFR) provides participants with perceptually equal image quality compared to typical FR methods while reducing fragments shading by 27.2% averagely, leading to the acceleration of 1/6 for graphics rendering. We anticipate that our study will enhance the rendering performance of VR by bridging the gap between retinotopy research in neuroscience and computer graphics in VR.},
  keywords={Solid modeling;Adaptation models;Visualization;Neuroscience;Computational modeling;Virtual reality;Visual systems;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Rendering},
  doi={10.1109/VR58804.2024.00109},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494136,
  author={Boffi, Paolo and Kouyoumdjian, Alexandre and Waldner, Manuela and Lanzi, Pier Luca and Viola, Ivan},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={BaggingHook: Selecting Moving Targets by Pruning Distractors Away for Intention-Prediction Heuristics in Dense 3D Environments}, 
  year={2024},
  volume={},
  number={},
  pages={913-923},
  abstract={Selecting targets in dense, dynamic 3D environments presents a significant challenge. In this study, we introduce two novel selection techniques based on distractor pruning to assist users in selecting targets moving unpredictably: BaggingHook and AutoBaggingHook. Both are built upon the Hook intention-prediction heuristic, which continuously measures the distance between the user’s cursor and each object to compute per-object scores and estimate the intended target. Our techniques reduce the number of targets in the environment, making heuristic convergence potentially faster. Once pruned away, distractors are also made semi-transparent to reduce occlusion and the overall difficulty of the task. However, their motion is not altered, so that users can still perceive the dynamics of the environment. We designed two pruning approaches: BaggingHook lets users manually prune distractors away, while AutoBaggingHook uses automated, score-based pruning. We conducted a user study in a virtual reality setting inspired by molecular dynamics simulations, featuring crowded scenes of objects moving fast and unpredictably, in 3D. We compared both proposed techniques to the Hook baseline under more challenging circumstances than it had previously been tested. Our results show that AutoBaggingHook was the fastest, and did not lead to higher error rates. BaggingHook, on the other hand, was preferred by the majority of participants, due to the greater degree of control it provides to users, leading some to see entertainment value in its use. This work shows the potential benefits of varying the types of inputs used in intention-prediction heuristics, not just to improve performance, but also to reduce occlusion, overall task load, and improve user experience.},
  keywords={Solid modeling;Three-dimensional displays;Error analysis;Heuristic algorithms;Dynamics;Virtual reality;User interfaces;Algorithms;Interaction Design;Human-Subjects Qualitative Studies;Human-Subjects Quantitative Studies;Mobile;AR/VR/Immersive;Specialized Input/Display Hardware},
  doi={10.1109/VR58804.2024.00110},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494164,
  author={Sin, Zackary P. T. and Jia, Ye and Li, Richard Chen and Leong, Hong Va and Li, Qing and Ng, Peter H. F.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={illumotion: An Optical-illusion-based VR Locomotion Technique for Long-Distance 3D Movement}, 
  year={2024},
  volume={},
  number={},
  pages={924-934},
  abstract={Locomotion has a marked impact on user experience in VR, but currently, common to-go techniques such as steering and teleportation have their limitations. Particularly, steering is prone to cybersickness, while teleportation trades presence for mitigating cybersickness. Inspired by how we manipulate a picture on a mobile phone, we propose illumotion, an optical-illusion-based method that, we believe, can provide an alternative to these two typical techniques. Instead of zooming in a picture by pinching two fingers, we can move forward by “zooming” toward part of the 3D virtual scene with pinched hands. Not only is the proposed technique easy to use, it also seems to minimize cybersickness to some degree.illumotion relies on the manipulation of optics; as such, it requires solving motion parameters in screen space and a model of how we perceive depth. To evaluate it, a comprehensive user study with 66 users was conducted. Results show that, compared with either teleportation, steering or both, illumotion has better performance, presence, usability, user experience and cybersickness alleviation. We believe the result is a clear indication that our novel optically-driven method is a promising candidate for generalized locomotion.},
  keywords={Three-dimensional displays;Cybersickness;Teleportation;User interfaces;Optical imaging;User experience;Mobile handsets;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00111},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494133,
  author={Or, Eden and Maidenbaum, Shachar},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={When Vision Lies - Navigating Virtual Environments with Unreliable Visual Information}, 
  year={2024},
  volume={},
  number={},
  pages={935-944},
  abstract={Humans typically utilize vision in a dominant role for navigation. However, what happens when vision becomes actively unreliable? Will it impair user performance, be suppressed, or be used advantageously? While such scenarios are rare in the real world, this question has important implications for multisensory integration in extended reality applications - e.g. virtual walls that a user sees but can walk through. We created virtual mazes which could be solved via audio or visual cues. We then manipulated the reliability of these sensory channels by including invisible walls which are not perceived but still blocked passage, and ghost walls which could be perceived but did not block participants. Participants navigated the exact same layouts under all conditions, and could solve these levels by ignoring the unreliable sensory modality and using only the other. Participants easily completed these mazes using vision-only, and with some difficulty via audition-only. Partially unreliable vision degraded performance, though still above audio-only demonstrating utilization of the unreliable visual cues. Mazes whose entire visual input was false degraded performance to the level of audio only, though participants subjectively reported it as easier then audio-only and did not close their eyes indicating that they still relied on vision. Testing a control in which visual information was both false and constantly moved, preventing its use as landmarks or optic flow, indeed caused participants to close their eyes, disregarding the false vision, but was accompanied by confounding nausea. In parallel, auditory incongruencies were easily suppressed across all unreliable auditory conditions. This demonstrates human attachment to visual information, even when mostly or completely false, and the ability to glean practical advantages from it unless it is completely stripped from usability. More broadly it lays a foundation for testing multisensory integration of sustained false sensory channels, and has implications for mixed reality design.},
  keywords={Visualization;Three-dimensional displays;Navigation;Virtual environments;User interfaces;Multisensory integration;Reliability;Virtual Environment;Virtual Reality;Multisensory;Multimodal;Vision;Audition;Sensory Substitution;Navigation},
  doi={10.1109/VR58804.2024.00112},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494195,
  author={Froehlich, Fabian and Hovey, Chris and Reza, Sameen and Plass, Jan L.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Beyond Slideshows – Investigating the Impact of Immersive Virtual Reality on Science Learning}, 
  year={2024},
  volume={},
  number={},
  pages={945-950},
  abstract={We investigated the effectiveness of immersive virtual reality (VR) for science learning by comparing an VR environment with traditional learning. One group learned about cell biology with a headmounted display using interactive simulation modules in VR, the other with a slideshow presenting the same materials. This study focused on the research question: “Does learning in immersive VR designed to take advantage of VR affordances lead to higher learning outcomes and affective outcomes compared to traditional instruction?” In a quasi-experimental between-subject control group design $(\mathrm{N}=63)$, we measured students’ recall, comprehension, interest, motivation, and experienced emotions. Results indicate that the VR group scored significantly higher than the control group on some dimensions of the post assessment. In addition, VR learners reported higher scores of positive affect and interest. The results of this study support the idea that an affordances approach to the design of VR for learning results in increased learning outcomes and that by using such an approach, VR can be a viable tool for science learning.},
  keywords={Solid modeling;Three-dimensional displays;Affordances;Computational modeling;Education;Virtual reality;User interfaces;Immersive VR;STEM Learning;Affective Learning;Technology-Enhanced Classrooms;H.1.2 [Models and Principles];User/Machine Systems;Human Factors;I.3.7 [Computing Graphics];Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VR58804.2024.00113},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494189,
  author={Uda, Masafumi and Nakamoto, Takamichi},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Influence of User’s Body in Olfactory Virtual Environment Generated by Real-Time CFD}, 
  year={2024},
  volume={},
  number={},
  pages={951-959},
  abstract={We have developed a virtual olfactory environment using an olfactory display and computational fluid dynamics (CFD) simulation to provide odor concentration change according to user’s movement in real-time in a virtual space. Although CFD can calculate the odor distribution in the complicated geometry, its computational cost was expensive and did not work in real-time in the previous study. In this study, real-time CFD based on GPU calculation was introduced to generate olfactory VR environment. Using real-time CFD we investigated influence of the user’s body with its location and orientation changing irregularly. In the sensory test to find the odor direction, the correct answer rate was over 70% when the body influence was considered, while it was just a chance hit when the influence was not considered. The experimental result indicates the usefulness of considering the effect of the user’s body since we cannot avoid that influence.},
  keywords={Geometry;Three-dimensional displays;Fluids;Computational fluid dynamics;Olfactory;Virtual environments;Graphics processing units;sense of smell;olfactory information;computational fluid dynamics;Computing methodologies-Modeling and simulation evaluation;Human-centered computing-Interaction paradigms-Virtual reality},
  doi={10.1109/VR58804.2024.00114},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494169,
  author={Zhu, Hanxin and Chen, Zhibo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency}, 
  year={2024},
  volume={},
  number={},
  pages={960-968},
  abstract={Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (i.e., multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods.},
  keywords={Geometry;Three-dimensional displays;Image color analysis;User interfaces;Rendering (computer graphics);Augmented reality;Neural Radiance Fields;Few-shot view synthesis;Multiplane Images;Cross-view consistency},
  doi={10.1109/VR58804.2024.00115},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494082,
  author={Ban, Hyunmin and Choi, Seungmi and Cha, Jun Yeong and Kim, Yeongwoong and Kim, Hui Yong},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={NHVC: Neural Holographic Video Compression with Scalable Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={969-978},
  abstract={Recently, neural network-based approaches for hologram generation and compression have gained popularity as they allow for efficient inference on GPUs without the need for iterative optimization required in traditional methods. In this paper, we introduce Neural Holographic Video Compression (NHVC), an end-to-end trainable and scalable model designed for high-quality phase hologram video generation and compression. NHVC consists of an auto-encoder-based phase hologram generator, a latent coder and-two hyper-prior coders. For each input image, the latent features are extracted through the encoder part of the phase generator and then entropy coded at the shared latent coder based on the hyper-prior information. The two hyper-prior coders employ a spatial and a spatio-temporal entropy model for I-frames and P-frames, respectively. With this architecture, our NHVC can offer task-scalability, allowing a single trained model to serve as a phase hologram generator, phase hologram image compressor, or phase hologram video compressor as required.Experimental results on phase hologram video compression with UVG dataset show that our model outperforms ‘HoloNet + VVC’ by 75.6% BD-Rate reduction, with modest 2K encoding and decoding speeds (5 fps and 12 fps, respectively). For the phase hologram video generation task, our model showed much higher-quality (almost 42dB PSNR) reconstruction using the UVG dataset, while the previous neural generation model HoloNet provides at most 36dB reconstruction quality. We also provide an extensive experimental study on several important design questions such as the need for quadruple extension (QE) in the neural compression model, the feasibility of motion estimation in the phase domain, and an alternative, the need for increasing receptive field to learn better phase features, and variable rate support with a single trained model. It is noteworthy that our model is the first and best neural phase video compression model providing such high-quality reconstruction and task-scalability.},
  keywords={Solid modeling;Image coding;Three-dimensional displays;Image resolution;Image color analysis;Video compression;Feature extraction;Computing methodologies;Computer graphics;Image manipulation;Image processing;Image compression},
  doi={10.1109/VR58804.2024.00116},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494168,
  author={Bozgeyikli, Evren and Bozgeyikli, Lal “Lila” and Gomes, Victor},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Googly Eyes: Exploring Effects of Displaying User’s Eyes Outward on a Virtual Reality Head-Mounted Display on User Experience}, 
  year={2024},
  volume={},
  number={},
  pages={979-989},
  abstract={Head-mounted displays (HMDs) in virtual reality (VR) occlude the upper face of the wearing users, leading to decreased nonverbal communication cues toward outside users. In this paper, we discuss Googly Eyes, a high-fidelity prototype that displays an illustration of the HMD-wearing user’s eyes in real-time in front of an HMD. We explored the effects of the Googly Eyes on the user experience of non-HMD users. For this, we designed and developed a collaborative asymmetrical co-located task performed by an HMD-wearing user and a non-HMD (tablet) user, and we conducted a between-subjects user study where we compared the Googly Eyes with a baseline HMD experience without any external eye depiction. In this paper, we discuss the system, task, user study details, and results along with implications for future studies.},
  keywords={Head-mounted displays;Three-dimensional displays;Prototypes;Virtual reality;Resists;User interfaces;User experience;Virtual reality;displaying eyes outward;eye tracking;co-located experiences;collaboration;co-presence;Human-centered computing;Human computer interaction (HCI);Interaction paradigms},
  doi={10.1109/VR58804.2024.00117},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494072,
  author={Skarbez, Richard and Jiang, Dai},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Scientometric History of IEEE VR}, 
  year={2024},
  volume={},
  number={},
  pages={990-999},
  abstract={As of IEEE VR 2023, there have been 30 installments of the IEEE Virtual Reality conference (VR) or its predecessor, the Virtual Reality Annual International Symposium (VRAIS). As such, it seems an opportune time to reflect on the intellectual history of the conference, and by extension, the VR research community. This article uses scientometric techniques to undertake such an intellectual history, using co-word analysis and citation analysis to identify core themes and trends in VR research over time. We identify the papers that have stood the test of time, the most esteemed authors and researchers in the IEEE VR community, and the topics that have shaped our field to date.},
  keywords={Visualization;Three-dimensional displays;Citation analysis;Virtual reality;User interfaces;Motors;Rendering (computer graphics);bibliometrics;scientometrics;history;survey;General and reference;Surveys and overviews;Social and professional topics;History of computing;Human-centered computing;Virtual reality},
  doi={10.1109/VR58804.2024.00118},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494123,
  author={Jin, Tao and Wu, Shengxi and Dasari, Mallesham and Apicharttrisorn, Kittipat and Rowe, Anthony},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={StageAR: Markerless Mobile Phone Localization for AR in Live Events}, 
  year={2024},
  volume={},
  number={},
  pages={1000-1010},
  abstract={Localizing mobile phone users precisely enough to provide AR content in theaters and concert venues is extremely challenging due to dynamic staging and variable lighting. Visual markers are often disruptive in terms of aesthetics, and static pre-defined feature maps are not robust to visual changes. In this paper, we study several techniques that leverage sparse fixed infrastructure to monitor and adapt to changes in the environment at runtime to enable robust AR quality pose tracking for large audiences. Our most basic technique uses one or more fixed cameras in the environment to prune away poor feature points due to motion and lighting from a static model. For more challenging environments, we propose transmitting dynamic 3D feature maps that adapt to changes in the scene in real-time. Users with a mobile phone camera can use these maps to accurately localize across highly dynamic environments without explicit markers. We show the performance trade-offs resulting from StageAR’s different reconstruction techniques, ranging from multiple stereo cameras to cameras paired with LiDAR. We evaluate each approach in our system across a wide variety of simulated and real environments at auditorium/theater scale and find that our most accurate technique can match the performance of large ($1.5 \times 1.5{\mathrm {m}}$) back-lit static markers without being visible to users.},
  keywords={Visualization;Three-dimensional displays;Tracking;Dynamics;Lighting;User interfaces;Cameras;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR58804.2024.00119},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494170,
  author={Gagnon, Holly C. and Finney, Hunter and Stefanucci, Jeanine K. and Bodenheimer, Bobby and Creem-Regehr, Sarah H.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reaching Between Worlds: Calibration and Transfer of Perceived Affordances from Virtual to Real Environments}, 
  year={2024},
  volume={},
  number={},
  pages={1011-1021},
  abstract={Accurate perception of one’s action capabilities, or affordance perception, is essential for successful interaction with both real and virtual environments. Affordance perception can potentially be improved by receiving feedback. It is unknown what specific types of feedback are needed for improvements in affordance perception to occur, particularly in virtual environments where cues may be impoverished. The current work studied perception of horizontal reachability in virtual and augmented reality (VR and AR), specifically whether it would improve with feedback, and if any improvement transferred to the real world. Multiple types of feedback were studied in VR or AR: exploratory behavior, static outcome, and action outcome feedback. Our results indicate that exploratory behavior is sufficient for improvement in perceived reachability in VR, but in AR, outcome feedback is necessary. In both VR and AR, outcome feedback was required for improvement in perceived reachability to transfer to the real world. These findings have practical implications for training in virtual environments. If virtual environments are used for training actions that ultimately need to be performed in the real world, outcome feedback should be provided.},
  keywords={Training;Three-dimensional displays;Affordances;Virtual environments;User interfaces;Reliability engineering;Calibration;Applied computing;Psychology;Human-centered computing;Mixed/augmented reality;Virtual reality;Empirical studies in HCI},
  doi={10.1109/VR58804.2024.00120},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494154,
  author={Gong, Ut and Jia, Hanze and Wang, Yujie and Tang, Tan and Xie, Xiao and Wu, Yingcai},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={VolleyNaut: Pioneering Immersive Training for Inclusive Sitting Volleyball Skill Development}, 
  year={2024},
  volume={},
  number={},
  pages={1022-1032},
  abstract={Participation in sports provides individuals with disabilities opportunities for social inclusion, improved physical and mental health, skill development, and increased self-confidence, ultimately empowering them. Sitting volleyball, a popular para-sport adapted from traditional volleyball, has been played in more than 75 countries since its development in 1956. However, the limited availability of dedicated sitting volleyball courts creates a significant gap for individuals with disabilities interested in playing the sport. To address the challenges encountered by amateur sitting volleyball players due to the lack of specialized facilities, we encompass a pioneering design study on VR para-sports training and introduce VolleyNaut - an innovative virtual reality (VR) training system. Developed in close collaboration with professional coaches, this immersive system faithfully replicates the daily drills and realistic ball pitches experienced by players. It offers four specialized basic defensive drill scenarios, contributing to skill adjustment and enhancement. In our user study, we recruited volleyball players from college teams and clubs to assess the engagement factor of VolleyNaut, and we also included national sitting volleyball players and coaches to evaluate the system’s effectiveness as a training tool. Our comprehensive analysis, combining quantitative and qualitative data, revealed consistently positive results across all user groups.},
  keywords={Training;Three-dimensional displays;Collaboration;Virtual reality;Mental health;User interfaces;Sports;Sitting volleyball;virtual reality;inclusive training;immersive sports training;accessible communities;SportsXR},
  doi={10.1109/VR58804.2024.00121},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494159,
  author={Oberdörfer, Sebastian and Birnstiel, Sandra and Latoschik, Marc Erich},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Influence of Virtual Shoe Formality on Gait and Cognitive Performance in a VR Walking Task}, 
  year={2024},
  volume={},
  number={},
  pages={1033-1043},
  abstract={Depending on their formality, clothes do not only change one’s appearance, but can also influence behavior and cognitive processes. Shoes are a special aspect of an outfit. Besides coming in various degrees of formality, their structure can affect human gait. Avatars used to embody users in immersive Virtual Reality (VR) can wear any kind of clothing. According to the Proteus Effect, the appearance of a user’s avatar can influence their behavior. Users change their behavior in accordance to the expected behavior of the avatar. In our study, we embody 39 participants with a generic avatar of the user’s gender wearing three different pairs of shoes as within condition. The shoes differ in degree of formality. We measure the gait during a 2-minute walking task during which participants wore the same real shoe and assess selective attention using the Stroop task. Our results show significant differences in gait between the tested virtual shoe pairs. We found small effects between the three shoe conditions with respect to selective attention. However, we found no significant differences with respect to correct items and response time in the Stroop task. Thus, our results indicate that virtual shoes are accepted by users and, although not eliciting any physical constraints, lead to changes in gait. This suggests that users not only adjust personal behavior according to the Proteus Effect, but also are affected by virtual biomechanical constraints. Also, our results suggest a potential influence of virtual clothing on cognitive performance.},
  keywords={Legged locomotion;Three-dimensional displays;Cognitive processes;Avatars;Footwear;User interfaces;Particle measurements;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Empirical studies in HCI},
  doi={10.1109/VR58804.2024.00122},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494126,
  author={Taibo, Javier and Iglesias-Guitian, Jose A.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive 3D Medical Visualization in Virtual Reality using Stereoscopic Volumetric Path Tracing}, 
  year={2024},
  volume={},
  number={},
  pages={1044-1053},
  abstract={Scientific visualizations using physically-based lighting models play a crucial role in enhancing both image quality and realism. In the domain of medical visualization, this trend has gained significant traction under the term cinematic rendering (CR). It enables the creation of 3D photorealistic reconstructions from medical data, offering great potential for aiding healthcare professionals in the analysis and study of volumetric datasets. However, the adoption of such advanced rendering for immersive virtual reality (VR) faces two main limitations related to their high computational demands. First, these techniques are frequently used to produce pre-recorded videos and offline content, thereby restricting interactivity to predefined volume appearance and lighting settings. Second, when deployed in head-tracked VR environments they can induce cyber-sickness symptoms due to the disturbing flicker caused by noisy Monte Carlo renderings. Consequently, the scope for meaningful interactive operations is constrained in this modality, in contrast with the versatile capabilities of classical direct volume rendering (DVR). In this work, we introduce an immersive 3D medical visualization system capable of producing photorealistic and fully interactive stereoscopic visualizations on head-mounted display (HMD) devices. Our approach extends previous linear regression denoising to enable real-time stereoscopic cinematic rendering within AR/VR settings. We demonstrate the capabilities of the resulting VR system, like its interactive rendering, appearance and transfer function editing.},
  keywords={Solid modeling;Three-dimensional displays;Stereo image processing;Lighting;Data visualization;Transfer functions;Virtual reality;Computing methodologies;Virtual reality;Ray tracing;Human-centered computing;Graphical user interfaces},
  doi={10.1109/VR58804.2024.00123},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10494171,
  author={Gao, BoYu and Zheng, Haojun and Zhao, Jingbo and Tu, Huawei and Kim, HyungSeok and Duh, Henry Been-Lirn},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Plausible Preference of Body-Centric Locomotion using Subjective Matching in Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={1054-1064},
  abstract={Body-centric locomotion in Virtual Reality (VR) involves multiple factors, including the point of view, avatar representations, tracked body parts for locomotion control and transfer functions that map body movement to the displacement of the virtual viewpoint. Understanding the role of these factors in evoking a plausible walking experience using within- or between-subject experimental designs based on questionnaires and/or objective measurements can be time-consuming and challenging due to the interrelated effects of these factors. This study employed the subjective matching method to evaluate the sense of plausible walking experience during body-centric locomotion in VR. Five relevant factors that may affect locomotion experience were identified by analyzing existing studies, i.e., point of view, the avatar appearance, body parts for locomotion control, transfer functions and the coefficients of transfer functions. A virtual locomotion experiment with these five factors based on subjective matching was conducted. Results showed that participants regarded the point of view as the most critical factor for walking experience enhancement, followed by body parts, transfer functions, the coefficients of transfer functions and finally the avatar appearance. Additionally, participants’ preferences for different body parts and the coefficients of transfer functions affected the choice of transfer functions. These results could serve as the guidelines for virtual locomotion experience design that involves combinations of multiple factors and can help achieve a plausible walking experience in VR.},
  keywords={Legged locomotion;Three-dimensional displays;Tracking;Avatars;Transfer functions;User interfaces;Guidelines;Human-centered computing;Virtual Reality;User study and evaluation methods},
  doi={10.1109/VR58804.2024.00124},
  ISSN={2642-5254},
  month={March},}
