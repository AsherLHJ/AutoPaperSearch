@INPROCEEDINGS{10108425,
  author={Zhang, Yizhong and Li, Zhiqi and Xu, Sicheng and Li, Chong and Yang, Jiaolong and Tong, Xin and Guo, Baining},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Recent research advance has significantly improved the visual real-ism of immersive 3D video communication. In this work we present a method to further enhance this immersive experience by adding the hand touch capability (“remote hand clapping”). In our system, each meeting participant sits in front of a large screen with haptic feedback. The local participant can reach his hand out to the screen and perform hand clapping with the remote participant as if the two participants were only separated by a virtual glass. A key challenge in emulating the remote hand touch is the realistic rendering of the participant's hand and arm as the hand touches the screen. When the hand is very close to the screen, the RGBD data required for realistic rendering is no longer available. To tackle this challenge, we present a dual representation of the user's hand. Our dual representation not only preserves the high-quality rendering usually found in recent image-based rendering systems but also allows the hand to reach to the screen. This is possible because the dual representation includes both an image-based model and a 3D geometry-based model, with the latter driven by a hand skeleton tracked by a side view camera. In addition, the dual representation provides a distance-based fusion of the image-based and 3D geometry-based models as the hand moves closer to the screen. The result is that the image-based and 3D geometry-based models mutually enhance each other, leading to realistic and seamless rendering. Our experiments demonstrate that our method provides consistent hand contact experience between remote users and improves the immersive experience of 3D video communication.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Tracking;Immersive experience;Glass;User interfaces;Human-centered computing-Collaborative and social computing;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR55154.2023.00016},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108433,
  author={Chen, Junjie and Li, Chenhui and Song, Sicheng and Wang, Changbo},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={iARVis: Mobile AR Based Declarative Information Visualization Authoring, Exploring and Sharing}, 
  year={2023},
  volume={},
  number={},
  pages={11-21},
  abstract={We present iARVis, a proof-of-concept toolkit for creating, experiencing, and sharing mobile AR-based information visualization environments. Over the past years, AR has emerged as a promising medium for information and data visualization beyond the physical media and the desktop, enabling interactivity and eliminating spatial limits. However, the creation of such environments remains difficult and frequently necessitates low-level programming expertise and lengthy hand encodings. We present a declarative approach for defining the augmented reality (AR) environment, including how information is automatically positioned, laid out, and interacted with, to improve the efficiency and flexibility of constructing AR-based information visualization environments. We provide fundamental layout and visual components such as the grid, rich text, images, and charts for the development of complex visualization widgets, as well as automatic targeting methods based on image and object tracking for the development of the AR environment. To increase design efficiency, we also provide features such as hot-reload and several creation levels for both novice and advanced users. We also investigate how the augmented reality-based visualization environment could persist and be shared through the internet and provide ways for storing, sharing, and restoring the environment to give a continuous and seamless experience. To demonstrate the viability and extensibility, we evaluate iARVis using a variety of use cases along with performance evaluation and expert reviews.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Layout;Data visualization;Media;Real-time systems;Visualization systems and tools;Augmented Reality},
  doi={10.1109/VR55154.2023.00017},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108465,
  author={Bao, Yiwei and Wang, Jiaxi and Wang, Zhimin and Lu, Feng},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring 3D Interaction with Gaze Guidance in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={22-32},
  abstract={Recent research based on hand-eye coordination has shown that gaze could improve object selection and translation experience under certain scenarios in AR. However, several limitations still exist. Specifically, we investigate whether gaze could help object selection with heavy 3D occlusions and help 3D object translation in the depth dimension. In addition, we also investigate the possibility of reducing the gaze calibration burden before use. Therefore, we develop new methods with proper gaze guidance for 3D interaction in AR, and also an implicit online calibration method. We conduct two user studies to evaluate different interaction methods and the results show that our methods not only improve the effectiveness of occluded objects selection but also alleviate the arm fatigue problem significantly in the depth translation task. We also evaluate the proposed implicit online calibration method and find its accuracy comparable to standard 9 points explicit calibration, which makes a step towards practical use in the real world.},
  keywords={Three-dimensional displays;Design methodology;Estimation;User interfaces;Fatigue;Calibration;Task analysis;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality},
  doi={10.1109/VR55154.2023.00018},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108413,
  author={Belani, Manshul and Singh, Harsh Vardhan and Parnami, Aman and Singh, Pushpendra},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Spatial Representation of Learning Content in Virtual Reality Learning Environments}, 
  year={2023},
  volume={},
  number={},
  pages={33-43},
  abstract={A recent surge in the application of Virtual Reality in education has made VR Learning Environments (VRLEs) prevalent in fields ranging from aviation, medicine, and skill training to teaching factual and conceptual content. In spite of multiple 3D affordances provided by VR, learning content placement in VRLEs has been mostly limited to a static placement in the environment. We conduct two studies to investigate the effect of different spatial representations of learning content in virtual environments on learning outcomes and user experience. In the first study, we studied the effects of placing content at four different places - world-anchored (TV screen placed in the environment), user-anchored (panel anchored to the wrist or head-mounted display of the user) and object-anchored (panel anchored to the object associated with current content) - in the VR environment with forty-two participants in the context of learning how to operate a laser cutting machine through an immersive tutorial. In the follow-up study, twenty-two participants from this study were given the option to choose from these four placements to understand their preferences. The effects of placements were examined on learning outcome measures - knowledge gain, knowledge transfer, cognitive load, user experience, and user preferences. We found that participants preferred user-anchored (controller condition) and object-anchored placement. While knowledge gain, knowledge transfer, and cognitive load were not found to be significantly different between the four conditions, the object-anchored placement scored significantly better than the TV screen and head-mounted display conditions on the user experience scales of attractiveness, stimulation, and novelty.},
  keywords={Wrist;Three-dimensional displays;TV;Head-mounted displays;Virtual environments;Tutorials;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Interaction design-Empirical studies in interaction design},
  doi={10.1109/VR55154.2023.00019},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108429,
  author={Xu, Yang and Jiang, Yuanfa and Wang, Shibo and Li, Kang and Geng, Guohua},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Delta Path Tracing for Real-Time Global Illumination in Mixed Reality}, 
  year={2023},
  volume={},
  number={},
  pages={44-52},
  abstract={Visual coherence between real and virtual objects is important in mixed reality (MR), and illumination consistency is one of the key aspects to achieve coherence. Apart from matching the illumination of the virtual objects with the real environments, the change of illumination on the real scenes produced by the inserted virtual objects should also be considered but is difficult to compute in real-time due to the heavy computation demands of global illumination. In this work, we propose delta path tracing (DPT), which only computes the radiance blocked by the virtual objects from the light sources at the primary hit points of Monte Carlo path tracing, then combines the blocked radiance and multi-bounce indirect illumination with the image of the real scene. Multiple importance sampling (MIS) between BRDF and environment map is performed to handle all-frequency environment maps captured by a panorama camera. Compared to conventional differential rendering methods, our method can remarkably reduce the number of times required to access the environment map and avoid rendering scenes twice. Therefore, the performance can be significantly improved. We implement our method using hardware-accelerated ray tracing on modern GPUs, and the results demonstrate that our method can render global illumination at real-time frame rates and produce plausible visual coherence between real and virtual objects in MR environments.},
  keywords={Visualization;Monte Carlo methods;Three-dimensional displays;Lighting;Mixed reality;Virtual reality;Coherence;Mixed / augmented reality;Ray tracing},
  doi={10.1109/VR55154.2023.00020},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108411,
  author={Fan, Cheng-Wei and Xu, Sen-Zhe and Yu, Peng and Zhang, Fang-Lue and Zhang, Song-Hai},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirected Walking Based on Historical User Walking Data}, 
  year={2023},
  volume={},
  number={},
  pages={53-62},
  abstract={With redirected walking (RDW) technology, people can explore large virtual worlds in smaller physical spaces. RDW controls the trajectory of the user's walking in the physical space through subtle adjustments, so as to minimize the collision between the user and the physical space. Previous predictive algorithms place constraints on the user's path according to the spatial layouts of the virtual environment and work well when applicable, while reactive algorithms are more general for scenarios involving free exploration or uncon-strained movements. However, even in relatively free environments, we can predict the user's walking to a certain extent by analyzing the user's historical walking data, which can help the decision-making of reactive algorithms. This paper proposes a novel RDW method that improves the effect of real-time unrestricted RDW by analyzing and utilizing the user's historical walking data. In this method, the physical space is discretized by considering the user's location and orientation in the physical space. Using the weighted directed graph obtained from the user's historical walking data, we dynamically update the scores of different reachable poses in the physical space during the user's walking. We rank the scores and choose the optimal target position and orientation to guide the user to the best pose. Since simulation experiments have been shown to be effective in many previous RDW studies, we also provide a method to simulate user walking trajectories and generate a dataset. Experiments show that our method outperforms multiple state-of-the-art methods in various environments of different sizes and spatial layouts.},
  keywords={Legged locomotion;Solid modeling;Three-dimensional displays;Layout;Virtual environments;Aerospace electronics;User interfaces;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR55154.2023.00021},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108438,
  author={Wysopal, Abby and Ross, Vivian and Passananti, Joyce and Yu, Kangyou and Huynh, Brandon and Höllerer, Tobias},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Level-of-Detail AR: Dynamically Adjusting Augmented Reality Level of Detail Based on Visual Angle}, 
  year={2023},
  volume={},
  number={},
  pages={63-71},
  abstract={Dynamically adjusting the content of augmented reality (AR) applications to efficiently display information best fitting the available screen estate may be important for user performance and satisfaction. Currently, there is not a common practice for dynamically adjusting the content of AR applications based on their apparent size in the user's view of the surround environment. We present a Level-of-Detail AR mechanism to improve the usability of AR applications at any relative size. Our mechanism dynamically renders textual and interactable content based on its legibility, interactability, and viewability respectively. When tested, Level-of-Detail AR functioned as intended out-of-the-box on 44 of the 45 standard user interface Unity prefabs in Microsoft's Mixed Reality Tool Kit. We additionally evaluated impact on task performance, user distance, and subjective satisfaction through a mixed-design user study with 45 participants. Statistical analysis of our results revealed significant task-dependent differences in user performance between the modes. User satisfaction was consistently higher for the Level-of-Detail AR condition.},
  keywords={Measurement;Visualization;Three-dimensional displays;Statistical analysis;Mixed reality;User interfaces;Task analysis;Level of Detail;Augmented Reality;Automation;User Study;Task Performance;User Satisfaction},
  doi={10.1109/VR55154.2023.00022},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108476,
  author={Rendle, Gareth and Kreskowski, Adrian and Froehlich, Bernd},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Volumetric Avatar Reconstruction with Spatio-Temporally Offset RGBD Cameras}, 
  year={2023},
  volume={},
  number={},
  pages={72-82},
  abstract={RGBD cameras can capture users and their actions in the real world for reconstruction of photo-realistic volumetric avatars that allow rich interaction between spatially distributed telepresence parties in virtual environments. In this paper, we present and evaluate a system design that enables volumetric avatar reconstruction at increased frame rates. We demonstrate that we can overcome the limited capturing frame rate of commodity RGBD cameras such as the Azure Kinect by dividing a set of cameras into two spatio-temporally offset reconstruction groups and implementing a real-time reconstruction pipeline to fuse the temporally offset RGBD image streams. Comparisons of our proposed system against capture configurations possible with the same number of RGBD cameras indicate that it is beneficial to use a combination of spatially and temporally offset RGBD cameras, allowing increased reconstruction frame rates and scene coverage while producing temporally consistent volumetric avatars.},
  keywords={Three-dimensional displays;Telepresence;Avatars;Virtual environments;User interfaces;Streaming media;Cameras;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR55154.2023.00023},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108469,
  author={Otono, Riku and Genay, Adélaïde and Perusquía-Hernández, Monica and Isoyama, Naoya and Uchiyama, Hideaki and Hachet, Martin and Lécuyer, Anatole and Kiyokawa, Kiyoshi},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={I'm Transforming! Effects of Visual Transitions to Change of Avatar on the Sense of Embodiment in AR}, 
  year={2023},
  volume={},
  number={},
  pages={83-93},
  abstract={Virtual avatars are more and more often featured in Virtual Reality (VR) and Augmented Reality (AR) applications. When embodying a virtual avatar, one may desire to change of appearance over the course of the embodiment. However, switching suddenly from one appearance to another can break the continuity of the user experience and potentially impact the sense of embodiment (SoE), especially when the new appearance is very different. In this paper, we explore how applying smooth visual transitions at the moment of the change can help to maintain the SoE and benefit the general user experience. To address this, we implemented an AR system allowing users to embody a regular-shaped avatar that can be transformed into a muscular one through a visual effect. The avatar's transformation can be triggered either by the user through physical action (“active” transition), or automatically launched by the system (“passive” transition). We conducted a user study to evaluate the effects of these two types of transformations on the SoE by comparing them to control conditions where there was no visual feedback of the transformation. Our results show that changing the appearance of one's avatar with an active transition (with visual feedback), compared to a passive transition, helps to maintain the user's sense of agency, a component of the SoE. They also partially suggest that the Proteus effects experienced during the embodiment were enhanced by these transitions. Therefore, we conclude that visual effects controlled by the user when changing their avatar's appearance can benefit their experience by preserving the SoE and intensifying the Proteus effects.},
  keywords={Human computer interaction;Visualization;Three-dimensional displays;Avatars;Switches;Visual effects;User experience;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI},
  doi={10.1109/VR55154.2023.00024},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108449,
  author={Cortes, Carlos Alfredo Tirado and Lin, Chin-Teng and Do, Tien-Thong Nguyen and Chen, Hsiang-Ting},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={An EEG-based Experiment on VR Sickness and Postural Instability While Walking in Virtual Environments}, 
  year={2023},
  volume={},
  number={},
  pages={94-104},
  abstract={Previous studies showed that natural walking reduces the susceptibility to VR sickness. However, many users still experience VR sickness when wearing VR headsets that allow free walking in room-scale spaces. This paper studies VR sickness and postural instability while the user walks in an immersive virtual environment using an electroencephalogram (EEG) headset and a full-body motion capture system. The experiment induced VR sickness by gradually increasing the translation gain beyond the user's detection threshold. A between-group comparison between participants with and without VR sickness symptoms found some significant differences in postural stability but found none on gait patterns during the walking. In the EEG analysis, the group with VR sickness showed a reduction of alpha power, a phenomenon previously linked to a higher workload and efforts to maintain postural control. In contrast, the group without VR sickness exhibited brain activities linked to fine cognitive-motor control. The EEG result provides new insights into the postural instability theory: participants with VR sickness could maintain their postural stability at the cost of a higher cognitive workload. Our result also indicates that the analysis of lower-frequency power could complement behavioral data for continuous VR sickness detection in both stationary and mobile VR setups.},
  keywords={Legged locomotion;Headphones;Three-dimensional displays;Costs;Virtual environments;User interfaces;Electroencephalography;VR Sickness;Postural Instability;Virtual Reality;Translational Gain;EEG},
  doi={10.1109/VR55154.2023.00025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108456,
  author={Zhao, Jingbo and Shao, Mingjun and Wang, Yaojun and Xu, Ruolin},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Recognition of In-Place Body Actions and Head Gestures using Only a Head-Mounted Display}, 
  year={2023},
  volume={},
  number={},
  pages={105-114},
  abstract={Body actions and head gestures are natural interfaces for interaction in virtual environments. Existing methods for in-place body action recognition often require hardware more than a head-mounted display (HMD), making body action interfaces difficult to be introduced to ordinary virtual reality (VR) users as they usually only possess an HMD. In addition, there lacks a unified solution to recognize in-place body actions and head gestures. This potentially hinders the exploration of the use of in-place body actions and head gestures for novel interaction experiences in virtual environments. We present a unified two-stream 1-D convolutional neural network (CNN) for recognition of body actions when a user performs walking-in-place (WIP) and for recognition of head gestures when a user stands still wearing only an HMD. Compared to previous approaches, our method does not require specialized hardware and/or additional tracking devices other than an HMD and can recognize a significantly larger number of body actions and head gestures than other existing methods. In total, ten in-place body actions and eight head gestures can be recognized with the proposed method, which makes this method a readily available body action interface (head gestures included) for interaction with virtual environments. We demonstrate one utility of the interface through a virtual locomotion task. Results show that the present body action interface is reliable in detecting body actions for the VR locomotion task but is physically demanding compared to a touch controller interface. The present body action interface is promising for new VR experiences and applications, especially for VR fitness applications where workouts are intended.},
  keywords={Head-mounted displays;Three-dimensional displays;Virtual environments;Resists;User interfaces;Hardware;Real-time systems;Body Action Recognition;Head Gesture Recognition;Virtual Locomotion},
  doi={10.1109/VR55154.2023.00026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108446,
  author={Bambušek, Daniel and Materna, Zdeněk and Kapinus, Michal and Beran, Vítězslav and Smrž, Pavel},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={How Do I Get There? Overcoming Reachability Limitations of Constrained Industrial Environments in Augmented Reality Applications}, 
  year={2023},
  volume={},
  number={},
  pages={115-122},
  abstract={The paper presents an approach for handheld augmented reality in constrained industrial environments, where it might be hard or even impossible to reach certain poses within a workspace. Therefore, a user might be unable to see or interact with some digital content in applications like visual robot programming, robotic program visualizations, or workspace annotation. To overcome this limitation, we propose a temporal switching to a non-immersive virtual reality that allows the user to see the virtual counterpart of the workspace from any angle and distance, where the viewpoint is controlled using a unique combination of on-screen controls complemented by the physical motion of the handheld device. Using such a combination, the user can position the virtual camera roughly to the desired pose using the on-screen controls and then continue working just as in augmented reality. To explore how people would use it and what the benefits would be over pure augmented reality, we chose a representative task of object alignment and conducted a study. The results revealed that mainly physical demands, which is often a limiting factor for handheld augmented reality, could be reduced and that the usability and utility of the approach are rated as high. In addition, suggestions for improving the user interface were proposed and discussed.},
  keywords={Visualization;Limiting;Service robots;Annotations;Switches;User interfaces;Control systems;Augmented Reality;Virtual Reality;Transitional Interface;Human Robot Interaction;Constrained Industrial Environmnets},
  doi={10.1109/VR55154.2023.00027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108085,
  author={Hombeck, Jan and Voigt, Henrik and Heggemann, Timo and Datta, Rabi R. and Lawonn, Kai},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tell Me Where To Go: Voice-Controlled Hands-Free Locomotion for Virtual Reality Systems}, 
  year={2023},
  volume={},
  number={},
  pages={123-134},
  abstract={As locomotion is an important factor in improving Virtual Reality (VR) immersion and usability, research in this area has been and continues to be a crucial aspect for the success of VR applications. In recent years, a variety of techniques have been developed and evaluated, ranging from abstract control, vehicle, and teleportation techniques to more realistic techniques such as motion, gestures, and gaze. However, when it comes to hands-free scenarios, for example to increase the overall accessibility of an application or in medical scenarios under sterile conditions, most of the announced techniques cannot be applied. This is where the use of speech as an intuitive means of navigation comes in handy. As systems become more capable of understanding and producing speech, voice interfaces become a valuable alternative for input on all types of devices. This takes the quality of hands-free interaction to a new level. However, intuitive user-assisted speech interaction is difficult to realize due to semantic ambiguities in natural language utterances as well as the high real-time requirements of these systems. In this paper, we investigate steering-based locomotion and selection-based locomotion using three speech-based, hands-free methods and compare them with leaning as an established alternative. Our results show that landmark-based locomotion is a convenient, fast, and intuitive way to move between locations in a VR scene. Furthermore, we show that in scenarios where landmarks are not available, number grid-based navigation is a successful solution. Based on this, we conclude that speech is a suitable alternative in hands-free scenar-ios, and exciting ideas are emerging for future work focused on developing hands-free ad hoc navigation systems for scenes where landmarks do not exist or are difficult to articulate or recognize.},
  keywords={Visualization;Three-dimensional displays;Navigation;Semantics;Virtual reality;Speech recognition;Teleportation;Human-centered computing-Human computer interaction (HCI);Computing methodologies-Artificial intelligence-Natural language processing Speech recognition},
  doi={10.1109/VR55154.2023.00028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108448,
  author={Wang, Yuxi and Ling, Haibin and Huang, Bingyao},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={CompenHR: Efficient Full Compensation for High-resolution Projector}, 
  year={2023},
  volume={},
  number={},
  pages={135-145},
  abstract={Full projector compensation is a practical task of projector-camera systems. It aims to find a projector input image, named compensation image, such that when projected it cancels the geometric and photometric distortions due to the physical environment and hardware. State-of-the-art methods use deep learning to address this problem and show promising performance for low-resolution setups. However, directly applying deep learning to high-resolution setups is impractical due to the long training time and high memory cost. To address this issue, this paper proposes a practical full compensation solution. Firstly, we design an attention-based grid refinement network to improve geometric correction quality. Secondly, we integrate a novel sampling scheme into an end-to-end compensation network to alleviate computation and introduce attention blocks to preserve key features. Finally, we construct a benchmark dataset for high-resolution projector full compensation. In experiments, our method demonstrates clear advantages in both efficiency and quality.},
  keywords={Deep learning;Training;Costs;Three-dimensional displays;Spatial augmented reality;Memory management;Virtual reality;Projector compensation;Spatial augmented reality;Projector-camera system},
  doi={10.1109/VR55154.2023.00029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108458,
  author={Nitsche, Michael and Bosley, Blaire and Primo, Susan and Park, Jisu and Carr, Daniel},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Remapping Control in VR for Patients with AMD}, 
  year={2023},
  volume={},
  number={},
  pages={146-151},
  abstract={Age-related Macular Degeneration (AMD) is the leading cause of vision loss among persons over 50. We present a two-part interface consisting of a VR-based visualization for AMD patients and an interconnected doctor interface to optimize this VR view. It focuses on remapping imagery to provide customized image optimizations. The system allows doctors to generate a tailored, patient-specific VR visualization. We pilot tested the doctor interface (n=10) with eye care professionals. The results indicate the potential of VR-based eye care for doctors to help visually-impaired patients, but also show a necessary training phase to establish new technologies in vision rehabilitation.},
  keywords={Training;Visualization;Three-dimensional displays;Medical services;Virtual reality;Computer applications;User interfaces;AMD;VR;Image Remapping;medical application;H.5.2 [Information Systems]: Information Interfaces and Presentation - User Interfaces;J.3 [Computer Applications]: Life and Medical Sciences},
  doi={10.1109/VR55154.2023.00030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108414,
  author={Wong, Sai-Keung and Volonte, Matias and Liu, Kuan-Yu and Ebrahimi, Elham and Babu, Sabarish V.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Visual Attention with Leading and Following Virtual Agents in a Collaborative Perception-Action Task in VR}, 
  year={2023},
  volume={},
  number={},
  pages={152-162},
  abstract={This paper presents a within-subject study to investigate the effects of leading and following behaviors on user visual attention behaviors when collaborating with a virtual agent (VA) during performing transportation tasks in immersive virtual environments. The task was to carry a target object from a location to a predefined location. There were two conditions, namely leader VA (LVA) and follower VA (FVA). The leader gave instructions to the follower to perform actions. In the FVA condition, users played the leader role, while they played the follower role in the LVA condition. The users and the VA communicated via spoken language. During the experiment, participants wore a head-mounted display and performed real walking in a room. In each condition, each participant performed 20 trials of object transportation for different types of objects. Our preliminary results revealed significant differences in the user visual attention behaviors between the follower and leader VA conditions during the transportation tasks.},
  keywords={Legged locomotion;Visualization;Solid modeling;Three-dimensional displays;Multimedia systems;Transportation;Virtual environments;Virtual Agents;Virtual Reality;Behavior Modeling;Human-Computer Interaction;Animation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Animations;Evaluation/methodology;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;},
  doi={10.1109/VR55154.2023.00031},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108488,
  author={Hoster, Johannes and Ritter, Dennis and Hildebrand, Kristian},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Style-aware Augmented Virtuality Embeddings (SAVE)}, 
  year={2023},
  volume={},
  number={},
  pages={163-172},
  abstract={We present an augmented virtuality (AV) pipeline that enables the user to interact with real-world objects through stylised representations which match the VR scene and thereby preserve immersion. It consists of three stages: First, the object of interest is reconstructed from images and corresponding camera poses recorded with the VR headset, or alternatively a retrieval model finds a fitting mesh from the ShapeNet dataset. Second, a style transfer technique adapts the mesh to the VR game scene in order to preserve consistent immersion. Third, the stylised mesh is superimposed on the real object in real time to ensure interactivity even if the real object is moved. Our pipeline serves as proof of concept for style-aware AV embeddings.},
  keywords={Surface reconstruction;Three-dimensional displays;Augmented virtuality;Shape;Pipelines;Mixed reality;User interfaces;Mixed reality-Virtual reality-Reconstruction},
  doi={10.1109/VR55154.2023.00032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108421,
  author={Liu, Junhua and Zhu, Boxiang and Wang, Fangxin and Jin, Yili and Zhang, Wenyi and Xu, Zihan and Cui, Shuguang},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={CaV3: Cache-assisted Viewport Adaptive Volumetric Video Streaming}, 
  year={2023},
  volume={},
  number={},
  pages={173-183},
  abstract={Volumetric video (VV) recently emerges as a new form of video application providing a photorealistic immersive 3D viewing experience with 6 degree-of-freedom (DoF), which empowers many applications such as VR, AR, and Metaverse. A key problem therein is how to stream the enormous size VV through the network with limited bandwidth. Existing works mostly focused on predicting the viewport for a tiling-based adaptive VV streaming, which however only has quite a limited effect on resource saving. We argue that the content repeatability in the viewport can be further leveraged, and for the first time, propose a client-side cache-assisted strategy that aims to buffer the repeatedly appearing VV tiles in the near future so as to reduce the redundant VV content transmission. The key challenges exist in three aspects, including (1) feature extraction and mining in 6 DoF VV context, (2) accurate long-term viewing pattern estimation and (3) optimal caching scheduling with limited capacity. In this paper, we propose CaV3, an integrated cache-assisted viewport adaptive VV streaming framework to address the challenges. CaV3 employs a Long-short term Sequential prediction model (LSTSP) that achieves accurate short-term, mid-term and long-term viewing pattern prediction with a multi-modal fusion model by capturing the viewer's behavior inertia, current attention, and subjective intention. Besides, CaV3 also contains a contextual MAB-based caching adaptation algorithm (CCA) to fully utilize the viewing pattern and solve the optimal caching problem with a proved upper bound regret. Compared to existing VV datasets only containing single or co-located objects, we for the first time collect a comprehensive dataset with sufficient practical unbounded 360° scenes. The extensive evaluation of the dataset confirms the superiority of CaV3, which outperforms the SOTA algorithm by 15.6%-43% in viewport prediction and 13%-40% in system utility.},
  keywords={Adaptation models;Solid modeling;Three-dimensional displays;Upper bound;Virtual reality;Streaming media;Predictive models},
  doi={10.1109/VR55154.2023.00033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108470,
  author={Guarese, Renan and Pretty, Emma and Fayek, Haytham and Zambetta, Fabio and van Schyndel, Ron},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evoking empathy with visually impaired people through an augmented reality embodiment experience}, 
  year={2023},
  volume={},
  number={},
  pages={184-193},
  abstract={To promote empathy with people that have disabilities, we propose a multi-sensory interactive experience that allows sighted users to embody having a visual impairment whilst using assistive technologies. The experiment involves blindfolded sighted participants interacting with a variety of sonification methods in order to locate targets and place objects in a real kitchen environment. Prior to the tests, we enquired about the perceived benefits of increasing said empathy from the blind and visually impaired (BVI) community. To test empathy, we adapted an Empathy and Sympathy Response scale to gather sighted people's self-reported and perceived empathy with the BVI community from both sighted (N = 77) and BVI people (N = 20) respectively. We re-tested sighted people's empathy after the experiment and found that their empathetic and sympathetic responses (N = 15) significantly increased. Furthermore, survey results suggest that the BVI community believes the use of these empathy-evoking embodied experiences may lead to the development of new assistive technologies.},
  keywords={Three-dimensional displays;Visual impairment;Assistive technologies;User interfaces;Sonification;Augmented reality;Human-centered computing-Accessibility-Accessibility technologies;Human-centered computing-Accessibility-Empirical studies in accessibility;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Auditory feedback;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
  doi={10.1109/VR55154.2023.00034},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108468,
  author={Feick, Martin and Regitz, Kora P. and Tang, Anthony and Jungbluth, Tobias and Rekrut, Maurice and Krüger, Antonio},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Noticeable Hand Redirection in Virtual Reality using Physiological and Interaction Data}, 
  year={2023},
  volume={},
  number={},
  pages={194-204},
  abstract={Hand redirection is effective so long as the introduced offsets are not noticeably disruptive to users. In this work we investigate the use of physiological and interaction data to detect movement discrepancies between a user's real and virtual hand, pushing towards a novel approach to identify discrepancies which are too large and therefore can be noticed. We ran a study with 22 participants, collecting EEG, ECG, EDA, RSP, and interaction data. Our results suggest that EEG and interaction data can be reliably used to detect visuo-motor discrepancies, whereas ECG and RSP seem to suffer from inconsistencies. Our findings also show that participants quickly adapt to large discrepancies, and that they constantly attempt to establish a stable mental model of their environment. Together, these findings suggest that there is no absolute threshold for possible non-detectable discrepancies; instead, it depends primarily on participants' most recent experience with this kind of interaction.},
  keywords={Three-dimensional displays;Virtual reality;Electrocardiography;User interfaces;Physiology;Electroencephalography;Cognitive science;Virtual Reality;Hand Redirection;Physiological Data;Detection Thresholds},
  doi={10.1109/VR55154.2023.00035},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108084,
  author={Singh, Rahul and Huzaifa, Muhammad and Liu, Jeffrey and Patney, Anjul and Sharif, Hashim and Zhao, Yifan and Adve, Sarita},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Power, Performance, and Image Quality Tradeoffs in Foveated Rendering}, 
  year={2023},
  volume={},
  number={},
  pages={205-214},
  abstract={Extended reality (XR) devices, including augmented, virtual, and mixed reality, provide a deeply immersive experience. However, practical limitations like weight, heat, and comfort put extreme constraints on the performance, power consumption, and image quality of such systems. In this paper, we study how these constraints form the tradeoff between Fixed Foveated Rendering (FFR), Gaze-Tracked Foveated Rendering (TFR), and conventional, non-foveated rendering. While existing papers have often studied these methods, we provide the first comprehensive study of their relative feasibility in practical systems with limited battery life and computational budget. We show that TFR with the added cost of the gaze-tracker can often be more expensive than FFR. Thus, we co-design a gaze-tracked foveated renderer considering its benefits in computation, power efficiency, and tradeoffs in image quality. We describe principled approximations for eye tracking which provide up to a 9x speedup in runtime performance with approximately a 20x improvement in energy efficiency when run on a mobile GPU. In isolation, these approximations appear to significantly degrade the gaze quality, but appropriate compensation in the visual pipeline can mitigate the loss. Overall, we show that with a highly optimized gaze-tracker, TFR is feasible compared to FFR, resulting in up to 1.25x faster frame times while also reducing total energy consumption by over 40%.},
  keywords={Image quality;Performance evaluation;Visualization;Three-dimensional displays;Runtime;Power demand;Pipelines;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR55154.2023.00036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108417,
  author={Ito, Kenichi and Hosoi, Juro and Ban, Yuki and Kikuchi, Takayuki and Nakagawa, Kyosuke and Kitagawa, Hanako and Murakami, Chizuru and Imai, Yosuke and Warisawa, Shin'ichi},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Wind comfort and emotion can be changed by the cross-modal presentation of audio-visual stimuli of indoor and outdoor environments}, 
  year={2023},
  volume={},
  number={},
  pages={215-225},
  abstract={The development of methods to simulate the sensation of wind that can promote relaxation and elicit positive emotional responses has become a topic of interest with the widespread adoption of virtual and augmented reality systems. Previous studies have simulated natural wind by varying wind speed in a controlled environment or moving a large flow of air through an area. In contrast to such approaches to modulate physical airflow, the use of multisensory stimuli to alter the impression and sense of comfort provided by a simulated wind has rarely been considered in previous research. If visual and auditory stimuli affect wind comfort, a multisensory design should be considered for relaxation systems that use wind effects. Therefore, we experimentally measured wind comfort and associated emotions when participants experienced outdoor and indoor virtual environments through immersive virtual reality to investigate whether cross-modal effects of variations in audio-visual stimuli would impact the relaxation effects associated with a virtual wind. The results show that the virtual environment of an outdoor meadow and the sound of natural wind significantly improved users' subjective experience of comfort and openness associated with the wind, as well as their emotional state. Simulated natural wind reduced mental stress compared to a condition without wind, as shown by questionnaires and biometric data. The results of this study indicate that multisensory stimuli conveying natural impressions and simulated natural wind are effective for wind-based relaxation.},
  keywords={Visualization;Three-dimensional displays;Wind speed;Virtual environments;Resists;Human factors;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR55154.2023.00037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108474,
  author={Cheng, Haonan and Liu, Shiguang and Zhang, Jiawan},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Lightweight Scene-aware Rain Sound Simulation for Interactive Virtual Environments}, 
  year={2023},
  volume={},
  number={},
  pages={226-236},
  abstract={We present a lightweight and efficient rain sound synthesis method for interactive virtual environments. Existing rain sound simulation methods require massive superposition of scene-specific precomputed rain sounds, which is excessive memory consumption for virtual reality systems (e.g. video games) with limited audio memory budgets. Facing this issue, we reduce the audio memory budgets by introducing a lightweight rain sound synthesis method which is only based on eight physically-inspired basic rain sounds. First, in order to generate sufficiently various rain sounds with limited sound data, we propose an exponential moving average based frequency domain additive (FDA) synthesis method to extend and modify the pre-computed basic rain sounds. Each rain sound is generated in the frequency domain before conversion back to the time domain, allowing us to extend the rain sound which is free of temporal distortions and discontinuities. Next, we introduce an efficient binaural rendering method to simulate the 3D perception that coheres with the visual scene based on a set of Near-Field Transfer Functions (NFTF). Various results demonstrate that the proposed method drastically decreases the memory cost (77 times compressed) and overcomes the limitations of existing methods in terms of interaction.},
  keywords={Solid modeling;Visualization;Video games;Rain;Three-dimensional displays;Frequency-domain analysis;Memory management;Human-centered computing;Human computer interaction;Interaction techniques;Auditory feedback;Applied computing;Arts and humanities;Sound and music computing},
  doi={10.1109/VR55154.2023.00038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108491,
  author={Ooi, Chun-Wei and Hiroi, Yuichi and Itoh, Yuta},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Compact Photochromic Occlusion Capable See-through Display with Holographic Lenses}, 
  year={2023},
  volume={},
  number={},
  pages={237-242},
  abstract={Occlusion is a crucial visual element in optical see-through (OST) augmented reality, however, implementing occlusion in OST displays while addressing various design trade-offs is a difficult problem. In contrast to the traditional method of using spatial light modulators (SLMs) for the occlusion mask, using photochromic materials as occlusion masks can effectively eliminate diffraction artifacts in see-through views due to the lack of electronic pixels, thus providing superior see-through image quality. However, this design requires UV illumination to activate the photochromic mate-rial, which traditionally requires multiple SLMs, resulting in a larger form factor for the system. This paper presents a compact photochromic occlusion-capable OST design using multilayer, wavelength-dependent holographic optical lenses (HOLs). Our approach employs a single digital mi-cromirror display (DMD) to form both the occlusion mask with UV light and a virtual image with visible light in a time-multiplexed man-ner. We demonstrate our proof-of-concept system on a bench-top setup and assess the appearance and contrasts of the displayed image. We also suggest potential improvements for current prototypes to encourage the community to explore this occlusion approach.},
  keywords={Visualization;Optical diffraction;Three-dimensional displays;Optical design;Prototypes;Holography;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Communication hardware;interfaces and storage-Displays and imagers},
  doi={10.1109/VR55154.2023.00039},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108444,
  author={Stellmacher, Carolin and Zenner, André and Nunez, Oscar Javier Ariza and Kruijff, Ernst and Schöning, Johannes},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Continuous VR Weight Illusion by Combining Adaptive Trigger Resistance and Control-Display Ratio Manipulation}, 
  year={2023},
  volume={},
  number={},
  pages={243-253},
  abstract={Handheld virtual reality (VR) controllers enable users to manipulate virtual objects in VR but do not convey a virtual object's weight. This hinders users from effectively experiencing lighter and heavier objects. While previous work explored either hardware-based interfaces or software-based pseudo-haptics, in this paper, we combine two techniques to improve the virtual weight perception in VR. By adapting the trigger resistance of the VR controller when grasping a virtual object and manipulating the control-display (C/D) ratio during lifting, we create a continuous weight sensation. In a psychophysical study (N=29), we compared our combined approach against the individual rendering techniques. Our results show that participants were significantly more sensitive towards smaller weight differences in the combined weight simulations compared to the individual methods. Additionally, participants were also able to determine weight differences significantly faster with both cues present compared to the single pseudo-haptic technique. While all three techniques were generally valued to be effective, the combined approach was favoured the most. Our findings demonstrate the meaningful benefit of combining physical and virtual techniques for virtual weight rendering over previously proposed methods.},
  keywords={Resistance;Solid modeling;Adaptation models;Three-dimensional displays;Virtual reality;Grasping;User interfaces;virtual reality;adaptive trigger resistance;virtual weight;control display ratio;psychophysical experiment},
  doi={10.1109/VR55154.2023.00040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108437,
  author={Wang, Junyi and Qi, Yue},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simultaneous Scene-independent Camera Localization and Category-level Object Pose Estimation via Multi-level Feature Fusion}, 
  year={2023},
  volume={},
  number={},
  pages={254-264},
  abstract={In AR/MR applications, camera localization and object pose estimation both play crucial roles. The universality of learning techniques, often referred to as scene-independent localization and category-level pose estimation, presents challenges for both tasks. The two missions maintain close relationships due to the spatial geometry constraint, but differing task requirements result in distinct feature extraction. In this paper, we focus on simultaneous scene-independent camera localization and category-level object pose estimation with a unified learning framework. The system consists of a localization branch called SLO-LocNet, a pose estimation branch called SLO-ObjNet, a feature fusion module for feature sharing between two tasks, and two decoders for creating coordinate maps. In SLO-LocNet, localization features are produced for anticipating the relative pose between two adjusted frames using inputs of color and depth images. Furthermore, we establish an image fusion module in order to promote feature sharing in depth and color branches. With SLO-ObjNet, we take the detected depth image and its corresponding point cloud as inputs, and produce object pose features for pose estimation. A geometry fusion module is created to combine depth and point cloud information simultaneously. Between the two tasks, the image fusion module is also exploited to accomplish feature sharing. In terms of the loss function, we present a mixed optimization function that is composed of the relative camera pose, geometry constraint, absolute and relative object pose terms. To verify how well our algorithm could perform, we conduct experiments on both localization and pose estimation datasets, covering 7 Scenes, ScanNet, REAL275 and YCB-Video. All experiments demonstrate superior performance to other existing methods. We specifically train the network on ScanNet and test it on 7 Scenes to demonstrate the universality performance. Additionally, the positive effects of fusion modules and loss function are also demonstrated.},
  keywords={Location awareness;Point cloud compression;Geometry;Image color analysis;Pose estimation;Cameras;Feature extraction;Scene-independent camera localization;Cagetory-level object pose estimation;Feature fusion;Multi-task learning;Geometry constraint},
  doi={10.1109/VR55154.2023.00041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108424,
  author={Sun, Qilei and Huang, Jiayou and Zhang, Haodong and Craig, Paul and Yu, Lingyun and Lim, Eng Gee},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Development of a Mixed Reality Acupuncture Training System}, 
  year={2023},
  volume={},
  number={},
  pages={265-275},
  abstract={This paper looks at how mixed reality can be used for the improvement and enhancement of Chinese acupuncture practice through the introduction of an acupuncture training simulator. A prototype system developed for our study allows practitioners to insert virtual needles using their bare hands into a full-scale 3D representation of the human body with labelled acupuncture points. This provides them with a safe and natural environment to develop their acupuncture skills simulating the actual physical process of acupuncture. It also helps them to develop their muscle memory for acupuncture and better develops their memory of acupuncture points through a more immersive learning experience. We describe some of the design decisions and technical challenges overcome in the development of our system. We also present the results of a comparative user evaluation with potential users aimed at assessing the viability of such a mixed reality system being used as part of their training and development. The results of our evaluation reveal the training system outperformed in the enhancement of spatial understanding as well as improved learning and dexterity in acupuncture practice. These results go some way to demonstrating the potential of mixed reality for improving practice in therapeutic medicine.},
  keywords={Training;Solid modeling;Three-dimensional displays;Mixed reality;Prototypes;Virtual reality;User interfaces;Mixed reality—immersive technologies—virtual environments—;Chinese acupuncture—Medical education},
  doi={10.1109/VR55154.2023.00042},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108419,
  author={Yamazaki, Yusuke and Hasegawa, Shoichi},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Providing 3D Guidance and Improving the Music-Listening Experience in Virtual Reality Shooting Games Using Musical Vibrotactile Feedback}, 
  year={2023},
  volume={},
  number={},
  pages={276-285},
  abstract={In this study, we aim to improve the experience of virtual reality (VR) shooting games by employing a 3D haptic guidance method using necklace-type and belt-type haptic devices. Such devices help to modulate the vibrations generated by and synchronized with musical signals according to the azimuth and height of a target in 3D space, which is expected to improve the gaming experience by providing 3D guidance and enhancing the music-listening experience. For the first step, we evaluated the method's potential by conducting an experiment in which participants were asked to shoot a randomly spawned target moving in 3D VR space. The experiment applied four conditions: the proposed method (Haptic), displaying 3D radar (Vision) to represent the visualization method, no guidance (None), and a combination of Haptic and Vision (VisHap). Outcomes related to the success rate and accomplishment time (of the shooting task), the number of head rotations, and participant responses to a follow-up questionnaire revealed that Haptic performed significantly better than None but was inferior to Vision, indicating that the proposed method succeeded in terms of effectively providing 3D guidance. VisHap performed roughly as well as Vision and was preferred to other conditions in most cases, indicating the general usefulness of the proposed method. Meanwhile, the findings from the questionnaire suggest that although the modular vibrations improved the music-listening experience during the shooting task, the impact on the overall gaming experience is unclear. This warrants further research.},
  keywords={Vibrations;Visualization;Three-dimensional displays;Music;Virtual reality;Radar;Games;Human-centered computing-Haptic devices;Human-centered computing-Virtual reality;Human-centered computing-Sound-based input / output},
  doi={10.1109/VR55154.2023.00043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108435,
  author={Yun, Haoran and Ponton, Jose Luis and Andujar, Carlos and Pelechano, Nuria},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency}, 
  year={2023},
  volume={},
  number={},
  pages={286-296},
  abstract={The use of self-avatars is gaining popularity thanks to affordable VR headsets. Unfortunately, mainstream VR devices often use a small number of trackers and provide low-accuracy animations. Previous studies have shown that the Sense of Embodiment, and in particular the Sense of Agency, depends on the extent to which the avatar's movements mimic the user's movements. However, few works study such effect for tasks requiring a precise interaction with the environment, i.e., tasks that require accurate manipulation, precise foot stepping, or correct body poses. In these cases, users are likely to notice inconsistencies between their self-avatars and their actual pose. In this paper, we study the impact of the animation fidelity of the user avatar on a variety of tasks that focus on arm movement, leg movement and body posture. We compare three different animation techniques: two of them using Inverse Kinematics to reconstruct the pose from sparse input (6 trackers), and a third one using a professional motion capture system with 17 inertial sensors. We evaluate these animation techniques both quantitatively (completion time, unintentional collisions, pose accuracy) and qualitatively (Sense of Embodiment). Our results show that the animation quality affects the Sense of Embodiment. Inertial-based MoCap performs significantly better in mimicking body poses. Surprisingly, IK-based solutions using fewer sensors outperformed MoCap in tasks requiring accurate positioning, which we attribute to the higher latency and the positional drift that causes errors at the end-effectors, which are more noticeable in contact areas such as the feet.},
  keywords={Legged locomotion;Three-dimensional displays;Tracking;Inertial sensors;User interfaces;Sensor phenomena and characterization;Animation;Virtual reality;Motion capture;Inverse kinematics;Embodiment;Avatar Animation},
  doi={10.1109/VR55154.2023.00044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108410,
  author={Mortezapoor, Soroosh and Vasylevska, Khrystyna and Vonach, Emanuel and Kaufmann, Hannes},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={CoboDeck: A Large-Scale Haptic VR System Using a Collaborative Mobile Robot}, 
  year={2023},
  volume={},
  number={},
  pages={297-307},
  abstract={We present CoboDeck - our proof-of-concept immersive virtual reality haptic system with free walking support. It provides prop-based encountered-type haptic feedback with a mobile robotic platform. Intended for use as a design tool for architects, it enables the user to directly and intuitively interact with virtual objects like walls, doors, or furniture. A collaborative robotic arm mounted on an omnidirectional mobile platform can present a physical prop that matches the position and orientation of a virtual counterpart anywhere in large virtual and real environments. We describe the concept, hardware, and software architecture of our system. Furthermore, we present the first behavioral algorithm tailored for the unique challenges of safe human-robot haptic interaction in VR, explicitly targeting availability and safety while the user is unaware of the robot and can change trajectory at any time. We explain our high-level state machine that controls the robot to follow a user closely and rapidly escape from him as required by the situation. We present our technical evaluation. The results suggest that our chasing approach saves time, decreases the travel distance and thus battery usage, compared to more traditional approaches for mobile platforms assuming a fixed parking position between interactions. We also show that the robot can escape from the user and prevent a possible collision within a mean time of 1.62 s. Finally, we confirm the validity of our approach in a practical validation and discuss the potential of the proposed system.},
  keywords={Three-dimensional displays;Collaboration;Virtual reality;Haptic interfaces;Safety;Behavioral sciences;Trajectory;Computer systems organization—Embedded and cyberphysical systems—Robotics;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Haptic devices;},
  doi={10.1109/VR55154.2023.00045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108439,
  author={Bhargava, Ayush and Venkatakrishnan, Roshan and Venkatakrishnan, Rohith and Solini, Hannah and Lucaites, Kathryn and Robb, Andrew C. and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Empirically Evaluating the Effects of Eye Height and Self-Avatars on Dynamic Passability Affordances in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={308-317},
  abstract={Over the past two decades self-avatars have been shown to affect the perception of both oneself and of environmental properties including the sizes and distances of elements in immersive virtual environments. However, virtual avatars that accurately match the body proportions of their users remain inaccessible to the general public. As such, most virtual experiences that represent the user have a generic avatar that does not fit the proportions of the users' body. This can negatively affect judgments involving affordances, such as passability and maneuverability, which pertain to the relationship between the properties of environmental elements relative to the properties of the user providing information about actions that can be enacted. This is especially true when the task requires the user to maneuver around moving objects like in games. Therefore, it is necessary to understand how different sized self-avatars affect the perception of affordances in dynamic virtual environments. To better understand this, we conducted an experiment investigating how a self-avatar that is either the same size, 20% shorter, or 20% taller, than the user's own body affects passability judgments in a dynamic virtual environment. Our results suggest that the presence of self-avatars results in better regulatory and safer road crossing behavior, and helps participants synchronize self-motion to external stimuli quicker than in the absence of self-avatars.},
  keywords={Three-dimensional displays;Affordances;Avatars;Roads;Virtual environments;External stimuli;Games;Human-centered-computing;Empirical-studies-in-HCI},
  doi={10.1109/VR55154.2023.00046},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108475,
  author={Cannavò, Alberto and Pratticò, Filippo Gabriele and Bruno, Alberto and Lamberti, Fabrizio},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={AR-MoCap: Using Augmented Reality to Support Motion Capture Acting}, 
  year={2023},
  volume={},
  number={},
  pages={318-327},
  abstract={Technology is disrupting the way films involving visual effects are produced. Chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are only a few examples of the many changes introduced in the cinema industry over the last years. Although these technologies are getting commonplace, they are presenting new, unexplored challenges to the actors. In particular, when mocap is used to record the actors' movements with the aim of animating digital character models, an increase in the workload can be easily expected for people on stage. In fact, actors have to largely rely on their imagination to understand what the digitally created characters will be actually seeing and feeling. This paper focuses on this specific domain, and aims to demonstrate how Augmented Reality (AR) can be helpful for actors when shooting mocap scenes. To this purpose, we devised a system named AR-MoCap that can be used by actors for rehearsing the scene in AR on the real set before actually shooting it. Through an Optical See-Through Head-Mounted Display (OST-HMD), an actor can see, e.g., the digital characters of other actors wearing mocap suits overlapped in real-time to their bodies. Experimental results showed that, compared to the traditional approach based on physical props and other cues, the devised system can help the actors to position themselves and direct their gaze while shooting the scene, while also improving spatial and social presence, as well as perceived effectiveness.},
  keywords={Solid modeling;Three-dimensional displays;User interfaces;Visual effects;Optical imaging;Motion pictures;Motion capture;Collaborative virtual production;acting rehearsal and performance;motion capture;body ownership;virtual characters;visual effects;augmented reality;Human-centered computing—Human computer interaction (HCI)—Mixed / augmented reality—;Human-centered computing—Human computer interaction (HCI)—Collaborative interaction—;Computing methodologies—Computer vision—;Image and video acquisition—Motion capture;Human-centered computing—Applied computing-Arts and humanities—Performing arts;Human-centered computing—Applied computing-Arts and humanities—Media arts},
  doi={10.1109/VR55154.2023.00047},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108472,
  author={Li, Gang and Pohlmann, Katharina and McGill, Mark and Chen, Chao Ping and Brewster, Stephen and Pollick, Frank},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Neural Biomarkers in Young Adults Resistant to VR Motion Sickness: A Pilot Study of EEG}, 
  year={2023},
  volume={},
  number={},
  pages={328-335},
  abstract={VR (Virtual Reality) Motion Sickness (VRMS) refers to purely visually-induced motion sickness. Not everyone is susceptible to VRMS, but if experienced, nausea will often lead users to withdraw from the ongoing VR applications. VRMS represents a serious challenge in the field of VR ergonomics and human factors. Like other neuro-ergonomics researchers did before, this paper considers VRMS as a brain state problem as various etiologies of VRMS support the claim that VRMS is caused by disagreement between the vestibular and visual sensory inputs. However, what sets this work apart from the existing literature is that it explores anti-VRMS brain patterns via electroencephalogram (EEG) in VRMS-resistant individuals. Based on existing datasets of a previous study, we found enhanced theta activity in the left parietal cortex in VRMS-resistant individuals (N= 10) compared to VRMS-susceptible individuals (N=10). Even though the sample size per se is not large, this finding achieved medium effect size. This finding offers new hypotheses regarding how to reduce VRMS by the enhancement of brain functions per se (e.g., via non-invasive transcranial electrostimulation techniques) without the need to redesign the existing VR content.},
  keywords={Visualization;Three-dimensional displays;Correlation;Ergonomics;Virtual reality;Human factors;Motion sickness;VR motion sickness;brain state problem;EEG;resistance to VR motion sickness;Neuro-ergonomics;vestibular system;VR motion sickness;cybersickness;EEG},
  doi={10.1109/VR55154.2023.00048},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108482,
  author={Liang, Wei and Wang, Luhui and Yu, Xinzhe and Li, Changyang and Alghofaili, Rawan and Lang, Yining and Yu, Lap-Fai},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimizing Product Placement for Virtual Stores}, 
  year={2023},
  volume={},
  number={},
  pages={336-346},
  abstract={The recent popularity of consumer-grade virtual reality devices has enabled users to experience immersive shopping in virtual environments. As in a real-world store, the placement of products in a virtual store should appeal to shoppers, which could be time-consuming, tedious, and non-trivial to create manually. Thus, this work introduces a novel approach for automatically optimizing product placement in virtual stores. Our approach considers product exposure and spatial constraints, applying an optimizer to search for optimal product placement solutions. We conducted qualitative scene rationality and quantitative product exposure experiments to validate our approach with users. The results show that the proposed approach can synthesize reasonable product placements and increase product exposures for different virtual stores.},
  keywords={Visualization;Three-dimensional displays;Layout;Virtual environments;Lighting;User interfaces;Optimization;Human-centered computing-Human computer interaction (HCI)-;-Computing methodologies-Virtual reality},
  doi={10.1109/VR55154.2023.00049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108478,
  author={Wang, Jingying and Qiu, Yilin and Chen, Keyu and Ding, Yu and Pan, Ye},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Fully Automatic Blendshape Generation for Stylized Characters}, 
  year={2023},
  volume={},
  number={},
  pages={347-355},
  abstract={Avatars are one of the most important elements in virtual environments. Real-time facial retargeting technology is of vital importance in AR/VR interactions, the filmmaking, and the entertainment industry, and blendshapes for avatars are one of its important materials. Previous works either focused on the characters with the same topology, which cannot be generalized to universal avatars, or used optimization methods that have high demand on the dataset. In this paper, we adopt the essence of deep learning and feature transfer to realize deformation transfer, thereby generating blendshapes for target avatars based on the given sources. We proposed a Variational Autoencoder (VAE) to extract the latent space of the avatars and then use a Multilayer Perceptron (MLP) model to realize the translation between the latent spaces of the source avatar and target avatars. By decoding the latent code of different blendshapes, we can obtain the blendshapes for the target avatars with the same semantics as that of the source. We qualitatively and quantitatively compared our method with both classical and learning-based methods. The results revealed that the blendshapes generated by our method achieves higher similarity to the groundtruth blendshapes than the state-of-art methods. We also demonstrated that our method can be applied to expression transfer for stylized characters with different topologies.},
  keywords={Deep learning;Training;Solid modeling;Three-dimensional displays;Avatars;Semantics;Virtual environments;Avatars;Blendshapes;facial animation;stylized char-acters;Human-centered computing-Visualization-Visu-alization techniques-Treemaps;Human-centered computing-Visualization-Visualization design and evaluation methods},
  doi={10.1109/VR55154.2023.00050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108479,
  author={Yi, Zimu and Xie, Ke and Lyu, Jiahui and Gong, Minglun and Huang, Hui},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Where to Render: Studying Renderability for IBR of Large-Scale Scenes}, 
  year={2023},
  volume={},
  number={},
  pages={356-366},
  abstract={Image-based rendering (IBR) technique enables presenting real scenes interactively to viewers and hence is a key component for implementing VR telepresence. The quality of IBR results depends on the set of pre-captured views, the rendering algorithm used, and the camera parameters of the novel view to be synthesized. Numerous methods were proposed for optimizing the set of captured images and enhancing the rendering algorithms. However, from which regions IBR methods can synthesize satisfactory results is not yet well studied. In this work, we introduce the concept of renderability, which predicts the quality of IBR results at any given viewpoint and view direction. Consequently, the renderability values evaluated for the 5D camera parameter space form a field, which effectively guides viewpoint/trajectory selection for IBR, especially for challenging large-scale 3D scenes. To demonstrate this capability, we designed 2 VR applications: a path planner that allows users to navigate through sparsely captured scenes with controllable rendering quality and a view selector that provides an overview for a scene from diverse and high quality perspectives. We believe the renderability concept, the proposed evaluation method, and the suggested applications will motivate and facilitate the use of IBR in various interactive settings.},
  keywords={Three-dimensional displays;Telepresence;Navigation;Virtual reality;User interfaces;Aerospace electronics;Rendering (computer graphics);Computer graphics techniques-Image-based rendering-Scene rendering;Evaluation methods-Renderability-View selection-Rendering path planning},
  doi={10.1109/VR55154.2023.00051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108496,
  author={Alghofaili, Rawan and Nguyen, Cuong and Krs, Vojtĕch and Carr, Nathan and Mĕch, Radomír and Yu, Lap-Fai},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={WARPY: Sketching Environment-Aware 3D Curves in Mobile Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={367-377},
  abstract={Three-dimensional curve drawing in Augmented Reality (AR) enables users to create 3D curves that fit within the real-world scene. It has applications in 3D design, sculpting, and animation. However, the task complexity increases when the desirable path for the curve is obstructed by the physical environment or by what the camera can see. For example, it is difficult to draw a curve that wraps around an object or scales to out-of-reach places. We propose WARPY, an environment-aware 3D curve drawing tool for mobile AR. Our system enables users to draw freeform curves from a distance in AR by combining 2D-to-3D sketch inference with geometric proxies. Geometric Proxies can be obtained via 3D scanning or from a list of pre-defined primitives. WARPY also provides a multi-view mode to enable users to sketch a curve from multiple viewpoints, which is useful if the target curve cannot fit within the camera's field of view. We conducted two user studies and found that WARPY can be a viable tool to help users create complex and large curves in AR.},
  keywords={Geometry;Three-dimensional displays;Spirals;Shape;User interfaces;Cameras;Animation;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality},
  doi={10.1109/VR55154.2023.00052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108467,
  author={Bang, Sunyoung and Woo, Woontack},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enhancing the Reading Experience on AR HMDs by Using Smartphones as Assistive Displays}, 
  year={2023},
  volume={},
  number={},
  pages={378-386},
  abstract={The reading experience on current augmented reality (AR) head mounted displays (HMDs) is often impeded by the devices' low perceived resolution, translucency, and small field of view, especially in situations involving lengthy text. Although many researchers have proposed methods to resolve this issue, the inherent characteristics prevent these displays from delivering a readability on par with that of more traditional displays. As a solution, we explore the use of smartphones as assistive displays to AR HMDs. To validate the feasibility of our approach, we conducted a user study in which we compared a smartphone-assisted hybrid interface against using the HMD only for two different text lengths. The results demonstrate that the hybrid interface yields a lower task load regardless of the text length, although it does not improve task performance. Furthermore, the hybrid interface provides a better experience regarding user comfort, visual fatigue, and perceived readability. Based on these results, we claim that joining the spatial output capabilities of the HMD with the high-resolution display of the smartphone is a viable solution for improving the reading experience in AR.},
  keywords={Process design;Visualization;Three-dimensional displays;Resists;Switches;User interfaces;Fatigue;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented re-ality;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/VR55154.2023.00053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108427,
  author={Tong, Wai and Xia, Meng and Wong, Kam Kwai and Bowman, Doug A. and Pong, Ting-Chuen and Qu, Huamin and Yang, Yalong},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving}, 
  year={2023},
  volume={},
  number={},
  pages={387-397},
  abstract={This paper provided empirical knowledge of the user experience for using collaborative visualization in a distributed asymmetrical setting through controlled user studies. With the ability to access various computing devices, such as Virtual Reality (VR) head-mounted displays, scenarios emerge when collaborators have to or prefer to use different computing environments in different places. However, we still lack an understanding of using VR in an asymmetric setting for collaborative visualization. To get an initial understanding and better inform the designs for asymmetric systems, we first conducted a formative study with 12 pairs of participants. All participants collaborated in asymmetric (PC-VR) and symmetric settings (PC-PC and VR-VR). We then improved our asymmetric design based on the key findings and observations from the first study. Another ten pairs of participants collaborated with enhanced PC-VR and PC-PC conditions in a follow-up study. We found that a well-designed asymmetric collaboration system could be as effective as a symmetric system. Surprisingly, participants using PC perceived less mental demand and effort in the asymmetric setting (PC-VR) compared to the symmetric setting (PC-PC). We provided fine-grained discussions about the trade-offs between different collaboration settings.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Collaboration;Prototypes;Virtual reality;User interfaces;asymmetric collaborative visualization;virtual reality;data visualization;problem solving},
  doi={10.1109/VR55154.2023.00054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108494,
  author={Teng, Xue and Allison, Robert S. and Wilcox, Laurie M.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Manipulation of Motion Parallax Gain Distorts Perceived Distance and Object Depth in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={398-408},
  abstract={Virtual reality (VR) is distinguished by the rich, multimodal, im-mersive sensory information and affordances provided to the user. However, when moving about an immersive virtual world the vi-sual display often conflicts with other sensory cues due to design, the nature of the simulation, or to system limitations (for example impoverished vestibular motion cues during acceleration in racing games). Given that conflicts between sensory cues have been as-sociated with disorientation or discomfort, and theoretically could distort spatial perception, it is important that we understand how and when they are manifested in the user experience. To this end, this set of experiments investigates the impact of mismatch between physical and virtual motion parallax on the per-ception of the depth of an apparently perpendicular dihedral angle (a fold) and its distance. We applied gain distortions between visual and kinesthetic head motion during lateral sway movements and measured the effect of gain on depth, distance and lateral space compression. We found that under monocular viewing, observers made smaller object depth and distance settings especially when the gain was greater than 1. Estimates of target distance declined with increasing gain under monocular viewing. Similarly, mean set depth decreased with increasing gain under monocular viewing, except at 6.0 m. The effect of gain was minimal when observers viewed the stimulus binocularly. Further, binocular viewing (stereopsis) improved the precision but not necessarily the accuracy of gain perception. Overall, the lateral compression of space was similar in the stereoscopic and monocular test conditions. Taken together, our results show that the use of large presentation distances (at 6 m) combined with binocular cues to depth and distance enhanced humans' tolerance to visual and kinesthetic mismatch.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Sensitivity;Shape;Stereo image processing;Virtual reality;Depth Perception;Egocentric Distance;Motion Gain;Motion Parallax},
  doi={10.1109/VR55154.2023.00055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108412,
  author={Miller, Mark Roman and DeVeaux, Cyan and Han, Eugy and Ram, Nilam and Bailenson, Jeremy N.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Large-Scale Study of Proxemics and Gaze in Groups}, 
  year={2023},
  volume={},
  number={},
  pages={409-417},
  abstract={Scholars who study nonverbal behavior have focused an incredible amount of work on proxemics, how close people stand to one another, and mutual gaze, whether or not they are looking at one another. Moreover, many studies have demonstrated a correlation between gaze and distance, and so-called equilibrium theory posits that people modulate gaze and distance to maintain proper levels of nonverbal intimacy. Virtual reality scholars have also focused on these two constructs, both for theoretical reasons, as distance and gaze are often used as proxies for psychological constructs such as social presence, and for methodological reasons, as head orientation and body position are automatically produced by most VR tracking systems. However, to date, the studies of distance and gaze in VR have largely been conducted in laboratory settings, observing behavior of a small number of participants for short periods of time. In this experimental field study, we analyze the proxemics and gaze of 232 participants over two experimental studies who each contributed up to about 240 minutes of tracking data during eight weekly 30-minute social virtual reality sessions. Participants' non-verbal behaviors changed in conjunction with context manipulations and over time. Interpersonal distance increased with the size of the virtual room; and both mutual gaze and interpersonal distance increased over time. Overall, participants oriented their heads toward the center of walls rather than to corners of rectangularly-aligned environments. Finally, statistical models demonstrated that individual differences matter, with pairs and groups maintaining more consistent differences over time than would be predicted by chance. Implications for theory and practice are discussed.},
  keywords={Solid modeling;Social computing;Three-dimensional displays;Correlation;Affordances;Virtual environments;Psychology;Human-centered computing-Human computer interaction (HCI-Interaction paradigms-Virtual reality;Human-centered computing-Collaborative and social computing-Empirical studies in collaborative and social computing},
  doi={10.1109/VR55154.2023.00056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108460,
  author={Kavaklı, Koray and Itoh, Yuta and Urey, Hakan and Akşit, Kaan},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Realistic Defocus Blur for Multiplane Computer-Generated Holography}, 
  year={2023},
  volume={},
  number={},
  pages={418-426},
  abstract={This paper introduces a new multiplane CGH computation method to reconstruct artifact-free high-quality holograms with natural-looking defocus blur. Our method introduces a new targeting scheme and a new loss function. While the targeting scheme accounts for defocused parts of the scene at each depth plane, the new loss function analyzes focused and defocused parts separately in reconstructed images. Our method support phase-only CGH calculations using various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative (e.g., Double Phase) CGH techniques. We achieve our best image quality using a modified gradient descent-based optimization recipe where we introduce a constraint inspired by the double phase method. We validate our method experimentally using our proof-of-concept holographic display, comparing various algorithms, including multi-depth scenes with sparse and dense contents.},
  keywords={Technological innovation;Three-dimensional displays;Virtual reality;Holography;User interfaces;Optical imaging;Holographic optical components;Hardware—Emerging Technologies—Emerging optical and photonic technology;Hardware—Communication hardware, interfaces and storage—Display and imagers},
  doi={10.1109/VR55154.2023.00057},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108481,
  author={Yi, Xin and Wang, Xueyang and Li, Jiaqi and Li, Hewu},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Examining the Fine Motor Control Ability of Linear Hand Movement in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={427-437},
  abstract={Linear hand movement in mid-air is one of the most fundamental interactions in virtual reality (e.g., when dragging/scaling/manipulating objects and drawing shapes). However, the lack of tactile feedback makes it difficult to precisely control the direction and amplitude of hand movement. In this paper, we conducted three user studies to progressively examine users' ability of fine motor control in 3D linear hand movement tasks. In Study 1, we examined participants' behavioural patterns when drawing straight lines in various directions and lengths, using both the hand and the controller. Results showed that the exhibited stroke length tended to be longer than perceived, regardless of the interaction tool. While displaying the trajectory could help reduce directional and length errors. In Study 2, we further tested the effect of different visual references and found that, compared with an empty room or cluttered scenarios, providing only a virtual table yielded higher input precision and user preference. In Study 3, we repeated Study 2 in real dragging and scaling tasks and verified the generalizability of the findings in terms of input error. Our core finding is that the user's hand moves significantly longer than the task length due to the underestimation of stroke length, yet the error of the Z-axis movement is smaller than that of the X-axis and the Y-axis, and a simple virtual desktop can effectively reduce errors.},
  keywords={Motor drives;Visualization;Solid modeling;Three-dimensional displays;Shape;Tactile sensors;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User models},
  doi={10.1109/VR55154.2023.00058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108489,
  author={Fidalgo, Catarina G. and Sousa, Maurício and Mendes, Daniel and Dos Anjos, Rafael Kuffner and Medeiros, Daniel and Singh, Karan and Jorge, Joaquim},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={MAGIC: Manipulating Avatars and Gestures to Improve Remote Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={438-448},
  abstract={Remote collaborative work has become pervasive in many settings, ranging from engineering to medical professions. Users are im-mersed in virtual environments and communicate through life-sized avatars that enable face-to-face collaboration. Within this context, users often collaboratively view and interact with virtual 3D models, for example to assist in the design of new devices such as cus-tomized prosthetics, vehicles or buildings. Discussing such shared 3D content face-to-face, however, has a variety of challenges such as ambiguities, occlusions, and different viewpoints that all decrease mutual awareness, which in turn leads to decreased task performance and increased errors. To address this challenge, we introduce MAGIC, a novel approach for understanding pointing gestures in a face-to-face shared 3D space, improving mutual understanding and awareness. Our approach distorts the remote user's gestures to correctly reflect them in the local user's reference space when face-to-face. To measure what two users perceive in common when using pointing gestures in a shared 3D space, we introduce a novel metric called pointing agreement. Results from a user study suggest that MAGIC significantly improves pointing agreement in face-to-face collaboration settings, improving co-presence and awareness of interactions performed in the shared space. We believe that MAGIC improves remote collaboration by enabling simpler communication mechanisms and better mutual awareness.},
  keywords={Performance evaluation;Solid modeling;Social computing;Three-dimensional displays;Avatars;Collaboration;Virtual environments;Human-centered computing-Collaborative and social computing-Collaborative and social computing theory;con-cepts and paradigms-Computer supported cooperative work},
  doi={10.1109/VR55154.2023.00059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108407,
  author={Li, Yuqi and Fu, Qiang and Heidrich, Wolfgang},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Extended Depth-of-Field Projector using Learned Diffractive Optics}, 
  year={2023},
  volume={},
  number={},
  pages={449-459},
  abstract={Projector Depth-of-Field (DOF) refers to the projection range of projector images in focus. It is a crucial property of projectors in spatial augmented reality (SAR) applications since wide projector DOF can increase the effective projection area on the projection surfaces with large depth variances and thus reduce the number of projectors required. Existing state-of-the-art methods attempt to create all-in-focus displays by adopting either a deep deblurring network or light modulation. Unlike previous work that considers the optimization of the deblurring model and physic modulation separately, in this paper, we propose an end-to-end joint optimization method to learn a diffractive optical element (DOE) placed in front of a projector lens and a compensation network for deblurring. Using the desired image and the captured projection result image, the compensation network can directly output the compensated image for display. We evaluate the proposed method in physical simulation and with a real experimental prototype, showing that the proposed method can extend the projector DOF by a minor modification to the projector and thus superior to the normal projection with a shallow DOF. The compensation method is also compared with the state-of-the-art methods and shows the advance in radiometric compensation in terms of computational efficiency and image quality.},
  keywords={Solid modeling;Three-dimensional displays;Spatial augmented reality;Prototypes;Optimization methods;Virtual reality;User interfaces;Computing methodologies-Computer graphics-Image manipulation-Image processing},
  doi={10.1109/VR55154.2023.00060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108428,
  author={Maslych, Mykola and Hmaiti, Yahya and Ghamandi, Ryan and Leber, Paige and Kattoju, Ravi Kiran and Belga, Jacob and LaViola, Joseph J.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward Intuitive Acquisition of Occluded VR Objects Through an Interactive Disocclusion Mini-map}, 
  year={2023},
  volume={},
  number={},
  pages={460-470},
  abstract={Standard selection techniques such as ray casting fail when virtual objects are partially or fully occluded. In this paper, we present two novel approaches that combine cone-casting, world-in-miniature, and grasping metaphors to disocclude objects in the representation local to the user. Through a within-subject study where we compared 4 selection techniques across 3 levels of object occlusion, we found that our techniques outperformed an alternative one that also focuses on maintaining the spatial relationships between objects. We discuss application scenarios and future research directions for these types of selection techniques.},
  keywords={Visualization;Casting;Three-dimensional displays;Virtual reality;Grasping;Maintenance engineering;User interfaces;visualization;object selection;virtual reality;occlusion;mini map;head mounted displays;user studies},
  doi={10.1109/VR55154.2023.00061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108455,
  author={Shi, Xuehuai and Wang, Lili and Wu, Jian and Ke, Wei and Lam, Chan-Tong},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Locomotion-aware Foveated Rendering}, 
  year={2023},
  volume={},
  number={},
  pages={471-481},
  abstract={Optimizing rendering performance improves the user's immersion in virtual scene exploration. Foveated rendering uses the features of the human visual system (HVS) to improve rendering performance without sacrificing perceptual visual quality. We collect and analyze the viewing motion of different locomotion methods, and describe the effects of these viewing motions on HVS's sensitivity, as well as the advantages of these effects that may bring to foveated rendering. Then we propose the locomotion-aware foveated rendering method (LaFR) to further accelerate foveated rendering by leveraging the advantages. In LaFR, we first introduce the framework of LaFR. Secondly, we propose an eccentricity-based shading rate controller that provides the shading rate control of the given region in foveated rendering. Thirdly, we propose a locomotion-aware log-polar mapping method, which controls the foveal average shading rate, the peripheral shading rate decrease speed, and the overall shading quantity with the locomotion-aware coefficients based on the eccentricity-based shading rate controller. LaFR achieves similar perceptual visual quality as the conventional foveated rendering while achieving up to 1.6× speedup. Compared with the full resolution rendering, LaFR achieves up to 3.8× speedup.},
  keywords={Visualization;Three-dimensional displays;Sensitivity;Virtual reality;Visual systems;User interfaces;Rendering (computer graphics);Virtual Reality;Foveated Rendering;Gaze-contingent Rendering;Perception},
  doi={10.1109/VR55154.2023.00062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108466,
  author={Batmaz, Anil Ufuk and Mughribi, Moaaz Hudhud and Sarac, Mine and Machuca, Mayra Barrera and Stuerzlinger, Wolfgang},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Measuring the Effect of Stereo Deficiencies on Peripersonal Space Pointing}, 
  year={2023},
  volume={},
  number={},
  pages={1-11},
  abstract={State-of-the-art Virtual Reality (VR) and Augmented Reality (AR) headsets rely on singlefocal stereo displays. For objects away from the focal plane, such displays create a vergence-accommodation conflict (VAC), potentially degrading user interaction performance. In this paper, we study how the VAC affects pointing at targets within arm's reach with virtual hand and raycasting interaction in current stereo display systems. We use a previously proposed experimental methodology that extends the ISO 9241–411:2015 multi-directional selection task to enable fair comparisons between selecting targets in different display conditions. We conducted a user study with eighteen participants and the results indicate that participants were faster and had higher throughput in the constant VAC condition with the virtual hand. We hope that our results enable designers to choose more efficient interaction methods in virtual environments.},
  keywords={Headphones;Three-dimensional displays;ISO Standards;Display systems;Virtual environments;User interfaces;Throughput;Human-centered computing-Human Computer Interaction (HCI);Human-centered computing-Virtual Reality;Human-centered computing-Pointing},
  doi={10.1109/VR55154.2023.00063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108434,
  author={Mithun, Niluthpol Chowdhury and Minhas, Kshitij S. and Chiu, Han-Pang and Oskiper, Taragay and Sizintsev, Mikhail and Samarasekera, Supun and Kumar, Rakesh},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cross-View Visual Geo-Localization for Outdoor Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={493-502},
  abstract={Precise estimation of global orientation and location is critical to ensure a compelling outdoor Augmented Reality (AR) experience. We address the problem of geo-pose estimation by cross-view matching of query ground images to a geo-referenced aerial satellite image database. Recently, neural network-based methods have shown state-of-the-art performance in cross-view matching. However, most of the prior works focus only on location estimation, ignoring orientation, which cannot meet the requirements in outdoor AR applications. We propose a new transformer neural network-based model and a modified triplet ranking loss for joint location and orientation estimation. Experiments on several benchmark cross-view geo-localization datasets show that our model achieves state-of-the-art performance. Furthermore, we present an approach to extend the single image query-based geo-localization approach by utilizing temporal information from a navigation pipeline for robust continuous geo-localization. Experimentation on several large-scale real-world video sequences demonstrates that our approach enables high-precision and stable AR insertion.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Satellites;Video sequences;Estimation;Benchmark testing;Cross View Visual Geo Localization;Ground Aerial Matching;Outdoor Augmented Reality;Transformer Neural Network;Visual Inertial Navigation},
  doi={10.1109/VR55154.2023.00064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108451,
  author={Bellgardt, Martin and Pape, Sebastian and Gilbert, David and Prochnau, Marcel and König, Georg and Kuhlen, Torsten W.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Optical Bench: Teaching Spherical Lens Layout in VR with Real-Time Ray Tracing}, 
  year={2023},
  volume={},
  number={},
  pages={503-508},
  abstract={Teaching in optical systems design is usually performed on an optical bench. While experimentation plays an important role in education, experiments involving expensive or dangerous components are usually limited to short, heavily supervised sessions. Computer simulations, on the other hand, offer high accessibility, but suffer from reduced realism and tangibility when presented on a 2D screen. For this reason, we present the virtual optical bench, an application that lets users explore spherical lens layouts in virtual reality (VR). We implemented a numerically accurate simulation of optical systems using Nvidia OptiX, as well as a prototypical VR application, which we then evaluated in an expert review with 6 optics experts. Based on their feedback, we re-implemented our VR application in Unreal Engine 4. The re-implementation has since been actively used for teaching optical layouts, where we performed a qualitative evaluation with 18 students. We show that our virtual optical bench achieves good usability and is perceived to enhance the understanding of course contents.},
  keywords={Solid modeling;Optical design;Optical feedback;Education;Layout;User interfaces;Adaptive optics;Applied computing—Education—Interactive learning environments},
  doi={10.1109/VR55154.2023.00065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108443,
  author={Yu, Ming-Fei and Zhang, Lei and Wang, Wu-Fan and Wang, Jia-Hui},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={SCP-SLAM: Accelerating DynaSLAM With Static Confidence Propagation}, 
  year={2023},
  volume={},
  number={},
  pages={509-518},
  abstract={DynaSLAM is the state-of-the-art visual simultaneous localization and mapping (SLAM) in dynamic environments. It adopts a convolutional neural network (CNN) for moving object detection, but usually incurs a very high computational cost because it performs semantic segmentation using the CNN model on every frame. This paper proposes SCP-SLAM, which accelerates DynaSLAM by running the CNN only on keyframes and propagating static confidence through other frames in parallel. The proposed static confidence characterizes the moving object features by the residual defined by inter-frame geometry transformation, which can be computed quickly. Our method combines the effectiveness of a CNN with the efficiency of static confidence in a tightly coupled manner. Extensive experiments on the publicly available TUM and Bonn RGB-D dynamic benchmark datasets demonstrate the efficacy of the method. Compared with DynaSLAM, it enables acceleration by a factor of ten on average, but retains comparable localization accuracy.},
  keywords={Visualization;Solid modeling;Simultaneous localization and mapping;Three-dimensional displays;Semantic segmentation;Computational modeling;Virtual reality;Human-centered computing;Visualization;Visualization techniques},
  doi={10.1109/VR55154.2023.00066},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108490,
  author={Lee, Hyunjin and Woo, Woontack},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Effects of Augmented Reality Notification Type and Placement in AR HMD while Walking}, 
  year={2023},
  volume={},
  number={},
  pages={519-529},
  abstract={Augmented reality (AR) helps users easily accept information when they are walking by providing virtual information in front of their eyes. However, it remains unclear how to present AR notifications considering the expected user reaction to interruption. Therefore, we investigated to confirm appropriate placement methods for each type by dividing it into notification types that are handled immediately (high) or that are performed later (low). We compared two coordinate systems (display-fixed and body-fixed) and three positions (top, right, and bottom) for the notification placement. We found significant effects of notification type and placement on how notifications are perceived during the AR notification experience. Using a display-fixed coordinate system responded faster for high notification types, whereas using a body-fixed coordinate system resulted in quick walking speed for low ones. As for the position, the high types had a higher notification performance at the bottom position, but the low types had enhanced walking performance at the right position. Based on the finding of our experiment, we suggest some recommendations for the future design of AR notification while walking.},
  keywords={Legged locomotion;Three-dimensional displays;Design methodology;Resists;User interfaces;Time factors;Task analysis;Human-centered computing-Human computer in-teraction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Ubiquitous and mobile computing-Ubiquitous and mobile computing design and evaluation methods},
  doi={10.1109/VR55154.2023.00067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108461,
  author={Liao, Shuqi and Zhou, Yuqi and Popescu, Voicu},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={AR Interfaces for Disocclusion—A Comparative Study}, 
  year={2023},
  volume={},
  number={},
  pages={530-540},
  abstract={An important application of augmented reality (AR) is the design of interfaces that reveal parts of the real world to which the user does not have line of sight. The design space for such interfaces is vast, with many options for integrating the visualization of the occluded parts of the scene into the user's main view. This paper compares four AR interfaces for disocclusion: X-ray, Cutaway, Picture-in-picture, and Multiperspective. The interfaces are compared in a within-subjects study (N = 33) over four tasks: counting dynamic spheres, pointing to the direction of an occluded person, finding the closest object to a given object, and finding pairs of matching numbers. The results show that Cutaway leads to poor performance in tasks where the user needs to see both the occluder and the occludee; that Picture-in-picture and Multiperspective have a visualization comprehensiveness advantage over Cutaway and X-ray, but a disadvantage in terms of directional guidance; that X-ray has a task completion time disadvantage due to the visualization complexity; and that participants gave Cutaway and Picture-in-picture high, and Multiperspective and X-ray low usability scores.},
  keywords={Visualization;Three-dimensional displays;User interfaces;Complexity theory;Task analysis;Usability;Augmented reality;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality},
  doi={10.1109/VR55154.2023.00068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108457,
  author={Truong-Allié, Camille and Herbeth, Martin and Paljic, Alexis},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A study of the influence of AR on the perception, comprehension and projection levels of situation awareness}, 
  year={2023},
  volume={},
  number={},
  pages={541-551},
  abstract={In this work, we examine how Augmented Reality (AR) impacts user's situation awareness (SA) on elements secondary to an AR-assisted main task, i.e. not directly concerned by the main task. These secondary elements can still provide relevant information that we do not want the user to miss. A good understanding of user's awareness about them is therefore interesting, especially in a context of a daily use of AR, in which not all elements of user's environment are controlled. In this regard, we measured SA about secondary elements in an industrial workshop where the AR-assisted main task is a pedestrian navigation. We compared SA between three navigation guidance conditions: a paper map, a virtual path, and a virtual path with virtual cues about secondary elements. These secondary elements were either hazardous areas, for example, for mandatory helmets, or items which could be on user's path, for example, misplaced carts, boxes… We adapted an existing SA method evaluation to a real-world environment. With this method, participants were queried about their SA on three levels: perception, comprehension and projection about different items. We found that the use of AR decreased user's SA about secondary elements, and that this degradation mainly occurs at the perception level: with AR, participants are less likely to detect secondary elements. Participants still felt the most secure with AR and virtual cues about secondary elements.},
  keywords={Hazardous areas;Solid modeling;Three-dimensional displays;Limiting;Head;Navigation;Resists;Augmented Reality;Situation Awareness;Head Mounted Display;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR55154.2023.00069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108422,
  author={Saint-Aubert, Justine and Argelaguet, Ferran and Macé, Marc and Pacchierotti, Claudio and Amedi, Amir and Lécuyer, Anatole},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Persuasive Vibrations: Effects of Speech-Based Vibrations on Persuasion, Leadership, and Co-Presence During Verbal Communication in VR}, 
  year={2023},
  volume={},
  number={},
  pages={552-560},
  abstract={In Virtual Reality (VR), a growing number of applications involve verbal communications with avatars, such as for teleconference, entertainment, virtual training, social networks, etc. In this context, our paper aims to investigate how tactile feedback consisting in vibrations synchronized with speech could influence aspects related to VR social interactions such as persuasion, co-presence and leadership. We conducted two experiments where participants embody a first-person avatar attending a virtual meeting in immersive VR. In the first experiment, participants were listening to two speaking virtual agents and the speech of one agent was augmented with vibrotactile feedback. Interestingly, the results show that such vibrotactile feedback could significantly improve the perceived co-presence but also the persuasiveness and leadership of the haptically-augmented agent. In the second experiment, the participants were asked to speak to two agents, and their own speech was augmented or not with vibrotactile feedback. The results show that vibrotactile feedback had again a positive effect on co-presence, and that participants perceive their speech as more persuasive in presence of haptic feedback. Taken together, our results demonstrate the strong potential of haptic feedback for supporting social interactions in VR, and pave the way to novel usages of vibrations in a wide range of applications in which verbal communication plays a prominent role.},
  keywords={Vibrations;Training;Leadership;Three-dimensional displays;Avatars;Tactile sensors;User interfaces;Audio;Haptic;Vibrotactile feedback;Speech;Co-Presence;Leadership;Persuasion},
  doi={10.1109/VR55154.2023.00070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108409,
  author={Ang, Samuel and Fernandez, Amanda and Rushforth, Michael and Quarles, John},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={You Make Me Sick! The Effect of Stairs on Presence, Cybersickness, and Perception of Embodied Conversational Agents}, 
  year={2023},
  volume={},
  number={},
  pages={561-570},
  abstract={Virtual reality (VR) technologies are used in a diverse range of applications. Many of these involve an embodied conversational agent (ECA), a virtual human who exchanges information with the user. Unfortunately, VR technologies remain inaccessible to many users due to the phenomenon of cybersickness: a collection of negative symptoms such as nausea and headache that can appear when immersed in a simulation. Many factors are believed to affect a user's level of cybersickness, but little is known regarding how these factors may influence a user's opinion of an ECA. In this study, we examined the effects of virtual stairs, a factor associated with increased levels of cybersickness. We recruited 39 participants to complete a simulated airport experience. This involved a simple navigation task followed by a brief conversation with a virtual airport customs agent in Spanish. Participants completed the experience twice, once walking across flat hallways, and once traversing a series of staircases. We collected self-reported ratings of cybersickness, presence, and perception of the ECA. We additionally collected physiological data on heart rate and galvanic skin response. Results indicate that the virtual staircases increased user level's of cybersickness and reduced their perceived realism of the ECA, but increased levels of presence.},
  keywords={Geometry;Solid modeling;Three-dimensional displays;Cybersickness;Navigation;Stairs;User interfaces;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR55154.2023.00071},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108086,
  author={Choudhary, Zubin and Norouzi, Nahal and Erickson, Austin and Schubert, Ryan and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Social Influence of Virtual Humans Unintentionally Conveying Conflicting Emotions}, 
  year={2023},
  volume={},
  number={},
  pages={571-580},
  abstract={The expression of human emotion is integral to social interaction, and in virtual reality it is increasingly common to develop virtual avatars that attempt to convey emotions by mimicking these visual and aural cues, i.e. the facial and vocal expressions. However, errors in (or the absence of) facial tracking can result in the rendering of incorrect facial expressions on these virtual avatars. For example, a virtual avatar may speak with a happy or unhappy vocal inflection while their facial expression remains otherwise neutral. In circumstances where there is conflict between the avatar's facial and vocal expressions, it is possible that users will incorrectly interpret the avatar's emotion, which may have unintended consequences in terms of social influence or in terms of the outcome of the interaction. In this paper, we present a human-subjects study (N = 22) aimed at understanding the impact of conflicting facial and vocal emotional expressions. Specifically we explored three levels of emotional valence (unhappy, neutral, and happy) expressed in both visual (facial) and aural (vocal) forms. We also investigate three levels of head scales (down-scaled, accurate, and up-scaled) to evaluate whether head scale affects user interpretation of the conveyed emotion. We find significant effects of different multimodal expressions on happiness and trust perception, while no significant effect was observed for head scales. Evidence from our results suggest that facial expressions have a stronger impact than vocal expressions. Additionally, as the difference between the two expressions increase, the less predictable the multimodal expression becomes. For example, for the happy-looking and happy-sounding multimodal expression, we expect and see high happiness rating and high trust, however if one of the two expressions change, this mismatch makes the expression less predictable. We discuss the relationships, implications, and guidelines for social applications that aim to leverage multimodal social cues.},
  keywords={Visualization;Three-dimensional displays;Avatars;Design methodology;User interfaces;Dynamic range;Rendering (computer graphics);Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods-User studies;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms-Virtual reality},
  doi={10.1109/VR55154.2023.00072},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108432,
  author={Akşit, Kaan and Itoh, Yuta},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={HoloBeam: Paper-Thin Near-Eye Displays}, 
  year={2023},
  volume={},
  number={},
  pages={581-591},
  abstract={An emerging alternative to conventional Augmented Reality (AR) glasses designs, Beaming displays promise slim AR glasses free from challenging design trade-offs, including battery-related limits or computational budget-related issues. These beaming displays remove active components such as batteries and electronics from AR glasses and move them to a projector that projects images to a user from a distance (1–2 meters), where users wear only passive optical eyepieces. However, earlier implementations of these displays delivered poor resolutions (7 cycles per degree) without any optical focus cues and were introduced with a bulky form-factor eyepiece ($\sim 50\ mm$ thick). This paper introduces a new milestone for beaming displays, which we call HoloBeam. In this new design, a custom holographic projector populates a micro-volume located at some distance (1–2 meters) with multiple planes of images. Users view magnified copies of these images from this small volume with the help of an eyepiece that is either a Holographic Optical Element (HOE) or a set of lenses. Our HoloBeam prototypes demonstrate the thinnest AR glasses to date with submillimeter thickness (e.g., HOE film is only $120\ \mu m$ thick). In addition, HoloBeam prototypes demonstrate near retinal resolutions (24 cycles per degree) with a 70 degrees-wide field of view.},
  keywords={Visualization;Image resolution;Three-dimensional displays;Prototypes;Glass;Holography;Optical imaging;Holographic Displays;Computer Generated Holography;Computational Displays;Augmented Reality;Virtual Reality;Near Eye Displays;AR glasses;VR Headsets;Optics;Learned Methods;Optimization;Machine Learning;Deep Learning},
  doi={10.1109/VR55154.2023.00073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108447,
  author={Cheng, Shiwei and Tian, Jieming},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Haptic Stimulation-Based Training Method to Improve the Quality of Motor Imagery EEG Signal in VR}, 
  year={2023},
  volume={},
  number={},
  pages={592-602},
  abstract={With the emergence of brain-computer interface (BCI) technology and virtual reality (VR), how to improve the quality of motor imagery (MI) electroencephalogram (EEG) signal has become a key issue for MI BCI applications under VR. In this paper, we proposed to enhance the quality of MI EEG signal by using haptic stimulation training. We designed a first-person perspective and a third-person perspective scene under VR, and the experimental results showed that the left- and right-hand MI EEG quality of the participants improved significantly compared with that before training, and the mean differentiation of the left- and right-hand MI tasks was improved by 21.8% and 15.7%, respectively. We implemented a BCI application system in VR and developed a game based on MI EEG for control of ball movement, in which the average classification accuracy by the participants after training in the first-person perspective reached 93.5%, which was a significant improvement over existing study.},
  keywords={Training;Three-dimensional displays;Games;Virtual reality;Electroencephalography;Real-time systems;Brain-computer interfaces;Brain computer interface;Virtual reality;Haptic stimulation},
  doi={10.1109/VR55154.2023.00074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108426,
  author={Iuchi, Masatoshi and Hirohashi, Yuito and Oku, Hiromasa},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Proposal for an aerial display using dynamic projection mapping on a distant flying screen}, 
  year={2023},
  volume={},
  number={},
  pages={603-608},
  abstract={In this study, we propose a method for an aerial display. The method uses a high-speed gaze control system and a laser display to perform projection mapping on a screen at a distance, which is suspended from a flying drone. A prototype system was developed and successfully demonstrated dynamic projection mapping on a screen attached to a flying drone at a distance of about 36 m, which indicated the effectiveness of the proposed method.},
  keywords={Meters;Three-dimensional displays;Image resolution;Prototypes;Virtual reality;User interfaces;Control systems;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Hardware-Communication hardware;interfaces and storage-Displays and imagers},
  doi={10.1109/VR55154.2023.00075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108450,
  author={Kundu, Ripan Kumar and Islam, Rifatul and Quarles, John and Hoque, Khaza Anuarul},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={LiteVR: Interpretable and Lightweight Cybersickness Detection using Explainable AI}, 
  year={2023},
  volume={},
  number={},
  pages={609-619},
  abstract={Cybersickness is a common ailment associated with virtual reality (VR) user experiences. Several automated methods exist based on machine learning (ML) and deep learning (DL) to detect cyber-sickness. However, most of these cybersickness detection methods are perceived as computationally intensive and black-box methods. Thus, those techniques are neither trustworthy nor practical for deploying on standalone energy-constrained VR head-mounted devices (HMDs). In this work, we present an explainable artificial intelligence (XAI)-based framework Lite VR for cybersickness detection, explaining the model's outcome, reducing the feature dimensions, and overall computational costs. First, we develop three cybersick-ness DL models based on long-term short-term memory (LSTM), gated recurrent unit (GRU), and multilayer perceptron (MLP). Then, we employed a post-hoc explanation, such as SHapley Additive Explanations (SHAP), to explain the results and extract the most dominant features of cybersickness. Finally, we retrain the DL models with the reduced number of features. Our results show that eye-tracking features are the most dominant for cybersickness detection. Furthermore, based on the XAI-based feature ranking and dimensionality reduction, we significantly reduce the model's size by up to 4.3×, training time by up to 5.6×, and its inference time by up to 3.8×, with higher cybersickness detection accuracy and low regression error (i.e., on Fast Motion Scale (FMS)). Our proposed lite LSTM model obtained an accuracy of 94% in classifying cyber-sickness and regressing (i.e., FMS 1–10) with a Root Mean Square Error (RMSE) of 0.30, which outperforms the state-of-the-art. Our proposed Lite VR framework can help researchers and practitioners analyze, detect, and deploy their DL-based cybersickness detection models in standalone VR HMDs.},
  keywords={Training;Solid modeling;Frequency modulation;Three-dimensional displays;Cybersickness;Computational modeling;Gaze tracking;Virtual Reality;Cybersickness Detection;Explainable Artificial Intelligence;Deep Learning;Model Reduction},
  doi={10.1109/VR55154.2023.00076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108459,
  author={Teo, Theophilus and Sakurada, Kuniharu and Sugimoto, Maki},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Enhancements towards Gaze Oriented Parallel Views in Immersive Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={620-630},
  abstract={Parallel view is a technique that allows a VR user to see multiple locations at a time. It enables the user to control several remote or virtual body parts while seeing parallel views to solve synchronous tasks. However, these techniques only explored the benefits and drawbacks of a user performing different tasks. In this paper, we explored enhancements on a singular or asynchronous task by utilizing information obtained in parallel views. We developed three prototypes where parallel views are fixed, moving in symmetric order, or following the user's eye gaze. We conducted a user study to compare each prototype against traditional VR (without parallel views) in three types of tasks: object search and interaction tasks in a 1) simple environment and 2) complex environment, and 3) object distances estimation task. We found parallel views improved multi-embodiment while each technique helped different tasks. No parallel view provided a clean interface, thus improving spatial presence, mental effort, and user performance. However, participants' feedback highlighted potential usefulness and a lower physical effort by using parallel views to solve complicated tasks.},
  keywords={Legged locomotion;Three-dimensional displays;Prototypes;Estimation;Immersive experience;User interfaces;Search problems;Human-centered computing—Visualization;Human-centered computing—Virtual reality},
  doi={10.1109/VR55154.2023.00077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108418,
  author={Wu, Sixuan and Li, Jiannan and Sousa, Maurício and Grossman, Tovi},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Guardian Awareness Techniques to Promote Safety in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={631-640},
  abstract={Virtual Reality (VR) can completely immerse users in a virtual world and provide little awareness of bystanders in the surrounding physical environment. Current technologies use predefined guardian area visualizations to set safety boundaries for VR interactions. However, bystanders cannot perceive these boundaries and may collide with VR users if they accidentally enter guardian areas. In this paper, we investigate four awareness techniques on mobile phones and smartwatches to help bystanders avoid invading guardian areas. These techniques include augmented reality boundary overlays and visual, auditory, and haptic alerts indicating bystanders' distance from guardians. Our findings suggest that the proposed techniques effectively keep participants clear of the safety boundaries. More specifically, using augmented reality overlays, participants could avoid guardians with less time, and haptic alerts caused less distraction.},
  keywords={Visualization;Three-dimensional displays;Shape;User interfaces;Mobile handsets;Haptic interfaces;Safety;Human-centered computing-Visualization-Visualization techniques-Treemaps;Human-centered computing-;Visualization-Visualization design and evaluation methods},
  doi={10.1109/VR55154.2023.00078},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108495,
  author={Li, Ou and Qiu, Han},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality in Supporting Charitable Giving: The Role of Vicarious Experience, Existential Guilt, and Need for Stimulation}, 
  year={2023},
  volume={},
  number={},
  pages={641-647},
  abstract={Although a growing number of charities have used virtual reality (VR) technology for fundraising activities, with better results than ever before, little research has been undertaken on what factors make VR beneficial in supporting charitable giving. The primary goal of this study is to investigate the underlying mechanism of VR in supporting charitable giving, which extends the current literature on VR and donation behaviors. The findings of this study indicated that VR charitable appeals increase actual money donations when compared to the traditional two-dimensional (2D) format and that this effect is achieved through a serial mediating effect of vicarious experience and existential guilt. Findings also identify the need for stimulation as a boundary condition, indicating that those with a higher (vs. lower) need for stimulation were more (vs. less) affected by the mediating mechanism of VR charitable appeals on donations. This work contributes to our understanding of the relationship between VR technology and charitable giving, as well as to future research on VR and its prosocial applications.},
  keywords={Computers;Three-dimensional displays;Two dimensional displays;Virtual reality;User interfaces;Boundary conditions;Behavioral sciences;Virtual reality;charitable giving;vicarious experience;guilt;need for stimulation;J.4 [Social and Behavioral Sciences];K.4.4 [Computers and Society]: Electronic Commerce},
  doi={10.1109/VR55154.2023.00079},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108420,
  author={Dasari, Mallesham and Lu, Edward and Farb, Michael W. and Pereira, Nuno and Liang, Ivan and Rowe, Anthony},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Scaling VR Video Conferencing}, 
  year={2023},
  volume={},
  number={},
  pages={648-657},
  abstract={Virtual Reality (VR) telepresence platforms are being challenged to support live performances, sporting events, and conferences with thousands of users across seamless virtual worlds. Current systems have struggled to meet these demands which has led to high-profile performance events with groups of users isolated in parallel sessions. The core difference in scaling VR environments compared to classic 2D video content delivery comes from the dynamic peer-to-peer spatial dependence on communication. Users have many pair-wise interactions that grow and shrink as they explore spaces. In this paper, we discuss the challenges of VR scaling and present an architecture that supports hundreds of users with spatial audio and video in a single virtual environment. We leverage the property of spatial locality with two key optimizations: (1) a Quality of Service (QoS) scheme to prioritize audio and video traffic based on users' locality, and (2) a resource manager that allocates client connections across multiple servers based on user proximity within the virtual world. Through real-world deployments and extensive evaluations under real and simulated environments, we demonstrate the scalability of our platform while showing improved QoS compared with existing approaches.},
  keywords={Three-dimensional displays;Telepresence;Scalability;Spatial audio;Virtual environments;Quality of service;User interfaces;VR.;Video.;Conferencing.},
  doi={10.1109/VR55154.2023.00080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108083,
  author={Nie, Tongyu and Adhanom, Isayas Berhe and Rosenberg, Evan Suma},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Like a Rolling Stone: Effects of Space Deformation During Linear Acceleration on Slope Perception and Cybersickness}, 
  year={2023},
  volume={},
  number={},
  pages={658-668},
  abstract={The decoupled relationship between the optical and inertial information in virtual reality is commonly acknowledged as a major factor contributing to cybersickness. Based on laws of physics, we noticed that a slope naturally affords acceleration, and the gravito-inertial force we experience when we are accelerating freely on a slope has the same relative direction and approximately the same magnitude as the gravity we experience when standing on the ground. This provides the opportunity to simulate a slope by manipulating the orientation of virtual objects accordingly with the accelerating optical flow. In this paper, we present a novel space deformation technique that deforms the virtual environment to replicate the structure of a slope when the user accelerates virtually. As a result, we can restore the physical relationship between the optical and inertial information available to the user. However, the changes to the geometry of the virtual environment during space deformation remain perceptible to users. Consequently, we created two different transition effects, pinch and tilt, which provide different visual experiences of ground bending. A human subject study (N=87) was conducted to evaluate the effects of space deformation on both slope perception and cyber-sickness. The results confirmed that the proposed technique created a strong feeling of traveling on a slope, but no significant differences were found on measures of discomfort and cybersickness.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Deformation;Cybersickness;Virtual environments;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Com-puting methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;},
  doi={10.1109/VR55154.2023.00081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108445,
  author={Quijano-Chavez, Carlos and Nedel, Luciana and Freitas, Carla M. D. S.},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Scatterplot Variants for Temporal Trends Visualization in Immersive Virtual Environments}, 
  year={2023},
  volume={},
  number={},
  pages={669-679},
  abstract={Trends are changes in variables or attributes over time, often represented by line plots or scatterplot variants, with time being one of the axes. Interpreting tendencies and estimating trends require observing the lines or points behavior regarding increments, decrements, or both (reversals) in the value of the observed variable. Previous work assessed variants of scatterplots like Animation, Small Multiples, and Overlaid Trails for comparing the effectiveness of trends representation using large and small displays and found differences between them. In this work, we study how best to enable the analyst to explore and perform temporal trend tasks with these same techniques in immersive virtual environments. We designed and conducted a user study based on the approaches followed by previous works regarding visualization and interaction techniques, as well as tasks for comparisons in three-dimensional settings. Results show that Overlaid Trails are the fastest overall, followed by Animation and Small Multiples, while accuracy is task-dependent. We also report results from interaction measures and questionnaires.},
  keywords={Visualization;Three-dimensional displays;Virtual environments;User interfaces;Market research;Animation;Behavioral sciences;Evaluation;graphical perception;immersive analytics;trend visualization;virtual reality},
  doi={10.1109/VR55154.2023.00082},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108464,
  author={Lee, Jong-In and Asente, Paul and Stuerzlinger, Wolfgang},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Designing Viewpoint Transition Techniques in Multiscale Virtual Environments}, 
  year={2023},
  volume={},
  number={},
  pages={680-690},
  abstract={Viewpoint transitions have been shown to improve users' spatial orientation and help them build a cognitive map when they are navigating an unfamiliar virtual environment. Previous work has investigated transitions in single-scale virtual environments, focusing on trajectories and continuity. We extend this work with an in-depth investigation of transition techniques in multiscale virtual environments (MVEs). We identify challenges in navigating MVEs with nested structures and assess how different transition techniques affect spatial understanding and usability. Through two user studies, we investigated transition trajectories, interactive control of transition movement, and speed modulation in a nested MVE. We show that some types of viewpoint transitions enhance users' spatial awareness and confidence in their spatial orientation and reduce the need to revisit a target point of interest multiple times.},
  keywords={Three-dimensional displays;Navigation;Virtual environments;Modulation;Focusing;User interfaces;Trajectory;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/VR55154.2023.00083},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10108453,
  author={Albrecht, Matthias and Assländer, Lorenz and Reiterer, Harald and Streuber, Stephan},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={MoPeDT: A Modular Head-Mounted Display Toolkit to Conduct Peripheral Vision Research}, 
  year={2023},
  volume={},
  number={},
  pages={691-701},
  abstract={Peripheral vision plays a significant role in human perception and orientation. However, its relevance for human-computer interaction, especially head-mounted displays, has not been fully explored yet. In the past, a few specialized appliances were developed to display visual cues in the periphery, each designed for a single specific use case only. A multi-purpose headset to exclusively augment peripheral vision did not exist yet. We introduce MoPeDT: Modular Peripheral Display Toolkit, a freely available, flexible, reconfigurable, and extendable headset to conduct peripheral vision research. MoPeDT can be built with a 3D printer and off-the-shelf components. It features multiple spatially configurable near-eye display modules and full 3D tracking inside and outside the lab. With our system, researchers and designers may easily develop and prototype novel peripheral vision interaction and visualization techniques. We demonstrate the versatility of our headset with several possible applications for spatial awareness, balance, interaction, feedback, and notifications. We conducted a small study to evaluate the usability of the system. We found that participants were largely not irritated by the peripheral cues, but the headset's comfort could be further improved. We also evaluated our system based on established heuristics for human-computer interaction toolkits to show how MoPeDT adapts to changing requirements, lowers the entry barrier for peripheral vision research, and facilitates expressive power in the combination of modular building blocks.},
  keywords={Headphones;Visualization;Three-dimensional displays;Head-mounted displays;Prototypes;Virtual reality;Printers;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—Interactive systems and tools—User interface toolkits},
  doi={10.1109/VR55154.2023.00084},
  ISSN={2642-5254},
  month={March},}
