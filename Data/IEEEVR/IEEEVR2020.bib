@INPROCEEDINGS{9089654,
  author={Choi, Youjin and Lee, Jeongmi and Lee, Sung-Hee},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Locomotion Style and Body Visibility of a Telepresence Avatar}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  abstract={Telepresence avatars enable users in different environments to interact with each other. In order to increase the effectiveness of these interactions, however, the movements of avatars must be adjusted accordingly to account for differences between user environments. For instance, if a user moves from one point to another in one environment, the avatar’s locomotion speed must be adjusted to move to the corresponding target point in another environment at the same time. Several locomotion styles can be used to achieve this speed change. This paper investigates how different avatar locomotion styles (speed, stride, and glide), body visibility levels (full body and head-to-knee), and views (front views and side views) influence human perceptions of the naturalness of motion, similarity to the user’s locomotion, and the degree of preserving the user’s intention. Our results indicate that 1) speed and stride styles are perceived as more natural than the glide style, while the glide style is more intention-preserving than the others, 2) a greater locomotion speed of the avatar is perceived as more natural, similar, and intention-preserving than slower motion, 3) the perception of naturalness has the greatest impact on people’s preferences for locomotion styles, and that 4) head-to-knee body visibility may enhance the perception of naturalness for the glide style.},
  keywords={Avatars;Legged locomotion;Telepresence;Torso;Three-dimensional displays;Conferences;Telepresence;Motion Retargeting;Perception;Virtual Avatar},
  doi={10.1109/VR46266.2020.00017},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089588,
  author={Nitsche, Michael and McBride, Pierce},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Manipulating Puppets in VR}, 
  year={2020},
  volume={},
  number={},
  pages={10-17},
  abstract={Archiving Performative Objects aimed at applying and conserving puppetry as creative practice in VR. It included 3D scanning and interaction design to capture puppets and their varying control schemes from the archives of the Center for Puppetry Arts. This paper reports on their design and implementation in a VR puppetry set up. Its focuses on the evaluation study (n=18) comparing the interaction of non-experts vs expert puppeteers. The data initially show little differences but a more detailed discussion indicates differing qualitative assessments of puppetry that support its value for VR. Results suggests successful creative activation especially among experts.},
  keywords={Three-dimensional displays;Human computer interaction;Cultural differences;Art;Media;Virtual reality;Solid modeling;Puppetry;Virtual Reality;interaction design},
  doi={10.1109/VR46266.2020.00018},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089510,
  author={Gonzalez-Franco, Mar and Cohn, Brian and Ofek, Eyal and Burin, Dalila and Maselli, Antonella},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Self-Avatar Follower Effect in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={18-25},
  abstract={When embodying a virtual avatar in immersive VR applications where body tracking is enabled, users typically are and feel in control the avatar movements. However, there are situations in which the technology could be tweaked to flip this relationship so that an embodied avatar could affect the user’s motor behavior without users noticing it. This has been shown in action retargeting applications and motor contagion experiments. Here we discuss a different way in which an embodied avatar could implicitly drive users movements: the self-avatar follower effect. We review previous evidences and present new experimental results showing how, whenever the virtual body does not overlay with their physical body, users tend to unconsciously follow their avatar, filling the gap if the system allows for it. We discuss this effect in the context of the relevant neuroscientific literature, and propose a theoretical account of the follower effect at the intersection of motor control and inference theories.},
  keywords={Avatars;Motor drives;Visualization;Computational modeling;Predictive models;Tracking;Human-centered computing;Virtual reality;Embodiment;Perception;Motor control},
  doi={10.1109/VR46266.2020.00019},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089612,
  author={Cho, SungIk and Kim, Seung-wook and Lee, JongMin and Ahn, JeongHyeon and Han, JungHyun},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of volumetric capture avatars on social presence in immersive virtual environments}, 
  year={2020},
  volume={},
  number={},
  pages={26-34},
  abstract={Recent advances in 3D reconstruction and tracking technologies have made it possible to volumetrically capture human body and performance at real time. In the field of human-computer interaction, however, no works have been reported on the user study made with such volumetric capture avatars. This paper investigates how the volumetric capture avatar affects users' sense of social presence in immersive virtual environments. In our experiments, the volumetric capture avatar of an actor is compared with the actor captured in 2D video and another 3D avatar obtained by pre-scanning the actor. The experiment results show that users have the highest sense of social presence with the volumetric capture avatar when performing dynamic tasks whereas they have higher sense of social presence with the volumetric capture avatar and 2D video than with the pre-scanned avatar when performing static tasks. These imply that the emerging volumetric capture techniques can be an attractive tool for mixed reality, telepresence, and many other 3D applications.},
  keywords={Avatars;Three-dimensional displays;Cameras;Two dimensional displays;Image reconstruction;Real-time systems;Resists;Human-centered computing;Visualization;Visualization techniques;Human-centered computing;Visualization;Visualization application domains},
  doi={10.1109/VR46266.2020.00020},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089485,
  author={Lu, Yiqin and Yu, Chun and Shi, Yuanchun},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Bubble Mechanism for Ray-Casting to Improve 3D Target Acquisition in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={35-43},
  abstract={Ray-casting, i.e., a ray cast from a hand-held controller to select targets, is widely used in 3D environments. Inspired by the bubble cursor [12] which dynamically resizes its selection range on 2D surfaces, we investigate a bubble mechanism for ray-casting in virtual reality. Bubble mechanism identifies the target nearest to the ray, with which users do not have to accurately shoot through the target. We first design the criterion of selection and the visual feedback of the bubble. We then conduct two experiments to evaluate ray-casting techniques with bubble mechanism in both simple and complicated 3D target acquisition tasks. Results show the bubble mechanism significantly improves ray-casting on both performance and preference, and our Bubble Ray technique with angular distance definition is competitive compared with other target acquisition techniques. We also discuss potential improvements to show more practical implementations of ray-casting with bubble mechanism.},
  keywords={Three-dimensional displays;Virtual reality;Two dimensional displays;Visualization;Task analysis;Euclidean distance;Conferences;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/VR46266.2020.00021},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089649,
  author={Valentini, Ivan and Ballestin, Giorgio and Bassano, Chiara and Solari, Fabio and Chessa, Manuela},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Improving Obstacle Awareness to Enhance Interaction in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={44-52},
  abstract={Currently, immersive virtual reality is experienced through head-mounted displays while the user is physically present into a real, cluttered environment. This causes the problem of avoiding dangerous collisions with obstacles in the real environment that are invisible to the user, and also hampers the interaction with real objects. Following the augmented virtuality paradigm, these obstacles should be embedded into the virtual environment. Thus, there is the need of knowing the 3D structure of the real environment to align it with the virtual one. In this paper, we present a method to create a virtual scenario composed of virtual objects having the same spatial occupancy of the corresponding real ones. The real scene is scanned to detect the position and bounding box of objects and obstacles, then virtual elements having similar spatial properties are added to the virtual scene. Two different real environment structure detection and clustering techniques are described and compared, both quantitatively and by considering users’ sense of presence with respect to the standard Chaperone technique. Our results show that the method is a good solution to maintain the real environment awareness while keeping an high level of immersivity and sense of presence.},
  keywords={Three-dimensional displays;Virtual environments;Solid modeling;Computational modeling;Transmission line matrix methods;Resists;Human-centered computing;Interaction paradigms;Virtual Reality;Human-centered computing;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR46266.2020.00022},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089663,
  author={Montano-Murillo, Roberto A. and Nguyen, Cuong and Kazi, Rubaiat Habib and Subramanian, Sriram and DiVerdi, Stephen and Martinez-Plasencia, Diego},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Slicing-Volume: Hybrid 3D/2D Multi-target Selection Technique for Dense Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={53-62},
  abstract={3D selection in dense VR environments (e.g., point clouds) is extremely challenging due to occlusion and imprecise mid-air input modalities (e.g., 3D controllers and hand gestures). In this paper, we propose "Slicing-Volume", a hybrid selection technique that enables simultaneous 3D interaction in mid-air, and a 2D pen-and-tablet metaphor in VR. Inspired by well-known slicing plane techniques in data visualization, our technique consists of a 3D volume that encloses target objects in mid-air, which are then projected to a 2D tablet view for precise selection on a tangible physical surface. While slicing techniques and tablets-in-VR have been previously explored, in this paper, we evaluated the potential of this hybrid approach to improve accuracy in highly occluded selection tasks, comparing different multimodal interactions (e.g., Mid-air, Virtual Tablet and Real Tablet). Our results showed that our hybrid technique significantly improved overall accuracy of selection compared to Mid-air selection only, thanks to the added haptic feedback given by the physical tablet surface, rather than the added visualization given by the tablet view.},
  keywords={Three-dimensional displays;Task analysis;Two dimensional displays;Stability analysis;Navigation;Virtual reality;Data visualization;3D selection;hybrid systems;slicing plane;virtual reality;bimanual interaction;tablet},
  doi={10.1109/VR46266.2020.00023},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089479,
  author={Schmitz, Anastasia and MacQuarrie, Andrew and Julier, Simon and Binetti, Nicola and Steed, Anthony},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Directing versus Attracting Attention: Exploring the Effectiveness of Central and Peripheral Cues in Panoramic Videos}, 
  year={2020},
  volume={},
  number={},
  pages={63-72},
  abstract={Filmmakers of panoramic videos frequently struggle to guide attention to Regions of Interest (ROIs) due to consumers’ freedom to explore. Some researchers hypothesize that peripheral cues attract reflexive/involuntary attention whereas cues within central vision engage and direct voluntary attention. This mixed-methods study evaluated the effectiveness of using central arrows and peripheral flickers to guide and focus attention in panoramic videos. Twenty-five adults wore a head-mounted display with an eye tracker and were guided to 14 ROIs in two panoramic videos. No significant differences emerged in regard to the number of followed cues, the time taken to reach and observe ROIs, ROI-related memory and user engagement. However, participants’ gaze travelled a significantly greater distance toward ROIs within the first 500 ms after flicker-onsets compared to arrow-onsets. Nevertheless, most users preferred the arrow and perceived it as significantly more rewarding than the flicker. The findings imply that traditional attention paradigms are not entirely applicable to panoramic videos, as peripheral cues appear to engage both involuntary and voluntary attention. Theoretical and practical implications as well as limitations are discussed.},
  keywords={Videos;Virtual reality;Visualization;Focusing;Head-mounted displays;Modulation;Color;Cinematic Virtual Reality;360° video;head-mounted display;guiding attention;memory;eye-tracking},
  doi={10.1109/VR46266.2020.00024},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089439,
  author={Marañes, Carlos and Gutierrez, Diego and Serrano, Ana},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the impact of 360° movie cuts in users’ attention}, 
  year={2020},
  volume={},
  number={},
  pages={73-82},
  abstract={Virtual Reality (VR) has grown since the first devices for personal use became available on the market. However, the production of cinematographic content in this new medium is still in an early exploratory phase. The main reason is that cinematographic language in VR is still under development, and we still need to learn how to tell stories effectively. A key element in traditional film editing is the use of different cutting techniques, in order to transition seamlessly from one sequence to another. A fundamental aspect of these techniques is the placement and control over the camera. However, VR content creators do not have full control of the camera. Instead, users in VR can freely explore the 360° of the scene around them, which potentially leads to very different experiences. While this is desirable in certain applications such as VR games, it may hinder the experience in narrative VR. In this work, we perform a systematic analysis of users’ viewing behavior across cut boundaries while watching professionally edited, narrative 360° videos. We extend previous metrics for quantifying user behavior in order to support more complex and realistic footage, and we introduce two new metrics that allow us to measure users’ exploration in a variety of different complex scenarios. From this analysis, (i) we confirm that previous insights derived for simple content hold for professionally edited content, and (ii) we derive new insights that could potentially influence VR content creation, informing creators about the impact of different cuts in the audience’s behavior.},
  keywords={Human computer interaction;Videos;Motion pictures;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089591,
  author={Wallgrün, Jan Oliver and Bagher, Mahda M. and Sajjadi, Pejman and Klippel, Alexander},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparison of Visual Attention Guiding Approaches for 360° Image-Based VR Tours}, 
  year={2020},
  volume={},
  number={},
  pages={83-91},
  abstract={Mechanisms for guiding a user’s visual attention to a particular point of interest play a crucial role in areas such as collaborative VR and AR, cinematic VR, and automated or live guided tour experiences in xR-based education. The attention guiding mechanism serves as a communication tool that helps users find entities currently not visible in their view, referenced for instance by another user or in some accompanying audio commentary. We report on a user study in which we compared three different visual guiding mechanisms (arrow, butterfly guide, and radar) in the context of 360° image-based educational VR tour applications of real-world sites. A fourth condition with no guidance tool available was added as a baseline. We investigate the question: How do the different approaches compare in terms of target finding performance and participants’ assessments of the experiences. While all three mechanisms were perceived as improvements over the no-guidance condition and resulted in significantly improved target finding times, the arrow mechanism stands out as the most generally accepted and favored approach, whereas the other two (butterfly guide and radar) received a more polarized assessment due to their specific strengths and drawbacks.},
  keywords={Visualization;Three-dimensional displays;Virtual reality;Radar;Human computer interaction;Two dimensional displays;Head;Human-centered computing—User studies;Human-centered computing—Virtual reality;Human-centered computing— Empirical studies in HCI;Human-centered computing—Empirical studies in visualization},
  doi={10.1109/VR46266.2020.00026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089519,
  author={Chen, Dongwen and Qing, Chunmei and Xu, Xiangmin and Zhu, Huansheng},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SalBiNet360: Saliency Prediction on 360° Images with Local-Global Bifurcated Deep Network}, 
  year={2020},
  volume={},
  number={},
  pages={92-100},
  abstract={With the development of the virtual reality applications, predicting human visual attention on 360° images is valuable to content creators and encoding algorithms, and becomes essential to understand user behaviour. In this paper, we propose a local-global bifurcated deep network for saliency prediction on 360° images, which is named as SalBiNet360. In the global deep sub-network, multiple multi-scale contextual modules and a multilevel decoder are utilized to integrate the features from the middle and deep layers of the network. In the local deep sub-network, only one multi-scale contextual module and a single-level decoder are utilized to reduce the redundancy of local saliency maps. Finally, fused saliency maps are generated by linear combination of the global and local saliency maps. Experiments on two publicly available datasets illustrate that the proposed SalBiNet360 outperforms the tested state-of-the-art methods.},
  keywords={Feature extraction;Observers;Predictive models;Two dimensional displays;Solid modeling;Decoding;Visualization;360° images;SalBiNet360;virtual reality (VR)},
  doi={10.1109/VR46266.2020.00027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089444,
  author={Matsumoto, Keigo and Langbehn, Eike and Narumi, Takuji and Steinicke, Frank},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Detection Thresholds for Vertical Gains in VR and Drone-based Telepresence Systems}, 
  year={2020},
  volume={},
  number={},
  pages={101-107},
  abstract={Several redirected walking techniques have been introduced and analyzed in recent years, while the main focus was on manipulations in horizontal directions, in particular, by means of curvature, rotation, and translation gains. However, less research has been conducted on the manipulation of vertical movements and its possible use as a redirection technique. Actually, vertical movements are fundamentally important, e.g., for remotely steering a drone using a virtual reality headset.In this paper, we explored vertical gains, a novel redirection technique, which enables us to purposefully manipulate the mapping of the user’s physical vertical movements to movements in the virtual space and the remote space. This approach allows natural and more active physical control of a real drone. To demonstrate the usability of vertical gains, we implemented a telepresence drone and vertical redirection techniques for stretching and crouching actions using common VR devices. We conducted two user studies to investigate the effective manipulation ranges and its usability: one study using a virtual environment (VE), and one using a camera stream from a telepresence drone. The results revealed that our technique could manipulate a users vertical movement without her/his noticing.},
  keywords={Drones;Telepresence;Legged locomotion;Resists;Virtual reality;Aerospace electronics;Three-dimensional displays;Drone;Vertical movement;Redirection;Telepresence},
  doi={10.1109/VR46266.2020.00028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089599,
  author={Tsuchiya, Kei and Koizumi, Naoya},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Optical Design for Avatar-User Co-axial Viewpoint Telepresence}, 
  year={2020},
  volume={},
  number={},
  pages={108-116},
  abstract={We propose a mid-air image system for telepresence. Virtual reality (VR) social networks enable users to interact with each other through CG avatars and choose their appearances freely. However, this is only possible in VR space. We propose a system that takes the avatar from VR space to real space with the help of mid-air imaging technology. In this system, the micro-mirror array plates (MMAPs) display the mid-air image and optically transfer the camera viewpoint to capture users from the mid-air image position. Luminance measurement and modulation transfer function (MTF) measurement were performed to evaluate the image capturing performance of this system. As a result, we found that the MMAPs ccause a decrease in brightness and an increase in blur. In addition, the stray light generated by the MMAPs was in the captured video. We also confirmed that face detection works correctly on the captured video by adjusting the ISO sensitivity of the camera. Furthermore, we designed an application for telepresence called Levitar, which uses a dual camera to output the captured video to the HMD and controls the camera gaze direction.},
  keywords={Cameras;Avatars;Optical imaging;High-speed optical techniques;Telepresence;Optical sensors;Optical refraction;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers},
  doi={10.1109/VR46266.2020.00029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089518,
  author={Best, Andrew and Narang, Sahil and Manocha, Dinesh},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SPA: Verbal Interactions between Agents and Avatars in Shared Virtual Environments using Propositional Planning}, 
  year={2020},
  volume={},
  number={},
  pages={117-126},
  abstract={We present a novel approach for generating plausible verbal interactions between virtual human-like agents and user avatars in shared virtual environments. Sense-Plan-Ask, or SPA, extends prior work in propositional planning and natural language processing to enable agents to plan with uncertain information, and leverage question and answer dialogue with other agents and avatars to obtain the needed information and complete their goals. The agents are additionally able to respond to questions from the avatars and other agents using natural-language enabling real-time multi-agent multi-avatar communication environments.Our algorithm can simulate tens of virtual agents at interactive rates interacting, moving, communicating, planning, and replanning. We find that our algorithm creates a small runtime cost and enables agents to complete their goals more effectively than agents without the ability to leverage natural-language communication. We demonstrate quantitative results on a set of simulated benchmarks and detail the results of a preliminary user-study conducted to evaluate the plausibility of the virtual interactions generated by SPA. Overall, we find that participants prefer SPA to prior techniques in 84% of responses including significant benefits in terms of the plausibility of natural-language interactions and the positive impact of those interactions.},
  keywords={Avatars;Planning;Natural languages;Uncertainty;Computational modeling;Robot sensing systems;Virtual environments;Computing methodologies;Artificial intelligence;Distributed artificial intelligence;Multi-agent systems},
  doi={10.1109/VR46266.2020.00030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089539,
  author={Subramanyam, Shishir and Li, Jie and Viola, Irene and Cesar, Pablo},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing the Quality of Highly Realistic Digital Humans in 3DoF and 6DoF: A Volumetric Video Case Study}, 
  year={2020},
  volume={},
  number={},
  pages={127-136},
  abstract={Virtual Reality (VR) and Augmented Reality (AR) applications have seen a drastic increase in commercial popularity. Different representations have been used to create 3D reconstructions for AR and VR. Point clouds are one such representation characterized by their simplicity and versatility, making them suitable for real time applications, such as reconstructing humans for social virtual reality. In this study, we evaluate how the visual quality of digital humans, represented using point clouds, is affected by compression distortions. We compare the performance of the upcoming point cloud compression standard against an octree-based anchor codec. Two different VR viewing conditions enabling 3- and 6 degrees of freedom are tested, to understand how interacting in the virtual space affects the perception of quality. To the best of our knowledge, this is the first work performing user quality evaluation of dynamic point clouds in VR; in addition, contributions of the paper include quantitative data and empirical findings. Results highlight how perceived visual quality is affected by the tested content, and how current data sets might not be sufficient to comprehensively evaluate compression solutions. Moreover, shortcomings in how point cloud encoding solutions handle visually-lossless compression are discussed.},
  keywords={Three-dimensional displays;Transform coding;Image color analysis;Measurement;Codecs;Geometry;Image coding;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00031},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089579,
  author={Cao, Antong and Wang, Lili and Liu, Yi and Popescu, Voicu},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Feature Guided Path Redirection for VR Navigation}, 
  year={2020},
  volume={},
  number={},
  pages={137-145},
  abstract={Path redirection for virtual reality (VR) navigation allows the user to explore a large virtual environment (VE) while the VR application is hosted in a limited physical space. Static mapping redirection methods deform the virtual scene to fit the physical space. The challenge is to deform the virtual scene in a reasonable way, making the distortions friendly to the user’s visual perception. In this paper we propose a feature-guided path redirection method that finds and takes into account the visual features of 3D virtual scenes. In a first offline step, a collection of view-independent and view-dependent visual features of the VE are extracted and stored in a visual feature map. Then, in a second offline step, the navigation path is deformed to fit in the confines of the available physical space through a mass-spring system optimization, according to distortion sensitive factors derived from the visual feature map. Finally, a novel detail preserving rendering algorithm is employed to preserve the original visual detail as the user navigates the VE on the redirected path. We tested our method on several scenes, where our method showed a reduced VE 3D mesh distortion, when compared to the path redirection methods without feature guidance.},
  keywords={Visualization;Distortion;Feature extraction;Geometry;Navigation;Three-dimensional displays;Legged locomotion;Virtual reality;Navigation;Path redirection},
  doi={10.1109/VR46266.2020.00032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089569,
  author={Dong, Tianyang and Chen, Xianwei and Song, Yifan and Ying, Wenyuan and Fan, Jing},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dynamic Artificial Potential Fields for Multi-User Redirected Walking}, 
  year={2020},
  volume={},
  number={},
  pages={146-154},
  abstract={Immersive Virtual Reality (VR) systems that combine Head Mounted Displays (HMDs) and a position tracking system support the multiple users or participants to collaborate in the same physical space for a large-scale virtual environment. Because the multiple users sharing the same physical space are in a dynamic state, the key technique of multi-user VR system is how to solve the problem of potential collisions among the users who are moving both virtually and physically. In order to better solve the collision problem caused by such dynamic changes, this work presents a new strategy of multi-user redirected walking using dynamic artificial potential fields, which generates repulsion to ‘push’ users away from obstacles and other users, and uses gravity to ‘attract’ users to an open or unobstructed space. In this method, the users not only get repulsive forces from walls, but also from other users and their future states that are called avatars. At the same time, the users will get gravitational force from the steering target. The target selection considers the size of open space, the distance between the steering target and the boundary of physical space, and the distance between the steering target and the center of the physical space. Therefore, the system can steer users to an open area in the physical space to further reduce collisions. To verify the validity of our method, we developed a software to statistically analyze the influence of different factors, such as the physical space size and the number of users. Data from experiments shows that our method reduces the potential user resets by about 20%.},
  keywords={Legged locomotion;Space vehicles;Virtual environments;Force;Heuristic algorithms;Prediction algorithms;Redirected Walking;Virtual Reality;Head-Mounted Display;Multiple users;Virtual Roaming},
  doi={10.1109/VR46266.2020.00033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089532,
  author={Lee, Dong-Yong and Cho, Yong-Hun and Min, Dae-Hong and Lee, In-Kwon},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimal Planning for Redirected Walking Based on Reinforcement Learning in Multi-user Environment with Irregularly Shaped Physical Space}, 
  year={2020},
  volume={},
  number={},
  pages={155-163},
  abstract={Redirected Walking (RDW) enables users to walk in both virtual and physical tracking spaces simultaneously, which is an effective method to increase presence in Virtual Reality (VR). Recently, RDW technologies have been developed in a multi-user environment where multiple users share the same physical tracking space and simultaneously explore the same virtual space. Meanwhile, in the Steer-To-Optimal-Target (S2OT) method, user actions are planned in RDW by introducing machine learning models such as reinforcement learning. In this paper, we propose a new predictive RDW algorithm "Multiuser-Steer-to-Optimal-Target (MS2OT)" that extends the S2OT method into an environment with multiple users and various types of tracking space. In addition to the steering actions used in S2OT, MS2OT considers pre-reset actions and uses more steering targets and an improved reward function. The locations of multiple users and tracking space information are treated as visual information to be the state of the reinforcement learning model in MS2OT. Hence, the artificial neural network of a multilayer three-dimensional convolutional neural network with a dueling double deep network architecture is learned through Q-Learning. MS2OT significantly reduces the total number of resets compared to the conventional RDW algorithms such as S2C and APF-RDW in a multi-user environment and improves the total distance and average distance between resets during the same period. Experimental results show that MS2OT can process up to 32 users in real-time.},
  keywords={Prediction algorithms;Space vehicles;Learning (artificial intelligence);Legged locomotion;Target tracking;Shape;Virtual reality;Redirected walking;resetting;virtual environments;multi-user;collision avoidance;reinforcement learning},
  doi={10.1109/VR46266.2020.00034},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089554,
  author={Min, Dae-Hong and Lee, Dong-Yong and Cho, Yong-Hun and Lee, In-Kwon},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shaking Hands in Virtual Space: Recovery in Redirected Walking for Direct Interaction between Two Users}, 
  year={2020},
  volume={},
  number={},
  pages={164-173},
  abstract={Various studies have been conducted to realize realistic direct interaction in the virtual environment. In this study, we focus on a situation wherein two users using the same physical space explore the same virtual environment using redirected walking (RDW) technology. For two users to meet each other in a virtual environment to realize realistic direct interaction, they must simultaneously meet each other in physical space. However, if the RDW algorithm is applied to each user independently, the relative positions and orientations of the two users can be significantly different in the virtual and physical spaces. We present a recovery algorithm that adjusts the relative position and orientation such that they become the same in the two spaces. Our recovery algorithm uses either modified subtle RDW techniques or overt recovery techniques in three cases depending on the relative position and orientation of the two users. Once the recovered state is reached, the two users can go forward to meet each other and directly interact in the virtual and physical spaces simultaneously. Based on the experiment results, we can confirm that the application of our recovery technology to the system increases the user’s satisfaction in usability and the presence of coexistence in the virtual environment with other users.},
  keywords={Virtual environments;Legged locomotion;Haptic interfaces;Redirected walking;Multi-user redirected walking;Direct interaction;Recovery in redirected walking},
  doi={10.1109/VR46266.2020.00035},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089587,
  author={Hobson, Tanner and Duncan, Jeremiah and Raji, Mohammad and Lu, Aidong and Huang, Jian},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Alpaca: AR Graphics Extensions for Web Applications}, 
  year={2020},
  volume={},
  number={},
  pages={174-183},
  abstract={In this work, we propose a framework to simplify the creation of Augmented Reality (AR) extensions for web applications, without modifying the original web applications. We implemented the framework in an open source package called Alpaca. AR extensions developed using Alpaca appear as a web-browser extension, and automatically bridge the Document Object Model (DOM) of the web with the SceneGraph model of AR. To transform the web application into a multi-device, mixed-space web application, we designed a restrictive and minimized interface for cross-device event handling. We demonstrate our approach to develop mixed-space applications using three examples. These applications are, respectively, for exploring Google Books, exploring biodiversity distribution hosted by the National Park Service of the United States, and exploring YouTube’s recommendation engine. The first two cases show how a 3rd-party developer can create AR extensions without making any modifications to the original web applications. The last case serves as an example of how to create AR extensions when a developer creates a web application from scratch. Alpaca works on the iPhone X, the Google Pixel, and the Microsoft HoloLens.},
  keywords={Three-dimensional displays;Graphics;Browsers;Runtime;Servers;Two dimensional displays;Synchronization;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computer systems organization;Architectures;Distributed architectures;Client-server architectures},
  doi={10.1109/VR46266.2020.00036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089504,
  author={Batmaz, Anil Ufuk and Mutasim, Aunnoy K and Malekmakan, Morteza and Sadr, Elham and Stuerzlinger, Wolfgang},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Touch the Wall: Comparison of Virtual and Augmented Reality with Conventional 2D Screen Eye-Hand Coordination Training Systems}, 
  year={2020},
  volume={},
  number={},
  pages={184-193},
  abstract={Previous research on eye-hand coordination training systems has investigated user performance on a wall, 2D touchscreens, and in Virtual Reality (VR). In this paper, we designed an eye-hand coordination reaction test to investigate and compare user performance in three different virtual environments (VEs) – VR, Augmented Reality (AR), and a 2D touchscreen. VR and AR conditions also included two feedback conditions – mid-air and passive haptics. Results showed that compared to AR, participants were significantly faster and made fewer errors both in 2D and VR. However, compared to VR and AR, throughput performance of the participants was significantly higher in the 2D touchscreen condition. No significant differences were found between the two feedback conditions. The results show the importance of assessing precision and accuracy in eye-hand coordination training and suggest that it is currently not advisable to use AR headsets in such systems.},
  keywords={Training;Haptic interfaces;Task analysis;Two dimensional displays;Throughput;Mathematical model;Human-centered computing;Human Computer Interaction (HCI);Human-centered computing;Virtual Reality;Human-centered computing;Pointing;Human-centered computing;Touch screens},
  doi={10.1109/VR46266.2020.00037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089476,
  author={Jakl, Andreas and Lienhart, Anna-Maria and Baumann, Clemens and Jalaeefar, Arian and Schlager, Alexander and Schöffer, Lucas and Bruckner, Franziska},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enlightening Patients with Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={195-203},
  abstract={Enlightening Patients with Augmented Reality (EPAR) enhances patient education with new possibilities offered by Augmented Reality. Medical procedures are becoming increasingly complex and printed information sheets are often hard to understand for patients. EPAR developed an augmented reality prototype that helps patients with strabismus to better understand the processes of examinations and eye surgeries. By means of interactive storytelling, three identified target groups based on user personas were able to adjust the level of information transfer based on their interests. We performed a 2-phase evaluation with a total of 24 test subjects, resulting in a final system usability score of 80.0. For interaction prompts concerning virtual 3D content, visual highlights were considered to be sufficient. Overall, participants thought that an AR system as a complementary tool could lead to a better understanding of medical procedures.},
  keywords={Education;Three-dimensional displays;Augmented reality;Surgery;Human computer interaction;Usability;Prototypes;Human-centered computing;Mixed / augmented reality Human-centered computing;Interface design prototyping Human-centered computing;Interaction design theory;concepts and paradigms Human-centered computing;Usability testing},
  doi={10.1109/VR46266.2020.00038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089517,
  author={Xu, Wenge and Liang, Hai-Ning and Chen, Yuzheng and Li, Xiang and Yu, Kangyou},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Visual Techniques for Boundary Awareness During Interaction in Augmented Reality Head-Mounted Displays}, 
  year={2020},
  volume={},
  number={},
  pages={204-211},
  abstract={Mid-air hand interaction has long been proposed as a ‘natural’ input method for Augmented Reality (AR) systems. Current AR Head-Mounted Displays (HMDs) have a limited area for hand-based interactions. Because of this, users may easily move their hand(s) outside this tracked area during interaction, especially in dynamic tasks (e.g., when translating an object). Compared to common midair interaction issues, such as gesture recognition, arm/hand fatigue, and unnatural ways of interacting with virtual objects (e.g., selecting a distant object), boundary awareness issues in AR devices have received little attention. In this research, we explore visual techniques for boundary awareness in AR HMDs, focusing on object translation tasks. Through a systematic formative study, we first identify the challenges that users might face when interacting with AR HMDs without any boundary awareness information (i.e., how current systems work). Based on the findings, we then propose four methods (i.e., static surfaces, dynamic surface(s), static coordinated lines, and dynamic coordinate line(s)) and evaluate them against the benchmark (i.e., baseline condition without boundary awareness) to make users aware of the tracked interaction area. Our results show that visual methods for boundary awareness can help with dynamic mid-air hand interactions in AR HMDs, but their effectiveness and application are user-dependent.},
  keywords={Tracking;Visualization;Cameras;Task analysis;Gesture recognition;Sensors;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Human-centered computing;Visualization;Visualization techniques},
  doi={10.1109/VR46266.2020.00039},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089635,
  author={Lin, Chengyuan and Rojas-Muñoz, Edgar and Cabrera, Maria Eugenia and Sanchez-Tamayo, Natalia and Andersen, Daniel and Popescu, Voicu and Barragan Noguera, Juan Antonio and Zarzaur, Ben and Murphy, Pat and Anderson, Kathryn and Douglas, Thomas and Griffis, Clare and Wachs, Juan},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={How About the Mentor? Effective Workspace Visualization in AR Telementoring}, 
  year={2020},
  volume={},
  number={},
  pages={212-220},
  abstract={Augmented Reality (AR) benefits telementoring by enhancing the communication between the mentee and the remote mentor with mentor authored graphical annotations that are directly integrated into the mentee’s view of the workspace. An important problem is conveying the workspace to the mentor effectively, such that they can provide adequate guidance. AR headsets now incorporate a frontfacing video camera, which can be used to acquire the workspace. However, simply providing to the mentor this video acquired from the mentee’s first-person view is inadequate. As the mentee moves their head, the mentor’s visualization of the workspace changes frequently, unexpectedly, and substantially. This paper presents a method for robust high-level stabilization of a mentee first-person video to provide effective workspace visualization to a remote mentor. The visualization is stable, complete, up to date, continuous, distortion free, and rendered from the mentee’s typical viewpoint, as needed to best inform the mentor of the current state of the workspace. In one study, the stabilized visualization had significant advantages over unstabilized visualization, in the context of three number matching tasks. In a second study, stabilization showed good results, in the context of surgical telementoring, specifically for cricothyroidotomy training in austere settings.},
  keywords={Cameras;Visualization;Geometry;Task analysis;Feeds;Resists;Three-dimensional displays;Human-centered computing;Visualization;Graphics systems and interfaces;Mixed/augmented reality},
  doi={10.1109/VR46266.2020.00040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089453,
  author={Englmeier, David and Dörner, Julia and Butz, Andreas and Höllerer, Tobias},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Tangible Spherical Proxy for Object Manipulation in Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={221-229},
  abstract={In this paper, we explore how a familiarly shaped object can serve as a physical proxy to manipulate virtual objects in Augmented Reality (AR) environments. Using the example of a tangible, handheld sphere, we demonstrate how irregularly shaped virtual objects can be selected, transformed, and released. After a brief description of the implementation of the tangible proxy, we present a buttonless interaction technique suited to the characteristics of the sphere. In a user study (N = 30), we compare our approach with three different controller-based methods that increasingly rely on physical buttons. As a use case, we focused on an alignment task that had to be completed in mid-air as well as on a flat surface. Results show that our concept has advantages over two of the controller-based methods regarding task completion time and user ratings. Our findings inform research on integrating tangible interaction into AR experiences.},
  keywords={Three-dimensional displays;Human computer interaction;Shape;Manipulators;Task analysis;Augmented reality;Haptic interfaces;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Haptic devices;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms— Mixed / augmented reality},
  doi={10.1109/VR46266.2020.00041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089593,
  author={Mercado, Victor and Marchai, Maud and Lécuyer, Anatole},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Evaluation of Interaction Techniques Dedicated to Integrate Encountered-Type Haptic Displays in Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={230-238},
  abstract={Encountered-Type Haptic Displays (ETHDs) represent a field of haptic displays with the premise of not using any type of actuator directly in contact with the user skin, thus providing an alternative integration of haptic displays in virtual environments. In this paper, we present novel interaction techniques (ITs) dedicated to ETHDs. The techniques aim at addressing the issues commonly presented for these devices such as limited contact areas, lags and unexpected collisions with the user. First, our paper proposes a design framework based on several parameters defining the interactive process between user and ETHD (input, movement control, displacement and contact). Five techniques based on different ramifications of the design space framework were conceived, respectively named: Swipe, Drag, Clutch, Bubble and Follow. Then, a use-case scenario was designed to depict the usage of these techniques on the task of touching and coloring a wide, flat surface. Finally, a user study based on the coloring task was conducted to assess the performance and user experience for each IT. Results were in favor of Drag and Clutch techniques which are based on manual surface displacement, absolute position selection and intermittent contact interaction. Taken together our results and design methodology pave the way to the design of future ITs for ETHDs in virtual environments.},
  keywords={Haptic interfaces;Task analysis;Shape;Aerospace electronics;Rendering (computer graphics);Three-dimensional displays;Virtual environments;Encountered-Type Haptic Displays;Interaction Techniques;Haptic Rendering;Human-Machine Interaction},
  doi={10.1109/VR46266.2020.00042},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089598,
  author={Zhao, Lu and Liu, Yue and Ye, Dejiang and Ma, Zhuoluo and Song, Weitao},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Implementation and Evaluation of Touch-based Interaction Using Electrovibration Haptic Feedback in Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={239-247},
  abstract={Presentation of more haptic information to improve the users interactive capability is an important and challenging task in the virtual environment. As an emerging haptic feedback technology, electrovibration is an underlying solution to enhance the systems interactivity and improve user experience. However, such a solution is rarely studied in a virtual environment. In this work, we explore a new VR interaction method based on electrovibration technology with a touch screen and conduct evaluations about it. The key idea is to incorporate a set of manipulation gestures and three types of electrovibration in the VR interaction to help users acquire different kinds of tactile perception in the virtual manipulation. We present the evaluation in which we compare user performance measured first in a Fitts law task to evaluate different electrovibration types and then in a virtual office application to assess the interactive user interface. Our results show that the precision of interactions is significantly improved with the electrovibration haptic feedback. To the best of our knowledge, this is the first work to introduce electrovibration haptic feedback into the VR human-computer interaction and our work enlightens the potential of the electrovibration touchscreen-based interaction in virtual environments.},
  keywords={Haptic interfaces;Three-dimensional displays;Virtual environments;Task analysis;Thumb;Haptic feedback;electrovibration;interactive mode;virtual reality},
  doi={10.1109/VR46266.2020.00043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089505,
  author={Cai, Shaoyu and Ke, Pingchuan and Narumi, Takuji and Zhu, Kening},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={ThermAirGlove: A Pneumatic Glove for Thermal Perception and Material Identification in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={248-257},
  abstract={We present ThermAirGlove (TAGlove), a pneumatic glove which provides thermal feedback for users, to support the haptic experience of grabbing objects of different temperatures and materials in virtual reality (VR). The system consists of a glove with five inflatable airbags on the fingers and the palm, two temperature chambers (one hot and one cold), and the closed-loop pneumatic thermal control system. Our technical experiments showed that the highest temperature-changing speed of TAGlove system was 2.75°C/s for cooling, and the pneumatic-control mechanism could generate the thermal cues of different materials (e.g., foam, glass, copper, etc.). The user-perception experiments showed that the TAGlove system could provide five distinct levels of thermal sensation (ranging from very cool to very warm). The user-perception experiments also showed that the TAGlove could support users’ material identification among foam, glass, and copper with the average accuracy of 87.2%, with no significant difference compared to perceiving the real physical objects. The user studies on VR experience showed that using TAGlove in immersive VR could significantly improve users’ experience of presence compared to the situations without any temperature or material simulation.},
  keywords={Temperature control;Temperature sensors;Copper;Haptic interfaces;Skin;Glass;Human-centered computing—Virtual reality;Human-centered computing—Haptic devices},
  doi={10.1109/VR46266.2020.00044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089455,
  author={Xiao, Shan and Ye, Xupeng and Guo, Yaqiu and Gao, Boyu and Long, Jinyi},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Transfer of Coordination Skill to the Unpracticed Hand in Immersive Environments}, 
  year={2020},
  volume={},
  number={},
  pages={258-265},
  abstract={Physical practice with one hand results in performance gains of the other (un-practiced) hand in a unilateral motor task. Yet how it induces performance gains of interlimb coordination in the bimanual movements between trained limb and the opposite, untrained limb is unclear. The present study designed a game-like interactive system for physical practice, in which an avatar’s hands could be controlled itself or by the subject during a bimanual movement task in an immersive virtual reality environment. Participants practiced with the bimanual task by simultaneously drawing non-symmetric three-sided squares (e.g., U and C) to learn limb coordination with the following training strategies: (1) performing and seeing a bimanual task (BH-BH); (2) performing a unimanual task with right hand and seeing a bimanual action (RH-BH); (3) not performing a task but seeing a bimanual action (noH-BH); (4) performing and seeing a unimanual task (RH-RH). We found that the learning performance was better after BH-BH and RH-BH compared with other training strategies. In addition, we examined the effects of virtual hand representations on the learning performance after RH-BH. We found that the performance after training was increased with the realism level of virtual hands. These findings suggest that the proposed approach of RH-BH with realistic virtual hand would result in transfer of coordination skill to the unpracticed hand, which puts forward a new approach for learning and rehabilitation of coordination skill in patients with unilateral motor deficit in immersive environments.},
  keywords={Training;Task analysis;Visualization;Thumb;Shape;Virtual reality;Avatar hands;bimanual movement;coordination skill;virtual reality},
  doi={10.1109/VR46266.2020.00045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089499,
  author={DELRIEU, Thibauld and Weistroffer, Vincent and Gazeau, Jean Pierre},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Precise and realistic grasping and manipulation in Virtual Reality without force feedback}, 
  year={2020},
  volume={},
  number={},
  pages={266-274},
  abstract={This paper introduces a physically-based approach of grasping and manipulation regarding virtual objects that would enable fine and stable grasping without haptic force feedback. The main contribution is to enhance an existing method which couples a virtual kinematic hand with a visual hand tracking system. The mismatches between the tracked and virtual hands often yield unstable grasps, especially for small objects. This is overcome by the implementation of grasping assistance based on virtual springs between the tracked and virtual hands. The assistance is triggered based on an analysis of usual grasping criteria, to determine whether a grasp is feasible or not. The proposed method has been validated in a supervised experiment which showed that our assistance improves speed and accuracy for a "pick and place" task involving an exhaustive object set, sized for precision grasp. Moreover, users’ feedback shows a clear preference for the present approach in terms of naturalness and efficiency.},
  keywords={Grasping;Couplings;Visualization;Kinematics;Task analysis;Virtual reality;Robustness;[Human-centered computing]: Virtual Reality— Virtual grasping;[Human-centered computing]: User interface design—Dexterous interaction Precision grasp;[Human-centered computing]: User studies},
  doi={10.1109/VR46266.2020.00046},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089474,
  author={Kang, Hyo Jeong and Shin, Jung-hye and Ponto, Kevin},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparative Analysis of 3D User Interaction: How to Move Virtual Objects in Mixed Reality}, 
  year={2020},
  volume={},
  number={},
  pages={275-284},
  abstract={Using one’s hands can be a natural and intuitive method for interacting with 3D objects in a mixed reality environment. This study explores three hand-interaction techniques, including the gaze and pinch, touch and grab, and worlds-in-miniature interaction for selecting and moving virtual furniture in the 3D scene. Overall, a comparative analysis reveals that the worlds-in-miniature provided the best usability and task performance than other studied techniques. We also conducted in-depth interviews and analyzed participants’ hand gestures in order to identify desired attributes for 3D hand interaction design. Findings from interviews suggest that, when it comes to enjoyment and discoverability, users prefer directly manipulating the virtual furniture to interacting with objects remotely or using in-direct interactions such as gaze. Another insight this study provides is the critical roles of the virtual object’s visual appearance in designing natural hand interaction. Gesture analysis reveals that shapes of furniture, as well as its perceived features such as weight, largely determined the participant’s instinctive form of hand interaction (i.e., lift, grab, push). Based on these findings, we present design suggestions that can aid 3D interaction designers to develop a natural and intuitive hand interaction for mixed reality.},
  keywords={Three-dimensional displays;Task analysis;Virtual reality;Tracking;Usability;Meters;Head;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/VR46266.2020.00047},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089644,
  author={Chen, Di Laura and Balakrishnan, Ravin and Grossman, Tovi},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Disambiguation Techniques for Freehand Object Manipulations in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={285-292},
  abstract={Manipulating virtual objects using bare hands has been an attractive interaction paradigm in virtual and augmented reality due to its intuitive nature. However, one limitation of freehand input lies in the ambiguous resulting effect of the interaction. The same gesture performed on a virtual object could invoke different operations on the object depending on the context, object properties, and user intention. We present an experimental analysis of a set of disambiguation techniques in a virtual reality environment, comparing three input modalities (head gaze, speech, and foot tap) paired with three different timings in which options appear to resolve ambiguity (before, during, and after an interaction). The results indicate that using head gaze for disambiguation during an interaction with the object achieved the best performance.},
  keywords={Task analysis;Three-dimensional displays;Timing;Virtual reality;Human computer interaction;Feedforward systems;Freehand gestures;VR;uncertainty;ambiguity;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input},
  doi={10.1109/VR46266.2020.00048},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089573,
  author={Volonte, Matias and Hsu, Yu-Chun and Liu, Kuan-Yu and Mazer, Joe P. and Wong, Sai-Keung and Babu, Sabarish V.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Interacting with a Crowd of Emotional Virtual Humans on Users’ Affective and Non-Verbal Behaviors}, 
  year={2020},
  volume={},
  number={},
  pages={293-302},
  abstract={In this contribution we examined the effects on users during interaction with a virtual human crowd in an immersive virtual reality environment. We developed an agent-based crowd model with rich properties including eye gaze, facial expression, body motion, and verbal and non-verbal behaviors. The scenario was a virtual market in which the users needed to gather specific items. In a betweensubjects design, users interacted with a virtual human crowd that showed opposite valenced emotional expressions. There are four conditions in the between-subjects design. These includes different affective virtual crowds namely, Positive, Negative, Neutral, and a Mix condition. The Mix group is defined by a combination of Positive, Negative and Neutral emotional expressive characters. Depending on the specific condition, the virtual humans showed specific verbal and non-verbal behaviors. During the experiment we collected objective measures such as skin electrodermal activity, total time in the simulation, the number of interactions with the agents and performance measures. The subjective measures were the differential emotional survey (DES), and a user experience survey. We reported our findings with an in-depth analysis.},
  keywords={Avatars;Virtual reality;Legged locomotion;Animation;Computer graphics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Animations, Evaluation/methodology;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality},
  doi={10.1109/VR46266.2020.00049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089655,
  author={Mostajeran, Fariba and Balci, Melik Berk and Steinicke, Frank and Kühn, Simone and Gallinat, Jürgen},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effects of Virtual Audience Size on Social Anxiety during Public Speaking}, 
  year={2020},
  volume={},
  number={},
  pages={303-312},
  abstract={Prior studies have explored the possibility of inducing social anxiety (SA) in virtual reality (VR). Among various existing protocols for this purpose, the Trier Social Stress Test (TSST) has been proven to be robust in evoking SA in the majority of participants in both in vivo as well as VR conditions. The TSST consists of giving a speech and performing mental arithmetic calculations each for five minutes in front of three persons. In this paper, we present an adaptation of TSST to investigate the effects of different numbers of virtual humans (VHs) (i.e., three, six, or fifteen) on perceived SA. In addition, we compare the results with an in vivo TSST with three real persons in the audience. Twenty four participants took part in this experiment. As a result, physiological arousal could be observed with VR inducing SA yet less than in vivo TSST. Furthermore, some of the subjective measures showed a high state of anxiety experienced during the experiment. An effect of the virtual audience size could be observed only in heart rate (HR) as a virtual audience size of three VHs induced the highest HR responses which was significantly different from an audience of size six and fifteen.},
  keywords={Stress;In vivo;Protocols;Public speaking;Heart rate;Virtual reality;Psychology;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089506,
  author={Maruhn, Philipp and Dietrich, André and Prasch, Lorenz and Schneider, Sonja},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analyzing Pedestrian Behavior in Augmented Reality — Proof of Concept}, 
  year={2020},
  volume={},
  number={},
  pages={313-321},
  abstract={With recent advancements in head-mounted displaying technologies, virtual reality pedestrian simulators have become a common tool for traffic safety research. In contrast to field studies, test track studies and traffic observations, simulators enable researchers to analyze pedestrian behavior in a safe and controlled environment. However, creating the necessary virtual environments is time-consuming, especially in terms of meeting today’s expectations regarding graphical level of detail and realism. Furthermore, VR experiments often lack a body representation or require additional sensors to create an avatar. Due to the laboratory setting, VR simulators might fail to convey the feeling of standing on an actual street. In addition, simulators on the one hand and real-world testing on the other hand leave a methodological gap on the reality-virtuality continuum. This paper presents a novel approach for an augmented reality pedestrian simulator. With this simulator, the participant experiences virtual vehicles, augmented on a real scenario, allowing for safe and controlled testing in a realistic setting. In a between-subject design, 13 participants experienced a gap acceptance scenario with virtual vehicles, while 30 participants experienced the same scenario with real vehicles in the same environment. These participants were instructed to initiate a street crossing if they considered that the gap between the two experimental vehicles was safe to cross the street. Results indicate similar, but also offset behavior for both conditions. Lower acceptance rates and later crossing initiation times could be observed in the augmented reality condition. Still, it was shown that augmented reality renders a promising tool for pedestrian research but also features limitations depending on the use case.},
  keywords={Roads;Virtual environments;Cameras;Streaming media;Augmented reality;Safety;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers},
  doi={10.1109/VR46266.2020.00051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089637,
  author={Berton, Florian and Hoyet, Ludovic and Olivier, Anne-Hélène and Bruneau, Julien and Le Meur, Olivier and Pettre, Julien},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Eye-Gaze Activity in Crowds: Impact of Virtual Reality and Density}, 
  year={2020},
  volume={},
  number={},
  pages={322-331},
  abstract={When we are walking in crowds, we mainly use visual information to avoid collisions with other pedestrians. Thus, gaze activity should be considered to better understand interactions between people in a crowd. In this work, we use Virtual Reality (VR) to facilitate motion and gaze tracking, as well as to accurately control experimental conditions, in order to study the effect of crowd density on eye-gaze behavior. Our motivation is to better understand how interaction neighborhood (i.e., the subset of people actually influencing one’s locomotion trajectory) changes with density. To this end, we designed two experiments. The first one evaluates the biases introduced by the use of VR on the visual activity when walking among people, by comparing eye-gaze activity while walking in a real and virtual street. We then designed a second experiment where participants walked in a virtual street with different levels of pedestrian density. We demonstrate that gaze fixations are performed at the same frequency despite increases in pedestrian density, while the eyes scan a narrower portion of the street. These results suggest that in such situations walkers focus more on people in front and closer to them. These results provide valuable insights regarding eye-gaze activity during interactions between people in a crowd, and suggest new recommendations in designing more realistic crowd simulations.},
  keywords={Legged locomotion;Virtual reality;Visualization;Collision avoidance;Trajectory;Navigation;Solid modeling;Gaze Activity;Locomotion;Crowd;Virtual Reality;Eye-tracking;Collision Avoidance;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design;evaluation methods},
  doi={10.1109/VR46266.2020.00052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089660,
  author={Buck, Lauren E. and Park, Sohee and Bodenheimer, Bobby},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Determining Peripersonal Space Boundaries and Their Plasticity in Relation to Object and Agent Characteristics in an Immersive Virtual Environment}, 
  year={2020},
  volume={},
  number={},
  pages={332-342},
  abstract={In this paper we examine the extent of functional reaching space, or peripersonal space, in immersive three-dimensional virtual reality. In the real world a person’s peripersonal space boundaries can be altered by factors in the environment and by social context. We completed two studies with visual and tactile stimuli to determine peripersonal space boundaries. These studies investigated whether peripersonal space boundaries in an immersive virtual environment are consistent with those in the real world, and could be altered by object and virtual agent interactions. We found that while peripersonal space boundaries were consistent with those in the real world, they were responsive to object and agent interactions. Moreover, while people’s reactions to the objects and agents varied, the peripersonal space boundaries remained consistent. These findings have potential implications for the design of virtual environments.},
  keywords={Virtual environments;Visualization;Task analysis;Tools;Three-dimensional displays;Avatars;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR46266.2020.00053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089500,
  author={Lee, Geonsun and Kang, HyeongYeop and Lee, JongMin and Han, JungHyun},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A User Study on View-sharing Techniques for One-to-Many Mixed Reality Collaborations}, 
  year={2020},
  volume={},
  number={},
  pages={343-352},
  abstract={In a one-to-many mixed reality collaboration environment, where multiple local users wearing AR headsets are supervised by a remote expert wearing a VR HMD, we evaluated three view-sharing techniques: 2D video, 360 video, and 3D model augmented with 2D video. Through a pilot test, the weaknesses of the techniques were identified, and additional features were integrated into them. Then, their performances were compared in two different collaboration scenarios based on search and assembling. In the first scenario, a local user performed both search and assembling. In the second scenario, two local users had dedicated roles, one for search and the other for assembling. The experiment results showed that the 3D model augmented with 2D video was time-efficient, usable, less demanding and most preferred in one-to-many mixed reality collaborations.},
  keywords={Collaboration;Three-dimensional displays;Cameras;Virtual reality;Two dimensional displays;Resists;Solid modeling;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Collaborative and social computing;Collaborative and social computing theory;concepts and paradigms;Computer supported cooperative work},
  doi={10.1109/VR46266.2020.00054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089441,
  author={Keshavarzi, Mohammad and Yang, Allen Y. and Ko, Woojin and Caldas, Luisa},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimization and Manipulation of Contextual Mutual Spaces for Multi-User Virtual and Augmented Reality Interaction}, 
  year={2020},
  volume={},
  number={},
  pages={353-362},
  abstract={Spatial computing experiences are physically constrained by the geometry and semantics of the local user environment. This limitation is elevated in remote multi-user interaction scenarios, where finding a common virtual ground physically accessible for all participants becomes challenging. Locating a common accessible virtual ground is difficult for the users themselves, particularly if they are not aware of the spatial properties of other participants. In this paper, we introduce a framework to generate an optimal mutual virtual space for a multi-user interaction setting where remote users’ room spaces can have different layout and sizes. The framework further recommends movement of surrounding furniture objects that expand the size of the mutual space with minimal physical effort. Finally, we demonstrate the performance of our solution on real-world datasets and also a real HoloLens application. Results show the proposed algorithm can effectively discover optimal shareable space for multi-user virtual interaction and hence facilitate remote spatial computing communication in various collaborative workflows.},
  keywords={Three-dimensional displays;Optimization;Semantics;Layout;Avatars;Collaboration;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Human-centered computing—Human computer interaction —Interaction paradigms—Collaborative interaction;Applied computing—Decision analysis—Multi-criterion optimization and decision-making;Theory of computation—Mathematical optimization—Optimization with randomized search heuristics—Evolutionary algorithms},
  doi={10.1109/VR46266.2020.00055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089521,
  author={Hsu, Ting-Wei and Tsai, Ming-Han and Babu, Sabarish V. and Hsu, Pei-Hsien and Chang, Hsuan-Ming and Lin, Wen-Chieh and Chuang, Jung-Hong},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Initial Evaluation of a VR based Immersive and Interactive Architectural Design Discussion System}, 
  year={2020},
  volume={},
  number={},
  pages={363-371},
  abstract={Design discussion is a very important course in architecture education. In this paper, we developed a VR based architecture design discussion system that allows members to visualize and discuss the architectural models and to modify the models during discussion. Since the system is designed to work on top of Rhinoceros and Grasshopper, the object database is updated right after the object modification. Members communicate via voice, object manipulations, and mid-air sketching as well as on-surface sketching in the virtual environment. Several tools have been designed to enhance the sense of presence and to make the discussion more effective. We also developed a rollback mechanism to help users intuitively and quickly revert to a previous state of discussion to make some changes or to start a new direction of discussion. To evaluate the system, we conducted an initial user study with 14 participants to assess the user experience, user impression and effectiveness of the system. The feedback from participants were positive and suggested that the system could be effective and useful for supporting architecture design discussion.},
  keywords={Three-dimensional displays;Virtual environments;Collaboration;Education;Architecture;Human-centered computing;Virtual reality;Human-centered computing;Computer supported cooperative work;Human-centered computing;User interface design;Applied computing;E-learning},
  doi={10.1109/VR46266.2020.00056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089611,
  author={Menzner, Tim and Gesslein, Travis and Otte, Alexander and Grubert, Jens},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Above Surface Interaction for Multiscale Navigation in Mobile Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={372-381},
  abstract={Virtual Reality enables the exploration of large information spaces. In physically constrained spaces such as airplanes or buses, controller-based or mid-air interaction in mobile Virtual Reality can be challenging. Instead, the input space on and above touchscreen enabled devices such as smartphones or tablets could be employed for Virtual Reality interaction in those spaces.In this context, we compared an above surface interaction technique with traditional 2D on-surface input for navigating large planar information spaces such as maps in a controlled user study (n = 20). We find that our proposed above surface interaction technique results in significantly better performance and user preference compared to pinch-to-zoom and drag-to-pan when navigating planar information spaces.},
  keywords={Navigation;Two dimensional displays;Three-dimensional displays;Sensors;Aerospace electronics;Virtual reality;User interfaces;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design;evaluation methods},
  doi={10.1109/VR46266.2020.00057},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089561,
  author={Wang, Ziyao and Wei, Haikun and Zhang, KanJian and Xie, Liping},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real Walking in Place: HEX-CORE-PROTOTYPE Omnidirectional Treadmill}, 
  year={2020},
  volume={},
  number={},
  pages={382-387},
  abstract={Locomotion is one of the most important problems in virtual reality. Real walking experience is the key to immersively explore the virtual world. Several strategies have been proposed to solve the problem, but most are not suitable to solve the locomotion problem in Room-Scale VR. The omnidirectional treadmill is an effective way to provide a natural walking experience within the Room-Scale VR. This paper proposes a novel omnidirectional treadmill named HEX-CORE-PROTOTYPE (HCP). The principle of synthesis and decomposition of velocity is applied to form an omnidirectional velocity field. Our system could provide a full degree of freedom and real walking experience in place. Compared to the current best system, the height of HCP is only 40% of it. The application shows the effectiveness of our system to solve the locomotion problem in Room-Scale VR.},
  keywords={Legged locomotion;Gears;Aerospace electronics;Servomotors;Belts;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality, Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR46266.2020.00058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089462,
  author={Vasylevska, Khrystyna and Kovács, Bálint István and Kaufmann, Hannes},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Bridges: Simulating Smooth Uneven Surfaces in VR}, 
  year={2020},
  volume={},
  number={},
  pages={388-397},
  abstract={Virtual reality (VR) is limited in many ways and often is incomparable to real-world experience. Walkable smooth uneven surfaces are inherent to reality but extremely lacking in VR. At the same time, VR offers a lot of possibilities for manipulations. In this paper, we focus on human height and slant perception of the uneven surfaces with multi-sensory stimulation in VR. By employing viewport manipulations, haptic, and vibrotactile stimuli, we explore the possibility to simulate uneven surfaces different from the physical props used.Our results suggest that the use of a rounded prop helps to create a more convincing illusion of an uneven surface that is significantly higher than the physical one. The multi-sensory stimulation brings both height and slant estimations closer to the values suggested by the visual cues if there is no conflict with the haptic sensations. The use of a flat prop is less realistic and leads to massive height and slant underestimations as opposed to those suggested by visual cues. However, if the curved prop cannot be used, a flat surface might still be used to simulate small dents and bumps.},
  keywords={Bridges;Visualization;Cameras;Legged locomotion;Haptic interfaces;Vibrations;Virtual reality;Human-centered computing;User studies, Human-centered computing;Virtual reality, Computing methodologies;Perception},
  doi={10.1109/VR46266.2020.00059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089463,
  author={Zielasko, Daniel and Law, Yuen C. and Weyers, Benjamin},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Take a Look Around – The Impact of Decoupling Gaze and Travel-direction in Seated and Ground-based Virtual Reality Utilizing Torso-directed Steering}, 
  year={2020},
  volume={},
  number={},
  pages={398-406},
  abstract={Leaning (the upper body) has several times been shown to be a suitable virtual travel technique when being seated; in both, flying as well as ground-based scenarios. The direction of the steering method most commonly used is gaze/head-directed. However, this does not allow to inspect the environment independently from the direction of movement. The change to torso-directed steering allows for the latter and additionally does not take anything from the natural character of the leaning metaphor. We empirically investigated the impact of this freedom in a ground-based scenario and complemented the conditions with a virtual body-directed method and then crossed all with device-based control conditions. In the conducted study (n = 25), we found the torso-directed methods objectively performed the best (traveled distance, completion time & number of collisions), and found torso-directed leaning subjectively rated the most usable one.},
  keywords={Torso;Task analysis;Velocity control;Visualization;Virtual reality;Three-dimensional displays;Wireless communication;Human-centered computing;Virtual reality, Human-centered computing;Empirical studies in interaction design},
  doi={10.1109/VR46266.2020.00060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089475,
  author={Niu, Yuzhen and Zheng, Qingyang and Liu, Wenxi and Guo, Wenzhong},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Recurrent Enhancement of Visual Comfort for Casual Stereoscopic Photography}, 
  year={2020},
  volume={},
  number={},
  pages={407-415},
  abstract={Creating stereoscopic 3D media content has wide applications in virtual reality. In this paper, we are interested in a challenging application, casual stereoscopic photography, that allows ordinary users to create a stereoscopic photo using two images captured by a hand-held monocular camera. To handle the geometric constraints and disparity adjustment for casually captured left and right images, we present a coarse-to-fine framework. In the coarse stage, we propose a unified reinforcement learning-based method, in which the produced stereo image is iteratively adjusted and evaluated in the term of visual comfort. In addition, to further enhance the visual comfort of the stereoscopic image produced in the coarse stage, we introduce another independent recurrent network to fine-tune its disparity range. Lastly, we perform comprehensive experiments to evaluate our method and demonstrate the applicability of our model for real images.},
  keywords={Stereo image processing;Visualization;Cameras;Photography;Learning systems;Rendering (computer graphics);Virtual reality;Human-centered computing;Computer vision;Image and video acquisition;3D imaging},
  doi={10.1109/VR46266.2020.00061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089629,
  author={Günther, Tobias and Hilgers, Inga-Lisa and Groh, Rainer and Schmauder, Martin},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visualization and evaluation of ergonomic visual field parameters in first person virtual environments}, 
  year={2020},
  volume={},
  number={},
  pages={416-424},
  abstract={Especially in the field of mechanical engineering, the market pressure for small and medium-sized enterprises increases because of faster developments and more complex designs. Nevertheless, standards and ergonomic safety regulations must be observed. In recent years, various applications were presented to help users understand and comply with inconvenient requirements. However, the time-consuming tools are primarily for ergonomics experts and often overwhelm engineers from small and medium-sized enterprises. We present an immersive concept that allows inexperienced users to quickly assess the ergonomic parameters of the visual field, represented by easy-to-understand visualizations. The solution is compared with standard market and scientific approaches.},
  keywords={Ergonomics;Tools;Solid modeling;Visualization;Prototypes;Three-dimensional displays;Virtual environments;Human-centered computing;Ergonomics, Human-centered computing;Virtual reality, Human-centered computing;User centered design, Human-centered computing;Assistance},
  doi={10.1109/VR46266.2020.00062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089620,
  author={Choudhary, Zubin and Kim, Kangsoo and Schubert, Ryan and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Big Heads: Analysis of Human Perception and Comfort of Head Scales in Social Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={425-433},
  abstract={Virtual reality (VR) technologies provide a shared platform for collaboration among users in a spatial context. To enhance the quality of social signals during interaction between users, researchers and practitioners started augmenting users’ interpersonal space with different types of virtual embodied social cues. A prominent example is commonly referred to as the "Big Head" technique, in which the head scales of virtual interlocutors are slightly increased to leverage more of the display’s visual space to convey facial social cues. While beneficial in improving interpersonal social communication, the benefits and thresholds of human perception of facial cues and comfort in such Big Head environments are not well understood, limiting their usefulness and subjective experience.In this paper, we present a human-subject study that we conducted to understand the impact of an increased or decreased head scale in social VR on participants’ ability to perceive facial expressions as well as their sense of comfort and feeling of "uncanniness." We explored two head scaling methods and compared them with respect to perceptual thresholds and user preferences. We further show that the distance to interlocutors has an important effect on the results. We discuss implications and guidelines for practical applications that aim to leverage VR-enhanced social cues.},
  keywords={Avatars;Games;Virtual reality;Computer graphics;Human computer interaction;Virtual environments;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR46266.2020.00063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089640,
  author={Erickson, Austin and Kim, Kangsoo and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Dark Mode Graphics on Visual Acuity and Fatigue with Virtual Reality Head-Mounted Displays}, 
  year={2020},
  volume={},
  number={},
  pages={434-442},
  abstract={Current virtual reality (VR) head-mounted displays (HMDs) are characterized by a low angular resolution that makes it difficult to make out details, leading to reduced legibility of text and increased visual fatigue. Light-on-dark graphics modes, so-called "dark mode" graphics, are becoming more and more popular over a wide range of display technologies, and have been correlated with increased visual comfort and acuity, specifically when working in low-light environments, which suggests that they might provide significant advantages for VR HMDs.In this paper, we present a human-subject study investigating the correlations between the color mode and the ambient lighting with respect to visual acuity and fatigue on VR HMDs. We compare two color schemes, characterized by light letters on a dark background (dark mode), or dark letters on a light background (light mode), and show that the dark background in dark mode provides a significant advantage in terms of reduced visual fatigue and increased visual acuity in dim virtual environments on current HMDs. Based on our results, we discuss guidelines for user interfaces and applications.},
  keywords={Visualization;Fatigue;Color;Image color analysis;Virtual reality;Lighting;Resists;Computer graphics—Graphics systems and interfaces—Virtual reality;Human-centered computing— Interaction paradigms—Virtual reality},
  doi={10.1109/VR46266.2020.00064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089562,
  author={Guo, Jie and Weng, Dongdong and Fang, Hui and Zhang, Zhenliang and Ping, Jiamin and Liu, Yue and Wang, Yongtian},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Differences of Visual Discomfort Caused by Long-term Immersion between Virtual Environments and Physical Environments}, 
  year={2020},
  volume={},
  number={},
  pages={443-452},
  abstract={To investigate the effects of visual discomfort caused by long-term immersing in virtual environments (VEs), we conducted a comparative study to evaluate users’ visual discomfort in an eight-hour working rhythm and compared the differences between the VEs and the physical environments. Twenty-seven participants performed four different visual tasks with a head-mounted display (HMD) for the VE condition and with a monitor for the physical condition. Their subjective visual discomfort and objective oculomotor indicators were measured to evaluate their visual performances. The results show that the subjective visual fatigue symptoms, the objective pupil size, and the relative accommodation response vary across time for the two conditions, in which VEs affects visual fatigue the most compared to the physical environments. The results also show that pupil size is negatively related to subjective visual fatigue, and the long-term work based on displays only influences the maximum accommodation response of participants. This work is a supplement to the necessary but insufficient-researched field of visual fatigue in long-term immersing in VEs, which should be valuable to researchers involved in the evaluation of visual fatigue using HMDs.},
  keywords={Visualization;Fatigue;Two dimensional displays;Resists;Three-dimensional displays;Virtual environments;Optical imaging;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089480,
  author={Esmaeili, Shaghayegh and Benda, Brett and Ragan, Eric D.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Detection of Scaled Hand Interactions in Virtual Reality: The Effects of Motion Direction and Task Complexity}, 
  year={2020},
  volume={},
  number={},
  pages={453-462},
  abstract={In virtual reality (VR), natural physical hand interaction allows users to interact with virtual content using physical gestures. While the most straightforward use of tracked hand motion maintains a one-to-one mapping between the physical and virtual world, some cases might benefit from changing this mapping through scaled or redirected interactions that modify the mapping between user’s physical movements and the magnitude of corresponding virtual movements. However, large deviations in interaction fidelity may potentially provide distractions or a loss of perceived realism. Therefore, it is important to know the extent to which remapping techniques can be applied to scaled interactions in VR without users detecting the difference. In this paper, we extend prior research on redirected hand techniques by investigating user perception of scaled hand movements and estimating detection thresholds for different types of hand motion in VR. We conducted two experiments with a two-alternative forced-choice (2AFC) design to estimate the detection thresholds of remapped interaction. The first experiment tested the perception of motion scaling for simple hand movements, and the second experiment involved more complex reaching motions in a cognitively demanding game scenario. We present estimated detection thresholds for scale values that can be applied to virtual hand movements without users noticing the difference. Our findings show that detection thresholds differ significantly based on the type of hand movement (horizontal, vertical, and depth).},
  keywords={Haptic interfaces;Task analysis;Virtual environments;Tracking;Legged locomotion;Visualization;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Empirical studies in HCI;Information interfaces and presentation— Multimedia Information Systems—Artificial;augmented;and virtual realities},
  doi={10.1109/VR46266.2020.00066},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089548,
  author={Jung, Sungchul and Wood, Andrew L. and Hoermann, Simon and Abhayawardhana, Pramuditha L. and Lindeman, Robert W.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Multi-sensory Stimuli on Confidence Levels for Perceptual-cognitive Tasks in VR}, 
  year={2020},
  volume={},
  number={},
  pages={463-472},
  abstract={Supporting perceptual-cognitive tasks is an important part of our daily lives. We use rich, multi-sensory feedback through sight, sound, touch, smell, and taste to support better perceptual-cognitive things we do, such as sports, cooking, and searching for a location, and to increase our confidence in performing those tasks in daily life. Same with real life, the demand for perceptual-cognitive tasks exists in serious VR simulations such as surgical or safety training systems. However, in contrast to real life, VR simulations are typically limited to visual and auditory cues, while sometimes adding simple tactile feedback. This could make it difficult to make confident decisions in VR.In this paper, we investigate the effects of multi-sensory stimuli, namely visuals, audio, two types of tactile (floor vibration and wind), and smell in terms of the confidence levels on a location-matching task which requires a combination of perceptual and cognitive work inside a virtual environment. We also measured the level of presence when participants visited virtual places with different combinations of sensory feedback. Our results show that our multi-sensory VR system was superior to a typical VR system (vision and audio) in terms of the sense of presence and user preference. However, the subjective confidence levels were higher in the typical VR system.},
  keywords={Vibrations;Multisensory integration;Cognition;Virtual reality;User interfaces;Wind;Multisensory VR;Perception;Confidence;Cognition;Floor vibration;Wind;Smell},
  doi={10.1109/VR46266.2020.00067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089568,
  author={Liao, Haodong and Xie, Ning and Li, Huiyuan and Li, Yuhang and Su, Jianping and Jiang, Feng and Huang, Weipeng and Shen, Heng Tao},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Data-Driven Spatio-Temporal Analysis via Multi-Modal Zeitgebers and Cognitive Load in VR}, 
  year={2020},
  volume={},
  number={},
  pages={473-482},
  abstract={Virtual Reality (VR) produces a highly realistic simulation environment to engage users with Immersive Virtual Environments (IVEs). To interact effectively with users, VR builds intensive media through the multi-modal sense functions in the lower level, such as visual, auditory, tactile, and olfactory senses. However, the higher-level perceptions, e.g., the temporal duration, the sense of presence, and the cognitive load are less explored. These higher-level perceptions are part of the critical evaluation criteria for VR design. In this paper, we divide the external zeitgebers into visual and auditory zeitgebers. We then combine these zeitgebers with the attention-oriented cognitive load to investigate their effects on temporal estimation and presence, particularly in IVEs. We propose a data-driven method to build a multi-modal predictive equation for time estimation and presence, in an effort to figure out the essential elements of users' spatial and temporal perception in VR. We also design a complicated application and validate the predictive equation. Our feature-based model is able to guide the VR application design in terms of the subjective time length judgment and presence of users as well as achieve a better VR user experience.},
  keywords={Estimation;Task analysis;Visualization;Mathematical model;Circadian rhythm;Lighting;Human-centered computing;Virtual Reality;Human-centered computing;Emprical studies in HCI},
  doi={10.1109/VR46266.2020.00068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089482,
  author={Oberdörfer, Sebastian and Heidrich, David and Latoschik, Marc Erich},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Think Twice: The Influence of Immersion on Decision Making during Gambling in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={483-492},
  abstract={Immersive Virtual Reality (VR) is increasingly being explored as an alternative medium for gambling games to attract players. Typically, gambling games try to impair a player's decision making, usually for the disadvantage of the players' financial outcome. An impaired decision making results in the inability to differentiate between advantageous and disadvantageous options. We investigated if and how immersion impacts decision making using a VR-based realization of the Iowa Gambling Task (IGT) to pinpoint potential risks and effects of gambling in VR. During the IGT, subjects are challenged to draw cards from four different decks of which two are advantageous. The selections made serve as a measure of a participant's decision making during the task. In a novel user study, we compared the effects of immersion on decision making between a low-immersive desktop-3D-based IGT realization and a high immersive VR version. Our results revealed significantly more disadvantageous decisions when playing the immersive VR version. This indicates an impairing effect of immersion on simulated real life decision making and provides empirical evidence for a high risk potential of gambling games targeting immersive VR.},
  keywords={Decision making;Games;Task analysis;Virtual reality;Atmospheric measurements;Particle measurements;Human computer interaction;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Human-centered computing;Interaction paradigms;Virtual Reality},
  doi={10.1109/VR46266.2020.00069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089604,
  author={Erickson, Austin and Bruder, Gerd and Wisniewski, Pamela J. and Welch, Gregory F.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Examining Whether Secondary Effects of Temperature-Associated Virtual Stimuli Influence Subjective Perception of Duration}, 
  year={2020},
  volume={},
  number={},
  pages={493-499},
  abstract={Past work in augmented reality has shown that temperature-associated AR stimuli can induce warming and cooling sensations in the user, and prior work in psychology suggests that a person’s body temperature can influence that person’s sense of subjective perception of duration. In this paper, we present a user study to evaluate the relationship between temperature-associated virtual stimuli presented on an AR-HMD and the user’s sense of subjective perception of duration and temperature. In particular, we investigate two independent variables: the apparent temperature of the virtual stimuli presented to the participant, which could be hot or cold, and the location of the stimuli, which could be in direct contact with the user, in indirect contact with the user, or both in direct and indirect contact simultaneously. We investigate how these variables affect the users’ perception of duration and perception of body and environment temperature by having participants make prospective time estimations while observing the virtual stimulus and answering subjective questions regarding their body and environment temperatures. Our work confirms that temperature-associated virtual stimuli are capable of having significant effects on the users’ perception of temperature, and highlights a possible limitation in the current augmented reality technology in that no secondary effects on the users’ perception of duration were observed.},
  keywords={Psychology;Augmented reality;Head-mounted displays;Virtual reality;Computer graphics;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089537,
  author={Yu, Difeng and Zhou, Qiushi and Tag, Benjamin and Dingler, Tilman and Velloso, Eduardo and Goncalves, Jorge},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Engaging Participants during Selection Studies in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={500-509},
  abstract={Selection studies are prevalent and indispensable for VR research. However, due to the tedious and repetitive nature of many such experiments, participants can become disengaged during the study, which is likely to impact the results and conclusions. In this work, we investigate participant disengagement in VR selection experiments and how this issue affects the outcomes. Moreover, we evaluate the usefulness of four engagement strategies to keep participants engaged during VR selection studies and investigate how they impact user performance when compared to a baseline condition with no engagement strategy. Based on our findings, we distill several design recommendations that can be useful for future VR selection studies or user tests in other domains that employ similar repetitive features.},
  keywords={Task analysis;Human computer interaction;Human factors;Virtual reality;Games;Human-centered computing;HCI design and evaluation methods;User studies;Human-centered computing;Virtual reality;Human-centered computing;Pointing},
  doi={10.1109/VR46266.2020.00071},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089442,
  author={Lougiakis, Christos and Katifori, Akrivi and Roussou, Maria and Ioannidis, Ioannis-Panagiotis},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Virtual Hand Representation on Interaction and Embodiment in HMD-based Virtual Environments Using Controllers}, 
  year={2020},
  volume={},
  number={},
  pages={510-518},
  abstract={Many studies have been conducted in the past few years that focus on interaction and embodiment in the field of virtual reality. However, despite the recent widespread use and continuing rise of controller-based head-mounted display (HMD) hardware for VR, there is little research on the use of handheld controllers in this context. We explore the effects of different virtual hand representations on interaction and the user’s sense of embodiment, extending the work of Argelaguet et al. in 2016, in this case using controllers. We designed an experiment where users perform the task of selecting and moving a cube from and to specific positions on a table inside an immersive virtual environment, interacting with three representations: the abstract shape of a Sphere, the 3D model of the Controller, and a realistic human-looking Hand. For each representation, users were asked to perform the same task with and without obstacles (Brick Wall, Barbed Wire, Electric Current). Statistical analysis of the results show that although no significant differences were identified in the sense of agency, the users’ performance with the Sphere was significantly worse compared to the other two, and in the case of the positioning task the Controller outperformed the others. Additionally, the Hand generated the strongest sense of ownership, and it was the favorite representation.},
  keywords={Virtual environments;Virtual reality;Human computer interaction;Grasping;Human-centered computing;Virtual reality;Human-centered computing;User studies;Human-centered computing;Empirical studies in interaction design;Computing methodologies;Perception},
  doi={10.1109/VR46266.2020.00072},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089645,
  author={Bhargava, Ayush and Solini, Hannah and Lucaites, Kathryn and Bertrand, Jeffrey W. and Robb, Andrew and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparative Evaluation of Viewing and Self-Representation on Passability Affordances to a Realistic Sliding Doorway in Real and Immersive Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={519-528},
  abstract={As Virtual Reality (VR) devices become more accessible, a multitude of VR applications engage users in highly immersive virtual environments that feature realistic graphics, real-life scenarios, and self-avatars. Many of these simulations require users to make spontaneous affordance judgments such as stepping over obstacles, passing through gaps, etc. which are shown to be affected by the nature of our self-representation in the virtual world. As the technology for creating self-avatars becomes more widely available, it is important to explore how various affordance judgments are affected by the presence of self-avatars. In this work, we investigate the effects of body-scaled self-avatars on the affordance of passability in a natural setting. We implemented a gender-matched body-scaled self-avatar using HTC Vive trackers and evaluated how passability judgments for a sliding doorway in VR, with and without an avatar, compared to the real world judgments. The results suggest that passability judgments are more conservative in VR as compared to the real world. However, the presence of a self-avatar does not significantly affect passability judgments made in VR. This does not align with previous findings which show that having a self-avatar improves judgments and estimates.},
  keywords={Affordances;Virtual environments;Avatars;Virtual reality;Real-time systems;Interactive systems;Self-Avatars;Affordance;Passability;Virtual Reality;Human-centered computing;Empirical studies in HCI;Human-centered computing;Interaction design},
  doi={10.1109/VR46266.2020.00073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089596,
  author={Kim, Kangsoo and de Melo, Celso M. and Norouzi, Nahal and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reducing Task Load with an Embodied Intelligent Virtual Assistant for Improved Performance in Collaborative Decision Making}, 
  year={2020},
  volume={},
  number={},
  pages={529-538},
  abstract={Collaboration in a group has the potential to achieve more effective solutions for challenging problems, but collaboration per se is not an easy task, rather a stressful burden if the collaboration partners do not communicate well with each other. While Intelligent Virtual Assistants (IVAs), such as Amazon Alexa, are becoming part of our daily lives, there are increasing occurrences in which we collaborate with such IVAs for our daily tasks. Although IVAs can provide important support to users, the limited verbal interface in the current state of IVAs lacks the ability to provide effective non-verbal social cues, which is critical for improving collaborative performance and reducing task load.In this paper, we investigate the effects of IVA embodiment on collaborative decision making. In a within-subjects study, participants performed a desert survival task in three conditions: (1) performing the task alone, (2) working with a disembodied voice assistant, and (3) working with an embodied assistant. Our results show that both assistant conditions led to higher performance over when performing the task alone, but interestingly the reported task load with the embodied assistant was significantly lower than with the disembodied voice assistant. We discuss the findings with implications for effective and efficient collaborations with IVAs while also emphasizing the increased social presence and richness of the embodied assistant.},
  keywords={Human computer interaction;Augmented reality;Decision making;Ubiquitous computing;Virtual assistants;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Ubiquitous and mobile computing;Ubiquitous and mobile devices;Personal digital assistants;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/VR46266.2020.00074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089656,
  author={Li, Wanwan and Talavera, Javier and Samayoa, Amilcar Gomez and Lien, Jyh-Ming and Yu, Lap-Fai},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Synthesis of Virtual Wheelchair Training Scenarios}, 
  year={2020},
  volume={},
  number={},
  pages={539-547},
  abstract={In this paper, we propose an optimization-based approach for automatically generating virtual scenarios for wheelchair training in virtual reality. To generate a virtual training scenario, our approach automatically generates a realistic furniture layout for a scene as well as a training path that the user needs to go through by controlling a simulated wheelchair. The training properties of the path, namely, its desired length, the extent of rotation, and narrowness, are optimized so as to deliver the desired training effects. We conducted an evaluation to validate the efficacy of the proposed virtual reality training approach. Users showed improvement in wheelchair control skills in terms of proficiency and precision after receiving the proposed virtual reality training.},
  keywords={Training;Wheelchairs;Virtual reality;Solid modeling;Layout;Optimization;Task analysis;Virtual Reality;Modeling and Simulation;Wheelchair Training Simulator},
  doi={10.1109/VR46266.2020.00075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089572,
  author={Pan, Junjun and Zhang, Leiyu and Yu, Peng and Shen, Yang and Wang, Haipeng and Hao, Haimin and Qin, Hong},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-time VR Simulation of Laparoscopic Cholecystectomy based on Parallel Position-based Dynamics in GPU}, 
  year={2020},
  volume={},
  number={},
  pages={548-556},
  abstract={In recent years, virtual reality (VR) based training has greatly changed surgeons learning mode. It can simulate the surgery from the visual, auditory, and tactile aspects. VR medical simulator can greatly reduce the risk of the real patient and the cost of hospitals. Laparoscopic cholecystectomy is one of the typical representatives in minimal invasive surgery (MIS). Due to the large incidence of cholecystectomy, the application of its VR-based simulation is vital and necessary for the residents' surgical training. In this paper, we present a VR simulation framework based on position-based dynamics (PBD) for cholecystectomy. To further accelerate the deformation of organs, PBD constraints are solved in parallel by a graph coloring algorithm. We introduce a bio-thermal conduction model to improve the realism of the fat tissue electrocautery. Finally, we design a hybrid multi-model connection method to handle the interaction and simulation of the liver-gallbladder separation. This simulation system has been applied to laparoscopic cholecystectomy training in several hospitals. From the experimental results, users can operate in real-time with high stability and fidelity. The simulator is also evaluated by a number of digestive surgeons through preliminary studies. They believed that the system can offer great help to the improvement of surgical skills.},
  keywords={Surgery;Biological system modeling;Strain;Biological tissues;Image color analysis;Deformable models;Solid modeling;Human-centered computing—Human computer interaction—Interactive systems and tools—User interface programming;Computer systems organization—Real-time systems—Real- time system architecture},
  doi={10.1109/VR46266.2020.00076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089542,
  author={Xiao, Xiao and Zhao, Shang and Meng, Yan and Soghier, Lamia and Zhang, Xiaoke and Hahn, James},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Physics-based Virtual Reality Simulation Framework for Neonatal Endotracheal Intubation}, 
  year={2020},
  volume={},
  number={},
  pages={557-565},
  abstract={Neonatal endotracheal intubation (ETI) is a complex procedure. Low intubation success rates for pediatric residents indicate the current training regimen is inadequate for achieving positive patient outcomes. Computer-based training systems in this field have been limited due to the complex nature of simulating in real-time, the anatomical structures, soft tissue deformations and frequent tool interactions with large forces which occur during actual patient intubation. This paper addresses the issues of neonatal ETI training in an attempt to bridge the gap left by traditional training methods. We propose a fully interactive physics-based virtual reality (VR) simulation framework for neonatal ETI that converts the training of this medical procedure to a completely immersive virtual environment where both visual and physical realism were achieved. Our system embeds independent dynamics models and interaction devices in separate modules while allowing them to interact with each other within the same environment, which offers a flexible solution for multi-modal medical simulation scenarios. The virtual model was extracted from CT scans of a neonatal patient, which provides realistic anatomical structures and was parameterized to allow variations in a range of features that affect the level of difficulty. Moreover, with this manikin-free VR system, we can capture and visualize an even larger set of performance parameters in relation to the internal geometric change of the virtual model for real-time guidance and post-trial assessment. Lastly, validation study results from a group of neonatologists are presented demonstrating that VR is a promising platform to train medical professionals effectively for this procedure.},
  keywords={Pediatrics;Training;Computational modeling;Haptic interfaces;Atmospheric modeling;Three-dimensional displays;Deformable models;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Computing methodologies;Animation;Physical simulation;Computing methodologies;Modeling and simulation;Simulation types and techniques;Real-time simulation;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices},
  doi={10.1109/VR46266.2020.00077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089445,
  author={Li, Meng and Ganni, Sandeep and Ponten, Jeroen and Albayrak, Armagan and Rutkowski, Anne-F and Jakimowicz, Jack},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analysing usability and presence of a virtual reality operating room (VOR) simulator during laparoscopic surgery training}, 
  year={2020},
  volume={},
  number={},
  pages={566-572},
  abstract={Immersive Virtual Reality (VR) laparoscopy simulation is emerging to enhance the attractiveness and realism of surgical procedural training. This study analyses the usability and presence of a Virtual Operating Room (VOR) setup via user evaluation and sets out the key elements for an immersive environment during a laparoscopic procedural training.In the VOR setup, a VR headset displayed a 360-degree computer-generated Operating Room (OR) around a VR laparoscopic simulator during laparoscopy procedures. Thirty-seven surgeons and surgical trainees performed the complete cholecystectomy task in the VOR. Questionnaires (i.e., Localized Postural Discomfort scale, Questionnaire for Intuitive Use, NASA-Task Load Index, and Presence Questionnaire) followed by a semi-structured interview were used to collect the data.The participants could intuitively adapt to the VOR and were satisfied when performing their tasks (M=3.90, IQR=0.70). The participants, particularly surgical trainees, were highly engaged to accomplish the task. Despite the higher mental workload on four subscales (p <; 0.05), the surgical trainees had a lower effort of learning (4 vs 3.33, p <; 0.05) compared to surgeons. The participants experienced very slight discomfort in seven body segments (0.59-1.16). In addition, they expected improvements for team interaction and personalized experience within the setup.The VOR showed potential to become a useful tool in providing immersive training during laparoscopy procedure simulation based on the usability and presence noted in the study. Future developments of user interfaces, VOR environment, team interaction and personalization should result in improvements of the system.},
  keywords={Surgery;Laparoscopes;Task analysis;Training;Usability;Virtual reality;Instruments;Laparoscopy simulation;Virtual reality operating room;Surgical training;Presence;Usability;User evaluation;Human-centered computing [Virtual Reality];Human computer interaction;User evaluation;Human-centered computing;[Applied Computing];Life and medical science},
  doi={10.1109/VR46266.2020.00078},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089623,
  author={Finney, Hunter and Jones, J. Adam},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Asymmetric Effects of the Ebbinghaus Illusion on Depth Judgments}, 
  year={2020},
  volume={},
  number={},
  pages={573-578},
  abstract={The Ebbinghaus illusion, also known as Titchner Circles, is a well- known perceptual illusion affecting the perceived size of a disc enclosed by an annulus of either larger or smaller discs. Though many have found highly consistent results with regard to the effect of the illusion on size perception, there have been mixed results when studying its effect on action-based tasks. In this paper, we present a study utilizing a head-worn virtual environment to examine the effect of the Ebbinghaus illusion on depth judgments as measured using a blind-reaching task. We found that participants’ size judgments were symmetrically affected by the classic "large annulus" and "small annulus" configurations, but their distance judgments were asymmetrically affected. Large annulus configurations had no significant effect on distance judgments while small annulus configurations resulted in significant underestimation of target distances. Despite this asymmetry, both configurations resulted in response times of similar magnitude that were significantly longer than those of the non-illusory control condition.},
  keywords={Task analysis;Virtual environments;Atmospheric measurements;Particle measurements;Visualization;Time factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;I.4.8 [Scene Analysis]: Depth Cues;H.5.1 [Information Systems]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;H.1.2 [Information Systems]: User/Machine Systems—Human Factors},
  doi={10.1109/VR46266.2020.00079},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089501,
  author={Buck, Lauren E. and McNamara, Timothy P. and Bodenheimer, Bobby},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dyadic Acquisition of Survey Knowledge in a Shared Virtual Environment}, 
  year={2020},
  volume={},
  number={},
  pages={579-587},
  abstract={Navigation and wayfinding are often accomplished collectively, with groups of people. Yet most studies of navigation and wayfinding behavior, and of the acquisition of spatial knowledge more generally, focus on the individual. In this paper we extend the investigation of these topics to dyads. In particular, we focus on how well straight-line distances and directions between objects (survey knowledge) were learned by individuals and by the same individuals when comprising a dyad. Our experiment was carried out in a shared virtual environment and we report on the technical issues in conducting such a collaborative experiment in a shared virtual environment, such as the choice of locomotion mode and the provision of full-body self-avatars. Our findings indicate that dyads outperform individuals in their acquisition of survey knowledge.},
  keywords={Virtual environments;Navigation;Collaboration;Task analysis;Avatars;Tracking;Human-centered computing;Human com puter interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Collaborative interaction},
  doi={10.1109/VR46266.2020.00080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089546,
  author={Liu, Jiazhou and Prouzeau, Arnaud and Ens, Barrett and Dwyer, Tim},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Evaluation of Interactive Small Multiples Data Visualisation in Immersive Spaces}, 
  year={2020},
  volume={},
  number={},
  pages={588-597},
  abstract={We explore the adaptation of 2D small-multiples visualisation on flat screens to 3D immersive spaces. We use a "shelves" metaphor for layout of small multiples and consider a design space across a number of layout and interaction dimensions. We demonstrate the applicability of a prototype system informed by this design space to data sets from different domains. We perform two user studies comparing the effect of the shelf curvature dimension from our design space on users’ ability to perform comparison and trend analysis tasks. Our results suggest that, with fewer multiples, a flat layout is more performant despite the need for participants to walk further. With an increase in the number of multiples, this performance difference disappears due to the time participants had to spend walking. In the latter case, users prefer a semi-circular layout over either a fully surrounding or a flat arrangement.},
  keywords={Layout;Three-dimensional displays;Data visualization;Buildings;Two dimensional displays;Atmospheric modeling;Visualization;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Evaluation/methodology},
  doi={10.1109/VR46266.2020.00081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089543,
  author={Simeone, Adalberto L. and Christian Nilsson, Niels and Zenner, André and Speicher, Marco and Daiber, Florian},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Space Bender: Supporting Natural Walking via Overt Manipulation of the Virtual Environment}, 
  year={2020},
  volume={},
  number={},
  pages={598-606},
  abstract={Manipulating the appearance of a Virtual Environment to enable natural walking has so far focused on modifications that are intended to be unnoticed by users. In our research, we took a radically different approach by embracing the overt nature of the change. To explore this method, we designed the Space Bender, a natural walking technique for room-scale VR. It builds on the idea of overtly manipulating the Virtual Environment by "bending" the geometry whenever the user comes in proximity of a physical boundary. Our aim was to evaluate the feasibility of this approach in terms of performance and subjective feedback. We compared the Space Bender to two other similarly situated techniques: Stop and Reset and Teleportation, in a task requiring participants to traverse a 100 m path. Results show that the Space Bender was significantly faster than Stop and Reset, and preferred to the Teleportation technique, highlighting the potential of overt manipulation to facilitate natural walking.},
  keywords={Legged locomotion;Teleportation;Virtual environments;Three-dimensional displays;Geometry;Engines;Human-centered computing;Interaction Paradigms;Virtual Reality},
  doi={10.1109/VR46266.2020.00082},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089581,
  author={Dominic, James and Robb, Andrew},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Effects of Screen-Fixed and World-Fixed Annotation on Navigation in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={607-615},
  abstract={In this paper, we consider the effect of different types of virtual annotations on performance during a navigation task in virtual reality. Two major types of annotations were shown to users: screen-fixed annotations that remained fixed in the user’s field of view, and world- fixed annotations that are linked to specific locations in the world. We also considered three different levels of navigation information, including destination markers, maps visualizing the layout of the space being navigated, and path markers showing the optimal route to the destination. We ran a within-subjects study where participants completed three trials with each of the six combinations of annotation type and information level, for a total of 18 trials in a virtual environment. Average speed, distance traveled, and the time taken to reach the destination were recorded during each trial. Participants were also asked to point back to where they started the trial upon reaching the destination, as a measure of spatial memory. Finally, participants were tasked with completing a secondary activity while navigating, so as to assess what effect annotation types had on multitasking performance. Participants navigated significantly more quickly when using world-fixed annotations; however an interaction effect was observed between the type of annotation and the level of information, which suggests that world-fixed annotations are not inherently better than screen-fixed annotations; instead, it is important to consider both the type of annotation and what information it displays.},
  keywords={Task analysis;Three-dimensional displays;Space exploration;Aircraft navigation;Virtual reality;Layout;Human factors and ergonomics;locomotion and navigation},
  doi={10.1109/VR46266.2020.00083},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089446,
  author={Whitlock, Matt and Smart, Stephen and Szafir, Danielle Albers},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Graphical Perception for Immersive Analytics}, 
  year={2020},
  volume={},
  number={},
  pages={616-625},
  abstract={Immersive Analytics (IA) uses immersive virtual and augmented reality displays for data visualization and visual analytics. Designers rely on studies of how accurately people interpret data in different visualizations to make effective visualization choices. However, these studies focus on data analysis in traditional desktop environments. We lack empirical grounding for how to best visualize data in immersive environments. This study explores how people interpret data visualizations across different display types by measuring how quickly and accurately people conduct three analysis tasks over five visual channels: color, size, height, orientation, and depth. We identify key quantitative differences in performance and user behavior, indicating that stereo viewing resolves some of the challenges of visualizations in 3D space. We also find that while AR displays encourage increased navigation, they decrease performance with color-based visualizations. Our results provide guidelines on how to tailor visualizations to different displays in order to better leverage the affordances of IA modalities.},
  keywords={Data visualization;Task analysis;Image color analysis;Visualization;Navigation;Three-dimensional displays;Two dimensional displays;Human-centered computing—Visualization—Empirical studies in visualization;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR46266.2020.00084},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089452,
  author={Mousas, Christos and Kao, Dominic and Koilias, Alexandros and Rekabdar, Banafsheh},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real and Virtual Environment Mismatching Induces Arousal and Alters Movement Behavior}, 
  year={2020},
  volume={},
  number={},
  pages={626-635},
  abstract={This paper examines a common problem found in a number of virtual reality setups—mismatches between real and virtual environments. Specifically, this paper investigates whether the mismatching between a real and a virtual environment in terms of appearance and physical constraints can affect the arousal (electrodermal activity) and movement behavior in the participants. For this study, one baseline condition and four mismatch conditions that examine different mismatching types were developed and tested in a between-group study design. The participants were immersed in a virtual environment and were asked to walk in a direction given to them along a provided path. During that time, electrodermal activity and the walking motion of participants were captured to assess potential alterations in their arousal and movement behavior respectively. Results obtained from this study indicate significant differences in the electrodermal activity and movement behavior of participants, especially when walking in a virtual environment that is mismatched both in appearance and physical constraints. Even though to a lesser degree, evidence was also found that correlates electrodermal activity with movement behavior. Limitations and future research directions are discussed.},
  keywords={Virtual environments;Virtual reality;Human computer interaction;Head-mounted displays;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR46266.2020.00085},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089661,
  author={Stauffert, Jan-Philipp and Niebling, Florian and Latoschik, Marc Erich},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simultaneous Run-Time Measurement of Motion-to-Photon Latency and Latency Jitter}, 
  year={2020},
  volume={},
  number={},
  pages={636-644},
  abstract={Latency in Virtual Reality (VR) applications can have numerous detrimental effects, e.g., a hampered user experience, a reduced user performance, or the occurrence of cybersickness. In VR environments, latency usually is measured as Motion-to-Photon (MTP) latency and reported as a mean value. This mean is taken during some specific intervals of sample runs with the target system, often detached in significant aspects from the final target scenario, to provide the necessary boundary conditions for the measurements. Additionally, the reported mean value is agnostic to dynamic and spiking latency behavior. This paper introduces an apparatus that is capable of determining per-frame MTP latency to capture dynamic MTP latency and latency jitter in addition to the commonly reported mean values of latency. The approach is evaluated by measuring MTP latency of a VR simulation based on the Unreal engine and the HTC Vive as a typical consumer-grade Head-Mounted Display (HMD). In contrast to previous approaches, the system does not rely on the HMD to be fixed to an external apparatus, can be used to assess any simulation setup, and can be extended to continuously measure latency during run-time. We evaluate the accuracy of our apparatus by injecting a controlled artificial latency in a VR simulation. We show that latency jitter artifacts already occur without system load, potentially caused by the tracking of the specific HMD, and how mean latency and jitter increase under system load, leading to dropped frames and an overall degraded system performance. The presented system can be used to monitor latency and latency jitter as critical simulation characteristics necessary to report and control to avoid unwanted effects and detrimental system performance.},
  keywords={Resists;Photodiodes;Cameras;Jitter;Tracking;Load modeling;Time measurement;D.4.8 [Operating Systems]: Performance—Measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR46266.2020.00086},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089437,
  author={Adhanom, Isayas Berhe and Navarro Griffin, Nathan and MacNeilage, Paul and Folmer, Eelke},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of a Foveated Field-of-view Restrictor on VR Sickness}, 
  year={2020},
  volume={},
  number={},
  pages={645-652},
  abstract={Virtual reality sickness typically results from visual-vestibular conflict. Because self-motion from optical flow is driven most strongly by motion at the periphery of the retina, reducing the user’s field-of-view (FOV) during locomotion has proven to be an effective strategy to minimize visual vestibular conflict and VR sickness. Current FOV restrictor implementations reduce the user’s FOV by rendering a restrictor whose center is fixed at the center of the head mounted display (HMD), which is effective when the user’s eye gaze is aligned with head gaze. However, during eccentric eye gaze, users may look at the FOV restrictor itself, exposing them to peripheral optical flow which could lead to increased VR sickness. To address these limitations, we develop a foveated FOV restrictor and we explore the effect of dynamically moving the center of the FOV restrictor according to the user’s eye gaze position. We conducted a user study (n=22) where each participant uses a foveated FOV restrictor and a head-fixed FOV restrictor while navigating a virtual environment. We found no statistically significant difference in VR sickness measures or noticeability between both restrictors. However, there was a significant difference in eye gaze behavior, as measured by eye gaze dispersion, with the foveated FOV restrictor allowing participants to have a wider visual scan area compared to the head-fixed FOV restrictor, which confined their eye gaze to the center of the FOV.},
  keywords={Optical sensors;Visualization;Virtual reality;Integrated optics;Navigation;Retina;Resists;Virtual Reality;VR Sickness;Field-of-view Manipulation;Eye Tracking},
  doi={10.1109/VR46266.2020.00087},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089534,
  author={Si-Mohammed, Hakim and Lopes-Dias, Catarina and Duarte, Maria and Argelaguet, Ferran and Jeunet, Camille and Casiez, Géry and Müller-Putz, Gernot R and Lécuyer, Anatole and Scherer, Reinhold},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Detecting System Errors in Virtual Reality Using EEG Through Error-Related Potentials}, 
  year={2020},
  volume={},
  number={},
  pages={653-661},
  abstract={When persons interact with the environment and experience or witness an error (e.g. an unexpected event), a specific brain pattern, known as error-related potential (ErrP) can be observed in the electroencephalographic signals (EEG). Virtual Reality (VR) technology enables users to interact with computer-generated simulated environments and to provide multi-modal sensory feedback. Using VR systems can, however, be error-prone. In this paper, we investigate the presence of ErrPs when Virtual Reality users face 3 types of visualization errors: (Te) tracking errors when manipulating virtual objects, (Fe) feedback errors, and (Be) background anomalies. We conducted an experiment in which 15 participants were exposed to the 3 types of errors while performing a center-out pick and place task in virtual reality. The results showed that tracking errors generate error-related potentials, the other types of errors did not generate such discernible patterns. In addition, we show that it is possible to detect the ErrPs generated by tracking losses in single trial, with an accuracy of 85%. This constitutes a first step towards the automatic detection of error-related potentials in VR applications, paving the way to the design of adaptive and self-corrective VR/AR applications by exploiting information directly from the user’s brain.},
  keywords={Virtual reality;Electroencephalography;Task analysis;Human computer interaction;Brain;Three-dimensional displays;Visualization;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR46266.2020.00088},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089634,
  author={Luong, Tiffany and Argelaguet, Ferran and Martin, Nicolas and Lecuyer, Anatole},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Introducing Mental Workload Assessment for the Design of Virtual Reality Training Scenarios}, 
  year={2020},
  volume={},
  number={},
  pages={662-671},
  abstract={Training is one of the major use cases of Virtual Reality (VR) due to the flexibility and reproducibility of VR simulations. However, the use of the user’s cognitive state, and in particular mental workload (MWL), remains largely unexplored in the design of training scenarios. In this paper, we propose to consider MWL for the design of complex training scenarios involving multiple parallel tasks in VR. The proposed approach is based on the assessment of the MWL elicited by each potential task configuration in the training application. Following the assessment, the resulting model is then used to create training scenarios able to modulate the user’s MWL over time. This approach is illustrated by a VR flight training simulator based on the Multi-Attribute Task Battery II, which solicits different cognitive resources, able to generate 12 different tasks configurations. A first user study (N = 38) was conducted to assess the MWL for each task configuration using self-reports and performance measurements. This assessment was then used to generate three training scenarios in order to induce different levels of MWL over time. A second user study (N = 14) confirmed that the proposed approach was able to induce the expected mental workload over time for each training scenario. These results pave the way to further studies exploring how MWL modulation can be used to improve VR training applications.},
  keywords={Task analysis;Training;Physiology;Virtual reality;Brain modeling;Electroencephalography;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00089},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089513,
  author={Venkatakrishnan, Roshan and Venkatakrishnan, Rohith and Bhargava, Ayush and Lucaites, Kathryn and Solini, Hannah and Volonte, Matias and Robb, Andrew and Babu, Sabarish V and Lin, Wen-Chieh and Lin, Yun-Xuan},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparative Evaluation of the Effects of Motion Control on Cybersickness in Immersive Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={672-681},
  abstract={The commercialization and lowering costs of consumer grade Virtual Reality (VR) devices has made the technology increasingly accessible to users around the world. The usage of VR technology is often accompanied by an undesirable side effect called cybersickness. Cyber-sickness is the feeling of discomfort that occurs during VR experiences, producing symptoms similar to those of motion sickness. It continues to remain one of the biggest hurdles to the widespread adoption of VR, making it increasingly important to explore and understand the factors that influence its onset. In this work, we investigated the influence of the presence/absence of motion control on the onset and severity of cybersickness in an HMD based VR driving simulation employing steering as a travel metaphor. Towards this end, we conducted a between subjects study manipulating the presence of control between three experimental conditions, two of which (Driving condition and Yoked Pair condition) formed a yoked control design where every pair of drivers and their yoked pairs were exposed to identical vehicular motion stimuli created by participants in the driving condition. In the other condition (Autonomous Car condition), participants experienced a program driven autonomous vehicle simulation. Results indicated that participants in the Driving condition experienced higher levels of cybersickness than participants in the Yoked Pair condition. While these results don’t conform to findings from previous research which suggests that having control over motion reduces cybersickness, it seems to point towards the importance of the fidelity of the control metaphor’s feedback response in alleviating cybersickness. Simply allowing one control their motion may not readily alleviate cybersickness but could instead increase it in such HMD based VR driving simulations. It may hence be important to consider how well the control metaphor and its feedback matches users’ expectations if we want to successfully mitigate cybersickness.},
  keywords={Virtual environments;Virtual reality;Feedback;Motion control;Simulation;Cybersickness;Human-centered computing;Empirical studies in HCI;Human-centered computing;Virtual reality},
  doi={10.1109/VR46266.2020.00090},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089551,
  author={Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Anaraky, Reza Ghaiumy and Volonte, Matias and Knijnenburg, Bart and Babu, Sabarish V},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Structural Equation Modeling Approach to Understand the Relationship between Control, Cybersickness and Presence in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={682-691},
  abstract={The commercialization of Virtual Reality (VR) devices is making the technology increasingly accessible to users around the world. Despite the success that VR is starting to see with its growing popularity, it has yet to become widely adopted and achieve its ultimate goal- convincingly simulate real life like experiences. The inability to generate adequate levels of presence and to prevent the manifestation of cybersickness are the two prominent barriers that have hindered VR from achieving its ultimate goal. While traditional research has examined factors that influence (correlate with) the onset and severity cybersickness, there is still a gap in our knowledge about the consequences of having motion control on cybersickness in immersive virtual environments (IVE’s) achieved using tracked Head Mounted Displays (HMD’s). Furthermore, outside of a correlational capacity, it is still unclear as to what causes cybersickness to affect presence in immersive virtual environments. The success of immersive virtual reality as a technology will hence largely come down to our ability to understand the interrelationship between these variables and then address the challenges they pose. Towards this end, we investigated how the affordance of motion control affects cybersickness and presence in an HMD based VR driving simulation by conducting a between subjects study where we manipulated the affordance of control between three experimental conditions. We leverage structural equation modeling in an attempt to build a framework that explains the relationship between virtual motion control, workload, cybersickness, time spent in the simulation, perceived time and presence. Our structural model helps explain why motion control could be an important factor to consider in addressing VR’s challenges and realizing its ultimate aim to simulate reality.},
  keywords={Virtual environments;Cybersickness;Virtual reality;Motion control;Human computer interaction;Human-centered computing;Empirical studies in HCI;Human-centered computing;Virtual reality},
  doi={10.1109/VR46266.2020.00091},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089533,
  author={Jiang, Haiyan and Weng, Dongdong},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HiPad: Text entry for Head-Mounted Displays Using Circular Touchpad}, 
  year={2020},
  volume={},
  number={},
  pages={692-703},
  abstract={Text entry in virtual reality (VR) is currently a common activity and a challenging problem. In this paper, we introduce HiPad, leveraging a circular touchpad with a circular virtual keyboard, to support the one-hand text entry in mobile head-mounted displays (HMDs). The design of HiPad’s layout is based on a circle and a square with rounded corners, where the outer circle is subdivided into six keys’ regions containing letters. This technique input text by a common hand-held controller with a circular touchpad for HMDs and disambiguates the word based on the sequence of keys pressed by the user. In our first study, three potential layouts are considered and evaluated, leading to the design containing six keys. By analyzing the touch behavior of users, we optimize the 6-keys layout and conduct the second study, showing that the optimized layout has better performance. Then the third study is conducted to evaluate the performance of 6-keys HiPad with VE-layout and TP-layout and to study the learning curves. The results show that novices can achieve 13.57 Words per Minute (WPM) with VE-layout and 11.60 WPM with TP-layout and the speeds increase by 74.42% for VE-layout users and by 81.53% for TP-layout users through a short 60-phrase training.},
  keywords={Layout;Keyboards;Training;Virtual reality;Visualization;Sensors;Conferences;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality;Human-centered computing;Interaction techniques;Text input},
  doi={10.1109/VR46266.2020.00092},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089450,
  author={Timmerman, Matthew and Sadagic, Amela and Irvine, Cynthia},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Peering Under the Hull: Enhanced Decision Making via an Augmented Environment}, 
  year={2020},
  volume={},
  number={},
  pages={704-712},
  abstract={Daily operation and management of complex systems typically include multiple working sessions during which a team presents a set of information and discusses issues relevant to their decision making. A complex set of operational technology (OT) networks installed onboard a Navy ship is an example of such a system. A crew’s ability to effectively communicate OT networks status to the ship commander, visualize, and discuss the options available in a given situation, has a significant impact on mission success. While the complexity of contemporary OT networks has dramatically increased, visualization tools have witnessed little improvement over several decades—they include sets of two-dimensional blueprints that are inherently hard to understand and conceptualize as three-dimensional (3D) information. To address this problem, we designed and implemented an augmented reality (AR) system that allowed a small team to visualize a 3D model of the ship with details of its computer networks. We recruited 30 individuals familiar with network management tasks central to our study and examined the usability of the tool on a set of real-world scenarios focused on network management. Analysis of objective and subjective data suggested that there was a general agreement among the participants that AR portrayal of the network was very supportive of their understanding of the physical-to-logical relationship within the network and that it fostered constructive collaboration among the team members. The reported levels of discomfort associated with oculomotor symptoms made the highest contribution to the total Simulator Sickness Questionnaire score; we believe that those symptoms should be given more attention in future studies with AR setups. The results provided in this empirical study offer early insights into the benefits and challenges of AR approaches applied to the decision making of small teams in high stakes scenarios and real-world situations.},
  keywords={Task analysis;Three-dimensional displays;Collaboration;Marine vehicles;Two dimensional displays;Tools;Computer networks;augmented reality;collaborative environment;usability;network visualization;small team collaboration;decision making;complex domains},
  doi={10.1109/VR46266.2020.00093},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089617,
  author={Nakamoto, Takamichi and Hirasawa, Tatsuya and Hanyu, Yukiko},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual environment with smell using wearable olfactory display and computational fluid dynamics simulation}, 
  year={2020},
  volume={},
  number={},
  pages={713-720},
  abstract={It is important to include an olfactory cue to enhance the reality in the virtual environment. We have developed the virtual olfactory environment where a user searches for an odor source. The virtual olfactory environment was prepared using computational fluid dynamics calculation. It enables us to have the dynamic odor concentration distribution even if we have complicated obstacles in the virtual environment. Moreover, we developed the wearable olfactory display made up of multiple micro dispensers and SAW (Surface Acoustic Wave) device so that the rapid switching of the smells could be achieved. The wearable olfactory display was attached beneath a head mount display to present a smell quickly. We made the virtual environment of the two-story building where four rooms were located at each floor. A user searched for the source of smoke smell located at one room among four ones at the second floor since we simulated the fire at the early stage. A half of the users could reach the correct source locations in the experiment.},
  keywords={Olfactory;Surface acoustic wave devices;Liquids;Surface acoustic waves;Virtual environments;Floors;Solid modeling;Wearable olfactory display;CFD;disaster simulator;micro dispenser;SAW atomizer},
  doi={10.1109/VR46266.2020.00094},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089461,
  author={Wei, Chunxue and Yu, Difeng and Dingler, Tilman},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reading on 3D Surfaces in Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={721-728},
  abstract={While text tends to lead a rather static life on paper and screens, virtual reality (VR) allows readers to interact with it in novel ways: the reading surface is no longer confined to a 2D plane. We conducted two user studies, in which we assessed text rendered on different surface shapes in VR and their effects on legibility and the reading experience. Comparing differently curved surfaces, these studies disclose the impact of warp angles and view box widths on reading comfort, speed, and distraction. Our results suggest that text should be warped around the horizontal rather than the vertical axis, and we provide recommendations for the extent of warp and view box width. In a proof-of-concept application, we used everyday 3D objects as text canvases and studied them through an information-seeking task. The studies’ implications inform VR interfaces and, more generally, the rendering of text on 3D objects.},
  keywords={Three-dimensional displays;Rendering (computer graphics);Virtual environments;Shape;Two dimensional displays;Task analysis;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/VR46266.2020.00095},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089492,
  author={Homps, François and Beugin, Yohan and Vuillemot, Romain},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={ReViVD: Exploration and Filtering of Trajectories in an Immersive Environment using 3D Shapes}, 
  year={2020},
  volume={},
  number={},
  pages={729-737},
  abstract={We present ReViVD, a tool for exploring and filtering large trajectory-based datasets using virtual reality. ReViVD’s novelty lies in using simple 3D shapes—such as cuboids, spheres and cylinders—as queries for users to select and filter groups of trajectories. Building on this simple paradigm, more complex queries can be created by combining previously made selection groups through a system of user-created Boolean operations. We demonstrate the use of ReViVD in different application domains, from GPS position tracking to simulated data (e. g., turbulent particle flows and traffic simulation). Our results show the ease of use and expressiveness of the 3D geometric shapes in a broad range of exploratory tasks. Re- ViVD was found to be particularly useful for progressively refining selections to isolate outlying behaviors. It also acts as a powerful communication tool for conveying the structure of normally abstract datasets to an audience.},
  keywords={Three-dimensional displays;Shape;Trajectory;Data visualization;Tools;Virtual reality;Two dimensional displays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;Human- centered computing;Visualization;Visualization systems and tools;Visualization toolkits},
  doi={10.1109/VR46266.2020.00096},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089646,
  author={Winther, Frederik and Ravindran, Linoj and Svendsen, Kasper Paabøl and Feuchtner, Tiare},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Evaluation of a VR Training Simulation for Pump Maintenance Based on a Use Case at Grundfos}, 
  year={2020},
  volume={},
  number={},
  pages={738-746},
  abstract={Encouraged by technological advancements, more and more companies consider virtual reality (VR) for training of their workforce in particular for situations that occur rarely, are dangerous, expensive, or very difficult to recreate in the real world. Thereby the need arises for understanding the potentials and limitations of VR training and establishing best practices. In pursuit of this, we have developed a VR Training simulation for a use case at Grundfos, in which apprentices learn a sequential maintenance task. We evaluated this simulation in a user study with 36 participants, comparing it to two traditional forms of training (Pairwise Training and Video Training). This paper describes the developed virtual training scenario and discusses design considerations for such VR simulations. Further, it presents the results of our evaluation, which support that VR Training is effective in teaching the procedure of a maintenance task. However, according to our evidence, traditional approaches with hands-on experience still lead to a significantly better outcome.},
  keywords={Training;Task analysis;Maintenance engineering;Solid modeling;Virtual reality;Computational modeling;Manuals;Human-centered computing—Virtual reality;Human-centered computing—User studies},
  doi={10.1109/VR46266.2020.00097},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089592,
  author={Murcia-López, María and Collingwoode-Williams, Tara and Steptoe, William and Schwartz, Raz and Loving, Timothy J. and Slater, Mel},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Virtual Reality Experiences Through Participant Choices}, 
  year={2020},
  volume={},
  number={},
  pages={747-755},
  abstract={When building virtual reality applications teams must choose between different configurations of the hardware and/or software aspects, and other factors, of the experience. In this paper we extend a framework for assessing how these factors contribute to quality of experience in an example evaluation. We consider how four factors related to avatar expressiveness affect quality of experience: Eye Gaze, Eye Blinking, Mouth Animation, and Microexpressions. 55 participants experienced an avatar delivering a presentation in virtual reality. At fixed times participants had the opportunity to spend a virtual budget to modify the factors to incrementally improve their quality of experience. They could stop making transitions when they felt further changes would make no further difference. From these transitions a Markov matrix was built, along with probabilities of a factor being present at a given level on participants’ final configurations. Most participants did not spend the full budget, suggesting that there was a point of equilibrium which did not require maximizing all factor levels. We discuss that point of equilibrium and present this work as an extended contribution to the evaluation of people’s responses to immersive virtual environments.},
  keywords={Avatars;Computer graphics;Virtual reality;I.3.7;Computer Graphics;Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VR46266.2020.00098},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089632,
  author={Gupta, Kunal and Hajika, Ryo and Pai, Yun Suen and Duenser, Andreas and Lochner, Martin and Billinghurst, Mark},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Measuring Human Trust in a Virtual Assistant using Physiological Sensing in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={756-765},
  abstract={With the advancement of Artificial Intelligence technology to make smart devices, understanding how humans develop trust in virtual agents is emerging as a critical research field. Through our research, we report on a novel methodology to investigate user’s trust in auditory assistance in a Virtual Reality (VR) based search task, under both high and low cognitive load and under varying levels of agent accuracy. We collected physiological sensor data such as electroencephalography (EEG), galvanic skin response (GSR), and heart-rate variability (HRV), subjective data through questionnaire such as System Trust Scale (STS), Subjective Mental Effort Questionnaire (SMEQ) and NASA-TLX. We also collected a behavioral measure of trust (congruency of users’ head motion in response to valid/ invalid verbal advice from the agent). Our results indicate that our custom VR environment enables researchers to measure and understand human trust in virtual agents using the matrices, and both cognitive load and agent accuracy play an important role in trust formation. We discuss the implications of the research and directions for future work.},
  keywords={Task analysis;Electroencephalography;Physiology;Heart rate variability;Games;Virtual reality;Shape;H.5.2 [User Interfaces]: Evaluation/methodology;H.1.2 [User/Machine Systems]: Human factors;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems— Artificial;augmented;and virtual realities},
  doi={10.1109/VR46266.2020.00099},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089531,
  author={Grzeskowiak, Fabien and Babel, Marie and Bruneau, Julien and Pettre, Julien},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward Virtual Reality-based Evaluation of Robot Navigation among People}, 
  year={2020},
  volume={},
  number={},
  pages={766-774},
  abstract={This paper explores the use of Virtual Reality (VR) to study humanrobot interactions during navigation tasks by both immersing a user and a robot in a shared virtual spaces. VR combines the advantages of being safe (as robots and humans interacting by the means of VR but can physically be in remote places) and ecological (realistic environments are perceived by the robot and the human, and natural behaviors can be observed). Nevertheless, VR can introduce perceptual biases in the interaction and affect in some ways the observed behaviors, which can be problematic when used to acquire experimental data. In our case, not only human perception is concerned, but also the one of the robot which requires to be simulated to perceive the VR world. Thus, the contribution of this paper is twofold. It first provides a technical solution to perform human robot interactions in navigation tasks through VR: we describe how we combine motion tracking, VR devices, as well as robot sensors simulation algorithms to immerse together a human and a robot in a shared virtual space. We then assess a simple interaction task that we replicate in real and in virtual conditions to perform a first estimation of the importance of the biases introduced by the use of VR on both a Human and a robot. Our conclusions are in favor of using VR to study human-robot interactions, and we are developing directions for future work.},
  keywords={Robot sensing systems;Collision avoidance;Human-robot interaction;Solid modeling;Task analysis;Virtual reality;Human Robot Interaction;Motion Tracking;Virtual Reality;Robots simulation},
  doi={10.1109/VR46266.2020.00100},
  ISSN={2642-5254},
  month={March},}
@INPROCEEDINGS{9089609,
  author={Ichii, Taro and Mitake, Hironori and Hasegawa, Shoichi},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={TEllipsoid: Ellipsoidal Display for Videoconference System Transmitting Accurate Gaze Direction}, 
  year={2020},
  volume={},
  number={},
  pages={775-781},
  abstract={We propose "TEllipsoid", an ellipsoidal display for video conference systems that can provide not only accurate eye-gaze transmission but also practicality in conferences, namely the convenience to use and the preservation of the identity of the displayed face.The display comprises an ellipsoidal screen, a small projector, and a convex mirror, where the bottom-installed projector projects the facial image of a remote participant onto the screen via the convex mirror. The facial image is made from photos shot from 360 degrees around the participant. Moreover, the image is modified to improve identity. The gaze representation is implemented by projecting the 3D model of eyeballs onto a virtual ellipsoidal screen.We evaluated the gaze transmissibility of the display in conference situations. As a result of experiments, we concluded that accurate gaze transmission is available in conferences when the angular distance of the adjacent participants is more than 38.5 degrees.},
  keywords={Face;Prototypes;Three-dimensional displays;Mirrors;Shape;Conferences;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Displays and imagers;Hardware—Communication hardware, interfaces and storage— Displays and imagers},
  doi={10.1109/VR46266.2020.00101},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089526,
  author={Baker, Lewis and Mills, Steven and Zollmann, Stefanie and Ventura, Jonathan},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={CasualStereo: Casual Capture of Stereo Panoramas with Spherical Structure-from-Motion}, 
  year={2020},
  volume={},
  number={},
  pages={782-790},
  abstract={Hand-held capture of stereo panoramas involves spinning the camera in a roughly circular path to acquire a dense set of views of the scene. However, most existing structure-from-motion pipelines fail when trying to reconstruct such trajectories, due to the small baseline between frames. In this work, we evaluate the use of spherical structure-from-motion for reconstructing handheld stereo panorama captures. The spherical motion constraint introduces a strong regularization on the structure-from-motion process which mitigates the small-baseline problem, making it well-suited to the use case of stereo panorama capture with a handheld camera. We demonstrate the effectiveness of spherical structure-from-motion for casual capture of high-resolution stereo panoramas and validate our results with a user study.},
  keywords={Image stitching;Image reconstruction;Stereo image processing;Three-dimensional displays;Human computer interaction;Virtual reality;Human-centered computing—Interaction paradigms—Virtual reality—;Computer vision—Image and video acquisition—3D imaging},
  doi={10.1109/VR46266.2020.00102},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089584,
  author={Gruen, Robert and Ofek, Eyal and Steed, Anthony and Gal, Ran and Sinclair, Mike and Gonzalez-Franco, Mar},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices}, 
  year={2020},
  volume={},
  number={},
  pages={791-799},
  abstract={Measuring Visual Latency in VR and AR devices has become increasingly complicated as many of the components will influence others in multiple loops and ultimately affect the human cognitive and sensory perception. In this paper we present a new method based on the idea that the performance of humans on a rapid motor task will remain constant, and that any added delay will correspond to the system latency. We ask users to perform a task inside different video see-through devices and also in front of a computer. We also calculate the latency of the systems using a hardware instrumentation-based measurement technique for bench-marking. Results show that this new form of latency measurement through human cognitive performance can be reliable and comparable to hardware instrumentation-based measurement. Our method is adaptable to many forms of user interaction. It is particularly suitable for systems, such as AR and VR, where externalizing signals is difficult, or where it is important to measure latency while the system is in use by a user.},
  keywords={Task analysis;Cameras;Tracking;Visualization;Instruments;Virtual reality;Hardware;Human-centered computing—Virtual reality—;—— Computing methodologies—Perception—},
  doi={10.1109/VR46266.2020.00103},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089486,
  author={Feng, Xianglong and Liu, Yao and Wei, Sheng},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={LiveDeep: Online Viewport Prediction for Live Virtual Reality Streaming Using Lifelong Deep Learning}, 
  year={2020},
  volume={},
  number={},
  pages={800-808},
  abstract={Live virtual reality (VR) streaming has become a popular and trending video application in the consumer market providing users with 360-degree, immersive viewing experiences. To provide premium quality of experience, VR streaming faces unique challenges due to the significantly increased bandwidth consumption. To address the bandwidth challenge, VR video viewport prediction has been proposed as a viable solution, which predicts and streams only the user’s viewport of interest with high quality to the VR device. However, most of the existing viewport prediction approaches target only the video-on-demand (VOD) use cases, requiring offline processing of the historical video and/or user data that are not available in the live streaming scenario. In this work, we develop a novel viewport prediction approach for live VR streaming, which only requires video content and user data in the current viewing session. To address the challenges of insufficient training data and real-time processing, we propose a live VR-specific deep learning mechanism, namely LiveDeep, to create the online viewport prediction model and conduct real-time inference. LiveDeep employs a hybrid approach to address the unique challenges in live VR streaming, involving (1) an alternate online data collection, labeling, training, and inference schedule with controlled feedback loop to accommodate for the sparse training data; and (2) a mixture of hybrid neural network models to accommodate for the inaccuracy caused by a single model. We evaluate LiveDeep using 48 users and 14 VR videos of various types obtained from a public VR user head movement dataset. The results indicate around 90% prediction accuracy, around 40% bandwidth savings, and premium processing time, which meets the bandwidth and real-time requirements of live VR streaming.},
  keywords={Streaming media;Predictive models;Bandwidth;Real-time systems;Data models;Machine learning;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR46266.2020.00104},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089490,
  author={Baker, Lewis and Ventura, Jonathan and Zollmann, Stefanie and Mills, Steven and Langlotz, Tobias},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SPLAT: Spherical Localization and Tracking in Large Spaces}, 
  year={2020},
  volume={},
  number={},
  pages={809-817},
  abstract={When implementing an Augmented Reality (AR) interface, it is essential to track camera motion in order to precisely register the virtual overlay in the view of the user. However, unlike most indoor AR scenarios, in many outdoor scenarios the user maintains a static position performing mostly rotational movements. Simultaneous Localization and Mapping (SLAM) methods typically used to solve the tracking problem require significant translational camera motion to perform reliably. The magnitude of the required translation is proportional to the size of the scene, exacerbating this problem in large environments such as open places or stadiums. In this paper, we present an alternative SLAM method, which combines spherical Structure-from-Motion and a robust 3D tracking method. We compare our method to ORB SLAM2 in synthetic and real tests, and show that our method can track more reliably in large spaces, with simpler calculation due to the spherical motion constraint. We discuss this issue in the context of implementing an AR interface for live sport events in stadiums or other open environments, but possible application scenarios for our technique go beyond and can be applied to handheld AR in many outdoor environments.},
  keywords={Tracking;Simultaneous localization and mapping;Cameras;Three-dimensional displays;Augmented reality;Mobile handsets;Computing methodologies—Tracking—;Human-centered computing—Mixed / augmented reality},
  doi={10.1109/VR46266.2020.00105},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089541,
  author={Sterzentsenko, Vladimiros and Doumanoglou, Alexandros and Thermos, Spyridon and Zioulis, Nikolaos and Zarpalas, Dimitrios and Daras, Petros},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Deep Soft Procrustes for Markerless Volumetric Sensor Alignment}, 
  year={2020},
  volume={},
  number={},
  pages={818-827},
  abstract={With the advent of consumer grade depth sensors, low-cost volumetric capture systems are easier to deploy. Their wider adoption though depends on their usability and by extension on the practicality of spatially aligning multiple sensors. Most existing alignment approaches employ visual patterns, e.g. checkerboards, or markers and require high user involvement and technical knowledge. More user-friendly and easier-to-use approaches rely on markerless methods that exploit geometric patterns of a physical structure. However, current SoA approaches are bounded by restrictions in the placement and the number of sensors. In this work, we improve markerless data-driven correspondence estimation to achieve more robust and flexible multi-sensor spatial alignment. In particular, we incorporate geometric constraints in an end-to-end manner into a typical segmentation based model and bridge the intermediate dense classification task with the targeted pose estimation one. This is accomplished by a soft, differentiable procrustes analysis that regularizes the segmentation and achieves higher extrinsic calibration performance in expanded sensor placement configurations, while being unrestricted by the number of sensors of the volumetric capture system. Our model is experimentally shown to achieve similar results with marker-based methods and outperform the mark-erless ones, while also being robust to the pose variations of the calibration structure. Code and pretrained models are available at https://vcl3d.github.io/StructureNet/.},
  keywords={Cameras;Three-dimensional displays;Calibration;Pose estimation;Solid modeling;Semantics;Computing methodologies;Artificial intelligence;Computer vision;Image segmentation;Computing methodologies;Artificial intelligence;Computer vision;Camera calibration;Computing methodologies;Artificial intelligence;Computer vision;3D imaging},
  doi={10.1109/VR46266.2020.00106},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089460,
  author={Wißmann, Niko and Mišiak, Martin and Fuhrmann, Arnulph and Latoschik, Marc Erich},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Accelerated Stereo Rendering with Hybrid Reprojection-Based Rasterization and Adaptive Ray-Tracing}, 
  year={2020},
  volume={},
  number={},
  pages={828-835},
  abstract={Stereoscopic rendering is a prominent feature of virtual reality applications to generate depth cues and to provide depth perception in the virtual world. However, straight-forward stereo rendering methods usually are expensive since they render the scene from two eye-points which in general doubles the frame times. This is particularly problematic since virtual reality sets high requirements for real-time capabilities and image resolution. Hence, this paper presents a hybrid rendering system that combines classic rasterization and real-time ray-tracing to accelerate stereoscopic rendering. The system reprojects the pre-rendered left half of the stereo image pair into the right perspective using a forward grid warping technique and identifies resulting reprojection errors, which are then efficiently resolved by adaptive real-time ray-tracing. A final analysis shows that the system achieves a significant performance gain, has a negligible quality impact, and is suitable even for higher rendering resolutions.},
  keywords={Rendering (computer graphics);Ray tracing;Real-time systems;Three-dimensional displays;Acceleration;Stereo image processing;Hardware;Computing methodologies;Computer Graphics;Rendering;Ray tracing;Rasterization;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR46266.2020.00107},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089608,
  author={Beams, Ryan and Collins, Brendan and Kim, Andrea S. and Badano, Aldo},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Angular Dependence of the Spatial Resolution in Virtual Reality Displays}, 
  year={2020},
  volume={},
  number={},
  pages={836-841},
  abstract={We compare two methods for characterizing the angular dependence of the spatial resolution in virtual reality head-mounted displays (HMDs) by measuring the line spread response (LSR) across the field of view (FOV) of the device. While slanted-edge is the standard method for determining the resolution of cameras, the standard approach for display devices is to used a line or edge aligned to the display pixel array. However, applying the LSR to head-mounted displays (HMDs) presents additional challenges due to the neareye optics. The LSRs of the HTC Vive and HTC Vive Pro were measured using a line of single white pixels by setting the red, green, and blue subpixels at maximum driving level. The white line was swept along a single direction over a 30° range in the FOV and the spatial resolution was measured using two approaches: wide-field and angle-scanning. In the wide-field method, the 30° FOV is imaged onto a stationary camera. In the second method, the camera is rotated across the FOV such that the white line remains static on the camera with the rotation axis located behind the lens to mimic the human visual system. The results show that the wide-field method overestimates the spatial resolution of the HMD by approximately 40% for angles larger than 10°. Consistent results obtained for the Vive and the Vive Pro indicate that the cause of the resolution limitation depends on the location in the FOV. The limitation in the center of the FOV is the pixel density, whereas, the off-axis spatial resolution is limited by optical components. Achieving high resolution VR HMDs requires system-wide design and technology improvement.},
  keywords={Cameras;Spatial resolution;Resists;Apertures;Distortion measurement;Optical imaging;Lenses;Human-centered computing;Virtual Reality;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR46266.2020.00108},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089636,
  author={Liu, Sinuo and Wang, Ben and Ban, Xiaojuan},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Multiple-scale Simulation Method for Liquid with Trapped Air under Particle-based Framework}, 
  year={2020},
  volume={},
  number={},
  pages={842-850},
  abstract={Trapped air in liquid is an important factor which affect the realism of fluid simulation. However, due to the complex physical properties, simulating the interaction and transformation between air and Liquid is extremely challenging and time-consuming. In this paper, we propose a multi-scale simulation method under particle-based framework to achieve the realistic and efficient simulation of air-liquid fluid. A unified generation rule is proposed according to the kinetic energy and the velocity difference between fluid particles. Two velocity-based dynamic models are then established for different size of air materials respectively. The Brownian motion of small scale air materials is achieved by Schilk random function. The interaction and air transfer between large scale air materials is achieved by inverse diffusion equation and a new high-order kernel function. Experimental results show that the proposed method can improve the fidelity and richness of the fluid simulation. The post-processing scheme makes it able to be integrated with existing particle method easily.},
  keywords={Atmospheric modeling;Solid modeling;Mathematical model;Liquids;Computational modeling;Visual effects;Couplings;Computing methodologies;Computer graphics;Animation;Physical simulation},
  doi={10.1109/VR46266.2020.00109},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089483,
  author={Imamov, Samat and Monzel, Daniel and Lages, Wallace S.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Where to display? How Interface Position Affects Comfort and Task Switching Time on Glanceable Interfaces}, 
  year={2020},
  volume={},
  number={},
  pages={851-858},
  abstract={A critical decision when designing glanceable information displays is where to place the content. Since blocking the center of the field of view with virtual information is not desirable, designers often opt for placement in the visual periphery. Another option is to only show virtual content when needed. However, no study has been made to systematically evaluate world-locked content position, considering both cognitive and physiological constraints. With this goal in mind, we designed a scenario that mimics context switching between a real world-task and an information display. We then conducted a within-subjects study to evaluate the effect of position, parameterized by horizontal angle, vertical angle, and distance from the user. Our results show that context switching time increases as the information is displayed far from the task position. The same happens with discomfort: content placed at eye level, or below, was faster and more comfortable than in other positions. We also found participants preferred content at medium distances, although they were also faster with content at far distances.},
  keywords={Task analysis;Switches;Virtual reality;Three-dimensional displays;Visualization;TV;Layout;Human-centered computing;Mixed and Augmented Reality Human-centered computing;Information Interfaces and Presentation;Miscellaneous},
  doi={10.1109/VR46266.2020.00110},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089553,
  author={Liu, Shiguang and Liu, Jin},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Outdoor Sound Propagation Based on Adaptive FDTD-PE}, 
  year={2020},
  volume={},
  number={},
  pages={859-867},
  abstract={In outdoor scenes, the inhomogeneity of the atmosphere and the ground effect have a great impact on sound propagation, but these two effects are usually ignored in previous methods. We propose an adaptive FDTD-PE method to simulate sound propagation in 3D scenes taking into account atmospheric inhomogeneity and the ground effect to produce more realistic sound propagation results. In the simulation, the ground is considered as a porous medium with a certain thickness. The scene is categorized into a number of two-dimensional vertical ground planes in the three-dimensional cylindrical coordinate system. These planes are decomposed into the near-source complex regions and the far-source regions, which are solved by the FDTD solver and the parabolic equation (PE) solver, respectively. Furthermore, a novel encoding method was designed to process sound pressure data. In the far-source regions, the one-way sound propagation is only affected by the ground and atmosphere inhomogeneity, so we encode sound pressure data through function fitting. Finally, an efficient sound rendering method with this encoding representation is developed to perform auralization in the frequency-domain. We validated our method in various outdoor scenes, and the results indicate that our method can realistically simulate outdoor sound propagation, with quite higher speed and lower storage.},
  keywords={Atmospheric modeling;Acoustics;Finite difference methods;Nonhomogeneous media;Time-domain analysis;Solid modeling;Adaptation models;Applied computing;Arts and humanities;Sound and music computing;Computing methodologies;Computer graphics;Animation},
  doi={10.1109/VR46266.2020.00111},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089578,
  author={Rahman, Yitoshee and Asish, Sarker M. and Fisher, Nicholas P. and Bruce, Ethan C. and Kulshreshth, Arun K. and Borst, Christoph W.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Eye Gaze Visualization Techniques for Identifying Distracted Students in Educational VR}, 
  year={2020},
  volume={},
  number={},
  pages={868-877},
  abstract={Virtual Reality (VR) headsets with embedded eye trackers are appearing as consumer devices (e.g. HTC Vive Eye, FOVE). These devices could be used in VR-based education (e.g., a virtual lab, a virtual field trip) in which a live teacher guides a group of students. The eye tracking could enable better insights into students’ activities and behavior patterns. For real-time insight, a teacher’s VR environment can display student eye gaze. These visualizations would help identify students who are confused/distracted, and the teacher could better guide them to focus on important objects. We present six gaze visualization techniques for a VR-embedded teacher’s view, and we present a user study to compare these techniques. The results suggest that a short particle trail representing eye trajectory is promising. In contrast, 3D heatmaps (an adaptation of traditional 2D heatmaps) for visualizing gaze over a short time span are problematic.},
  keywords={Data visualization;Visualization;Gaze tracking;Heating systems;Image color analysis;Three-dimensional displays;Monitoring;Human-centered computing;Visualization techniques;Human-centered computing;Visualization design and evaluation methods;Human-centered computing;Usability Testing},
  doi={10.1109/VR46266.2020.00009},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089559,
  author={Rauschenberger, Robert and Barakat, Brandon},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Health and Safety of VR Use by Children in an Educational Use Case}, 
  year={2020},
  volume={},
  number={},
  pages={878-884},
  abstract={The present study examined the potential health and safety effects of short-term virtual reality (VR) use by children in an educational use case scenario (that is, relatively brief episodes of use across a limited number of sequential days), such as how VR may be used in the classroom or at a museum. Ophthalmological, vestibular functioning, balance, hand-eye coordination, 3D spatial representation, and subjective comfort effects were assessed using a variety of optometric, psychophysical, and self-report measures. Thirty child participants (ages 10 to 12 years) were immersed in VR for 30 minutes daily across five consecutive days of use. Measurements were taken prior to the onset of VR use (baseline), at the end of the fifth day of VR use (to assess potential acute effects), and 24 hours after the fifth day of VR use (to assess potential longer-lasting effects).There were no statistically significant adverse effects found, with the exception of slightly elevated scores on a self-reported measure of subjective comfort, which, however, were below the range of scores reported in past research as being indicative of subject discomfort. In other words, the current study found no empirical evidence that short-term use of VR in an educational use setting by children ages 10 through 12 years is associated with any adverse visual, spatial representational, or balance aftereffects, or that it causes undue nausea, oculomotor discomfort, or disorientation. The present study does not address longer-term use or potential psychological effects of different VR content.},
  keywords={Atmospheric measurements;Particle measurements;Virtual reality;Health and safety;Pediatrics;Area measurement;Three-dimensional displays;Virtual reality;children;education;health and safety;SSQ;optometry;balance;hand-eye coordination;spatial representation;[Human-centered computing—Interaction Paradigms]:Virtual Reality;[User Characteristics]:Age—Children},
  doi={10.1109/VR46266.2020.00010},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089606,
  author={Rottermanner, Gernot and de Jesus Oliveira, Victor Adriel and Lechner, Patrik and Graf, Philipp and Kreiger, Mylene and Wagner, Markus and Iber, Michael and Rokitansky, Carl-Herbert and Eschbacher, Kurt and Grantz, Volker and Settgast, Volker and Judmaier, Peter},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Evaluation of a Tool to Support Air Traffic Control with 2D and 3D Visualizations}, 
  year={2020},
  volume={},
  number={},
  pages={885-892},
  abstract={Air traffic control officers (ATCOs) are specialized workers responsible to monitor and guide airplanes in their assigned airspace. Such a task is highly visual and mainly supported by 2D visualizations. In this paper, we designed and assessed an application for visualizing air traffic in both orthographic (2D) and perspective (3D) views. A user study was then performed to compare these two types of representations in terms of situation awareness, workload, performance, and user acceptance. Results show that the 3D view yielded both higher situation awareness and less workload than the 2D view condition. However, such a performance does not match the opinion of the ATCOs about the 3D representation.},
  keywords={Three-dimensional displays;Airplanes;Prototypes;Two dimensional displays;Visualization;Task analysis;User interfaces;Human-centered computing;User studies;Humancentered computing;Graphical user interfaces;User interface design;User centered design},
  doi={10.1109/VR46266.2020.00011},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089451,
  author={Zhao, Jiayan and LaFemina, Peter and Carr, Julia and Sajjadi, Pejman and Wallgrün, Jan Oliver and Klippel, Alexander},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Learning in the Field: Comparison of Desktop, Immersive Virtual Reality, and Actual Field Trips for Place-Based STEM Education}, 
  year={2020},
  volume={},
  number={},
  pages={893-902},
  abstract={Field trips are a key component of learning in STEM disciplines such as geoscience to develop skills, integrate knowledge, and prepare students for lifelong learning. Given the reported success of technology-based learning and the prevalence of new forms of technology, especially with immersive virtual reality (iVR) entering the mainstream, virtual field trips (VFTs) are increasingly being considered as an effective form of teaching to either supplement or replace actual field trips (AFTs). However, little research has investigated the implications of VFTs in place-based STEM education, and empirical evidence is still limited about differences between students’ learning experiences and outcomes in VFTs experienced on desktop displays and field trips experienced in iVR. We report on a study that divided an introductory geoscience laboratory course into three groups with the first two groups experiencing a VFT either on desktop (dVFT) or in iVR (iVFT), while the third group went on an AFT. We compared subjective experiences (assessed via questionnaires) and objective learning outcomes for these groups. Our results suggest that, although students reported higher motivation and being more present in the iVFT group, they did not learn more compared to those in the dVFT group; both VFT groups yielded higher scores for learning experience and perceived learning outcomes than the actual field site visit. These findings demonstrate positive learning effects of VFTs relative to AFTs and provide evidence that geology VFTs need not be limited to iVR setups, which lead to considerable equipment costs and increased implementation complexity. Discussing the results, we reflect on the implications of our findings and point out future research directions.},
  keywords={Interactive systems;Education;Virtual environments;STEM;Virtual reality;Virtual field trips;immersion;place-based education},
  doi={10.1109/VR46266.2020.00012},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089472,
  author={Lehman, Sarah M. and Ling, Haibin and Tan, Chiu C.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={ARCHIE: A User-Focused Framework for Testing Augmented Reality Applications in the Wild}, 
  year={2020},
  volume={},
  number={},
  pages={903-912},
  abstract={In this paper, we present ARCHIE, a framework for testing augmented reality applications in the wild. ARCHIE collects user feedback and system state data in situ to help developers identify and debug issues important to testers. It also supports testing of multiple application versions (called "profiles") in a single evaluation session, prioritizing those versions which the tester finds more appealing. To evaluate ARCHIE, we implemented four distinct test case applications and used these applications to examine the performance overhead and context switching cost of incorporating our framework into a pre-existing code base. With these, we demonstrate that ARCHIE provides no significant overhead for AR applications, and introduces at most 2% processing overhead when switching among large groups of testable profiles.},
  keywords={Testing;Interviews;Smart phones;Augmented reality;Google;User interfaces;Debugging;Software and its engineering—Software testing and debugging;Human-centered computing—User interface toolkits;Human-centered computing—Mixed / augmented reality},
  doi={10.1109/VR46266.2020.00013},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089491,
  author={Pouke, Matti and Mimnaugh, Katherine J. and Ojala, Timo and LaValle, Steven M.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Plausibility Paradox For Scaled-Down Users In Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={913-921},
  abstract={This paper identifies a new phenomenon: when users interact with simulated objects in a virtual environment where the user is much smaller than usual, there is a mismatch between the object physics that they expect and the object physics that would be correct at that scale. We report the findings of our study investigating the relationship between perceived realism and a physically accurate approximation of reality in a virtual reality experience in which the user has been scaled down by a factor of ten. We conducted a within-subjects experiment in which 44 subjects performed a simple interaction task with objects under two different physics simulation conditions. In one condition, the objects, when dropped and thrown, behaved accurately according to the physics that would be correct at that reduced scale in the real world, our true physics condition. In the other condition, the movie physics condition, the objects behaved in a similar manner as they would if no scaling of the user had occurred. We found that a significant majority of the users considered the latter condition to be the more realistic one. We argue that our findings have implications for many virtual reality and telepresence applications involving operation with simulated or physical objects in small scales.},
  keywords={Physics;Motion pictures;Virtual environments;Ubiquitous computing;Cameras;Visualization;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented and virtual realities},
  doi={10.1109/VR46266.2020.00014},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089552,
  author={Gagnon, Holly C. and Na, Dun and Heiner, Keith and Stefanucci, Jeanine and Creem-Regehr, Sarah and Bodenheimer, Bobby},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Role of Viewing Distance and Feedback on Affordance Judgments in Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={922-929},
  abstract={The effectiveness of Augmented Reality (AR) for spatial applications is likely influenced by the extent to which users perceive that they can act on virtual objects as if they are real. While there are some promising initial results that action capabilities in AR are perceived similarly to the real world, AR differs from completely real-world viewing both in the visual information available within a restricted field of view (FOV) and in the ways that perceptual-motor feedback can be provided for the outcome of actions. We addressed both of these differences in a study of judgments of passing through an AR aperture defined by two moveable virtual walls presented via the Microsoft HoloLens. First, we investigated the possible effects of viewing distance on affordance judgments, predicting greater error in judgments at a close distance because of the inability to see the entirety of the aperture within the FOV. Second, we explored the effect of feedback on affordance judgments, predicting that verbal feedback about whether a judgment was correct would be sufficient to see improvements in judgments over time. In contrast to our predictions for distance, we found that judgments for passing through were closer to actual shoulder width when viewed at a distance near the aperture (0.85 m) compared to a farther viewing point (3.20 m). Verbal feedback reduced error over trials, but only at the farther distance, possibly because the judgments at the near distance were close to the ceiling of performance. These findings have implications for tasks that require accurate perception of action capabilities in AR, such as training procedures.},
  keywords={Apertures;Virtual environments;Psychology;Augmented reality;Augmented reality;affordances;perception},
  doi={10.1109/VR46266.2020.00112},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089433,
  author={Lu, Feiyu and Davari, Shakiba and Lisle, Lee and Li, Yuan and Bowman, Doug A.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={930-939},
  abstract={Augmented reality head-worn displays (AR HWDs) have the potential to assist personal computing and the acquisition of everyday information. In this research, we propose Glanceable AR, an interaction paradigm for accessing information in AR HWDs. In Glanceable AR, secondary information resides at the periphery of vision to stay unobtrusive and can be accessed by a quick glance whenever needed. We propose two novel hands-free interfaces: "head-glance", in which virtual contents are fixed to the user’s body and can be accessed by head rotation, and "gaze-summon" in which contents can be "summoned" into central vision by eye-tracked gazing at the periphery. We compared these techniques with a baseline heads-up display (HUD), which we call "eye-glance" interface in two dual-task scenarios. We found that the head-glance and eye-glance interfaces are more preferred and more efficient than the gaze-summon interface for discretionary information access. For a continuous monitoring task, the eye-glance interface was preferred. We discuss the implications of our findings for designing Glanceable AR interfaces in AR HWDs.},
  keywords={Augmented reality;User interfaces;Human computer interaction;Head-mounted displays;Human-centered computing;Mixed / augmented reality;Human-centered computing;User interface design},
  doi={10.1109/VR46266.2020.00113},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9089540,
  author={Bhandari, Naval and O’Neill, Eamonn},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Influence of Perspective on Dynamic Tasks in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={939-948},
  abstract={Users are increasingly able to move around and perform tasks in virtual environments (VEs). Such movements and tasks are typically represented in a VE using either a first-person perspective (1PP) or a third-person perspective (3PP). In Virtual Reality (VR), 1PP is almost universally used. 3PP can be represented as either egocentric or allocentric. However, there is little empirical evidence about which view may be better suited to dynamic tasks in particular. This paper compares the use of 1PP, egocentric 3PP and allocentric 3PP for dynamic tasks in VR. Our results indicate that 1PP provides the best spatial perception and performance across several dynamic tasks. This advantage is less pronounced as the task becomes more dynamic.},
  keywords={Task analysis;Cameras;Avatars;Navigation;Virtual environments;Dynamic task;virtual reality;first-person perspective;third-person perspective;egocentric;allocentric;virtual environment;dynamic view;immersion;presence;spatial perception;task performance.},
  doi={10.1109/VR46266.2020.00114},
  ISSN={2642-5254},
  month={March},}
