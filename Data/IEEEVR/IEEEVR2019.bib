@INPROCEEDINGS{8798326,
  author={Jung, Raehyuk and Lee, Aiden Seung Joon and Ashtari, Amirsaman and Bazin, Jean-Charles},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Deep360Up: A Deep Learning-Based Approach for Automatic VR Image Upright Adjustment}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={Spherical VR cameras can capture high-quality immersive VR images with a 360° field of view. However, in practice, when the camera orientation is not straight, the acquired VR image appears tilted when displayed on a VR headset, which diminishes the quality of the VR experience. To overcome this problem, we present a deep learning-based approach that can automatically estimate the orientation of a VR image and return its upright version. In contrast to existing methods, our approach does not require the presence of lines or horizon in the image, and thus can be applied on a wide range of scenes. Extensive experiments and comparisons with state-of-the-art methods have successfully confirmed the validity of our approach.},
  keywords={Cameras;Headphones;Deep learning;Three-dimensional displays;Training;Standards;Estimation;Upright adjustment;deep learning;VR content;Computing methodologies—Computer graphics—Image manipulation—Image processing},
  doi={10.1109/VR.2019.8798326},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798281,
  author={da Silveira, Thiago L. T. and Jung, Claudio R.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dense 3D Scene Reconstruction from Multiple Spherical Images for 3-DoF+ VR Applications}, 
  year={2019},
  volume={},
  number={},
  pages={9-18},
  abstract={We propose a novel method for estimating the 3D geometry of indoor scenes based on multiple spherical images. Our technique produces a dense depth map registered to a reference view so that depth-image-based-rendering (DIBR) techniques can be explored for providing three-degrees-of-freedom plus immersive experiences to virtual reality users. The core of our method is to explore large displacement optical flow algorithms to obtain point correspondences, and use cross-checking and geometric constraints to detect and remove bad matches. We show that selecting a subset of the best dense matches leads to better pose estimates than traditional approaches based on sparse feature matching, and explore a weighting scheme to obtain the depth maps. Finally, we adapt a fast image-guided filter to the spherical domain for enforcing local spatial consistency, improving the 3D estimates. Experimental results indicate that our method quantitatively outperforms competitive approaches on computer-generated images and synthetic data under noisy correspondences and camera poses. Also, we show that the estimated depth maps obtained from only a few real spherical captures of the scene are capable of producing coherent synthesized binocular stereoscopic views by using traditional DIBR methods.},
  keywords={Three-dimensional displays;Cameras;Optical imaging;Feature extraction;Image reconstruction;Optical distortion;Geometry;Computing methodologies—Computer vision problems—Reconstruction;Computing methodologies—Computer graphics—Image manipulation—Image-based rendering;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality},
  doi={10.1109/VR.2019.8798281},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797974,
  author={Matthews, Brandon J. and Thomas, Bruce H. and Von Itzstein, Stewart and Smith, Ross T.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Remapped Physical-Virtual Interfaces with Bimanual Haptic Retargeting}, 
  year={2019},
  volume={},
  number={},
  pages={19-27},
  abstract={This paper proposes a novel interface for virtual reality in which physical interface components are mapped to multiple virtual counterparts using haptic retargeting illusions. This gives virtual reality interfaces the ability to have correct haptic sensations for many virtual buttons although in the physical space there is only one. This is a generic system that can be applied to areas including design, interaction tasks, product prototype development and interactive games in virtual reality. The system presented extends existing retargeting algorithms to support asymmetric bimanual interactions. A new warp technique, called interface warp, was developed to support remapped virtual reality user interfaces. Through an experimental user study, we explore the effects of bimanual retargeting and the interface warp technique on task response time, errors, presence, perceived manipulation compared to unimanual (single handed) retargeting and other existing warp techniques. The results demonstrated faster task response time and less errors for the interface warp technique and shows no significant effect of bimanual interactions.},
  keywords={Haptic interfaces;Virtual reality;Task analysis;User interfaces;Mathematical model;Visualization;Shape;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Haptic I/O;H.1.2 [Models and Principles]: User/Machine Systems—Human Factors},
  doi={10.1109/VR.2019.8797974},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797744,
  author={Brickler, David and Volonte, Matias and Bertrand, Jeffrey W. and Duchowski, Andrew T. and Babu, Sabarish V.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Stereoscopic Viewing and Haptic Feedback, Sensory-Motor Congruence and Calibration on Near-Field Fine Motor Perception-Action Coordination in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={28-37},
  abstract={We present an empirical evaluation on how stereoscopic viewing and haptic feedback deferentially affects fine motor perception-action coordination in a pick-and-place task in Virtual Reality (VR). The factors considered were stereoscopic viewing, haptic feedback, sensory-motor congruence and mismatch, and calibration on perception-action coordination in near field fine motor task performance in VR. Quantitative measures of placement error, distance, collision, and time to complete trials were recorded and analyzed. Overall, we found that participants' manual dexterous task performance was enhanced in the presence of both stereoscopic viewing and haptic feedback. However, we found that time to complete task was greatly enhanced by the presence of haptic feedback, and economy and efficiency of movement of the end effector as well as the manipulated object was enhanced by the presence of both haptic feedback and stereoscopic viewing. Whereas, number of collisions and placement accuracy were greatly enhanced by the presence of stereoscopic viewing in near-field fine motor perception-action coordination. Our research additionally shows that mismatch in sensory-motor stimuli can detrimentally affect the number of collisions, and efficiency of end effector and object movements in near-field fine motor activities, and can be further negatively affected by the absence of haptic feedback and stereoscopic viewing. In spite of reduced cue situations in VR, and the absence or presence of stereoscopic viewing and haptic feedback, we found that participants tend to calibrate or adapt their perception-action coordination rapidly with a set of at least 5 trials.},
  keywords={Haptic interfaces;Stereo image processing;Task analysis;Surgery;Three-dimensional displays;Training;Solid modeling;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797744},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798300,
  author={Kang, HyeongYeop and Lee, Geonsun and Han, JungHyun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Manipulation for Underwater Drag Force Perception in Immersive Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={38-46},
  abstract={In this paper, we propose to reproduce drag forces in a virtual underwater environment. To this end, we first compute the drag forces to be exerted on human limbs in a physically correct way. Adopting a pseudo-haptic approach that generates visual discrepancies between the real and virtual limb motions, we compute the extent of drag forces that are applied to the virtual limbs and can be naturally perceived. Through two tests, our drag force simulation method is compared with others. The results show that our method is effective in reproducing the sense of being immersed in water. Our study can be utilized for various types of virtual underwater applications such as scuba diving training and aquatic therapy.},
  keywords={Drag;Force;Visualization;Haptic interfaces;Avatars;Angular velocity;Shoulder;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798300},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798143,
  author={Zenner, André and Krüger, Antonio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Estimating Detection Thresholds for Desktop-Scale Hand Redirection in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={47-55},
  abstract={Virtual reality (VR) interaction techniques like haptic retargeting offset the user's rendered virtual hand from the real hand location to redirect the user's physical hand movement. This paper explores the order of magnitude of hand redirection that can be applied without the user noticing it. By deriving lower-bound estimates of detection thresholds, we quantify the range of unnoticeable redirection for the three basic redirection dimensions, horizontal, vertical and gain-based hand warping. In a two-alternative forced choice (2AFC) experiment, we individually explore these three hand warping dimensions each in three different scenarios: a very conservative scenario without any distraction and two conservative but more realistic scenarios that distract users from the redirection. Additionally, we combine the results of all scenarios to derive robust recommendations for each redirection technique. Our results indicate that within a certain range, desktop-scale VR hand redirection can go unnoticed by the user, but that this range is narrow. The findings show that the virtual hand can be unnoticeably displaced horizontally or vertically by up to 4.5° in either direction, respectively. This allows for a range of ca. 9°, in which users cannot reliably detect applied redirection. For our gain-based hand redirection technique, we found that gain factors between g = 0.88 and g = 1.07 can go unnoticed, which corresponds to a user grasping up to 13.75% further or up to 6.18% less far than in virtual space. Our findings are of value for the development of VR applications that aim to redirect users in an undetectable manner, such as for haptic retargeting.},
  keywords={Haptic interfaces;Visualization;Three-dimensional displays;Virtual reality;Legged locomotion;Indexes;User interfaces;Virtual reality;hand redirection;detection thresholds;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology, Interaction styles},
  doi={10.1109/VR.2019.8798143},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797983,
  author={Thomas, Jerald and Rosenberg, Evan Suma},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A General Reactive Algorithm for Redirected Walking Using Artificial Potential Functions}, 
  year={2019},
  volume={},
  number={},
  pages={56-62},
  abstract={Redirected walking enables users to locomote naturally within a virtual environment that is larger than the available physical space. These systems depend on steering algorithms that continuously redirect users within limited real world boundaries. While a majority of the most recent research has focused on predictive algorithms, it is often necessary to utilize reactive approaches when the user's path is unconstrained. Unfortunately, previously proposed reactive algorithms assume a completely empty space with convex boundaries and perform poorly in complex real world spaces containing obstacles. To overcome this limitation, we present Push/Pull Reactive (P2R), a novel algorithm that uses an artificial potential function to steer users away from potential collisions. We also introduce three new reset strategies and conducted an experiment to evaluate which one performs best when used with P2R. Simulation results demonstrate that the proposed approach outperforms the previous state-of-the-art reactive algorithm in non-convex spaces with and without interior obstacles.},
  keywords={Prediction algorithms;Legged locomotion;Force;Virtual environments;Heuristic algorithms;Layout;Redirected Walking;Virtual Reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8797983},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798121,
  author={Lee, Dong-Yong and Cho, Yong-Hun and Lee, In-Kwon},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-time Optimal Planning for Redirected Walking Using Deep Q-Learning}, 
  year={2019},
  volume={},
  number={},
  pages={63-71},
  abstract={This work presents a novel control algorithm of redirected walking called steer-to-optimal-target (S2OT) for effective real-time planning in redirected walking. S2OT is a method of redirection estimating the optimal steering target that can avoid the collision on the future path based on the user's virtual and physical paths. We design and train the machine learning model for estimating optimal steering target through reinforcement learning, especially, using the technique called Deep Q-Learning. S2OT significantly reduces the number of resets caused by collisions between user and physical space boundaries compared to well-known algorithms such as steer-to-center (S2C) and Model Predictive Control Redirection (MPCred). The results are consistent for any combinations of room-scale and large-scale physical spaces and virtual maps with or without predefined paths. S2OT also has a fast computation time of 0.763 msec per redirection, which is sufficient for redirected walking in real-time environments.},
  keywords={Planning;Legged locomotion;Heuristic algorithms;Real-time systems;Virtual environments;Reinforcement learning;Neural networks;Computer Graphics—Three-Dimensional Graphics and Realism—Virtual reality;Information Interfaces and Presentation—Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798121},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797818,
  author={Messinger, Justin and Hodgson, Eric and Bachmann, Eric R.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Tracking Area Shape and Size on Artificial Potential Field Redirected Walking}, 
  year={2019},
  volume={},
  number={},
  pages={72-80},
  abstract={Immersive Virtual Environment systems that utilize Head Mounted Displays and a large tracking area have the advantage of being able to use natural walking as a locomotion interface. In such systems, difficulties arise when the virtual world is larger than the tracking area and users approach area boundaries. Redirected walking (RDW) is a technique that distorts the correspondence between physical and virtual world motion to steer users away from boundaries and obstacles, including other co-immersed users. Recently, a RDW algorithm was proposed based on the use of artificial potential fields (APF), in which walls and obstacles repel the user. APF-RDW effectively supports multiple simultaneous users and, unlike other RDW algorithms, can easily account for tracking area dimensions and room shape when generating steering instructions. This work investigates the performance of a refined APF-RDW algorithm in different sized tracking areas and in irregularly shaped rooms, as compared to a Steer-to-Center (STC) algorithm and an un-steered control condition. Data was generated in simulation using logged paths of prior live users, and is presented for both single-user and multi-user scenarios. Results show the ability of APF-RDW to steer effectively in irregular concave shaped tracking areas such as L-shaped rooms or crosses, along with scalable multi-user support, and better performance than STC algorithms in almost all conditions.},
  keywords={Legged locomotion;Force;Shape;Task analysis;Target tracking;Navigation;Redirected walking;virtual environments;navigation;simulation},
  doi={10.1109/VR.2019.8797818},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797990,
  author={He, Dejing and Wang, Rui and Bao, Hujun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Rendering of Stereo-Consistent Contours}, 
  year={2019},
  volume={},
  number={},
  pages={81-87},
  abstract={Line drawing is an important and concise method to depict the shape of an object. Stereo line drawing, a combination of line drawing and stereo rendering, not only efficiently conveys shape but also provides users with a visual experience of a stereoscopic 3D world. Contours are the most important lines to draw. However, contours must be rendered consistently for two eyes because of their view-dependent nature; otherwise, they cause binocular rivalry and viewing discomfort. This paper proposes a novel solution to draw stereo-consistent contours in real time. First, we extend the concept of epipolar-slidability and derive a new criterion to check epipolar-slidability by the monotonicity of the trajectory of the viewpoints of contour points. Then, we design an algorithm to test the epipolar-slidability of contours by conducting an image space search rather than sampling multiple viewpoints. Results show that the proposed method has a much lower cost than that of previous works, therefore enables the real-time rendering and editing of stereo-consistent contours for users, such as changing camera viewpoints, editing object geometry, tweaking parameters to show contours with different details, etc.},
  keywords={Rendering (computer graphics);Real-time systems;Trajectory;Three-dimensional displays;Shape;Geometry;Solid modeling;Stereo contour rendering;binocular rivalry;stereo consistency;epipolar-slidability;real-time rendering;Computing methodologies—Computer graphics—Rendering},
  doi={10.1109/VR.2019.8797990},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798283,
  author={Fink, Laura and Hensel, Nora and Markov-Vetter, Daniela and Weber, Christoph and Staadt, Oliver and Stamminger, Marc},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hybrid Mono-Stereo Rendering in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={88-96},
  abstract={Rendering for Head Mounted Displays (HMD) causes a doubled computational effort, since serving the human stereopsis requires the creation of one image for the left and one for the right eye. The difference in this image pair, called binocular disparity, is an important cue for depth perception and the spatial arrangement of surrounding objects. Findings in the context of the human visual system (HVS) have shown that especially in the near range of an observer, binocular disparities have a high significance. But as with rising distance the disparity converges to a simple geometric shift, also the importance as depth cue exponentially declines. In this paper, we exploit this knowledge about the human perception by rendering objects fully stereoscopic only up to a chosen distance and monoscopic, from there on. By doing so, we obtain three distinct images which are synthesized to a new hybrid stereoscopic image pair, which reasonably approximates a conventionally rendered stereoscopic image pair. The method has the potential to reduce the amount of rendered primitives easily to nearly 50 % and thus, significantly lower frame times. Besides of a detailed analysis of the introduced formal error and how to deal with occurring artifacts, we evaluated the perceived quality of the VR experience during a comprehensive user study with nearly 50 participants. The results show that the perceived difference in quality between the shown image pairs was generally small. An in-depth analysis is given on how the participants reached their decisions and how they subjectively rated their VR experience.},
  keywords={Rendering (computer graphics);Stereo image processing;Cameras;Virtual reality;Hardware;Visualization;Resists;Virtual Reality;Stereoscopic Rendering;Depth Perception;Virtual Reality—Stereoscopic Rendering—Hybrid Rendering—Depth Perception},
  doi={10.1109/VR.2019.8798283},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798111,
  author={Müller, Christoph and Braun, Matthias and Ertl, Thomas},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimised Molecular Graphics on the HoloLens}, 
  year={2019},
  volume={},
  number={},
  pages={97-102},
  abstract={The advent of modern and affordable augmented reality head sets like Microsoft HoloLens has sparked new interest in using virtual and augmented reality technology in the analysis of molecular data. For all visualisation in immersive, mixed-reality scenarios, a sufficiently high rendering speed is an important factor, which leads to the issue of limited processing power available on fully untethered devices facing the situation of handling computationally expensive visualisations. Recent research shows that the space-filling model of even small data sets from the Protein Data Bank (PDB) cannot be rendered at desirable frame rates on the HoloLens. In this work, we report on how to improve the rendering speed of atom-based visualisation of proteins and how the rendering of more abstract representations of the molecules compares against it. We complement our findings with in-depth GPU and CPU performance numbers.},
  keywords={Rendering (computer graphics);Atomic measurements;Data visualization;Sprites (computer);Graphics processing units;Image color analysis;Solids;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed/augmented reality;Computing methodologies—Computer graphics—Graphics systems and interfaces—Graphics processors;Human-centered computing—Visualization—Scientific visualization},
  doi={10.1109/VR.2019.8798111},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798284,
  author={Schütz, Markus and Krösl, Katharina and Wimmer, Michael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Continuous Level of Detail Rendering of Point Clouds}, 
  year={2019},
  volume={},
  number={},
  pages={103-110},
  abstract={Real-time rendering of large point clouds requires acceleration structures that reduce the number of points drawn on screen. State-of-the art algorithms group and render points in hierarchically organized chunks with varying extent and density, which results in sudden changes of density from one level of detail to another, as well as noticeable popping artifacts when additional chunks are blended in or out. These popping artifacts are especially noticeable at lower levels of detail, and consequently in virtual reality, where high performance requirements impose a reduction in detail. We propose a continuous level-of-detail method that exhibits gradual rather than sudden changes in density. Our method continuously recreates a down-sampled vertex buffer from the full point cloud, based on camera orientation, position, and distance to the camera, in a point-wise rather than chunk-wise fashion and at speeds up to 17 million points per millisecond. As a result, additional details are blended in or out in a less noticeable and significantly less irritating manner as compared to the state of the art. The improved acceptance of our method was successfully evaluated in a user study.},
  keywords={Rendering (computer graphics);Three-dimensional displays;Cameras;Distortion;Octrees;Real-time systems;Virtual reality;Computing methodologies—Computer graphics—Rendering;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality},
  doi={10.1109/VR.2019.8798284},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797906,
  author={Cheng, Haonan and Liu, Shiguang},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Haptic Force Guided Sound Synthesis in Multisensory Virtual Reality (VR) Simulation for Rigid-Fluid Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={111-119},
  abstract={This paper tackles a challenging problem for interactive rigid-fluid interaction sound synthesis. One core issue of the rigid-fluid interaction in multisensory VR system is how to balance the algorithm efficiency, result authenticity and result synchronization. Since the sampling rate of audio is far greater than visual and haptic modalities, sound synthesis for a multisensory VR system is more difficult than visual simulation and haptic rendering, which still remains an open challenge until now. Therefore, this paper focuses on developing an efficient sound synthesis method tailored for a multisensory system. To improve the result authenticity while ensuring real time performance and result synchronization, we propose a novel haptic force guided granular sound synthesis method tailored for sounding in multisensory VR systems. To the best of our knowledge, this is the first step that exploits haptic force feedback from the tactile channel for guiding sound synthesis in a multisensory VR system. Specifically, we propose a modified spectral granular sound synthesis method, which can ensure real time simulation and improve the result authenticity as well. Then, to balance the algorithm efficiency and result synchronization, we design a multi-force (MF) granulation algorithm which avoids repeated analysis of fluid particle motion and thereby improves the synchronization performance. Various results show that the proposed sound synthesis method effectively overcomes the limitations of existing methods in terms of audio modality, which has great potential to provide powerful technological support for building a more immersive multisensory VR system.},
  keywords={Haptic interfaces;Rendering (computer graphics);Synchronization;Force;Computational modeling;Real-time systems;Solid modeling;Human-centered computing—Human computer interaction—Interaction devices—Haptic devices;Applied computing—Arts and humanities—Sound and music computing},
  doi={10.1109/VR.2019.8797906},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798247,
  author={Kim, Hansung and Remaggi, Luca and Jackson, Philip J.B. and Hilton, Adrian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Spatial Audio Reproduction for VR/AR Using Room Acoustic Modelling from 360° Images}, 
  year={2019},
  volume={},
  number={},
  pages={120-126},
  abstract={Recent progresses in Virtual Reality (VR) and Augmented Reality (AR) allow us to experience various VR/AR applications in our daily life. In order to maximise the immersiveness of user in VR/AR environments, a plausible spatial audio reproduction synchronised with visual information is essential. In this paper, we propose a simple and efficient system to estimate room acoustic for plausible reproducton of spatial audio using 360° cameras for VR/AR applications. A pair of 360° images is used for room geometry and acoustic property estimation. A simplified 3D geometric model of the scene is estimated by depth estimation from captured images and semantic labelling using a convolutional neural network (CNN). The real environment acoustics are characterised by frequency-dependent acoustic predictions of the scene. Spatially synchronised audio is reproduced based on the estimated geometric and acoustic properties in the scene. The reconstructed scenes are rendered with synthesised spatial audio as VR/AR content. The results of estimated room geometry and simulated spatial audio are evaluated against the actual measurements and audio calculated from ground-truth Room Impulse Responses (RIRs) recorded in the rooms.},
  keywords={Acoustics;Cameras;Three-dimensional displays;Geometry;Image segmentation;Semantics;Visualization;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Computing methodologies—Artificial intelligence—Computer vision—Scene understanding},
  doi={10.1109/VR.2019.8798247},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798080,
  author={Grandi, Jerônimo Gustavo and Debarba, Henrique Galvan and Maciel, Anderson},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities}, 
  year={2019},
  volume={},
  number={},
  pages={127-135},
  abstract={We present an assessment of asymmetric interactions in Collaborative Virtual Environments (CVEs). In our asymmetric setup, two co-located users interact with virtual 3D objects, one in immersive Virtual Reality (VR) and the other in mobile Augmented Reality (AR). We conducted a study with 36 participants to evaluate performance and collaboration aspects of pair work, and compare it with two symmetric scenarios, either with both users in immersive VR or mobile AR. To perform this experiment, we adopt a collaborative AR manipulation technique from literature and develop and evaluate a VR manipulation technique of our own. Our results indicate that pairs in asymmetric VR-AR achieved significantly better performance than the AR symmetric condition, and similar performance to VR symmetric. Regardless of the condition, pairs had similar work participation indicating a high cooperation level even when there is a visualization and interaction asymmetry between the participants.},
  keywords={Collaboration;Three-dimensional displays;Visualization;Task analysis;Augmented reality;User interfaces;Human-centered computing—Human computer interaction (HCI)—Interaction techniques;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/augmented reality Human-centered computing—Collaborative and social computing},
  doi={10.1109/VR.2019.8798080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797807,
  author={Weissker, Tim and Kulik, Alexander and Froehlich, Bernd},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Multi-Ray Jumping: Comprehensible Group Navigation for Collocated Users in Immersive Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={136-144},
  abstract={The collaborative exploration of virtual environments benefits from joint group navigation capabilities. In this paper, we focus on the design and evaluation of a short-range teleportation technique (jumping) for a group of collocated users wearing head-mounted displays. In a pilot study with expert users, we tested three naïve group jumping approaches and derived the requirements for comprehensible group jumping. We propose a novel Multi-Ray Jumping technique to meet these requirements and report results of two formal user studies, one exploring the effects of passive jumping on simulator sickness symptoms (N=20) and a second one investigating the advantages of our novel technique compared to naïve group jumping (N=22). The results indicate that Multi-Ray Jumping decreases spatial confusion for passengers, increases planning accuracy for navigators, and reduces cognitive load for both.},
  keywords={Navigation;Virtual environments;Head-mounted displays;Collaboration;Avatars;Visualization;Virtual reality;head-mounted displays;multi-user interaction;group navigation;collocation;teleportation;jumping;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Collaborative computing},
  doi={10.1109/VR.2019.8797807},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797733,
  author={Cavallo, Marco and Dholakia, Mishal and Havlena, Matous and Ocheltree, Kenneth and Podlaseck, Mark},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative Information Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={145-153},
  abstract={Immersive environments have gradually become standard for visualizing and analyzing large or complex datasets that would otherwise be cumbersome, if not impossible, to explore through smaller scale computing devices. However, this type of workspace often proves to possess limitations in terms of interaction, flexibility, cost and scalability. In this paper we introduce a novel immersive environment called Dataspace, which features a new combination of heterogeneous technologies and methods of interaction towards creating a better team workspace. Dataspace provides 15 high-resolution displays that can be dynamically reconfigured in space through robotic arms, a central table where information can be projected, and a unique integration with augmented reality (AR) and virtual reality (VR) headsets and other mobile devices. In particular, we contribute novel interaction methodologies to couple the physical environment with AR and VR technologies, enabling visualization of complex types of data and mitigating the scalability issues of existing immersive environments. We demonstrate through four use cases how this environment can be effectively used across different domains and reconfigured based on user requirements. Finally, we compare Dataspace with existing technologies, summarizing the trade-offs that should be considered when attempting to build better collaborative workspaces for the future.},
  keywords={Data visualization;Three-dimensional displays;Collaboration;Headphones;Two dimensional displays;Augmented reality;Human-centered computing—Visualization—Visualization Systems and Tools;Human-centered computing—Human computer interaction (HCI)—Interactive systems and tools},
  doi={10.1109/VR.2019.8797733},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797867,
  author={Zhao, Jiayan and Klippel, Alexander},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Scale - Unexplored Opportunities for Immersive Technologies in Place-based Learning}, 
  year={2019},
  volume={},
  number={},
  pages={155-162},
  abstract={Immersive technologies have the potential to overcome physical limitations and virtually deliver field site experiences, for example, into the classroom. Yet, little is known about the features of immersive technologies that contribute to successful place-based learning. Immersive technologies afford embodied experiences by mimicking natural embodied interactions through a user's egocentric perspective. Additionally, they allow for beyond reality experiences integrating contextual information that cannot be provided at actual field sites. The current study singles out one aspect of place-based learning: Scale. In an empirical evaluation, scale was manipulated as part of two immersive virtual field trip (iVFT) experiences in order to disentangle its effect on place-based learning. Students either attended an actual field trip (AFT) or experienced one of two iVFTs using a head-mounted display. The iVFTs either mimicked the actual field trip or provided beyond reality experiences offering access to the field site from an elevated perspective using pseudo-aerial 360° imagery. Results show that students with access to the elevated perspective had significantly better scores, for example, on their spatial situation model (SSM). Our findings provide first results on how an increased (geographic) scale, which is accessible through an elevated perspective, boosts the development of SSMs. The reported study is part of a larger immersive education effort. Inspired by the positive results, we discuss our plan for a more rigorous assessment of scale effects on both self- and objectively assessed performance measures of spatial learning.},
  keywords={Immersive learning;virtual field trips;scale;place-based education;Applied computing—Education—Interactive learning environments;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797867},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797708,
  author={Liao, Meng-Yun and Sung, Ching-Ying and Wang, Hao-Chuan and Lin, Wen-Chieh},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Classmates: Embodying Historical Learners' Messages as Learning Companions in a VR Classroom through Comment Mapping}, 
  year={2019},
  volume={},
  number={},
  pages={163-171},
  abstract={Online learning platforms such as MOOCs have been prevalent sources of self-paced learning to people nowadays. However, the lack of peer accompaniment and social interaction may increase learners' sense of isolation and loneliness. Prior studies have shown the positive effects on visualizing peer students' appearances with virtual avatars or virtualized online learners in VR learning environments. In this work, we propose to build virtual classmates, which were constructed by synthesizing previous learners' messages (time-anchored comments). Configurations of virtual classmates, such as the number of classmates participating in a VR class and the behavioral features of the classmates, can also be adjusted. To build the characteristics of virtual classmates, we propose a technique called comment mapping to aggregate prior online learners' comments to shape virtual classmates' behaviors. We conduct a study with 100 participants to evaluate the effects of the virtual classmates built with and without the comment mapping and the amount of virtual classmates rendered in VR. The findings of our study suggest design implications for developing virtual classmates in VR environments.},
  keywords={Avatars;Streaming media;Task analysis;Discussion forums;Virtual environments;Computer science;Games},
  doi={10.1109/VR.2019.8797708},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798338,
  author={Chen, Yi-Ting and Hsu, Chi-Hsuan and Chung, Chih-Han and Wang, Yu-Shuen and Babu, Sabarish V.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking Interface for Study and Reflection in VR Learning Environments}, 
  year={2019},
  volume={},
  number={},
  pages={172-180},
  abstract={In this contribution, we design, implement and evaluate the pedagogical benefits of a novel interactive note taking interface (iVRNote) in VR for the purpose of learning and reflection lectures. In future VR learning environments, students would have challenges in taking notes when they wear a head mounted display (HMD). To solve this problem, we installed a digital tablet on the desk and provided several tools in VR to facilitate the learning experience. Specifically, we track the stylus' position and orientation in the physical world and then render a virtual stylus in VR. In other words, when students see a virtual stylus somewhere on the desk, they can reach out with their hand for the physical stylus. The information provided will also enable them to know where they will draw or write before the stylus touches the tablet. Since the presented iVRNote featuring our note taking system is a digital environment, we also enable students save efforts in taking extensive notes by providing several functions, such as post-editing and picture taking, so that they can pay more attention to lectures in VR. We also record the time of each stroke on the note to help students review a lecture. They can select a part of their note to revisit the corresponding segment in a virtual online lecture. Figures and the accompanying video demonstrate the feasibility of the presented iVRNote system. To evaluate the system, we conducted a user study with 20 participants to assess the preference and pedagogical benefits of the iVRNote interface. The feedback provided by the participants were overall positive and indicated that the iVRNote interface could be potentially effective in VR learning experiences.},
  keywords={Three-dimensional displays;Writing;Tools;Tracking;Education;Portable computers;Task analysis;Virtual reality;classroom;on-line learning;Human-centered computing—Visualization—Visualization techniques—Education;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798338},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797889,
  author={Clifford, Rory M.S. and Jung, Sungchul and Hoermann, Simon and Billinghurst, Mark and Lindeman, Robert W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Creating a Stressful Decision Making Environment for Aerial Firefighter Training in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={181-189},
  abstract={The decisions made by an Air Attack Supervisor (AAS) helicopter co-pilots in aerial firefighting have critical and immediate impacts. It is difficult to always make fast, high quality decisions due to the mental and physical stress being experienced. Real world training exercises have limitations such as safety, cost, time and difficulty in reproducing events, making frequent training infeasible. Virtual Reality (VR) offers new training opportunities, but it is challenging to create a virtual environment with the analogous level of stress experienced in the real-world. In this paper, we investigate the use of a multi-user, collaborative, multi-sensory (vision, audio, tactile) VR system to produce a realistic training environment for practising aerial firefighting training scenarios. We focus on a comparison between our VR training system, an equivalent real-world field training and an existing radio-only exercise currently in use, where we compare Heart-Rate Variability (HRV) and self reported stress using the Short Stress State Questionnaire (SSSQ). We conducted the study with real trainee AAS firefighters to determine the effectiveness of the system. Our results show that there were no significant differences between the VR training exercise and the real-world exercise in terms of the level of stress, measured by HRV, and no significant difference between VR and radio-only exercises, as reported by the SSSQ.},
  keywords={Training;Stress;Stress measurement;Decision making;Helicopters;Task analysis;Heart rate;Virtual Reality;Training;Assessment;User Centered Design;Situated Learning;Aerial Firefighting;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797889},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797854,
  author={Taupiac, Jean-Daniel and Rodriguez, Nancy and Strauss, Olivier and Rabier, Martin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Ad-hoc Study on Soldiers Calibration Procedure in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={190-199},
  abstract={French Army infantrymen's are equipped today with a combat system called FELIN, which includes an infrared sighting device: the IR sight. One of the first manipulations learned by the soldier is the IR sight calibration. Currently, calibration training is a two-step process. The first step consists of practicing on a 2D WIMP software until making no mistakes. Then, the soldiers can apply his knowledge in the real situation on the shooting range. In this paper, we present an ad-hoc study of a learning method including a prototype in Virtual Reality for training on the FELIN IR sight calibration procedure. It has been experimented on real infantrymen learners in an infantry school. Results showed an attractive added value of Virtual Reality in this specific use case. It improved the learners' intrinsic motivation to repeat the training task as well as the learning efficiency. It also helped the training team to identify specific mistake types not detected by the traditional learning software.},
  keywords={Training;Virtual reality;Software;Calibration;Two dimensional displays;Prototypes;Task analysis},
  doi={10.1109/VR.2019.8797854},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797978,
  author={Cordeil, Maxime and Cunningham, Andrew and Bach, Benjamin and Hurter, Christophe and Thomas, Bruce H. and Marriott, Kim and Dwyer, Tim},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={IATK: An Immersive Analytics Toolkit}, 
  year={2019},
  volume={},
  number={},
  pages={200-209},
  abstract={We introduce IATK, the Immersive Analytics Toolkit, a software package for Unity that allows interactive authoring and exploration of data visualisation in immersive environments. The design of IATK was informed by interdisciplinary expert-collaborations as well as visual analytics applications and iterative refinement over several years. IATK allows for easy assembly of visualisations through a grammar of graphics that a user can configure in a GUI-in addition to a dedicated visualisation API that supports the creation of novel immersive visualisation designs and interactions. IATK is designed with scalability in mind, allowing visualisation and fluid responsive interactions in the order of several million points at a usable frame rate. This paper outlines our design requirements, IATK's framework design and technical features, its user interface, as well as application examples.},
  keywords={Data visualization;Grammar;Visualization;Three-dimensional displays;Geometry;Two dimensional displays;Human-centered computing—Visualization—Visualization techniques—;Human-centered computing—Visualization—visualisation design and evaluation methods},
  doi={10.1109/VR.2019.8797978},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797812,
  author={Ssin, Seung Youb and Walsh, James A. and Smith, Ross T. and Cunningham, Andrew and Thomas, Bruce H.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={GeoGate: Correlating Geo-Temporal Datasets Using an Augmented Reality Space-Time Cube and Tangible Interactions}, 
  year={2019},
  volume={},
  number={},
  pages={210-219},
  abstract={This paper introduces GeoGate, an Augmented Reality tabletop system that extends the Space-Time Cube and utilizes a ring-shaped tangible user interface to explore correlations between entities in multiple location datasets. We demonstrate GeoGate in the context of the maritime domain, where operators seek to find geo-temporal associations between trajectories recorded from a global positioning system, and light data extracted from night time satellite images. GeoGate utilizes a tabletop system displaying a traditional 2D map in conjunction with a Microsoft Hololens to present a single view of the data with a novel Augmented Reality extension of the Space-Time Cube. To validate GeoGate, we present the results of a user study comparing GeoGate with the existing 2D approach used in a normal desktop environment. The outcomes of the user study show that GeoGate's approach reduces mistakes in the interpretation of the correlations between various datasets, while the qualitative results show that such a system is preferable for the majority of geo-temporal maritime tasks compared.},
  keywords={Three-dimensional displays;Data visualization;Two dimensional displays;Trajectory;Complexity theory;Augmented reality;Uncertainty;Augmented Reality;Space Time Cube;Multivariate Network Visualization;Multiple Data-sets;Maritime Visualization;Geo-visualization;Tangible User Interface;Tabletop;H.1.2 [Information Systems]: User/Machine Systems—Human factors;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques},
  doi={10.1109/VR.2019.8797812},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797840,
  author={Dey, Arindam and Chatburn, Alex and Billinghurst, Mark},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploration of an EEG-Based Cognitively Adaptive Training System in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={220-226},
  abstract={Virtual Reality (VR) is effective in various training scenarios across multiple domains, such as education, health and defense. However, most of those applications are not adaptive to the real-time cognitive or subjectively experienced load placed on the trainee. In this paper, we explore a cognitively adaptive training system based on real-time measurement of task related alpha activity in the brain. This measurement was made by a 32-channel mobile Electroencephalography (EEG) system, and was used to adapt the task difficulty to an ideal level which challenged our participants, and thus theoretically induces the best level of performance gains as a result of training. Our system required participants to select target objects in VR and the complexity of the task adapted to the alpha activity in the brain. A total of 14 participants undertook our training and completed 20 levels of increasing complexity. Our study identified significant differences in brain activity in response to increasing levels of task complexity, but response time did not alter as a function of task difficulty. Collectively, we interpret this to indicate the brain's ability to compensate for higher task load without affecting behaviourally measured visuomotor performance.},
  keywords={Training;Electroencephalography;Task analysis;Adaptive systems;Real-time systems;Frequency measurement;Virtual reality;H.1.2 [Models and Principles]: User/Machine Systems—Human Factors;H.5.1 [Multimedia Information Systems]: Artificial—Augmented and Virtual Realities;Virtual Reality;Cognitively Adaptive Training;Electroencephalography;Alpha Activity},
  doi={10.1109/VR.2019.8797840},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797826,
  author={Peillard, Etienne and Thebaud, Thomas and Normand, Jean-Marie and Argelaguet, Ferran and Moreau, Guillaume and Lécuyer, Anatole},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Objects Look Farther on the Sides: The Anisotropy of Distance Perception in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={227-236},
  abstract={The topic of distance perception has been widely investigated in Virtual Reality (VR). However, the vast majority of previous work mainly focused on distance perception of objects placed in front of the observer. Then, what happens when the observer looks on the side? In this paper, we study differences in distance estimation when comparing objects placed in front of the observer with objects placed on his side. Through a series of four experiments (n=85), we assessed participants' distance estimation and ruled out potential biases. In particular, we considered the placement of visual stimuli in the field of view, users' exploration behavior as well as the presence of depth cues. For all experiments a two-alternative forced choice (2AFC) standardized psychophysical protocol was employed, in which the main task was to determine the stimuli that seemed to be the farthest one. In summary, our results showed that the orientation of virtual stimuli with respect to the user introduces a distance perception bias: objects placed on the sides are systematically perceived farther away than objects in front. In addition, we could observe that this bias increases along with the angle, and appears to be independent of both the position of the object in the field of view as well as the quality of the virtual scene. This work sheds a new light on one of the specificities of VR environments regarding the wider subject of visual space theory. Our study paves the way for future experiments evaluating the anisotropy of distance perception in real and virtual environments.},
  keywords={Visualization;Resists;Observers;Calibration;Anisotropic magnetoresistance;Virtual reality;Protocols;Perception;Distance;Virtual Reality;User Experiment;Psychophysical Study;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR.2019.8797826},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798095,
  author={Rosales, Carlos Salas and Pointon, Grant and Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah and Thompson, William B. and Bodenheimer, Bobby},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Distance Judgments to On- and Off-Ground Objects in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={237-243},
  abstract={Augmented reality (AR) technologies have the potential to provide individuals with unique training and visualizations, but the effectiveness of these applications may be influenced by users' perceptions of the distance to AR objects. Perceived distances to AR objects may be biased if these objects do not appear to make contact with the ground plane. The current work compared distance judgments of AR targets presented on the ground versus off the ground when no additional AR depth cues, such as shadows, were available to denote ground contact. We predicted that without additional information for height off the ground, observers would perceive the off-ground objects as placed on the ground, but at farther distances. Furthermore, this bias should be exaggerated when targets were viewed with one eye rather than two. In our experiment, participants judged the absolute egocentric distance to various cubes presented on or off the ground with an action-based measure, blind walking. We found that observers walked farther for off-ground AR objects and that this effect was exaggerated when participants viewed off-ground objects with monocular vision compared to binocular vision. However, we also found that the restriction of binocular cues influenced participants' distance judgments for on-ground AR objects. Our results suggest that distances to off-ground AR objects are perceived differently than on-ground AR objects and that the elimination of binocular cues further influences how users perceive these distances.},
  keywords={Augmented reality;Visualization;Legged locomotion;Observers;Virtual environments;Meters;Augmented reality;Virtual environments;distance perception;depth cues;I.3.7 [Computer Graphics]: Three—Dimensional Graphics and Realism—Virtual Reality;J.4 [Computer Applications]: Social and Behavioral Sciences—Psychology},
  doi={10.1109/VR.2019.8798095},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797911,
  author={Peer, Alex and Ponto, Kevin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mitigating Incorrect Perception of Distance in Virtual Reality through Personalized Rendering Manipulation}, 
  year={2019},
  volume={},
  number={},
  pages={244-250},
  abstract={Viewers of virtual reality appear to have an incorrect sense of space when performing blind directed-action tasks, such as blind walking or blind throwing. It has been shown that various manipulations can influence this incorrect sense of space, and that the degree of misperception varies by person. It follows that one could measure the degree of misperception an individual experiences and generate some manipulation to correct for it, though it is not clear that correct behavior in a specific blind directed action task leads to correct behavior in all tasks in general. In this work, we evaluate the effectiveness of correcting perceived distance in virtual reality by first measuring individual perceived distance through blind throwing, then manipulating sense of space using a vertex shader to make things appear more or less distant, to a degree personalized to the individual's perceived distance. Two variants of the manipulation are explored. The effects of these personalized manipulations are first evaluated when performing the same blind throwing task used to calibrate the manipulation. Then, in order to observe the effects of the manipulation on dissimilar tasks, participants perform two perceptual matching tasks which allow full visual feedback as objects, or the participants themselves, move through space.},
  keywords={Task analysis;Virtual environments;Measurement uncertainty;Atmospheric measurements;Particle measurements;Rendering (computer graphics);I.3.7 [Computing Methodologies]: Graphics Utilities—Virtual reality},
  doi={10.1109/VR.2019.8797911},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797923,
  author={Takezawa, Takuro and Iwai, Daisuke and Sato, Kosuke and Hara, Toshihiro and Takeda, Yusaku and Murase, Kenji},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Material Surface Reproduction and Perceptual Deformation with Projection Mapping for Car Interior Design}, 
  year={2019},
  volume={},
  number={},
  pages={251-258},
  abstract={Car interior design, such as dashboard, broadly consists of two parts. One is shape design, where the processes of 2D drawing, 3D modeling and evaluation with full-scale mockups are iterated, which takes a massive amount of time and cost. The other is material design such as surface property tuning, where designers compare material samples. This way, however, has a limitation on the number of material samples to compare and does not allow applying of the samples of interest to the whole mockups in early phases. In this paper, we apply projection mapping technique to boost the design process by altering the appearance of the surface of projected objects and enabling various shape and material evaluations in early phases. Our proposed system uses multiple projectors, one of which is 4K projector to reproduce fine leather surface. Utilizing physiological and psychological depth cues, the system allows the user to perceive the projected mockup as deformed. Psychological experiments confirm that users perceive deformation and have controlled impression on leather reproduced with certain parameters. In addition, we discuss the usability of the proposed system as a support system of car interior design.},
  keywords={Shape;Automobiles;Strain;Three-dimensional displays;Usability;Resists;Stereo image processing;Human-centered computing—Interaction design—Interaction design process and methods—Scenario-based design;Computing methodologies—Computer graphics—graphics systems and interfaces—Perception},
  doi={10.1109/VR.2019.8797923},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798154,
  author={Lee, Eun-Cheol and Cho, Yong-Hun and Lee, In-Kwon},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulating Water Resistance in a Virtual Underwater Experience Using a Visual Motion Delay Effect}, 
  year={2019},
  volume={},
  number={},
  pages={259-266},
  abstract={In this paper, we propose a new visual motion delay effect to enhance the presence of a user in a virtual underwater experience. To do this, we simulate the resistance in the underwater environment by delaying the hand and head movements of the user's avatar. The motion delay effect is implemented using two components: a drag force and a recovery force. The experimental results show that the combination of a drag force and a recovery force creates a realistic illusion of an underwater experience and enhances the user's presence, satisfaction, and immersion in the virtual underwater environment.},
  keywords={Force;Resistance;Drag;Visualization;Delay effects;Atmospheric modeling;Acceleration;Computer Graphics—Three-Dimensional Graphics and Realism—Virtual reality},
  doi={10.1109/VR.2019.8798154},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797925,
  author={Zhao, Yajie and Xu, Qingguo and Chen, Weikai and Du, Chao and Xing, Jun and Huang, Xinyu and Yang, Ruigang},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mask-off: Synthesizing Face Images in the Presence of Head-mounted Displays}, 
  year={2019},
  volume={},
  number={},
  pages={267-276},
  abstract={Wearable VR/AR devices provide users with fully immersive experience in a virtual environment, enabling possibilities to reshape the forms of entertainment and telepresence. While the body language is a crucial element in effective communication, wearing a head-mounted display (HMD) could severely hinder the eye contact and block facial expressions. We present a novel headset removal technique that enables high-quality occlusion-free communication in virtual environment. In particular, our solution synthesizes photoreal faces in the occluded region with faithful reconstruction of facial expressions and eye movements. Towards this goal, we develop a novel capture setup that consists of two near-infrared (NIR) cameras inside the HMD for eye capturing and one external RGB camera for recording visible face regions. To enable realistic face synthesis with consistent illuminations, we propose a data-driven approach to fuse the narrow-field-of-view NIR images with the RGB image captured from the external camera. In addition, to generate pho-torealistic eyes, a dedicated algorithm is proposed to colorize the NIR eye images and further rectify the color distortion caused by the non-linear mapping of IR light sensitivity. Experimental results demonstrate that our framework is capable to synthesize high-fidelity unoccluded facial images with accurate tracking of head motion, facial expression and eye movement.},
  keywords={Face;Cameras;Headphones;Three-dimensional displays;Image reconstruction;Resists;Tracking;AR/VR;headset removal;face inpainting;eye synthesis},
  doi={10.1109/VR.2019.8797925},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797862,
  author={George, Ceenu and Khamis, Mohamed and Buschek, Daniel and Hussmann, Heinrich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Third Dimension for Authentication in Immersive Virtual Reality and in the Real World}, 
  year={2019},
  volume={},
  number={},
  pages={277-285},
  abstract={Immersive Virtual Reality (IVR) is a growing 3D environment, where social and commercial applications will require user authentication. Similarly, smart homes in the real world (RW), offer an opportunity to authenticate in the third dimension. For both environments, there is a gap in understanding which elements of the third dimension can be leveraged to improve usability and security of authentication. In particular, investigating transferability of findings between these environments would help towards understanding how rapid prototyping of authentication concepts can be achieved in this context. We identify key elements from prior research that are promising for authentication in the third dimension. Based on these, we propose a concept in which users' authenticate by selecting a series of 3D objects in a room using a pointer. We created a virtual 3D replica of a real world room, which we leverage to evaluate and compare the factors that impact the usability and security of authentication in IVR and RW. In particular, we investigate the influence of randomized user and object positions, in a series of user studies (N=48). We also evaluate shoulder surfing by real world bystanders for IVR (N=75). Our results show that 3D passwords within our concept are resistant against shoulder surfing attacks. Interactions are faster in RW compared to IVR, yet workload is comparable.},
  keywords={Authentication;Three-dimensional displays;Password;Virtual reality;Resists;Usability;Human-centered computing—User studies;Human-centered computing—Virtual reality},
  doi={10.1109/VR.2019.8797862},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798112,
  author={de Moura, Douglas Yamashita and Sadagic, Amela},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effects of Stereopsis and Immersion on Bimanual Assembly Tasks in a Virtual Reality System}, 
  year={2019},
  volume={},
  number={},
  pages={286-294},
  abstract={Assembly tasks are an essential component in complex operations done by a large number of humans on a regular basis; examples include system maintenance (preventive and corrective), industrial production lines, and teleoperation. Having access to superior and low-cost solutions that can be used to train personnel who need to conduct these tasks is essential. Virtual reality (VR) technology, with its immersive and non-immersive display solutions combined with hand controllers suitable for bimanual operations, is especially appealing for training purposes in this domain. We designed and executed a user study in which we tested the influence of stereopsis and immersion on execution of bimanual assembly task and examined the effects of tested system configurations on symptoms of cybersickness. Our user study, with its between-subjects format, collected comprehensive data sets in four distinct experimental conditions: immersive stereoscopic (IS), immersive non-stereoscopic (INS), non-immersive stereoscopic (NIS), and non-immersive non-stereoscopic (NINS). The results of this study suggest that IS platforms are the most promising contenders for an efficient system solution, and that NINS solutions that use larger screens (like a TV set, in our case) may also be considered. It is encouraging that no significant simulator sickness issues were recorded in any condition. The results of this study provide important input and guidance that people who work in the training domain need to have before making decisions about the acquisition of new solutions for assembly task training.},
  keywords={Training;Task analysis;Maintenance engineering;Software;Virtual reality;Stereo image processing;TV;Bimanual assembly task;human performance;immersive VR;non-immersive VR;usability;cybersickness;H.5.1 [Information Interfaces & Presentations]: Multimedia Information Systems - Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798112},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798179,
  author={Prasolova-Førland, Ekaterina and Fominykh, Mikhail and Ekelund, Oscar Ihlen},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Empowering Young Job Seekers with Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={295-302},
  abstract={This paper presents the results of the Virtual Internship project that aims to help young job seekers get insights of different workplaces via immersive and interactive experiences. We designed a concept of `Immersive Job Taste' that provides a rich presentation of occupations with elements of workplace training, targeting a specific group of young job seekers, including high-school students and unemployed. We developed several scenarios and applied different virtual and augmented reality concepts to build prototypes for different types of devices. The intermediary and the final versions of the prototypes were evaluated by several groups of primary users and experts, including over 70 young job seekers and high school students and over 45 various professionals and experts. The data were collected using questionnaires and interviews. The results indicate a generally very positive attitude towards the concept of immersive job taste, although with significant differences between job seekers and experts. The prototype developed for room-scale virtual reality with controllers was generally evaluated better than those including cardboard with 360 videos or with animated 3D graphics and augmented reality glasses. In the paper, we discuss several aspects, such as the potential of immersive technologies for career guidance, fighting youth unemployment by better informing the young job seekers, and various practical and technology considerations.},
  keywords={Task analysis;Employment;Interviews;Fish;Training;Three-dimensional displays;Videos;Virtual Reality;Career guidance;unemployment},
  doi={10.1109/VR.2019.8798179},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798181,
  author={Galvane, Quentin and Lin, I-Sheng and Argelaguet, Fernando and Li, Tsai-Yen and Christie, Marc},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR as a Content Creation Tool for Movie Previsualisation}, 
  year={2019},
  volume={},
  number={},
  pages={303-311},
  abstract={Creatives in animation and film productions have forever been exploring the use of new means to prototype their visual sequences before realizing them, by relying on hand-drawn storyboards, physical mockups or more recently 3D modelling and animation tools. However these 3D tools are designed in mind for dedicated animators rather than creatives such as film directors or directors of photography and remain complex to control and master. In this paper we propose a VR authoring system which provides intuitive ways of crafting visual sequences, both for expert animators and expert creatives in the animation and film industry. The proposed system is designed to reflect the traditional process through (i) a storyboarding mode that enables rapid creation of annotated still images, (ii) a previsualisation mode that enables the animation of the characters, objects and cameras, and (iii) a technical mode that enables the placement and animation of complex camera rigs (such as cameras cranes) and light rigs. Our methodology strongly relies on the benefits of VR manipulations to re-think how content creation can be performed in this specific context, typically how to animate contents in space and time. As a result, the proposed system is complimentary to existing tools, and provides a seamless back-and-forth process between all stages of previsualisation. We evaluated the tool with professional users to gather experts' perspectives on the specific benefits of VR in 3D content creation.},
  keywords={Three-dimensional displays;Cameras;Tools;Animation;Motion pictures;Layout;Visualization;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798181},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797921,
  author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, 
  year={2019},
  volume={},
  number={},
  pages={312-320},
  abstract={We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm3) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.},
  keywords={Actuators;Haptic interfaces;Rendering (computer graphics);Skin;Electromagnetics;Force;Tactile sensors;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Haptic devices;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8797921},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798205,
  author={de Tinguy, Xavier and Pacchierotti, Claudio and Marchal, Maud and Lécuyer, Anatole},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward Universal Tangible Objects: Optimizing Haptic Pinching Sensations in 3D Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={321-330},
  abstract={Tangible objects are a simple yet effective way for providing haptic sensations in Virtual Reality. For achieving a compelling illusion, there should be a good correspondence between what users see in the virtual environment and what they touch in the real world. The haptic features of the tangible object should indeed match those of the corresponding virtual one in terms of, e.g., size, local shape, mass, texture. A straightforward solution is to create perfect tangible replicas of all the virtual objects in the scene. However, this is often neither feasible nor desirable. This paper presents an innovative approach enabling the use of few tangible objects to render many virtual ones. The proposed algorithm analyzes the available tangible and virtual objects to find the best grasps in terms of matching haptic sensations. It starts by identifying several suitable pinching poses on the considered tangible and virtual objects. Then, for each pose, it evaluates a series of haptically-salient characteristics. Next, it identifies the two most similar pinching poses according to these metrics, one on the tangible and one on the virtual object. Finally, it highlights the chosen pinching pose, which provides the best matching sensation between what users see and touch. The effectiveness of our approach is evaluated through a user study. Results show that the algorithm is able to well combine several haptically-salient object features to find convincing pinches between the given tangible and virtual objects.},
  keywords={Shape;Grasping;Three-dimensional displays;Virtual environments;Object recognition;Human-centered computing—Human computer interaction—Interaction devices—Haptic devices},
  doi={10.1109/VR.2019.8798205},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798255,
  author={Wang, Chiu-Hsuan and Hsieh, Chen-Yuan and Yu, Neng-Hao and Bianchi, Andrea and Chan, Liwei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HapticSphere: Physical Support To Enable Precision Touch Interaction in Mobile Mixed-Reality}, 
  year={2019},
  volume={},
  number={},
  pages={331-339},
  abstract={This work presents HapticSphere, a wearable spherical surface enabled by bridging a finger and the head-mounted display (HMD) with a passive string. Users perceive a physical support on a finger attached to a string, when extending their arm and reaching out to the string's maximum extension. This physical support assists users in precise touch interaction in the context of stationary and walking virtual or mixed-reality experiences. We propose three methods of attachment of the haptic string (directly on the head or on the body), and illustrate a novel single-step calibration algorithm that supports these configurations by estimating a grand haptic sphere, once a head-coordinated touch interaction is established. Two user studies were conducted to validate our approach and to compare the touch performance with physical support in sitting and walking conditions in the context of mobile mixed-reality scenarios. The results show that, in the walking condition, touch interaction with physical support significantly outperformed the visual-only condition.},
  keywords={Resists;Force;Virtual reality;Force feedback;Neck;Legged locomotion;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798255},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798036,
  author={Valkov, Dimitar and Linsen, Lars},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Vibro-tactile Feedback for Real-world Awareness in Immersive Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={340-349},
  abstract={In immersive virtual environments (IVE), users' visual and auditory perception is replaced by computer-generated stimuli. Thus, knowing the positions of real objects is crucial for physical safety. While some solutions exist, e. g., using virtual replicas or visible cues indicating the interaction space boundaries, these are limiting the IVE design or depend on the hardware setup. Moreover, most solutions cannot handle lost tracking, erroneous tracker calibration, or moving obstacles. However, these are common scenarios especially for the increasingly popular home virtual reality settings. In this paper, we present a stand-alone hardware device designed to alert IVE users for potential collisions with real-world objects. It uses distance sensors mounted on a head-mounted display (HMD) and vibro-tactile actuators inserted into the HMD's face cushion. We implemented different types of sensor-actuator mappings with the goal to find a mapping function that is minimally obtrusive in normal use, but efficiently alerting in risk situations.},
  keywords={Sensors;Vibrations;Actuators;Haptic interfaces;Hardware;Resists;Virtual environments;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Haptic devices},
  doi={10.1109/VR.2019.8798036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798240,
  author={Tran, Kien T.P. and Jung, Sungchul and Hoermann, Simon and Lindeman, Robert W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={MDI: A Multi-channel Dynamic Immersion Headset for Seamless Switching between Virtual and Real World Activities}, 
  year={2019},
  volume={},
  number={},
  pages={350-358},
  abstract={We present a study of the usability and induced workload of a new head-mounted display (HMD) which allows the wearer to more easily switch between tasks in the virtual and real world using multiple optical channels dynamically at the press of a button. Most HMDs provide full immersion by occluding the real world from the view of the user, and replacing it with a computer-generated virtual environment. One trade-off, however, is that such HMDs make it difficult to interact with real-world objects and people, or to react to events while being immersed. Tasks that are simple in the unmediated real world, such as taking a drink or responding to a text, become nearly impossible while wearing an occlusive HMD, and normally require the user to take off the HMD. To address this limitation, we propose an HMD which provides optical channels to view the near-field real world in the periphery of the HMD while still wearing the HMD by pressing a button. We call our approach Multi-channel Dynamic Immersion (MDI). We embedded transparency controllable LCD panels around the periphery of an HMD and installed a fish-eye lens in front of its forward-facing camera. We then compared MDI to three other methods of switching from working in VR to real world activities: (a) a front camera video-see-through method, (b) using dynamic LCDs only, and (c) taking off the HMD. Participants had to interrupt their work in a VR office to carry out typical real world daily activities, such as picking up a mug, responding to a phone call, or replying to a text message. Our results show no significant differences in task execution time between the four conditions. However, we observed a tendency favouring MDI as the easiest to use among the HMDs. In addition, we found a significantly higher rated ease of use for the two HMDs with controllable LCDs compared to the two occlusive HMDs. Our data also underscores that a substantial amount of time is lost when switching between work contexts in occlusive HMDs and the real world.},
  keywords={Resists;Optical switches;Task analysis;Visualization;Liquid crystal displays;Cameras;MDI VR HMD;Multi-channel dynamic immersion HMD;Comfort VR session;Virtual Reality and Real-world interaction;Real objects interaction from VR;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Prototyping;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology;H.5.2 [User Interfaces Interfaces and Presentation]: User Interfaces—User-centered design;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality},
  doi={10.1109/VR.2019.8798240},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798074,
  author={Cheng, Lung-Pan and Ofek, Eyal and Holz, Christian and Wilson, Andrew D.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VRoamer: Generating On-The-Fly VR Experiences While Walking inside Large, Unknown Real-World Building Environments}, 
  year={2019},
  volume={},
  number={},
  pages={359-366},
  abstract={Procedural generation in virtual reality (VR) has been used to adapt the virtual world to various indoor environments, fitting different geometries and interiors with virtual environments. However, such applications require that the physical environment be known or pre-scanned prior to use to then generate the corresponding virtual scene, thus restricting the virtual experience to a controlled space. In this paper, we present VRoamer, which enables users to walk unseen physical spaces for which VRoamer procedurally generates a virtual scene on-the-fly. Scaling to the size of office buildings, VRoamer extracts walkable areas and detects physical obstacles in real time, instantiates pre-authored virtual rooms if their sizes fit physically walkable areas or otherwise generates virtual corridors and doors that lead to undiscovered physical areas. The use of these virtual structures allows VRoamer to (1) temporarily block users' passage, thus slowing them down while increasing VRoamer's insight into newly discovered physical areas, (2) prevent users from seeing changes beyond the current virtual scene, and (3) obfuscate the appearance of physical environments. VRoamer animates virtual objects to reflect dynamically discovered changes of the physical environment, such as people walking by or obstacles that become apparent. In our proof-of-concept study, participants were able to walk long distances through a procedurally generated dungeon experience and reported high levels of immersion.},
  keywords={Legged locomotion;Geometry;Virtual environments;Aerospace electronics;Real-time systems;Cameras;Virtual reality;procedural generation;real walking;locomotion techniques;redirected walking;H.5.1 [Information Interfaces and Presentation]: Multimedia, Information Systems-Virtual Realities},
  doi={10.1109/VR.2019.8798074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797751,
  author={Hanson, Sara and Paris, Richard A. and Adams, Haley A. and Bodenheimer, Bobby},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Improving Walking in Place Methods with Individualization and Deep Networks}, 
  year={2019},
  volume={},
  number={},
  pages={367-376},
  abstract={Walking in place is a standard method for moving through large virtual environments when physical space or positional tracking is limited. This technique has become increasingly prominent with the advent of mobile virtual reality in which external tracking may not be present. In this paper, we revisit walking in place algorithms to address some of their technical challenges. Namely, our solutions attend to improving starting, stopping, and speed control for individual users. From a hand-tuned threshold based algorithm, we provide a new, fast method for individualizing the walking in place algorithm based on biomechanic measures of step rate. In addition, we introduce a new walking in place model based on a convolutional neural network trained to differentiate walking and standing. Over two experiments we assess these methods against a traditional threshold based algorithm on two mobile virtual reality platforms. The assessments are based on controllability, scale, and presence. Our results suggest that an adequately trained convolutional neural network can be an effective way of implementing walking in place.},
  keywords={Legged locomotion;Virtual environments;Tracking;Acceleration;Gears;Neural networks;Magnetic heads;Virtual environments;locomotion;walking in place;convolutional neural network;perception;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;J.4 [Computer Applications]: Social and Behavioral Sciences—Psychology},
  doi={10.1109/VR.2019.8797751},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797994,
  author={Stebbins, Travis and Ragan, Eric D.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirecting View Rotation in Immersive Movies with Washout Filters}, 
  year={2019},
  volume={},
  number={},
  pages={377-385},
  abstract={Immersive movies take advantage of virtual reality (VR) to bring new opportunities for storytelling that allow users to naturally turn their heads and bodies to view a 3D virtual world and follow the story in a surrounding space. However, while many designers often assume scenarios where viewers stand and are free to physically turn without constraints, this excludes many commonly desired usage settings where the user may wish to remain seated, such as the use of VR while relaxing on the couch or passing the time during a flight. For such situations, large amounts of physical turning may be uncomfortable due to neck strain or awkward twisting. Our research investigates a technique that automatically rotates the virtual scene to help redirect the viewer's physical rotation while viewing immersive narrative experiences. By slowly rotating the virtual content, viewers are encouraged to gradually turn physically to align their head positions to a more comfortable straight-ahead viewing direction in seated situations where physical turning is not ideal. We present our study of technique design and an evaluation of how the redirection approach affects user comfort, sickness, the amount of physical rotation, and likelihood of viewers noticing the rotational adjustments. Evaluation results show the rotation technique was effective at significantly reducing the amount of physical turning while watching immersive videos, and only 39% of participants noticed the automated rotation when the technique rotated at a speed of 3 degrees per second.},
  keywords={Videos;Motion pictures;Turning;Head;Legged locomotion;Virtual reality;Three-dimensional displays;Human-centered computing—Visualization—Visualization techniques—Virtual reality},
  doi={10.1109/VR.2019.8797994},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797989,
  author={Hayashi, Daigo and Fujita, Kazuyuki and Takashima, Kazuki and Lindeman, Robert W. and Kitamura, Yoshifumi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirected Jumping: Imperceptibly Manipulating Jump Motions in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={386-394},
  abstract={Jumping is a fundamental movement in our daily lives that is often used in many video games. However, little research has been done on jumping and its possible use as a redirection technique in virtual reality (VR). In this study we explore Redirected Jumping, a novel redirection technique which enables us to purposefully manipulate the mapping of the user's physical jumping movements (e.g., distance and direction) to movement in the virtual space, allowing richer and more active physical VR experiences within a limited tracking area. To demonstrate the possibilities afforded by Redirected Jumping, we implemented a jump detection algorithm and jumping redirection methods for three basic jumping actions (i.e., horizontal, vertical, and rotational jumps) using common VR devices. We conducted three user studies to investigate the effective manipulation ranges, and the results revealed that our methods can manipulate a user's jumping movements without his/her noticing, similar to walking.},
  keywords={Legged locomotion;Virtual reality;Meters;Tracking;Games;Sports;Foot;Virtual reality;virtual locomotion;redirected walking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality},
  doi={10.1109/VR.2019.8797989},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798286,
  author={Rewkowski, Nicholas and Rungta, Atul and Whitton, Mary and Lin, Ming},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Effectiveness of Redirected Walking with Auditory Distractors for Navigation in Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={395-404},
  abstract={Many virtual locomotion interfaces allowing users to move in virtual reality have been built and evaluated, such as redirected walking (RDW), walking-in-place (WIP), and joystick input. RDW has been shown to be among the most natural and immersive as it supports real walking, and many newer methods further adapt RDW to allow for customization and greater immersion. Most of these methods have been demonstrated to work with vision, in this paper we evaluate the ability for a general distractor-based RDW framework to be used with only auditory display. We conducted two studies evaluating the differences between RDW with auditory distractors and other distractor modalities using distraction ratio, virtual and physical path information, immersion, simulator sickness, and other measurements. Our results indicate that auditory RDW has the potential to be used with complex navigational tasks, such as crossing streets and avoiding obstacles. It can be used without designing the system specifically for audio-only users. Additionally, sense of presence and simulator sickness remain reasonable across all user groups.},
  keywords={Legged locomotion;Distortion;Navigation;Task analysis;Visualization;Virtual environments;Dogs;virtual locomotion—redirected walking—distractors},
  doi={10.1109/VR.2019.8798286},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798016,
  author={Lai, Po Kong and Xie, Shuang and Lang, Jochen and Laganière, Robert},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Panoramic Depth Maps from Omni-directional Stereo Images for 6 DoF Videos in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={405-412},
  abstract={In this paper we present an approach for 6 DoF panoramic videos from omni-directional stereo (ODS) images using convolutional neural networks (CNNs). More specifically, we use CNNs to generate panoramic depth maps from ODS images in real-time. These depth maps would then allow for re-projection of panoramic images thus providing 6 DoF to a viewer in virtual reality (VR). As the boundaries of a panoramic image must touch in order to envelope a viewer, we introduce a border weighted loss function as well as new error metrics specifically tailored for panoramic images. We show experimentally that training with our border weighted loss function improves performance by benchmarking a baseline skip-connected encoder-decoder style network as well as other state-of-the-art methods in depth map estimation from mono and stereo images. Finally, a practical application for VR using real world data is also demonstrated.},
  keywords={Cameras;Videos;Estimation;Real-time systems;Virtual reality;Training;Measurement;Computing methodologies—Virtual reality;Computer systems organization—Neural networks;Computing methodologies—Reconstruction},
  doi={10.1109/VR.2019.8798016},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797777,
  author={Mirhosseini, Seyedkoosha and Ghahremani, Parmida and Ojal, Sushant and Marino, Joseph and Kaufman, Arie},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploration of Large Omnidirectional Images in Immersive Environments}, 
  year={2019},
  volume={},
  number={},
  pages={413-422},
  abstract={Navigation is a major challenge in exploring data within immersive environments, especially of large omnidirectional spherical images. We propose a method of auto-scaling to allow users to navigate using teleportation within the safe boundary of their physical environment with different levels of focus. Our method combines physical navigation with virtual teleportation. We also propose a “peek then warp” behavior when using a zoom lens and evaluate our system in conjunction with different teleportation transitions, including a proposed transition for exploration of omnidirectional and 360-degree panoramic imagery, termed Envelop, wherein the destination view expands out from the zoom lens to completely envelop the user. In this work, we focus on visualizing and navigating large omnidirectional or panoramic images with application to GIS visualization as an inside-out omnidirectional image of the earth. We conducted two user studies to evaluate our techniques over a search and comparison task. Our results illustrate the advantages of our techniques for navigation and exploration of omnidirectional images in an immersive environment.},
  keywords={Navigation;Teleportation;Cameras;Data visualization;Lenses;Earth;Virtual environments;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Visualization—Visualization application domains—Geographic visualization},
  doi={10.1109/VR.2019.8797777},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797843,
  author={Keskinen, Tuuli and Mäkelä, Ville and Kallioniemi, Pekka and Hakulinen, Jaakko and Karhu, Jussi and Ronkainen, Kimmo and Mäkelä, John and Turunen, Markku},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Camera Height, Actor Behavior, and Viewer Position on the User Experience of 360° Videos}, 
  year={2019},
  volume={},
  number={},
  pages={423-430},
  abstract={360° videos can be viewed in an immersive manner with a head-mounted display (HMD). However, it is unclear how the viewing experience is affected by basic properties of 360° videos, such as how high they are recorded from, and whether there are people close to the camera. We conducted a 24-participant user study where we explored whether the viewing experience is affected by A) camera height, B) the proximity and actions of people appearing in the videos, and C) viewer position (standing/sitting). The results, surprisingly, suggest that the viewer's own height has little to no effect on the preferred camera height and the experience. The most optimal camera height situates at around 150 centimeters, which hits the comfortable height range for both sitting and standing viewers. Moreover, in some cases, people being close to the camera, or the camera being very low, has a negative effect on the experience. Our work contributes to understanding and designing immersive 360° experiences.},
  keywords={360° videos;camera height;omnidirectional videos;head-mounted displays;virtual environments;virtual reality;user experience;viewer height;viewer position;Human-centered computing—Virtual reality},
  doi={10.1109/VR.2019.8797843},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797876,
  author={Ikei, Yasushi and Yem, Vibol and Tashiro, Kento and Fujie, Toi and Amemiya, Tomohiro and Kitazaki, Michiteru},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Live Stereoscopic 3D Image with Constant Capture Direction of 360° Cameras for High-Quality Visual Telepresence}, 
  year={2019},
  volume={},
  number={},
  pages={431-439},
  abstract={To capture a remote 3D image, conventional stereo cameras attached to a robot head have been commonly used. However, when the head and cameras rotate, the captured image in buffers is degraded by latency and motion blur, which may cause VR sickness. In the present study, we propose a method named TwinCam in which we use two 360° cameras spaced at the standard interpupillary distance and keep the direction of the lens constant in the world coordinate even when the camera bodies are rotated to reflect the orientation of the observer's head and the position of the eyes. We consider that this method can suppress the image buffer size to send to the observer because each camera captures the omnidirectional image without lens rotation. This paper introduces the mechanical design of our camera system and its potential for visual telepresence through three experiments. Experiment 1 confirmed the requirement of a stereoscopic rather than monoscopic camera for highly accurate depth perception, and Experiments 2 and 3 proved that our mechanical camera setup can reduce motion blur and VR sickness.},
  keywords={Cameras;Streaming media;Head;Lenses;Robot vision systems;Resists;Stereo image processing;Stereoscopic 360° image;motion blur;latency;VR sickness;telepresence;Human-centered computing—Displays and imagers;Virtual reality;Computing methodologies—3D imaging},
  doi={10.1109/VR.2019.8797876},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798261,
  author={Tang, Jintao and Zhang, Xinyu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hybrid Projection For Encoding 360 VR Videos}, 
  year={2019},
  volume={},
  number={},
  pages={440-447},
  abstract={During the past five years, tons of economic 360 VR cameras (e.g., Ricoh Theta, Samsumg Gear360, LG 360, Insta 360) are sold in the market. While 360 VR videos become ubiquitous very soon, 360 VR video standardization is still under discussion in the digital industry, and more concrete efforts are desired to accelerate its standardization and applications. Though ERP has been widely used for projection and packing layout while encoding 360 VR videos, it has severe projection distortion near poles. In this paper, we introduce a new format for encoding and storing 360 VR videos using hybrid cylindrical projection after thoroughly analyzing the problems with ERP. We show that our new hybrid format can minimize stretching distortion and generate well balanced pixel distribution in the resulting projection.},
  keywords={Videos;Layout;Distortion;Encoding;Cameras;Standardization;Industries;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality},
  doi={10.1109/VR.2019.8798261},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798177,
  author={Rummukainen, Olli S. and Schlecht, Sebastian J. and Robotham, Thomas and Plinge, Axel and Habets, Emanuël A.P.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perceptual Study of Near-Field Binaural Audio Rendering in Six-Degrees-of-Freedom Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={448-454},
  abstract={Auditory localization cues in the near-field are significantly different than in the far-field. The near-field region is within an arm's length of the listener allowing to integrate proprioceptive cues to determine the location of an object in space. This perceptual study compares three non-individualized methods to apply head-related transfer functions (HRTFs) in six-degrees-of-freedom near-field audio rendering, namely, far-field measured HRTFs, multi-distance measured HRTFs, and spherical-model-based HRTFs with near-field extrapolation. To set our findings in context, we provide a real-world hand-held audio source for comparison along with a distance-invariant condition. Two modes of interaction are compared in an audio-visual virtual reality: one allowing the participant to move the audio object dynamically and the other with a stationary audio object but a freely moving listener.},
  keywords={Rendering (computer graphics);Loudspeakers;Virtual reality;Visualization;Solid modeling;Engines;Acoustic measurements;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual Reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR.2019.8798177},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797914,
  author={Rungta, Atul and Rewkowski, Nicholas and Klatzky, Roberta and Manocha, Dinesh},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={P-Reverb: Perceptual Characterization of Early and Late Reflections for Auditory Displays}, 
  year={2019},
  volume={},
  number={},
  pages={455-463},
  abstract={We introduce a novel, perceptually derived metric (P - Reverb) that relates the just-noticeable difference (JND) of the early sound field (also called early reflections) to the late sound field (known as late reflections or reverberation). Early and late reflections are crucial components of the sound field and provide multiple perceptual cues for auditory displays. We conduct two extensive user evaluations that relate the JNDs of early reflections and late reverberation in terms of the mean-free path of the environment and present a novel P - Reverb metric. Our metric is used to estimate dynamic reverberation characteristics efficiently in terms of important parameters like reverberation time (RT60). We show the numerical accuracy of our P - Reverb metric in estimating RT60. Finally, we use our metric to design an interactive sound propagation algorithm and demonstrate its effectiveness on various benchmarks.},
  keywords={Reverberation;Measurement;Erbium;Rendering (computer graphics);Ray tracing;Auditory displays},
  doi={10.1109/VR.2019.8797914},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797816,
  author={Alghofaili, Rawan and Solah, Michael S and Huang, Haikun and Sawahata, Yasuhito and Pomplun, Marc and Yu, Lap-Fai},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimizing Visual Element Placement via Visual Attention Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={464-473},
  abstract={Eye-tracking enables researchers to conduct complex analysis on human behavior. With the recent introduction of eye-tracking into consumer-grade virtual reality headsets, the barrier of entry to visual attention analysis in virtual environments has been lowered significantly. Whether for arranging artwork in a virtual museum, posting banners for virtual events or placing advertisements in virtual worlds, analyzing visual attention patterns provides a powerful means for guiding visual element placement. In this work, we propose a novel data-driven optimization approach for automatically analyzing visual attention and placing visual elements in 3D virtual environments. Using an eye-tracking virtual reality headset, we collect eye-tracking data which we use to train a regression model for predicting gaze duration. We then use the predicted gaze duration output of our regressors to optimize the placement of visual elements with respect to certain visual attention and design goals. Through experiments in several virtual environments, we demonstrate the effectiveness of our optimization approach for predicting gaze duration and for placing visual elements in different practical scenarios. Our approach is implemented as a useful plug-in that level designers can use to automatically populate visual elements in 3D virtual environments.},
  keywords={Visualization;Three-dimensional displays;Virtual environments;Headphones;Solid modeling;Public transportation;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual Reality},
  doi={10.1109/VR.2019.8797816},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797988,
  author={Mardanbegi, Diako and Mayer, Benedikt and Pfeuffer, Ken and Jalaliniya, Shahram and Gellersen, Hans and Perzl, Alexander},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={EyeSeeThrough: Unifying Tool Selection and Application in Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={474-483},
  abstract={In 2D interfaces, actions are often represented by fixed tools arranged in menus, palettes, or dedicated parts of a screen, whereas 3D interfaces afford their arrangement at different depths relative to the user and the user can move them relative to each other. In this paper, we introduce EyeSeeThrough as a novel interaction technique that utilizes eye-tracking in VR. The user can apply an action to an intended object by visually aligning the object with the tool at the line-of-sight, and then issue a confirmation command. The underlying idea is to merge the two-step process of 1) selection of a mode in a menu and 2) applying it to a target, into one unified interaction. We present a user study where we compare the method to the baseline two-step selection. The results of our user study showed that our technique outperforms the two step selection in terms of speed and comfort. We further developed a prototype of a virtual living room to demonstrate the practicality of the proposed technique.},
  keywords={Tools;Visualization;Three-dimensional displays;Task analysis;User interfaces;Space exploration;Virtual environments;Human-centered computing—Human-centered-computing—Gestural input},
  doi={10.1109/VR.2019.8797988},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798257,
  author={Willis, Dion and Powell, Wendy and Powell, Vaughan and Stevens, Brett},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Stimulus Disrupts the Spatial Localization of a Tactile Sensation in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={484-491},
  abstract={Phantom limb pain is a neuropathic condition in which a person feels pain in a limb that is not present. Cognitive treatments that visually recreate the limb in an attempt to create a cross modal interaction between vision, and touch/proprioception have shown to be effective at alleviating this pain. With improvements in technology, Virtual Mirror Therapy is starting to gain favor, however, there are currently no applications that utilize passive touch in the same way non-virtual reality applications do. This paper investigates whether a visual stimulus can relocate a tactile stimulus to a different location using principles from the rubber hand illusion and mirror therapy. We demonstrate that a displaced visual stimulus in virtual reality can disrupt accurate spatial perception of a physical vibrotactile sensation however the effects are small and require further investigation.},
  keywords={Mirrors;Medical treatment;Pain;Vibrations;Visualization;Phantoms;Virtual reality;Virtual mirror therapy;Rubber hand Illusion;Vibrotactile;Visuotactile interactions;cross modal interactions;Virtual reality;phantom limb pain;Human-centered computing~Virtual reality;Human-centered computing~Haptic devices;Human-centered computing~Empirical studies in HCI;Human-centered computing~Scenario-based design;Human-centered computing~Contextual design;Human-centered computing~Interface design prototyping},
  doi={10.1109/VR.2019.8798257},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797997,
  author={Hurd, Ocean and Kurniawan, Sri and Teodorescu, Mircea},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Video Game Paired with Physical Monocular Blurring as Accessible Therapy for Amblyopia}, 
  year={2019},
  volume={},
  number={},
  pages={492-499},
  abstract={This paper discusses a virtual reality (VR) therapeutic video game for treatment of the neurological eye disorder, Amblyopia. Amblyopia is often referred to as lazy eye, and it entails weaker vision in one eye due to a poor connection between the eye and the brain. Until recently it was thought to be untreatable in adults, but new research has proven that with consistent therapy even adults can improve their Amblyopia, especially through perceptual learning and video games. Even so, therapy compliance remains low due to the fact that conventional therapies are perceived as either invasive, dull and/or boring. Our game aims to make Amblyopia therapy more immersive, enjoyable and playful. The game was perceived by our users to be a fun and accessible alternative, as it involves adhering a Bangerter foil (an opaque sticker) on a VR headset to blur vision in an Amblyopic person's dominant eye while having them playa VR video game. To perform well in the video game, their brain must adapt to rely on seeing with their weaker eye, thereby reforging that neurological connection. While testing our game, we also studied users behavior to investigate what visual and kinetic components were more effective therapeutically. Our findings generally show positive results, showing that visual acuity in adults increases with 45 minutes of therapy. Amblyopia has many negative symptoms including poor depth perception (nec-essary for daily activities such as driving), so this therapy could be life changing for adults with Amblyopia.},
  keywords={Games;Vision defects;Medical treatment;Visualization;Virtual reality;Headphones;Testing;Visual acuity—Visual stereopsis—LogMAR—Crowding},
  doi={10.1109/VR.2019.8797997},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797828,
  author={Kern, Florian and Winter, Carla and Gall, Dominik and Käthner, Ivo and Pauli, Paul and Latoschik, Marc Erich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Virtual Reality and Gamification Within Procedurally Generated Environments to Increase Motivation During Gait Rehabilitation}, 
  year={2019},
  volume={},
  number={},
  pages={500-509},
  abstract={Virtual Reality (VR) technology offers promising opportunities to improve traditional treadmill-based rehabilitation programs. We present an immersive VR rehabilitation system that includes a head-mounted display and motion sensors. The application is designed to promote the experience of relatedness, autonomy, and competence. The application uses procedural content generation to generate diverse landscapes. We evaluated the effect of the immersive rehabilitation system on motivation and affect. We conducted a repeated measures study with 36 healthy participants to compare the immersive program to a traditional rehabilitation program. Participants reported significant greater enjoyment, felt more competent and experienced higher decision freedom and meaningfulness in the immersive VR gait training compared to the traditional training. They experienced significantly lower physical demand, simulator sickness, and state anxiety, and felt less pressured while still perceiving a higher personal performance. We derive three design implications for future applications in gait rehabilitation: Immersive VR provides a promising augmentation for gait rehabilitation. Gamification features provide a design guideline for content creation in gait rehabilitation. Relatedness and autonomy provide critical content features in gait rehabilitation.},
  keywords={Games;Task analysis;Training;Psychology;Virtual reality;Multiple sclerosis;Resists;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI;Applied computing—Health informatics},
  doi={10.1109/VR.2019.8797828},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797787,
  author={Lin, Lorraine and Normoyle, Aline and Adkins, Alexandra and Sun, Yu and Robb, Andrew and Ye, Yuting and Di Luca, Massimiliano and Jörg, Sophie},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Hand Size and Interaction Modality on the Virtual Hand Illusion}, 
  year={2019},
  volume={},
  number={},
  pages={510-518},
  abstract={Most commercial virtual reality applications with self avatars provide users with a “one-size fits all” avatar. While the height of this body may be scaled to the user's height, other body proportions, such as limb length and hand size, are rarely customized to fit an individual user. Prior research has shown that mismatches between users' avatars and their actual bodies can affect size perception and feelings of body ownership. In this paper, we consider how concepts related to the virtual hand illusion, user experience, and task efficiency are influenced by variations between the size of a user's actual hand and their avatar's hand. We also consider how using a tracked controller or tracked gestures affect these concepts. We conducted a 2×3 within-subjects study (n=20), with two levels of input modality: using tracked finger motion vs. a hand-held controller (Glove vs. Controller), and three levels of hand scaling (Small, Fit, and Large). Participants completed 2 block-assembly trials for each condition (for a total of 12 trials). Time, mistakes, and a user experience survey were recorded for each trial. Participants experienced stronger feelings of ownership and realism in the Glove condition. Efficiency was higher in the Controller condition and supported by play data of more time spent, blocks grabbed, and blocks dropped in the Glove condition. We did not find enough evidence for a change in agency and the intensity of the virtual hand illusion depending on hand size. Over half of the participants indicated preferring the Glove condition over the Controller condition, mentioning fun and efficiency as factors in their choices. Preferences on hand scaling were mixed but often attributed to efficiency. Participants liked the appearance of their virtual hand more while using the Fit instead of Large hands. Several interaction effects were observed between input modality and hand scaling, for example, for smaller hands, tracked hands evoked stronger feelings of ownership compared to using a controller. Our results show that the virtual hand illusion is stronger when participants are able to control a hand directly rather than with a hand-held device, and that the virtual reality task must first be considered to determine which modality and hand size are the most applicable.},
  keywords={Avatars;Tracking;Rubber;Task analysis;Games;Virtual environments;Human-centered computing—Virtual reality Human-centered computing—Gestural input;Human-centered computing—Interaction design;Computing methodologies—Perception},
  doi={10.1109/VR.2019.8797787},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798040,
  author={Ogawa, Nami and Narumi, Takuji and Hirose, Michitaka},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Hand Realism Affects Object Size Perception in Body-Based Scaling}, 
  year={2019},
  volume={},
  number={},
  pages={519-528},
  abstract={How does the representation of an embodied avatar influence the way in which one perceives the scale of a virtual environment? In virtual reality, it is common to embody avatars of various appearances, from abstract to realistic. It is known that changes in the realism of virtual hands affect the self-body perception, including body ownership. However, the influence of self-avatar realism on the perception of non-body objects has not been investigated. Considering the theory that the scale of the external environment is perceived relative to the size of one's body (body-based scaling), it can be hypothesized that the realism of an avatar affects not only body ownership but also the fidelity of the avatar with respect to our own body as a metric. Therefore, this study examines how avatar realism affects perceived object sizes as the size of the virtual hand changes. In the experiment, we manipulate the level of realism (realistic, iconic, and abstract) and size (veridical and enlarged) of the virtual hand and measure the perceived size of a cube. The results show that the size of the cube is perceived to be smaller when the virtual hand is enlarged compared to when it is veridical, indicating that the participants perceive the sizes of objects based on the size of the avatar, only in the case of a highly realistic hand. Our findings indicate that the more realistic the avatar, the stronger is the sense of embodiment including body ownership, which fosters scaling the size of objects using the size of the body as a fundamental metric. This provides evidence that self-avatar appearances affect how we perceive not only virtual bodies but also virtual spaces.},
  keywords={Avatars;Estimation;Visualization;Measurement;Three-dimensional displays;Haptic interfaces;Human-centered computing—Virtual reality},
  doi={10.1109/VR.2019.8798040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797716,
  author={Porssut, Thibault and Herbelin, Bruno and Boulic, Ronan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reconciling Being in-Control vs. Being Helped for the Execution of Complex Movements in VR}, 
  year={2019},
  volume={},
  number={},
  pages={529-537},
  abstract={Performing motor tasks in virtual environments is best achieved with motion capture and animation of a 3D character that participants control in real time and perceive as being their avatar in the virtual environment. A strong Sense of Embodiment (SoE) for the virtual body not only relies on the feeling that the virtual body is their own (body ownership), but also that the virtual body moves in the world according to their will and replicates precisely their body movement (sense of agency). Within that frame of mind our specific aim is to demonstrate that the avatar can even be programmed to be better at executing a given task or to perform a movement that is normally difficult or impossible to execute precisely by the user. More specifically, our experimental task consists in asking subjects to follow with the hand a target that is animated using non-biological motion; the unnatural nature of the movement leads to systematic errors by the subjects. The challenge here is to introduce a subtle distortion between the position of the real hand and the position of the virtual hand, so that the virtual hand succeeds in performing the task while still letting subjects believe they are fully in control. Results of two experiments (N=16) show that our implementation of a distortion function, that we name the attraction well, successfully led participants to report being in control of the movement (agency) and being embodied in the avatar (body ownership) even when the distortion was above a threshold that they can detect. Furthermore, a progressive introduction of the distortion (starting without help, and introducing distortion on the go) could even further increase its acceptance.},
  keywords={Distortion;Avatars;Task analysis;Haptic interfaces;Three-dimensional displays;Training;Virtual environments;Virtual Reality;Avatar;Embodiment;Agency;Body Ownership;Movement Distortion;K.6.1 [Management of Computing and Information Systems]: Project and People Management—Life Cycle;K.7.m [The Computing Profession]: Miscellaneous—Ethics},
  doi={10.1109/VR.2019.8797716},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798152,
  author={Walker, Michael E. and Szafir, Daniel and Rae, Irene},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Influence of Size in Augmented Reality Telepresence Avatars}, 
  year={2019},
  volume={},
  number={},
  pages={538-546},
  abstract={In this work, we explore how advances in augmented reality technologies are creating a new design space for long-distance telepresence communication through virtual avatars. Studies have shown that the relative size of a speaker has a significant impact on many aspects of human communication including perceived dominance and persuasiveness. Our system synchronizes the body pose of a remote user with a realistic, virtual human avatar visible to a local user wearing an augmented reality head-mounted display. We conducted a two-by-two (relative system size: equivalent vs. small; leader vs. follower), between participants study (N = 40) to investigate the effect of avatar size on the interactions between remote and local user. We found the equal-sized avatars to be significantly more influential than the small-sized avatars and that the small avatars commanded significantly less attention than the equal-sized avatars. Additionally, we found the assigned leadership role to significantly impact participant subjective satisfaction of the task outcome.},
  keywords={Avatars;Telepresence;Robots;Augmented reality;Collaboration;Leadership;Avatars, augmented reality;mixed reality;avatar-mediated communication;human-avatar interaction;avatar telepresence systems;avatar size;scenario-based design;team role;Human-centered computing—HCI—Interaction paradigms—Mixed / augmented reality Human-centered computing—HCI—Interaction paradigms—Virtual reality Human-centered computing—HCI—Interaction paradigms—Web-based interaction Human-centered computing—HCI—Interaction paradigms—Collaborative interaction Human-centered computing—HCI—HCI design and evaluation methods—User studies Human-centered computing—HCI—HCI design and evaluation methods—Laboratory experiments Human-centered computing—Interaction design—Interaction design process and methods—Scenario-based design},
  doi={10.1109/VR.2019.8798152},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797719,
  author={Yoon, Boram and Kim, Hyung-il and Lee, Gun A. and Billinghurst, Mark and Woo, Woontack},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Avatar Appearance on Social Presence in an Augmented Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={547-556},
  abstract={This paper investigates the effect of avatar appearance on Social Presence and users' perception in an Augmented Reality (AR) telepresence system. Despite the development of various commercial 3D telepresence systems, there has been little evaluation and discussions about the appearance of the collaborator's avatars. We conducted two user studies comparing the effect of avatar appearances with three levels of body part visibility (head & hands, upper body, and whole body) and two different character styles (realistic and cartoon-like) on Social Presence while performing two different remote collaboration tasks. We found that a realistic whole body avatar was perceived as being the best for remote collaboration, but an upper body or cartoon style could be considered as a substitute depending on the collaboration context. We discuss these results and suggest guidelines for designing future avatar-mediated AR remote collaboration systems.},
  keywords={Avatars;Collaboration;Three-dimensional displays;Telepresence;Augmented reality;Task analysis;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/augmented reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR.2019.8797719},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798019,
  author={Takatori, Hikaru and Hiraiwa, Masashi and Yano, Hiroaki and Iwata, Hiroo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Large-Scale Projection-Based Immersive Display: The Design and Implementation of LargeSpace}, 
  year={2019},
  volume={},
  number={},
  pages={557-565},
  abstract={In this paper, we introduce LargeSpace, the world's largest immersive display, and discuss the principles of its design. To clarify the design of large-scale projection-based immersive displays, we address the optimum screen shape, projection approach, and arrangement of projectors and tracking cameras. In addition, a novel distortion correction method for panoramic stereo rendering is described. The method can be applied to any projection-based immersive display with any screen shape, and can generate real-time panoramic-stereoscopic views from the viewpoints of tracked participants. To validate the design principles and the rendering algorithm, we implement the LargeSpace and confirm that the method can generate the correct perspective from any position inside the screen viewing area. We implement several applications and show that large-scale immersive displays can be used in the fields of art and experimental psychology.},
  keywords={Shape;Position measurement;Optical variables measurement;Mirrors;Virtual environments;Buildings;Cameras;Human-centered computing—Human computer interaction (HCI) —Interaction devices—Displays and imagers;Human-centered computing—Visualization—Visualization systems and tools;Applied computing—Arts and humanities—Media arts},
  doi={10.1109/VR.2019.8798019},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797752,
  author={Vasylevska, Khrystyna and Yoo, Hyunjin and Akhavan, Tara and Kaufmann, Hannes},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Eye-Friendly VR: How Bright Should It Be?}, 
  year={2019},
  volume={},
  number={},
  pages={566-574},
  abstract={Visual information plays an important part in the perception of the world around us. Recently, head-mounted displays (HMD) came to the consumer market and became a part of everyday life of thousands of people. Like with the desktop screens or hand-held devices before, the public is concerned with the possible health consequences of the prolonged usage and question the adequacy of the default settings. It has been shown that the brightness and contrast of a display should be adjusted to match the external light to decrease eye strain and other symptoms. Currently, there is a noticeable mismatch in brightness between the screen and dark background of an HMD that might cause eye strain, insomnia, and other unpleasant symptoms. In this paper, we explore the possibility to significantly lower the screen brightness in the HMD and successfully compensate for the loss of the visual information on a dimmed screen. We designed a user study to explore the connection between the screen brightness in HMD and task performance, cybersickness, users' comfort, and preferences. We have tested three levels of brightness: the default Full Brightness, the optional Night Mode and a significantly lower brightness with original content and compensated content. Our results suggest that although users still prefer the brighter setting, the HMDs can be successfully used with significantly lower screen brightness, especially if the low screen brightness is compensated.},
  keywords={Brightness;Resists;Image color analysis;Visualization;Strain;Color;Standards;Human-centered computing—User studies;Human-centered computing—Virtual reality;Computing methodologies—Perception},
  doi={10.1109/VR.2019.8797752},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798312,
  author={Trepkowski, Christina and Eibich, David and Maiero, Jens and Marquardt, Alexander and Kruijff, Ernst and Feiner, Steven},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Narrow Field of View and Information Density on Visual Search Performance in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={575-584},
  abstract={Many optical-see-through displays have a relatively narrow field of view. However, a limited field of view can constrain how information can be presented and searched through. To understand these constraints, we present a series of experiments that address the interrelationships between field of view, information density, and search performance. We do so by simulating various fields of view using two approaches: limiting the field of view presented on a Microsoft HoloLens optical-see-through head-worn display and dynamically changing the portion of a large tiled-display wall on which information is presented, for head-tracked users in both cases. Our results indicate a significant effect of information density and field of view on search performance, with potential search performance benefits of using a larger FOV between ca. 7-28%. Furthermore, while grids guided visual search, they did not significantly affect performance.},
  keywords={Visualization;Task analysis;Limiting;Three-dimensional displays;Semantics;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798312},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797975,
  author={Batmaz, Anil Ufuk and Machuca, Mayra Donaji Barrera and Pham, Duc Minh and Stuerzlinger, Wolfgang},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Do Head-Mounted Display Stereo Deficiencies Affect 3D Pointing Tasks in AR and VR?}, 
  year={2019},
  volume={},
  number={},
  pages={585-592},
  abstract={Most AR and VR headsets use stereoscopic displays to show virtual objects in 3D. However, the limitations of current stereo display systems affect depth perception through conflicting depth cues, which then also affect virtual hand interaction in peri-personal space, i.e., within arm's reach. We performed a Fitts' law experiment to better understand the impact of stereo display deficiencies of AR and VR headsets on pointing at close-by targets arranged laterally or along the line of sight. According to our results, the movement direction and the corresponding change in target depth affect pointing time and throughput; subjects' movements towards/away from their head were slower and less accurate than their lateral movements (left/right). However, even though subjects moved faster in AR, we did not observe a significant difference for pointing performance between AR and VR headsets, which means that previously identified differences in depth perception between these platforms seem to have no strong effect on interaction. Our results also help 3D user interface designers understand how changes in target depth affect users' performance in different movement directions in AR and VR.},
  keywords={Three-dimensional displays;Headphones;Task analysis;Visualization;Stereo image processing;TV;Display systems;3D pointing;virtual hand;selection;Fitts Law;AR;VR;Human-centered computing—Human-computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human-computer interaction (HCI)—Interaction paradigms—Mixed/Augmented Reality;Human-centered computing—Human-computer interaction (HCI)—Interaction techniques—Pointing;Human-centered computing—Interaction design},
  doi={10.1109/VR.2019.8797975},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798340,
  author={Satriadi, Kadek Ananta and Ens, Barrett and Cordeil, Maxime and Jenny, Bernhard and Czauderna, Tobias and Willett, Wesley},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality Map Navigation with Freehand Gestures}, 
  year={2019},
  volume={},
  number={},
  pages={593-603},
  abstract={Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.},
  keywords={Navigation;Fatigue;Aerospace electronics;Three-dimensional displays;Task analysis;Augmented reality;Collaboration;Human-centered computing— Mixed / augmented reality;Human-centered computing—Gestural input;Human-centered computing—Empirical studies in interaction design},
  doi={10.1109/VR.2019.8798340},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797824,
  author={Bonfert, Michael and Porzel, Robert and Malaka, Rainer},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Get a Grip! Introducing Variable Grip for Controller-Based VR Systems}, 
  year={2019},
  volume={},
  number={},
  pages={604-612},
  abstract={We propose an approach to facilitate adjustable grip for object interaction in virtual reality. It enables the user to handle objects with loose and firm grip using conventional controllers. Pivotal design properties were identified and evaluated in a qualitative pilot study. Two revised interaction designs with variable grip were compared to the status quo of invariable grip in a quantitative study. The users performed placing actions with all interaction modes. Performance, clutching, task load, and usability were measured. While the handling time increased slightly using variable grip, the usability score was significantly higher. No substantial differences were measured in positioning accuracy. The results lead to the conclusion that variable grip can be useful and improve realism depending on tasks, goals, and user preference.},
  keywords={Grasping;Task analysis;Gravity;Glass;Virtual reality;Usability;Control systems;Virtual reality;grip;grasping;object manipulation;Human-centered computing—Virtual reality;Human-centered computing—Empirical studies in interaction design},
  doi={10.1109/VR.2019.8797824},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798119,
  author={Günther, Tobias and Engeln, Lars and Busch, Sally J. and Groh, Rainer},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Elastic Feedback on the Perceived User Experience and Presence of Travel Methods in Immersive Environments}, 
  year={2019},
  volume={},
  number={},
  pages={613-620},
  abstract={Mid-air interaction achieved through handtracking or controller in Virtual Reality (VR) is performed without an explicit spatial reference. One solution for creating such a reference is elastic feedback, which we realized via tension springs attached to a tracked VR-controller. Thereby, directional force is exerted and perceived by the user. A study was conducted to investigate the effects of elastic feedback on users in virtual travel tasks. Therefore, the elastic mode of the input device was compared to an isotonic mode, without any perceivable force. Also, two virtual travelling methods, a driving mode and a flight mode, were investigated in the study. The main goal of the study was to analyze, how elastic feedback influences user experience and presence perception. Statistical analysis with ANOVA of data from 24 subjects revealed significant main effects for the elastic mode on both, the User Experience Questionnaire (UEQ) and the Igroup Presence Questionnaire (IPQ). In addition, the subjects missed fewer target objects on average during the travel task. However, analysis of task completion times indicated no relevant differences. Although not all individual scales of UEQ and IPQ showed significant outcomes, the result indicates that immersive travel interfaces could benefit from the use of elastic feedback.},
  keywords={Task analysis;Input devices;Three-dimensional displays;Haptic interfaces;Muscles;Force;Cameras;Human-centered computing—Empirical studies in HCI;Human-centered computing—Virtual reality;Human-centered computing—User centered design;Human-centered computing—Haptic devices},
  doi={10.1109/VR.2019.8798119},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798209,
  author={Perrin, Théo and Kerhervé, Hugo A. and Faure, Charles and Sorel, Anthony and Bideau, Benoit and Kulpa, Richard},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enactive Approach to Assess Perceived Speed Error during Walking and Running in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={622-629},
  abstract={The recent development of virtual reality (VR) devices such as head mounted displays (HMDs) increases opportunities for applications at the confluence of physical activity and gaming. Recently, the fields of sport and fitness have turned to VR, including for locomotor activities, to enhance motor and energetic resources, as well as motivation and adherence. For example, VR can provide visual feedbacks during treadmill running, thereby reducing monotony and increasing the feeling of movement and engagement with the activity. However, the relevance of using VR tools during locomotion depends on the ability of these systems to provide natural immersive feelings, specifically a coherent perception of speed. The objective of this study is to estimate the error between actual and perceived locomotor speed in VE using an enactive approach, i.e. allowing an active control of the environment. Sixteen healthy individuals participated in the experiment, which consisted in walking and running on a motorized treadmill at speeds ranging from 3 to 11 km/h with 0.5 km/h increments, in a randomized order while wearing a HMD device (HTC Vive) displaying a virtual racetrack. Participants were instructed to match VE speed with what they perceived was their ac-tuallocomotion speed (LS), using a handheld Vive controller. They were able to modify the optic flow speed (OFS) with a 0.02 km/h increment/decrement accuracy. An optic flow multiplier (OFM) was computed based on the error between OFS and LS. It represents the gain that exists between the visually perceived speed and the real locomotion speed experienced by participants for each trial. For all conditions, the average of OFM was 1.00±.25 to best match LS. This finding is at odds with previous works reporting an underestimation of speed perception in VR. It could be explained by the use of an enactive approach allowing an active and accurate matching of visually and proprioceptively perceived speeds by participants. But above all, our study showed that the perception of speed in VR is strongly individual, with some participants always overestimating and others constantly underestimating. Therefore, a general OFM should not be used to correct speed in VE to ensure congruence in speed perception, and we propose the use of individual models as recommendations for setting up locomotion-based VR applications.},
  keywords={Legged locomotion;Adaptive optics;Resists;Optical feedback;Virtual environments;Visualization;Virtual reality;speed perception assessment;running in virtual environment;enaction;physical activity in VR},
  doi={10.1109/VR.2019.8798209},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798065,
  author={Ku, Pin-Sung and Lin, Yu-Chih and Peng, Yi-Hao and Chen, Mike Y.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={PeriText: Utilizing Peripheral Vision for Reading Text on Augmented Reality Smart Glasses}, 
  year={2019},
  volume={},
  number={},
  pages={630-635},
  abstract={Augmented Reality (AR) provides real-time information by superimposing virtual information onto users' view of the real world. Our work is the first to explore how peripheral vision, instead of central vision, can be used to read text on AR and smart glasses. We present Peritext, a multiword reading interface using rapid serial visual presentation (RSVP). This enables users to observe the real world using central vision, while using peripheral vision to read virtual information. We first conducted a lab-based study to determine the effect of different text transformation by comparing reading efficiency among 3 capitalization schemes, 2 font faces, 2 text animation methods, and 3 different numbers of words for RSVP paradigm. We found that title case capitalization, sans-serif font and word-wise typewriter animation with multiword RSVP display resulted in better reading efficiency, which together formed our Peritext design. Another lab-based study followed, investigating the performance of the Peritext against control text, and the results showed significant better performance. Finally, we conducted a field study to collect user feedback while using Peritext in real-world walking scenarios, and all users reported a preference of 5° eccentricity over 8°.},
  keywords={Visualization;Animation;Legged locomotion;Augmented reality;Smart glasses;Monitoring;reading interface;augmented reality;peripheral vision;multiword;mobile;rapid serial visual presentation;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Human-centered computing—Visualization—Visualization design and evaluation methods Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/ augmented reality},
  doi={10.1109/VR.2019.8798065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797992,
  author={Klose, Elisa Maria and Mack, Nils Adrian and Hegenberg, Jens and Schmidt, Ludger},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text Presentation for Augmented Reality Applications in Dual-Task Situations}, 
  year={2019},
  volume={},
  number={},
  pages={636-644},
  abstract={We investigate how reading text in augmented reality (AR) glasses and the simultaneous execution of three real-world tasks interfere with each other. The three tasks are a visual stimulus-response task (VSRT), a simple walking task and a walking obstacle course. Also, we investigate the effects of different AR text positions on primary task and reading performance as well as subjective preference. We propose a novel out of sight body-locked text placement for AR text presentation to be used in dual-task situations and compare it to head-locked text placement, each in two heights. AR reading affected performance in all tasks and reading speed was affected in all dual-task conditions. Participants subjectively preferred the body-locked text presentation, while objective measures do not reflect that preference. Differences between the tasks and several interaction effects between task and AR text placement demonstrate the necessity to carefully consider the context of use when designing AR reading UIs. The presented study with 12 participants provides insights into the effects of AR glasses usage in dual-task situations and several design recommendations are derived from the results.},
  keywords={Task analysis;Glass;Legged locomotion;Augmented reality;Visualization;Smart phones;body-locked;head-locked;dual task;user study;text presentation;Augmented Reality;Human-Computer Interaction;Human Factors},
  doi={10.1109/VR.2019.8797992},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797852,
  author={MacQuarrie, Andrew and Steed, Anthony},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perception of Volumetric Characters' Eye-Gaze Direction in Head-Mounted Displays}, 
  year={2019},
  volume={},
  number={},
  pages={645-654},
  abstract={Volumetric capture allows the creation of near-video-quality content that can be explored with six degrees of freedom. Due to limitations in these experiences, such as the content being fixed at the point of filming, an understanding of eye-gaze awareness is critical. A repeated measures experiment was conducted that explored users' ability to evaluate where a volumetrically captured avatar (VCA) was looking. Wearing one of two head-mounted displays (HMDs), 36 participants rotated a VCA to look at a target. The HMD resolution, target position, and VCA's eye-gaze direction were varied. Results did not show a difference in accuracy between HMD resolutions, while the task became significantly harder for target locations further away from the user. In contrast to real-world studies, participants consistently misjudged eye-gaze direction based on target location, but not based on the avatar's head turn direction. Implications are discussed, as results for VCAs viewed in HMDs appear to differ from face-to-face scenarios.},
  keywords={Avatars;Receivers;Three-dimensional displays;Task analysis;Cameras;Solid modeling;Resists;User study;Virtual reality;Gaze perception;1.1.1 [Human-centered computing]—Empirical studies in HCI;1.1.3.2 [Human-centered computing]: Interaction devices—Displays and imagers;1.1.4 [Human-centered computing]—HCI theory, concepts and models},
  doi={10.1109/VR.2019.8797852},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798239,
  author={Krösl, Katharina and Elvezio, Carmine and Hürbe, Matthias and Karst, Sonja and Wimmer, Michael and Feiner, Steven},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={ICthroughVR: Illuminating Cataracts through Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={655-663},
  abstract={Vision impairments, such as cataracts, affect the way many people interact with their environment, yet are rarely considered by architects and lighting designers because of a lack of design tools. To address this, we present a method to simulate vision impairments, in particular cataracts, graphically in virtual reality (VR), using eye tracking for gaze-dependent effects. We also conduct a VR user study to investigate the effects of lighting on visual perception for users with cataracts. In contrast to existing approaches, which mostly provide only simplified simulations and are primarily targeted at educational or demonstrative purposes, we account for the user's vision and the hardware constraints of the VR headset. This makes it possible to calibrate our cataract simulation to the same level of degraded vision for all participants. Our study results show that we are able to calibrate the vision of all our participants to a similar level of impairment, that maximum recognition distances for escape route signs with simulated cataracts are significantly smaller than without, and that luminaires visible in the field of view are perceived as especially disturbing due to the glare effects they create. In addition, the results show that our realistic simulation increases the understanding of how people with cataracts see and could therefore also be informative for health care personnel or relatives of cataract patients.},
  keywords={Cataracts;Lenses;Lighting;Diseases;Visualization;Solid modeling;Gaze tracking;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Computing methodologies—Computer Graphics—Graphics systems and interfaces—Perception;Applied computing—Life and medical sciences—Health informatics},
  doi={10.1109/VR.2019.8798239},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798089,
  author={Sankaran, Naveen Kumar and Nisar, Harris J. and Zhang, Ji and Formella, Kyle and Amos, Jennifer and Barker, Lisa T. and Vozenilek, John A. and LaValle, Steven M. and Kesavadas, Thenkurussi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Efficacy Study on Interactive Mixed Reality (IMR) Software with Sepsis Prevention Medical Education}, 
  year={2019},
  volume={},
  number={},
  pages={664-670},
  abstract={Objective: In recent years, the training of novice medical professionals with simulated environments such as virtual reality (VR) and augmented reality (AR) has increased dramatically. However, the usability of these technologies is limited due to the complexity involved in creating the clinical content. To be comparable to a clinical environment, the simulation platform should include real-world learning parameters such as patient physiology, emotions, and clinical team behaviors. Incorporating such nondeterministic parameters has historically required faculty to possess advanced programming skills. Lack of effective software for instructors to easily develop VR curriculum content is a hurdle in developing VR based curriculum. Method: We address this challenge through a software platform that simplifies the creation of Interactive Mixed Reality (IMR) scenarios. Three educational components we were able to embed into an IMR scenario includes 1) integrated 360-degree video recording of a clinical encounter to provide a first-person perspective, 2) rich annotated knowledge content, and 3) assessment questionnaire. We developed a sepsis prevention education scenario using the IMR software to demonstrate the potential of enhancing simulated medical training by accelerating clinical exposure for novice students. Result: An IRB approved study was conducted with a group of 28 novice students to evaluate the efficacy of the IMR technology. The participants provided feedback by answering demographics, NASA-TLX and system usability scale questionnaires. Significance: Our software is a step towards improving VR based education content development process. Conclusion: The studies conducted here provide preliminary evidence that the IMR software is a usable technology based on the NASA-TLX and system usability studies conducted. Future work will compare our new educational strategy for medical training with live simulation scenarios inside a hospital room and a simple video-based curriculum.},
  keywords={Training;Task analysis;Virtual reality;Software;Solid modeling;Biomedical imaging;Mixed reality—Medical training—Medical simulation—Sepsis training—Emergency medicine},
  doi={10.1109/VR.2019.8798089},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797705,
  author={Prachyabrued, Mores and Wattanadhirach, Disathon and Dudrow, Richard B. and Krairojananan, Nat and Fuengfoo, Pusit},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward Virtual Stress Inoculation Training of Prehospital Healthcare Personnel: A Stress-Inducing Environment Design and Investigation of an Emotional Connection Factor}, 
  year={2019},
  volume={},
  number={},
  pages={671-679},
  abstract={Prehospital emergency healthcare personnel are responsible for finding, rescuing, and taking prehospital care of emergency patients. They are regularly exposed to stressful and traumatic lifesaving situations. The stress involved can impact their performance and can cause mental disorders in the long term. Stress inoculation training (SIT) inoculates individuals to potential stressors by letting them practice stress-coping skills in a controlled environment. Our work explores a story-driven stressful virtual environment design that can potentially be used for SIT in the new context of emergency healthcare personnel. Users role-play a first-time emergency worker on a rescue mission. The interactive storytelling is designed to engage users and elicit strong emotional responses, and follows the three-act structure commonly found in films and video games. To understand the stress-inducing and sense of presence qualities of our approach including the previously untested impact of an emotional connection factor, we conduct a between-subjects experiment involving 60 subjects. Results show that the approach successfully induces stress by increasing heart rate, galvanic skin response, and subjective stress rating. Questionnaire results indicate positive presence. One subject group engages in an initial friendly conversation with a virtual co-worker to establish an emotional connection. Another group includes no such conversation. The group with the emotional connection shows higher physiological stress levels and more occurrences of subject behaviors reflecting presence. Medical experts review our approach and suggest several applications that can benefit from its stress inducing ability.},
  keywords={Stress;Training;Personnel;Medical services;Virtual environments;Emotional responses;Biomedical monitoring;Stress;training;emergency healthcare personnel;virtual reality;storytelling;emotional connection;presence;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;J.3 [Life And Medical Sciences]: Health},
  doi={10.1109/VR.2019.8797705},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797721,
  author={Brument, Hugo and Podkosova, Iana and Kaufmann, Hannes and Olivier, Anne Hélène and Argelaguet, Ferran},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual vs. Physical Navigation in VR: Study of Gaze and Body Segments Temporal Reorientation Behaviour}, 
  year={2019},
  volume={},
  number={},
  pages={680-689},
  abstract={This paper investigates whether the body anticipation synergies in real environments (REs) are preserved during navigation in virtual environments (VEs). Experimental studies related to the control of human locomotion in REs during curved trajectories report a top-down reorientation strategy with the reorientation of the gaze anticipating the reorientation of head, the shoulders and finally the global body motion. This anticipation behavior provides a stable reference frame to the walker to control and reorient his/her body according to the future walking direction. To assess body anticipation during navigation in VEs, we conducted an experiment where participants, wearing a head-mounted display, performed a lemniscate trajectory in a virtual environment (VE) using five different navigation techniques, including walking, virtual steering (head, hand or torso steering) and passive navigation. For the purpose of this experiment, we designed a new control law based on the power-law relation between speed and curvature during human walking. Taken together our results showed a similar ordered top-down sequence of reorientation of the gaze, head and shoulders during curved trajectories between walking in REs and in VEs (for all the evaluated techniques). However, the anticipation mechanism was significantly higher for the walking condition compared to the others. The results presented in this paper pave the way to the better understanding of the underlying mechanisms of human navigation in VEs and to the design of navigation techniques more adapted to humans.},
  keywords={Trajectory;Indexes;Navigation;Conferences;Virtual reality;Three-dimensional displays;User interfaces;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8797721},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798070,
  author={Wienrich, Carolin and Döllinger, Nina and Kock, Simon and Gramann, Klaus},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={User-Centered Extension of a Locomotion Typology: Movement-Related Sensory Feedback and Spatial Learning}, 
  year={2019},
  volume={},
  number={},
  pages={690-698},
  abstract={When human operators locomote actively in virtual environments (VE), the movement range often has to be adapted to the limited dimensions of the physical space. This however might lead to a conflict between sensory information originating from user movements and sensory feedback provided through the virtual locomotion. To investigate whether different locomotion strategies that adapt virtual movement to the limited physical space impact cognitive processes, two experiments were conducted. The first experiment used walking in place, the second study scale of locomotion to investigate the impact of locomotion adaptation on the acquisition of spatial knowledge and user experience. We systematically analyzed body-based sensorial conflicts for the different adaptation strategies and reveal that neither walking in place nor scale of locomotion impacts spatial knowledge acquisition or user experience. We can conclude that visual cues indicating locomotion combined with body-based rotational cues seem to be sufficient for the acquisition of spatial knowledge and that locomotion with controllers seems efficient and preferable for users. The results link system-driven typologies with human-centered factors to guide systematic tests of locomotion techniques in virtual environments for future studies.},
  keywords={Legged locomotion;Floors;Visualization;Cognitive processes;User experience;Task analysis;Locomotion;Body-Based Sensorial Cues;Spatial Cognition;User Experience;H.1.2 User/Machine Systems: Human information processing;H.5.2 User Interfaces: Evaluation/methodology, User-centered design},
  doi={10.1109/VR.2019.8798070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798251,
  author={Kang, Hyeong Yeop and Lee, Geonsun and Kang, Dae Seok and Kwon, Ohung and Cho, Jun Yeup and Choi, Ho-Jung and Han, Jung Hyun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Jumping Further: Forward Jumps in a Gravity-reduced Immersive Virtual Environment}, 
  year={2019},
  volume={},
  number={},
  pages={699-707},
  abstract={In a cable-driven suspension system developed to simulate the reduced gravity of lunar or Martian surfaces, we propose to manipu-late/reduce the physical cues of forward jumps so as to overcome the limited workspace problem. The physical cues should be manipulated in a way that the discrepancy from the visual cues provided through the HMD is not noticeable by users. We identified the extent to which forward jumps can be manipulated naturally. We combined it with visual gains, which can scale visual cues without being noticed by users. The test results obtained in a prototype application show that we can use both trajectory manipulation and visual gains to overcome the spatial limit. We also investigated the user experiences when making significantly high and far jumps. The results will be helpful in designing astronaut-training systems and various VR entertainment content.},
  keywords={Trajectory;Visualization;Moon;Wires;Acceleration;Gravity;Tracking;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR.2019.8798251},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798025,
  author={Wang, Lili and Zhao, Han and Wang, Zesheng and Wu, Jian and Li, Bingqiang and He, Zhiming and Popescu, Voicu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Occlusion Management in VR: A Comparative Study}, 
  year={2019},
  volume={},
  number={},
  pages={708-716},
  abstract={VR applications rely on the user's ability to explore the virtual scene efficiently. In complex scenes, occlusions limit what the user can see from a given location, and the user has to navigate the viewpoint around occluders to gain line of sight to the hidden parts of the scene. When the disoccluded regions prove to be of no interest, the user has to retrace their path, making scene exploration inefficient. Furthermore, the user might not be able to assume a viewpoint that would reveal the occluded regions due to physical limitations, such as obstacles in the real world hosting the VR application, viewpoints beyond the tracked area, or viewpoints above the user's head that cannot be reached by walking. Several occlusion management methods have been proposed in visualization research, such as top view, X-ray, and multiperspective visualization, which help the user see more from the current position, having the potential to improve the exploration efficiency of complex scenes. This paper reports on a study that investigates the potential of these three occlusion management methods in the context of VR applications, compared to conventional navigation. Participants were required to explore two virtual scenes to purchase five items in a virtual Supermarket, and to find three people in a virtual parking garage. The task performance metrics were task completion time, total distance traveled, and total head rotation. The study also measured user spatial awareness, depth perception, and simulator sickness. The results indicate that users benefit from top view visualization which helps them learn the scene layout and helps them understand their position within the scene, but the top view does not let the user find targets easily due to occlusions in the vertical direction, and due to the small image footprint of the targets. The X-ray visualization method worked better in the garage scene, a scene with a few big occluders and a low occlusion depth complexity' and less well in the Supermarket scene, a scene with many small occluders that create high occlusion depth complexity. The multi-perspective visualization method achieves better performance than the top view method and the X-ray method, in both scenes. There are no significant differences between the three methods and the conventional method in terms of spatial awareness, depth perception, and simulator sickness.},
  keywords={Visualization;Cameras;Task analysis;X-ray imaging;Navigation;Legged locomotion;Resists;Scene Exploration;Occlusion Management;Virtual Reality;Top View;X-ray;Multiperspective Visualization;Human-centered computing—Human-centered interaction—Virtual reality;Computer graphics—Ocllusion management—Visualization},
  doi={10.1109/VR.2019.8798025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798204,
  author={Berton, Florian and Olivier, Anne-Hélène and Bruneau, Julien and Hoyet, Ludovic and Pettre, Julien},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Studying Gaze Behaviour during Collision Avoidance with a Virtual Walker: Influence of the Virtual Reality Setup}, 
  year={2019},
  volume={},
  number={},
  pages={717-725},
  abstract={Simulating realistic interactions between virtual characters has been of interest to research communities for years, and is particularly important to automatically populate virtual environments. This problem requires to accurately understand and model how humans interact, which can be difficult to assess. In this context, Virtual Reality (VR) is a powerful tool to study human behaviour, especially as it allows assessing conditions which are both ecological and controlled. While VR was shown to allow realistic collision avoidance adaptations, in the frame of the ecological theory of perception and action, interactions between walkers can not solely be characterized through motion adaptations but also through the perception processes involved in such interactions. The objective of this paper is therefore to evaluate how different VR setups influence gaze behaviour during collision avoidance tasks between walkers. To this end, we designed an experiment involving a collision avoidance task between a participant and another walker (real confederate or virtual character). During this task, we compared both the partici-pant`s locomotion and gaze behaviour in a real environment and the same situation in different VR setups (including a CAVE, a screen and a Head-Mounted Display). Our results show that even if some quantitative differences exist, gaze behaviour is qualitatively similar between VR and real conditions. Especially, gaze behaviour in VR setups including a HMD is more in line with the real situation than the other setups. Furthermore, the outcome on motion adaptations confirms previous work, where collision avoidance behaviour is qualitatively similar in VR and real conditions. In conclusion, our results show that VR has potential for qualitative analysis of locomotion and gaze behaviour during collision avoidance. This opens perspectives in the design of new experiments to better understand human behaviour, in order to design more realistic virtual humans.},
  keywords={Collision avoidance;Task analysis;Legged locomotion;Trajectory;Navigation;Kinematics;Virtual reality;Gaze Activity;Locomotion;Virtual Human;Virtual Reality;Eye-tracking;Collision Avoidance;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798204},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798043,
  author={Mousas, Christos and Koilias, Alexandros and Anastasiou, Dimitris and Rekabdar, Banafsheh and Anagnostopoulos, Christos-Nikolaos},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Self-Avatar and Gaze on Avoidance Movement Behavior}, 
  year={2019},
  volume={},
  number={},
  pages={726-734},
  abstract={The present study investigates users' movement behavior in a virtual environment when they attempted to avoid a virtual character. At each iteration of the experiment, four conditions (Self-Avatar LookAt, No Self-Avatar LookAt, Self-Avatar No LookAt, and No Self-Avatar No LookAt) were applied to examine users' movement behavior based on kinematic measures. During the experiment, 52 participants were asked to walk from a starting position to a target position. A virtual character was placed at the midpoint. Participants were asked to wear a head-mounted display throughout the task, and their locomotion was captured using a motion capture suit. We analyzed the captured trajectories of the participants' routes on four kinematic measures to explore whether the four experimental conditions influenced the paths they took. The results indicated that the Self-Avatar LookAt condition affected the path the participants chose more significantly than the other three conditions in terms of length, duration, and deviation, but not in terms of speed. Overall, the length and duration of the task, as well as the deviation of the trajectory from the straight line, were greater when a self-avatar represented participants. An additional effect on kinematic measures was found in the LookAt (Gaze) conditions. Implications for future research are discussed.},
  keywords={Legged locomotion;Virtual reality;Collision avoidance;Trajectory;Task analysis;Resists;Kinematics;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8798043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798135,
  author={Martin-Gomez, Alejandro and Eck, Ulrich and Navab, Nassir},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visualization Techniques for Precise Alignment in VR: A Comparative Study}, 
  year={2019},
  volume={},
  number={},
  pages={735-741},
  abstract={Many studies explored the effectiveness of augmented, virtual, and mixed reality for object placement tasks. Two main approaches for assisting users during object alignment exist: static visualization techniques and interactive guides. This paper presents a comparative evaluation of four static visualization techniques used to render virtual objects when precise alignment in 6 degrees of freedom (DoF) is required. The selection of these techniques is based on the amount of occlusion caused by the visual guides during the alignment task. To the best of our knowledge, no previous work exists that evaluates which visualization technique is most suitable to support users while precisely aligning objects in virtual environments. We designed a virtual reality scenario considering two conditions -with and without time constraints- in which users aligned pairs of objects. To evaluate the users performance, quantitative and qualitative scores were collected. Our results suggest that visualization techniques with low levels of occlusion can improve alignment performance and increase user acceptance.},
  keywords={Visualization;Task analysis;Solid modeling;Solids;Virtual reality;Three-dimensional displays;Rendering (computer graphics);Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Human-centered computing—Visualization—Visualization design and evaluation methods;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8798135},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797725,
  author={Gruenefeld, Uwe and Koethe, Ilja and Lange, Daniel and Weiß, Sebastian and Heuten, Wilko},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Techniques for Visualizing Moving Out-of-View Objects in Head-mounted Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={742-746},
  abstract={Current head-mounted displays (HMDs) have a limited field-of-view (FOV). A limited FOV further decreases the already restricted human visual range and amplifies the problem of objects receding from view (e.g., opponents in computer games). However, there is no previous work that investigates how to best perceive moving out-of-view objects on head-mounted displays. In this paper, we compare two visualization approaches: (1) Overview+detail, with 3D Radar, and (2) Focus+context, with EyeSee360, in a user study to evaluate their performances for visualizing moving out-of-view objects. We found that using 3D Radar resulted in a significantly lower movement estimation error and higher usability, measured by the system usability scale. 3D Radar was also preferred by 13 out of 15 participants for visualization of moving out-of-view objects.},
  keywords={Visualization;Three-dimensional displays;Radar;Games;Estimation error;Usability;Meters;Human-centered computing—Visualization—Visualization techniques;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI},
  doi={10.1109/VR.2019.8797725},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797871,
  author={Nam, Jung Who and McCullough, Krista and Tveite, Joshua and Espinosa, Maria Molina and Perry, Charles H. and Wilson, Barry T. and Keefe, Daniel F.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Worlds-in-Wedges: Combining Worlds-in-Miniature and Portals to Support Comparative Immersive Visualization of Forestry Data}, 
  year={2019},
  volume={},
  number={},
  pages={747-755},
  abstract={Virtual reality (VR) environments are typically designed so users feel present in a single virtual world at a time, but this creates a problem for applications that require visual comparisons (e.g., forest scientists comparing multiple data-driven virtual forests). To address this, we present Worlds-in-Wedges, a 3D user interface and visualization technique that supports comparative immersive visualization by dividing the virtual space surrounding the user into volumetric wedges. There are three visual/interactive levels. The first, worlds-in-context, visualizes high-level relationships between the worlds (e.g., a map for worlds that are related in space). The second level, worlds-in-miniature, is a multi-instance implementation of the World-in-Miniature technique extended to support mutlivari-ate glyph visualization. The third level, worlds-in-wedges, displays multiple large-scale worlds in wedges that act as volumetric portals. The interface supports navigation, selection, and view manipulation. Since the techniques were inspired directly by problems facing forest scientists, the interface was evaluated by building a complete multivariate data visualization of the US Forest Service Forest Inventory and Analysis public dataset. Scientist user feedback and lessons from iterative design are reported.},
  keywords={Data visualization;Forestry;Visualization;Three-dimensional displays;Portals;Computer science;Task analysis;worlds-in-miniature;3D user interface;presence;comparative visualization;Human-centered computing—Virtual reality;Human-centered computing—Interaction techniques;Human-centered computing—Scientific visualization;Human-centered computing—Geographic visualization},
  doi={10.1109/VR.2019.8797871},
  ISSN={2642-5254},
  month={March},}
@INPROCEEDINGS{8798108,
  author={Toothman, Nicholas and Neff, Michael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Avatar Tracking Errors on User Experience in VR}, 
  year={2019},
  volume={},
  number={},
  pages={756-766},
  abstract={There is evidence that adding motion-tracked avatars to virtual environments increases users' sense of presence. High quality motion capture systems are cost sensitive for the average user and low cost resource-constrained systems introduce various forms of error to the tracking. Much research has looked at the impact of particular kinds of error, primarily latency, on factors such as body ownership, but it is still not known what level of tracking error is permissible in these systems to afford compelling social interaction. This paper presents a series of experiments employing a sizable subject pool (n=96) that study the impact of motion tracking errors on user experience for activities including social interaction and virtual object manipulation. Diverse forms of error that arise in tracking are examined, including latency, popping (jumps in position), stuttering (positions held in time) and constant noise. The focus is on error on a person's own avatar, but some conditions also include error on an interlocutor, which appears underexplored. The picture that emerges is complex. Certain forms of error impact performance, a person's sense of embodiment' enjoyment and perceived usability, while others do not. Notably, evidence was not found that tracking errors impact social presence, even when those errors are severe.},
  keywords={Task analysis;Tracking;Avatars;Delays;Jitter;Headphones;Atmospheric measurements},
  doi={10.1109/VR.2019.8798108},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798018,
  author={Lang, Yining and Liang, Wei and Yu, Lap-Fai},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Agent Positioning Driven by Scene Semantics in Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={767-775},
  abstract={When a user interacts with a virtual agent via a mixed reality device, such as a Hololens or a Magic Leap headset, it is important to consider the semantics of the real-world scene in positioning the virtual agent, so that it interacts with the user and the objects in the real world naturally. Mixed reality aims to blend the virtual world with the real world seamlessly. In line with this goal, in this paper, we propose a novel approach to use scene semantics to guide the positioning of a virtual agent. Such considerations can avoid unnatural interaction experiences, e.g., interacting with a virtual human floating in the air. To obtain the semantics of a scene, we first reconstruct the 3D model of the scene by using the RGB-D cameras mounted on the mixed reality device (e.g., a Hololens). Then, we employ the Mask R-CNN object detector to detect objects relevant to the interactions within the scene context. To evaluate the positions and orientations for placing a virtual agent in the scene, we define a cost function based on the scene semantics, which comprises a visibility term and a spatial term. We then apply a Markov chain Monte Carlo optimization technique to search for an optimized solution for placing the virtual agent. We carried out user study experiments to evaluate the results generated by our approach. The results show that our approach achieved a higher user evaluation score than that of the alternative approaches.},
  keywords={Virtual reality;Semantics;Three-dimensional displays;Solid modeling;Optimization;Geometry;Cameras;Mixed Reality—Scene Understanding—Virtual Agent Positioning},
  doi={10.1109/VR.2019.8798018},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798263,
  author={Alchalabi, Bilal and Faubert, Jocelyn and Labbe, David R.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={EEG Can Be Used to Measure Embodiment When Controlling a Walking Self-Avatar}, 
  year={2019},
  volume={},
  number={},
  pages={776-783},
  abstract={It has recently been shown that inducing the ownership illusion and then manipulating the movements of one's self-avatar can lead to compensatory motor control strategies in gait rehabilitation. In order to maximize this effect, there is a need for a method that measures, and monitors embodiment levels of participants immersed in VR to induce and maintain a strong ownership illusion. The objective of this study was to propose a novel approach to measuring embodiment by presenting visual feedback that conflicts with motor control to embodied subjects. Twenty healthy participants were recruited. During experimentations, participants wore an EEG cap and motion capture markers, with an avatar displayed in a HMD from a first-person perspective. They were cued to either perform, watch or imagine a single step forward or to initiate walking on the treadmill. For some of the trials, the avatar took a step with the contralateral limb or stopped walking before the participant stopped (modified feedback). Results show that subjective levels of embodiment correlate strongly with the difference in μ - ERS power over the motor and pre-motor cortex between the modified and non-modified feedback trials.},
  keywords={Avatars;Legged locomotion;Electroencephalography;Atmospheric measurements;Particle measurements;Foot;Virtual reality;rhythm EEG;event-related-potentials;gait rehabilitation;mirror neuron system},
  doi={10.1109/VR.2019.8798263},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798208,
  author={Danieau, Fabien and Gubins, Ilja and Olivier, Nicolas and Dumas, Olivier and Denis, Bernard and Lopez, Thomas and Mollet, Nicolas and Frager, Brian and Avril, Quentin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Generation and Stylization of 3D Facial Rigs}, 
  year={2019},
  volume={},
  number={},
  pages={784-792},
  abstract={In this paper, we present a fully automatic pipeline for generating and stylizing high geometric and textural quality facial rigs. They are automatically rigged with facial blendshapes for animation, and can be used across platforms for applications including virtual reality, augmented reality, remote collaboration, gaming and more. From a set of input facial photos, our approach is to be able to create a photorealistic, fully rigged character in less than seven minutes. The facial mesh reconstruction is based on state-of-the art photogrammetry approaches. Automatic landmarking coupled with ICP registration with regularization provide direct correspondence and registration from a given generic mesh to the acquired facial mesh. Then, using deformation transfer, existing blendshapes are transferred from the generic to the reconstructed facial mesh. The reconstructed face is then fit to the full body generic mesh. Extra geometry such as jaws, teeth and nostrils are retargeted and transferred to the character. An automatic iris color extraction algorithm is performed to colorize a separate eye texture, animated with dynamic UVs. Finally, an extra step applies a style to the photorealis-tic face to enable blending of personalized facial features into any other character. The user's face can then be adapted to any human or non-human generic mesh. A pilot user study was performed to evaluate the utility of our approach. Up to 65% of the participants were successfully able to discern the presence of one's unique facial features when the style was not too far from a humanoid shape.},
  keywords={Face;Pipelines;Cameras;Three-dimensional displays;Geometry;Strain;Computational modeling;I.2.10 [artificial intelligence]: Vision and Scene Understanding—Intensity, color, photometry, and thresholding;I.3.7 [computer graphics]: Three-Dimensional Graphics and, Realism—Animation;character;animation;pipeline;virtual reality},
  doi={10.1109/VR.2019.8798208},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798021,
  author={Heidrich, David and Oberdörfer, Sebastian and Latoschik, Marc Erich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effects of Immersion on Harm-inducing Factors in Virtual Slot Machines}, 
  year={2019},
  volume={},
  number={},
  pages={793-801},
  abstract={Slot machines are one of the most played games by pathological gamblers. New technologies, e.g. immersive Virtual Reality (VR), offer more possibilities to exploit erroneous beliefs in the context of gambling. However, the risk potential of VR-based gambling has not been researched, yet. A higher immersion might increase harmful aspects, thus making VR realizations more dangerous. Measuring harm-inducing factors reveals the risk potential of virtual gambling. In a user study, we analyze a slot machine realized as a desktop 3D and as an immersive VR version. Both versions are compared in respect to effects on dissociation, urge to gamble, dark flow, and illusion of control. Our study shows significantly higher values of dissociation, dark flow, and urge to gamble in the VR version. Presence significantly correlates with all measured harm-inducing factors. We demonstrate that VR-based gambling has a higher risk potential. This creates the importance of regulating VR-based gambling.},
  keywords={Games;Pathology;Virtual reality;Three-dimensional displays;Indexes;Visualization;Current measurement;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI;Human-centered computing—Interaction paradigms—Virtual Reality},
  doi={10.1109/VR.2019.8798021},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797977,
  author={Reinhardt, Daniel and Haesler, Steffen and Hurtienne, Jörn and Wienrich, Carolin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Entropy of Controller Movements Reflects Mental Workload in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={802-808},
  abstract={Virtual Reality can impose cognitive demands on users and influence their task performance. These cognitive demands, however, have been difficult to measure precisely without inducing breaks of presence. Based on findings in psychological science on how motion trajectories reflect underlying cognitive processes, we investigated entropy (i.e. the degree of movement irregularity) as an unobtrusive measure of mental workload. Entropy values were obtained from a time-series history of controller movement data. Mental workload is considered high over a given time interval, when the measured entropy is high as well. By manipulating the difficulty of a simple rhythm game we could show that the results are comparable to the results of the NASA-TLX questionnaire, which is currently used as the gold standard in VR for measuring mental workload. Thus, our results pave the way for further investigating the entropy of controller movements as a precise measurement of mental workload in VR.},
  keywords={Task analysis;Entropy;Physiology;Games;Current measurement;Atmospheric measurements;Particle measurements;Sample Entropy;entropy of controller movements;virtual reality;non-intrusive measure;evaluation method;mental workload;H.5.2 [Information Interfaces and Presentation (e.g., HCI)]: User Interfaces—Evaluation\methodology},
  doi={10.1109/VR.2019.8797977},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798029,
  author={Luong, Tiffany and Martin, Nicolas and Argelaguet, Ferran and Lécuyer, Anatole},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Studying the Mental Effort in Virtual Versus Real Environments}, 
  year={2019},
  volume={},
  number={},
  pages={809-816},
  abstract={Is there an effect of Virtual Reality (VR) Head-Mounted Display (HMD) on the user's mental effort? In this paper, we compare the mental effort in VR versus in real environments. An experiment (N=27) was conducted to assess the effect of being immersed in a virtual environment (VE) using an HMD on the user's mental effort while performing a standardized cognitive task (the well-known N-back task, with three levels of difficulty, N ∈ {1,2,3}). In addition to test the effect of the environment (i.e., virtual versus real), we also explored the impact of performing a dual task (i.e., sitting versus walking) in both environments on mental effort. The mental effort was assessed through self-report, task performance, behavioural and physiological measures. In a nutshell, the analysis of all measurements revealed no significant effect of being immersed in the VE on the users' mental effort. In contrast, natural walking significantly increased the users' mental effort. Taken together, our results support the fact there is no specific additional mental effort related to the immersion in a VE using a VR HMD.},
  keywords={Task analysis;Resists;Training;Legged locomotion;Physiology;Virtual environments;Visualization;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8798029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798158,
  author={Liu, Shi-Hong and Yu, Neng-Hao and Chan, Liwei and Peng, Yi-Hao and Sun, Wei-Zen and Chen, Mike Y.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={PhantomLegs: Reducing Virtual Reality Sickness Using Head-Worn Haptic Devices}, 
  year={2019},
  volume={},
  number={},
  pages={817-826},
  abstract={Virtual Reality (VR) sickness occurs when exposure to a virtual environment causes symptoms that are similar to motion sickness, and has been one of the major user experience barriers of VR. To reduce VR sickness, prior work has explored dynamic field-of-view modification and galvanic vestibular stimulation (GVS) that recou-ples the visual and vestibular systems. We propose a new approach to reduce VR sickness, called PhantomLegs, that applies alternating haptic cues that are synchronized to users' footsteps in VR. Our prototype consists of two servos with padded swing arms, one set on each side of the head, that lightly taps the head as users walk in VR. We conducted a three-session, multi-day user study with 30 participants to evaluate its effects as users navigate through a VR environment while physically being seated. Results show that our approach significantly reduces VR sickness during the initial exposure while remaining comfortable to users.},
  keywords={Visualization;Legged locomotion;Haptic interfaces;Resists;Servomotors;Virtual reality;Vibrations;Human-centered computing—Virtual Reality—Interaction techniques—Locomotion;Human-centered computing—Virtual Reality—Interaction techniques—Haptics},
  doi={10.1109/VR.2019.8798158},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798334,
  author={Jeong, Daekyo and Yoo, Sangbong and Yun, Jang},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cybersickness Analysis with EEG Using Deep Learning Algorithms}, 
  year={2019},
  volume={},
  number={},
  pages={827-835},
  abstract={Cybersickness is a symptom of dizziness that occurs while experiencing Virtual Reality (VR) technology and it is presumed to occur mainly by crosstalk between the sensory and cognitive systems. However, since the sensory and cognitive systems cannot be measured objectively, it is difficult to measure cybersickness. Therefore, methodologies for measuring cybersickness have been studied in various ways. Traditional studies have collected answers to questionnaires or analyzed EEG data using machine learning algorithms. However, the system relying on the questionnaires lacks objectivity, and it is difficult to obtain highly accurate measurements with the machine learning algorithms in previous studies. In this work, we apply and compare Deep Neural Network (DNN) and Convolutional Neural Network (CNN) deep learning algorithms for objective cy-bersickness measurement from EEG data. We also propose a data preprocessing for learning and signal quality weights allowing us to achieve high performance while learning EEG data with the deep learning algorithms. Besides, we analyze video characteristics where cybersickness occurs by examining the 360 video stream segments causing cybersickness in the experiments. Finally, we draw common patterns that cause cybersickness.},
  keywords={Electroencephalography;Deep learning;Motion measurement;Machine learning algorithms;Atmospheric measurements;Particle measurements;Emotion recognition;Human-centered computing—Virtual reality;Computing methodologies—Machine learning approaches},
  doi={10.1109/VR.2019.8798334},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798192,
  author={Adams, Haley and Noble, Jack and Morrel, William G. and Rivas, Alejandro and Shinn, Justin R. and Labadie, Robert and Bodenheimer, Bobby},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Play it by Ear: An Immersive Ear Anatomy Tutorial}, 
  year={2019},
  volume={},
  number={},
  pages={836-837},
  abstract={The anatomy of the ear and the bones surrounding it are intricate yet critical for medical professionals to know. Current best practices teach ear anatomy through two-dimensional representations, which poorly characterize the three-dimensional (3D), spatial nature of the anatomy and make it difficult to learn and visualize. In this work, we describe an immersive, stereoscopic visualization tool for the anatomy of the ear based on real patient data. We describe the interface and its construction. And we compare how well medical students learn ear anatomy in the simulation compared with more traditional learning methods. Our preliminary results suggest that virtual reality may be an effective tool for anatomy education in this context.},
  keywords={Ear;Biomedical imaging;Standards;Solid modeling;Virtual reality;Tutorials;virtual reality;medical imaging;visualization;spatial perception;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality;J.4 [Computer Applications]: Social and Behavioral Sciences—Psychology},
  doi={10.1109/VR.2019.8798192},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797981,
  author={Allmacher, Christoph and Dudczig, Manuel and Knopp, Sebastian and Klimant, Philipp},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality for Virtual Commissioning of Automated Guided Vehicles}, 
  year={2019},
  volume={},
  number={},
  pages={838-839},
  abstract={The development process of automated guided vehicles (AGV) can be supported by virtual commissioning, which comprises the validation of functionality using a simulation of the AGV. In case of collaborating AGVs, testing their functionalities require the simulation to contain not only the AGV but also a human and an interaction device. Therefore, this paper presents a setup, that integrates a human using motion capturing and emulates a smartwatch as interaction device. Furthermore, the simulation is visualized by a head-mounted display and provides additional information for the test case assessment and analysis. Thus, it enables the virtual commissioning of collaborative AGVs.},
  keywords={Software;Tracking;Solid modeling;Autonomous vehicles;Virtual environments;Production;Virtual commissioning;virtual reality;automated guided vehicle;motion capturing;Human-centered computing—Interaction design—Systems and tools for interaction design;Software and its engineering—Software creation and management—Software verification and validation},
  doi={10.1109/VR.2019.8797981},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797755,
  author={Andreasen, Niels Koch and Baceviciute, Sarune and Pande, Prajakt and Makransky, Guido},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Instruction Followed by Enactment Can Increase Procedural Knowledge in a Science Lesson}, 
  year={2019},
  volume={},
  number={},
  pages={840-841},
  abstract={A 2×2 between-subjects experiment (a) investigated and compared the instructional effectiveness of immersive virtual reality (VR) versus video as media for teaching scientific procedural knowledge, and (b) examined the efficacy of enactment as a generative learning strategy in combination with the respective instructional media. A total of 117 high school students (74 females) were randomly distributed across four instructional groups - VR and enactment, video and enactment, only VR, and only video. Outcome measures included declarative knowledge, procedural knowledge, knowledge transfer, and subjective ratings of perceived enjoyment. Results indicated that there were no main effects or interactions for the outcomes of declarative knowledge or transfer. However, there was a significant interaction between media and method for the outcome of procedural knowledge with the VR and enactment group having the highest performance. Furthermore, media also seemed to have a significant effect on student perceived enjoyment, indicating that the groups enjoyed the VR simulation significantly more than the video. The results deepen our understanding of how we learn with immersive technology, as well as suggest important implications for implementing VR in schools.},
  keywords={Media;Biological system modeling;Virtual reality;Education;Solid modeling;Psychology;Knowledge transfer;Virtual Reality;generative learning strategy;enactment;learning;procedural knowledge},
  doi={10.1109/VR.2019.8797755},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797785,
  author={Aoki, Hirooki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Occurrence of Pseudo-Haptics by Swimming in a Virtual Reality Environment}, 
  year={2019},
  volume={},
  number={},
  pages={842-843},
  abstract={This study focuses on the investigation of pseudo-haptics while swimming in a highly immersive virtual reality environment created using a head-mounted display to investigate the conditions under which pseudo-haptics occur while performing considerable physical exercises. When the users perform the outward sweep motion of the breaststroke, the spheres floating in the virtual reality space move toward the users. The experimental results confirm that pseudo-haptics can be controlled using this setup by adjusting the ratio of the amount of movement of users' hands and the amount of movement of the virtual reality spheres.},
  keywords={Virtual reality;Sports;Aerospace electronics;Visualization;Head-mounted displays;Haptic interfaces;Mice;Pseudo-haptics;virtual reality space;swimming motion},
  doi={10.1109/VR.2019.8797785},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797899,
  author={Bae, Byung-Chull and Jang, Su-ji and Ahn, Duck-Ki and Seo, Gapyuel},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A VR Interactive Story Using POV and Flashback for Empathy}, 
  year={2019},
  volume={},
  number={},
  pages={844-845},
  abstract={In this paper we introduce our ongoing project to design and implement an interactive storytelling in VR. Our particular design intent is to invoke narrative empathy through two narrative devices - change of POV (Point of View) and flashback. Focusing on narrative empathy, we explore design considerations for our project, including story, characters and objects, events and space of possibility, focalization (or POV), flashback and VR implementation.},
  keywords={Games;Coherence;Kernel;Presses;Virtual reality;Current measurement;Focusing;Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism—Virtual Reality},
  doi={10.1109/VR.2019.8797899},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797794,
  author={Baker, Lewis and Zollmann, Stefanie and Ventura, Jonathan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spherical Structure-from-Motion for Casual Capture of Stereo Panoramas}, 
  year={2019},
  volume={},
  number={},
  pages={846-847},
  abstract={Hand-held capture of stereo panoramas involves spinning the camera in a roughly circular path to acquire a dense set of views of the scene. However, most existing structure-from-motion pipelines fail when trying to reconstruct such trajectories, due to the small baseline between frames. In this work, we propose to use spherical structure-from-motion for reconstructing handheld stereo panorama captures. Our initial results show that spherical motion constraints are critical for reconstructing small-baseline, circular trajectories.},
  keywords={Cameras;Image reconstruction;Pipelines;Trajectory;Three-dimensional displays;Computer vision;Conferences;Human-centered computing—Interaction paradigms—Virtual Reality;Artificial intelligence—Computer vision—Computer vision problems—Tracking},
  doi={10.1109/VR.2019.8797794},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798323,
  author={Barberis, Adrian and Bennet, Trystan and Minear, Meredith},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={“Ready Player One”: Enhancing Omnidirectional Treadmills for Use in Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={848-849},
  abstract={As large-scale immersive virtual environments become more common, technologies develop to allow increasingly naturalistic travel in these spaces. One such technology, is the omnidirectional treadmill. Though the hardware has been realized, it still lacks a concrete software paradigm that ensures comfort and performance for the user. We identified three potentially key settings for omnidirectional treadmill locomotion in virtual reality (VR) environments that may lead to more comfort, increased degrees of freedom, and better performance: movement speed, treadmill sensor sensitivity, and decoupling the head's rotation from that of the body. To integrate these settings, we developed an original first-person movement script and conducted a prestudy to discover if enough variance between individuals existed to necessitate calibration on a per-individual basis. Initial prestudy results suggest that these three values do show enough variance to recommend they be calibrated on a per-individual basis and opens up the opportunity for further research into the potential benefits of these variables on locomotion in environments that use omnidirectional treadmills.},
  keywords={Virtual environments;Tracking;Cameras;Calibration;Smoothing methods;Software;Virtual reality;Interaction design;Software;Computer applications},
  doi={10.1109/VR.2019.8798323},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798290,
  author={Becker, Jonathan and Meyer, Uli and Eichler, Tobias and Draheim, Susanne},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Supernatural VR Environment for Spatial User Rotation}, 
  year={2019},
  volume={},
  number={},
  pages={850-851},
  abstract={VR environments with supernatural properties which expand or replace the laws of physics could be used to understand how the brain organises and interprets sensory stimulation. We built an application with a supernatural room that allows users to walk up the wall and on the ceiling. During preliminary tests, we optimised the application so that it rarely causes cybersickness. User reports and observed user reaction such as swaying indicate that users accepted the rotation as a self-rotation, as opposed to an animated rotation of the room around the user. Therefore the application is viable for future studies on spatial orientation, pathfinding and cognitive maps.},
  keywords={Gravity;Visualization;Three-dimensional displays;User interfaces;Foot;Tracking;Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8798290},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798237,
  author={Bian, Yulong and Zhou, Chao and Chen, Yeqing and Zhao, Yanshuai and Liu, Juan and Yang, Chenglei and Meng, Xiangxu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Explore the Weak Association between Flow and Performance Based on a Visual Search Task Paradigm in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={852-853},
  abstract={Weak association model indicates that distraction caused by the disjunction between the primary task and interactive artifacts may be a key factor directly leading to weak association between flow and task performance in virtual reality (VR) activities. To test the idea, this paper proposed a VR visual search paradigm based on which we constructed a VR oceanic treasure hunting system. Experiment 1 proved that distraction caused by the incongruence was indeed a direct antecedent of weak association. Next, we slightly adjusted the system by providing visual cues to achieve task-oriented selective attention. Experiment 2 found this helped enhance the task performance without damaging flow experience.},
  keywords={Task analysis;Visualization;Virtual reality;Solid modeling;Human factors;Resource management;Atmospheric measurements;Flow experience;weak association;virtual reality;visual search task design guideline;H.1.2. [Models and Principles]: User/Machine Systems — Human factors; Introduction},
  doi={10.1109/VR.2019.8798237},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798165,
  author={Blackwell, Lindsay and Ellison, Nicole and Elliott-Deflo, Natasha and Schwartz, Raz},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Harassment in Social VR: Implications for Design}, 
  year={2019},
  volume={},
  number={},
  pages={854-855},
  abstract={We interviewed VR users (n=25) about their experiences with harassment, abuse, and discomfort in social YR. We find that users' definitions of `online harassment' are subjective and highly personal, making it difficult to govern social spaces at the platform or application level. We also find that embodiment and presence make harassment feel more intense. Finally, we find that shared norms for appropriate behavior in social VR are still emergent, and that users distinguish between newcomers who unknowingly violate expectations for appropriateness and those users who aim to cause intentional harm.},
  keywords={Avatars;Social networking (online);Human computer interaction;Real-time systems;Cultural differences;Interviews;online harassment;social VR;embodiment;presence;moderation;CCS → Human-centered computing → Human computer interaction (HCI) → Empirical studies in HCI},
  doi={10.1109/VR.2019.8798165},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797679,
  author={Blanpied, Evan and Good, Jessica and Peck, Tabitha},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shooter Bias and Socioeconomic Status in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={856-857},
  abstract={This poster details an ongoing experiment aiming to test the prevalence of shooter bias in virtual reality. Further, we examine the interaction between shooter bias and perceived socioeconomic status. Even though shooter bias is a well-documented topic in psychology research, little experimentation has used virtual reality, instead opting for unrealistic two-dimensional simulations. This study will yield new insight into shooter bias, especially concerning virtual reality as a tool for use in future work, and will provide new understanding about the relationship between socioeconomic status and shooter bias.},
  keywords={Avatars;Psychology;Solid modeling;Games;Weapons;Law enforcement;Shooter bias—Immersion—Presence—Video games Violence—Racial bias—Threat perception},
  doi={10.1109/VR.2019.8797679},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798214,
  author={Boem, Alberto and Enzaki, Yuuki and Yano, Hiroaki and Iwata, Hiroo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Human Perception of a Haptic Shape-changing Interface with Variable Rigidity and Size}, 
  year={2019},
  volume={},
  number={},
  pages={858-859},
  abstract={This paper studies the characteristics of the human perception of a haptic shape-changing interface, capable of altering its size and rigidity simultaneously for presenting characteristics of virtual objects physically. The haptic interface is composed of an array of computer-controlled balloons, with two mechanisms, one for changing size and one for changing rigidity. We manufactured two balloons and conducted psychophysical experiments with twenty subjects to measure perceived sensory thresholds and haptic perception of the change of size and rigidity. The results show that subjects can correctly discriminate different conditions with an acceptable level of accuracy. Our results also suggest that the proposed system can present an ample range of rigidities and variations of the size in a way that is compatible with the human haptic perception of physical materials. Currently, shape-changing interfaces do not hold a defined position in the current VR / AR research. Our results provide basic knowledge for developing novel types of haptic interfaces that can enhance the haptic perception of virtual objects, allowing rich embodied interactions, and synchronize the virtual and the physical world through computationally-controlled materiality.},
  keywords={Haptic interfaces;Indexes;Virtual reality;Measurement;Task analysis;Informatics;Human computer interaction;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Haptic devices'Treemaps;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI},
  doi={10.1109/VR.2019.8798214},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798238,
  author={Boustila, Sabah and Guégan, Thomas and Takashima, Kazuki and Kitamura, Yoshifumi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text Typing in VR Using Smartphones Touchscreen and HMD}, 
  year={2019},
  volume={},
  number={},
  pages={860-861},
  abstract={In this work, we were interested in using smartphone touchscreen keyboard for text typing in virtual environments (VEs) with head-mounted display. We carried out an experiment comparing the smartphone to the ordinary devices: gamepad and HTC Vive Controllers. We represented the touchscreen keyboard in the VE with a virtual interface and the fingertips with tracked green circles. A confirm-on-release paradigm was employed for text typing. Results showed that the smartphone did not fully outperformed the other devices. However, unlike the other devices, smartphones users tended to correct progressively their error while typing thanks to their familiarity with the device.},
  keywords={Smart phones;Keyboards;Error analysis;Resists;Virtual reality;Prototypes;Writing;Human-centered computing—Virtual reality—Touch screens;Human-centered computing—User interface design},
  doi={10.1109/VR.2019.8798238},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797980,
  author={Boustila, Sabah and Zheng, Hao and Shuxia, Bai and Takashima, Kazuki and Kitamura, Yoshifumi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Scalable WorkSpace Based on Virtual and Physical Movable Wall}, 
  year={2019},
  volume={},
  number={},
  pages={862-863},
  abstract={We propose an approach for flexibly scalable workspace that changes viewer's perceived size of the workspace by simulating a virtually movable wall on the real wall. We conducted an experiment comparing viewers depth estimates between extended workspace virtually and physically. Besides, we investigated walls motion effects; discrete wall motion or continuous wall motion. Distances were estimated verbally and using perceptual matching method. Verbal depth estimates were equivalent in the virtual extended workspace and the physical one, with a high accuracy (estimation errors were less than 5%). However, perceptual matching estimates were signicantly different between virtual and physical scaling. Overall, they were slightly overestimated. No difference of the motion effect was found. Our results clearly suggest the potential of the workspace scaling by just giving such visual impression of extended workspace using perspective projection technique.},
  keywords={Visualization;Meters;Estimation error;Task analysis;Games;Conferences;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797980},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798303,
  author={Brito, Caio and Mitchell, Kenny},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Repurposing Labeled Photographs for Facial Tracking with Alternative Camera Intrinsics}, 
  year={2019},
  volume={},
  number={},
  pages={864-865},
  abstract={Acquiring manually labeled training data for a specific application is expensive and while such data is often fully available for casual camera imagery, it is not a good fit for novel cameras. To overcome this, we present a repurposing approach that relies on spherical image warping to retarget an existing dataset of landmark labeled casual photography of people's faces with arbitrary poses from regular camera lenses to target cameras with significantly different intrinsics, such as those often attached to the head mounted displays (HMDs) with wide-angle lenses necessary to observe mouth and other features at close proximity and infrared only sensing for eye observations. Our method can predict landmarks of the HMD wearer in facial sub-regions in a divide-and-conquer fashion with particular focus on mouth and eyes. We demonstrate animated avatars in realtime using the face landmarks as input without user-specific nor application-specific dataset.},
  keywords={Cameras;Mouth;Resists;Shape;Lips;Lenses;Distortion;Real-time facial performance capture;virtual reality;head-mounted display;retargeting;Computing methodologies—Motion Capture—HMDs;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality},
  doi={10.1109/VR.2019.8798303},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798216,
  author={Caluya, Nicko R. and Santos, Marc Ericson C.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Kantenbouki VR: A Virtual Reality Authoring Tool for Learning Localized Weather Reporting}, 
  year={2019},
  volume={},
  number={},
  pages={866-867},
  abstract={Localized weather reporting based on human observation is a skill practiced by fishermen, seafarers, airport ground staff, among others. Weather reporting largely depends on accurately identifying cloud types, and judging visibility and distances. This skill is developed by first-hand experience of various weather phenomena. As such, beginners would have difficulty practicing this reporting task without experiencing the phenomena, without proper guidance of experts, and without appropriate tools to support their learning. We present Kantenbouki VR, a virtual reality authoring tool, which can solve various issues in the process of learning weather reporting based on human observation. Using a 360° camera capture as an image background in the application, users can create annotations via freeform drawings accompanied by a cloud type label. To test our idea, we conducted an exploratory design study. Observations and results show that users learned how to use the system quickly, and approached the annotation tasks differently. Features that can be improved include the fidelity of the cloud images, as well as mapping of actions to controller buttons.},
  keywords={Meteorology;Cloud computing;Task analysis;Virtual reality;Authoring systems;Tools;Three-dimensional displays;H.5.1. Information Interfaces and Presentation: Multimedia Information Systems Artificial, augmented, and virtual realities;H.5.2. Information Interfaces and Presentation: Multimedia Information Systems Ergonomics, Evaluation/methodology, Theory and methods},
  doi={10.1109/VR.2019.8798216},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797864,
  author={Calvert, James and Abadia, Rhodora and Tauseef, Syed Mohammad},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Testing of a Virtual Reality Enabled Experience that Enhances Engagement and Simulates Empathy for Historical Events and Characters}, 
  year={2019},
  volume={},
  number={},
  pages={868-869},
  abstract={Our study uses Virtual Reality (VR) to transport high school students into the mountains of Papua New Guinea during World War Two - the Kokoda campaign - to witness first-hand the hardships faced by both Australian and Japanese soldiers. By using photogrammetry of real locations and artefacts, in combination with animated characters, Kokoda VR places the students in the centre of the action. Results from data collected in two Australian high schools has shown that a linear narrative in the VR condition increases feelings of empathy for the soldiers, over the 360° video desktop application. Students using VR also reported higher levels of engagement than students using 360° video and the study found a positive correlation between high engagement and increased empathy in VR.},
  keywords={Virtual reality;Education;Australia;History;Production;Testing;Correlation;Human-centered computing → Virtual reality;Human-centered computing → User studies},
  doi={10.1109/VR.2019.8797864},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797958,
  author={Casarin, Julien and Ladrech, Antoine and Tchilinguirian, Tristan and Bechmann, Dominique},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A UMI3D-based Interactions Analytics System for XR Devices and Interaction Techniques}, 
  year={2019},
  volume={},
  number={},
  pages={870-871},
  abstract={In this paper we present an interaction analytics system we are working on. With this system we intend to simplify the evaluation and classification of eXtented Reality devices and interaction techniques. Our final objective is to release it as an open Cloud platform that will allow researchers to compare their respective results in the field of Human-Computer Interaction with ease. To achieve this, we use the UMI3D exchange protocol to design device-independent 3D environments, and a Cloud analytics platform which stores experimental raw data from these environments as well as from the different devices. Finally, we present our first results using the platform to compute statistical analyzes and create dashboards comparing a set of devices and interaction techniques.},
  keywords={Task analysis;Three-dimensional displays;Protocols;Cloud computing;Performance evaluation;Real-time systems;H.3.5 [Online Information Services]: Data sharing;I.3.6 [Methodology and Techniques]: Interaction techniques;H.5.2 [User Interfaces]: Evaluation/methodology;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8797958},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798148,
  author={Cavallo, Marco and Forbes, Angus G.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={CAVE-AR: A VR Authoring System to Interactively Design, Simulate, and Debug Multi-user AR Experiences}, 
  year={2019},
  volume={},
  number={},
  pages={872-873},
  abstract={Despite advances in augmented reality (AR), the process of creating meaningful experiences with this technology is still extremely challenging. Due to different tracking implementations and hardware constraints, developing AR applications either requires low-level programming skills, or is done through specific authoring tools that largely sacrifice the possibility of customizing the AR experience. Existing development workflows also do not support previewing or simulating the AR experience, requiring a lengthy process of trial and error by which content creators deploy and physically test applications in each iteration. To mitigate these limitations, we propose CAVE-AR, a novel virtual reality system for authoring, simulating and debugging custom augmented reality experiences. Available both as a standalone or a plug-in tool, CAVE-AR is based on the concept of representing in the same global reference system both in AR content and tracking information, mixing geographical information, architectural features, and sensor data to simulate the context of an AR experience. Thanks to its novel abstraction of existing tracking technologies, CAVE-AR operates independently of users' devices, and integrates with existing programming tools to provide maximum flexibility. Our VR application provides designers with ways to create and modify an AR application, even while others are in the midst of using it. CAVE-AR further allows the designer to track how users are behaving, preview what they are currently seeing, and interact with them through several different channels. To illustrate our proposed development workflow and demonstrate the advantages of our authoring system, we introduce two CAVE-AR use cases in which an augmented reality application is created and tested. In particular, we compare the CAVE-AR workflow to traditional development methods and demonstrate the importance of simulation and live application debugging.},
  keywords={Authoring systems;Augmented reality;Debugging;Hardware;Tools;Mobile handsets;Human-centered computing—Human computer interaction (HCI)—Interactive systems and tools;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality},
  doi={10.1109/VR.2019.8798148},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797849,
  author={Cepok, J. and Arzaroli, R. and Marnholz, K. and Große, C. S. and Reuter, H. and Nelson, K. and Lorenz, M. and Weller, R. and Zachmann, G.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of VR on Intentions to Change Environmental Behavior}, 
  year={2019},
  volume={},
  number={},
  pages={874-875},
  abstract={We present a study investigating the question whether and how people's intention to change their environmental behavior depends on the degrees of immersion and freedom of navigation when they experience a virtual coral reef. The most striking result is, perhaps, that the highest level of immersion combined with the highest level of navigation did not lead to the highest intentions to change behavior.},
  keywords={Navigation;Mediation;Virtual reality;Ecosystems;Analysis of variance;Media;Animals},
  doi={10.1109/VR.2019.8797849},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797868,
  author={Chen, Hui and Ballal, Tarig and Al-Naffouri, Tareq},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Decomposition Approach for Complex Gesture Recognition Using DTW and Prefix Tree}, 
  year={2019},
  volume={},
  number={},
  pages={876-877},
  abstract={Gestures are effective tools for expressing emotions and conveying information to the environment. Sequence matching and machine-learning based algorithm are two main methods to recognize continuous gestures. Machine-learning based recognition systems are not flexible to new gestures because the models have to be trained again. On the other hand, the computational time that matching methods required increases with the complexity and the class of the gestures. In this work, we propose a decomposition approach for complex gesture recognition utilizing DTW and prefix tree. This system can recognize 100 gestures with an accuracy of 97.38%.},
  keywords={Gesture recognition;Solid modeling;Feature extraction;Classification algorithms;Indexes;Computational complexity;Prediction algorithms;Gesture Recognition;Time Sequence;DTW;Prefix Tree;Human-machine-interaction;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—Gestural input},
  doi={10.1109/VR.2019.8797868},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798146,
  author={Chen, Weiya and Hu, Yangliu and Ladevèze, Nicolas and Bourdot, Patrick},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Quick Estimation of Detection Thresholds for Redirected Walking with Method of Adjustment}, 
  year={2019},
  volume={},
  number={},
  pages={878-879},
  abstract={A method that allows quick estimation of Redirection Detection Thresholds (RDTs) is not only useful for identifying factors that contribute to the detection of redirections, but can also provide timely inputs for personalized redirected walking control. In aim to achieve quick RDT estimation, we opted for a classical psychophysical method - the Method of Adjustment (MoA), and compared it against commonly used method for RDT estimation (i.e. MCS-2AFC) to see their difference. Preliminary results show that MoA allows to save about 33% experiment time when compared with MCS-2AFC while getting overall similar RDT estimations on the same population.},
  keywords={Estimation;Legged locomotion;Task analysis;Visualization;Standards;Sociology;Statistics;Redirected Walking;Detection Threshold;Method of Adjustment;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798146},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798142,
  author={Cho, Hochul and Kim, Jangyoon and Woo, Woontack},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Novel View Synthesis with Multiple 360 Images for Large-Scale 6-DOF Virtual Reality System}, 
  year={2019},
  volume={},
  number={},
  pages={880-881},
  abstract={We present a novel view synthesis method that allows users to experience a large-scale Six-Degree-of-Freedom (6-DOF) virtual environment. Our main contributions are the construction of a large-scale 6-DOF virtual environment using multiple 360 images as well as synthesis of a scene from novel viewpoints. Novel view synthesis from a single 360 image can give free viewpoint experience with full 6-DOF of head motion to players, but the moveable space is limited within a context of the image. We propose a novel view synthesis process that references multiple 360 images via reconstructing a large-scale real world based virtual data map and perform a weighted blending for interpolating multiple novel view images. Our results show that our approach provides a wider area of virtual environment as well as a smooth transition between each reference 360 images.},
  keywords={Virtual environments;Three-dimensional displays;Cameras;Head;Buildings;Image quality;Computing methodologies—Computer graphics—Image manipulation—Image-based rendering},
  doi={10.1109/VR.2019.8798142},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797741,
  author={Choi, Kup-Sze},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Wound Care Training for Clinical Nursing Education: An Initial User Study}, 
  year={2019},
  volume={},
  number={},
  pages={882-883},
  abstract={Wound care is an essential nursing competency, where dressing change is an important component. Compliance with aseptic procedures and techniques is necessary to reduce the risk of infection. Proficiency in the skills can be developed through adequate practice. In this paper, use of virtual reality is proposed to provide more practice opportunity. An immersive virtual environment is developed to simulate the steps of changing a simple wound dressing. Positive comments are obtained from an initial user study on usability with an experienced nurse and an undergraduate nursing student. Comprehensive evaluation will be conducted to further improve the simulation.},
  keywords={Wounds;Training;Solid modeling;Virtual environments;Virtual reality;wound care;dressing change;nursing education;I.3.8 [Computer Graphics]: Applications;I.6.3 [Simulation and Modeling]: Applications;J.3 [Life and Medical Sciences]—Health;K.3.1 [Computers and Education]: Computer Uses in Education—Computer-assisted instruction},
  doi={10.1109/VR.2019.8797741},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798228,
  author={Chun, Wooyoung and Choi, Gyujin and An, Jaepung and Seo, Woong and Park, Sanghun and Ihm, Insung},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={On Sharing Physical Geometric Space between Augmented and Virtual Reality Environments}, 
  year={2019},
  volume={},
  number={},
  pages={884-885},
  abstract={Despite the expected synergistic effects, augmented and virtual reality (AR and VR, respectively) technologies still tend to be discrete entities. In this paper, we describe our effort to enable both AR and VR users to share the same physical geometric space. The geometric transformation between two world spaces, defined separately by AR and VR systems, are estimated using a specially designed tracking board. Once initially obtained, the transformation will allow users to collaborate with each other within an integrated physical environment while making the best use of both AR and VR technologies.},
  keywords={Geometry;Virtual reality;Estimation;Solid modeling;Three-dimensional displays;Visualization;Aerospace electronics;Human-centered computing—Human computer interaction—Interaction paradigms—Mixed/augmented reality},
  doi={10.1109/VR.2019.8798228},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797814,
  author={Daher, Salam and Hochreiter, Jason and Norouzi, Nahal and Schubert, Ryan and Bruder, Gerd and Gonzalez, Laura and Anderson, Mindi and Diaz, Desiree and Cendan, Juan and Welch, Greg},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Matching vs. Non-Matching Visuals and Shape for Embodied Virtual Healthcare Agents}, 
  year={2019},
  volume={},
  number={},
  pages={886-887},
  abstract={Embodied virtual agents serving as patient simulators are widely used in medical training scenarios, ranging from physical patients to virtual patients presented via virtual and augmented reality technologies. Physical-virtual patients are a hybrid solution that combines the benefits of dynamic visuals integrated into a human-shaped physical form that can also present other cues, such as pulse, breathing sounds, and temperature. Sometimes in simulation the visuals and shape do not match. We carried out a human-participant study employing graduate nursing students in pediatric patient simulations comprising conditions associated with matching/non-matching of the visuals and shape.},
  keywords={Visualization;Shape;Pediatrics;TV;Training;Medical services;Load modeling;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality},
  doi={10.1109/VR.2019.8797814},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797922,
  author={Dai, Feng and Zhu, Chen and Ma, Yike and Cao, Juan and Zhao, Qiang and Zhang, Yongdong},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Freely Explore the Scene with 360° Field of View}, 
  year={2019},
  volume={},
  number={},
  pages={888-889},
  abstract={By providing 360° field of view, spherical panoramas are widely used in virtual reality (VR) systems and street view services. However, due to bandwidth or storage limitations, existing systems only provide sparsely captured panoramas and have limited interaction modes. Although there are methods that can synthesize novel views based on captured panoramas, the generated novel views all lie on the lines connecting existing views. Therefore these methods do not support free viewpoint navigation. In this paper, we propose a new panoramic image based rendering method. Our method takes pre-captured images as input and can synthesize panoramas at novel views that are far from input camera positions. Thus, it supports to freely explore the scene with 360° field of view.},
  keywords={Rendering (computer graphics);Cameras;Three-dimensional displays;Image reconstruction;Structure from motion;Real-time systems;Navigation;Computing methodologies—Computer graphics—Image manipulation—Image-based rendering;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality},
  doi={10.1109/VR.2019.8797922},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798264,
  author={De Simone, Francesca and Li, Jie and Debarba, Henrique Galvan and Ali, Abdallah El and Gunkel, Simon N.B and Cesar, Pablo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Watching Videos Together in Social Virtual Reality: An Experimental Study on User's QoE}, 
  year={2019},
  volume={},
  number={},
  pages={890-891},
  abstract={In this paper, we describe a user study in which pairs of users watch a video trailer and interact with each other, using two social Virtual Reality (sVR) systems, as well as in a face-to-face condition. The sVR systems are: Facebook Spaces, based on puppet-like customized avatars, and a video-based sVR system using photo-realistic virtual user representations. We collect subjective and objective data to analyze users' Quality of Experience (QoE) and compare their interaction in VR to that observed during the real-life scenario. Our results show that the experience delivered by the video-based sVR system is comparable with real-life settings, while the puppet-based avatars limit the perceived quality of the interaction. Our protocol for QoE assessment is fully documented to allow replication in similar experiments.},
  keywords={Avatars;Videos;Resists;Quality of experience;Facebook;Face;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology},
  doi={10.1109/VR.2019.8798264},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797918,
  author={Dong, Jiahui and Zhang, Jun and Ma, Xiao and Ren, Pengyu and Qian, Zhenyu Cheryl and Chen, Yingjie Victor},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Training with Passive Haptic Feedback for CryoEM Sample Preparation}, 
  year={2019},
  volume={},
  number={},
  pages={892-893},
  abstract={We present an immersive virtual reality training system cryoVR with passive haptic feedback for training biological scientists preparing bio-sample for Cryo-Electron Microscopy (CryoEM). CryoEM requires the user to conduct careful operations on expensive delicate equipment. To minimize the risk and interruption of crucial research work, we tried to mimic the real lab using VR to let trainer practice in a virtual environment. We used 3D printed objects to provide passive haptic feedback in order to achieve a more realistic training experience. Participants are able to interact with the equipment in the virtual environment by moving or touching physical models. We developed all the necessary operations with haptic feedback, including moving, clicking, pouring, rotating, polling and pushing. By following instructions provided by our virtual reality simulator and interacting with physical objects, trainees will learn how to operate CryoEM with low cost and risk. By developing our training system, we explore the benefits, limitations and precautions of embedding haptic feedback to scientific VR training.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Medical services;Education;Virtual reality;haptic feedback;CP3;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality Haptic feedback},
  doi={10.1109/VR.2019.8797918},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798319,
  author={Dong, Tianyang and Song, Yifan and Shen, Yuqi and Fan, Jing},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulation and Evaluation of Three-User Redirected Walking Algorithm in Shared Physical Spaces}, 
  year={2019},
  volume={},
  number={},
  pages={894-895},
  abstract={Shifting from single-person experiences to multi-user interactions is an inevitable trend of virtual reality technology. Existing methods primarily address the problem of one- or two-user redirected walking and do not respond to additional challenges related to potential collisions among three or more users who are moving both virtually and physically. To apply redirected walking to multiple users who are immersed in virtual reality experiences, we present a novel algorithm of three-user redirected walking in shared physical spaces. In addition, we present the steps to apply three-user redirected walking to multiplayer VR scene, where the users are divided into different groups based on the users' motion states. Therefore, this strategy can be applied to each group to address the challenges of redirected walking when there are more than three users. The results show that sharing a space using our three-user redirected walking algorithm is completely feasible.},
  keywords={Legged locomotion;Collision avoidance;Navigation;Virtual environments;Computer simulation;Approximation algorithms;Virtual Reality;Redirected Walking;Multi-user;Collision Avoidance;Locomotion;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Virtual Reality;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques},
  doi={10.1109/VR.2019.8798319},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798344,
  author={Dorado, José and Figueroa, Pablo and Chardonnet, Jean-Rémy and Merienne, Frédéric and Hernández, Tiberio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perceived Space and Spatial Performance during Path-Integration Tasks in Consumer-Oriented Virtual Reality Environments}, 
  year={2019},
  volume={},
  number={},
  pages={896-897},
  abstract={Studies using virtual reality environments (VE) have shown that subjects can perform path integration tasks with acceptable performance. However, in these studies, subjects could walk naturally across large tracking areas, or researchers provided them with large- immersive displays. Unfortunately, these configurations are far from current consumer-oriented VEs (COVEs), and little is known about how their limitations influence this task. Using a triangle completion paradigm, we assessed the subjects' spatial performance when developing path integration tasks in two consumer-oriented displays (an HTC Vive and a GearVR) and two consumer-oriented interaction devices (a Virtuix Omni motion platform and a Touchpad Control). Our results show that when locomotion is available (motion platform condition), there exist significant effects regarding the display and the path. In contrast, when locomotion is mediated no effect was found. Some future research directions are therefore proposed.},
  keywords={Task analysis;Optical imaging;Adaptive optics;Optical sensors;Three-dimensional displays;Navigation;Virtual reality;I.3.7 [Computer Graphics]: Three—Dimensional Graphics and Realism—Virtual reality},
  doi={10.1109/VR.2019.8798344},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798197,
  author={Drogemuller, Adam and Verhulst, Adrien and Volmer, Benjamin and Thomas, Bruce H. and Inami, Masahiko and Sugimoto, Maki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Remapping a Third Arm in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={898-899},
  abstract={This paper presents development on a conceptual method to remap supernumerary limbs using Virtual Reality (VR) as a platform for experimentation. Our VR system allows users to control a third arm through their own limbs such as their head, arms, and feet with the ability to switch between them. To realize and experiment with our remapping method, we used the Oculus Rift in conjunction with OptiTrack to track users in a room-scaled virtual environment. We present some initial findings from a small pilot study and conclude with suggestions for future work.},
  keywords={Switches;Task analysis;Foot;Legged locomotion;Virtual reality;Australia;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Supernumerary Limbs;Third Arm},
  doi={10.1109/VR.2019.8798197},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798187,
  author={Du, Ruofei and Li, David and Varshney, Amitabh},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interactive Fusion of 360° Images for a Mirrored World}, 
  year={2019},
  volume={},
  number={},
  pages={900-901},
  abstract={Reconstruction of the physical world in real time has been a grand challenge in computer graphics and 3D vision. In this paper, we introduce an interactive pipeline to reconstruct a mirrored world at two levels of detail. Given a pair of latitude and longitude coordinates, our pipeline streams and caches depth maps, street view panoramas, and building polygons from Google Maps and OpenStreetMap APIs. At a fine level of detail for close-up views, we render textured meshes using adjacent local street views and depth maps. When viewed from afar, we apply projection mappings to 3D geometries extruded from building polygons for a coarse level of detail. In contrast to teleportation, our system allows users to virtually walk through the mirrored world at the street level. We present an application of our approach by incorporating it into a mixed-reality social platform, Geollery, and validate our real-time strategies on various platforms including mobile phones, workstations, and head-mounted displays.},
  keywords={Three-dimensional displays;Image reconstruction;Buildings;Google;Geometry;Virtual reality;Pipelines;virtual reality;360° image;3D reconstruction;mixed reality;projection mapping;mirrored world;I.3.3 [Computer Graphics]: Image Manipulation—Image-based rendering;I.4.6 [Computer Graphics]: Graphics Systems and Interfaces—Virtual realities},
  doi={10.1109/VR.2019.8798187},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797924,
  author={Du, Ruofei and Lee, Eric and Varshney, Amitabh},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tracking-Tolerant Visual Cryptography}, 
  year={2019},
  volume={},
  number={},
  pages={902-903},
  abstract={We introduce a novel secure display system, which uses visual cryptography [4] with tolerance for tracking. Our system brings cryptographic privacy from text to virtual worlds [3]. Much like traditional encryption that uses a public key and a private key, our system uses two images that are both necessary for visual decryption of the data. The public image could be widely shared on a printed page, on a traditional display (desktop, tablet, or smartphone), or in a multi-participant virtual world, while the other private image can be exclusively on a user's personal AR or VR display. Only the recipient is able to visually decrypt the data by fusing both images. In contrast to prior art, our system is able to provide tracking tolerance, making it more practically usable in modern VR and AR systems. We model the probability of misalignment caused by head or body jitter as a Gaussian distribution. Our algorithm diffuses the second image using the normalized probabilities, thus enabling the visual cryptography to be tolerant of alignment errors due to tracking.},
  keywords={Visualization;Cryptography;Head;Jitter;Gaussian distribution;Two dimensional displays;Kernel;visual cryptography;augmented reality (AR);tracking;H.5.1 [Information Interfaces and Presentation (e.g., HCI)]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms},
  doi={10.1109/VR.2019.8797924},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798200,
  author={Du, Shengyu and Ge, Ting and Pei, Jingyi and Wang, Jianmin and Yin, Changqing and Zhu, Yongning},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Panoramic Fluid Painting}, 
  year={2019},
  volume={},
  number={},
  pages={904-905},
  abstract={The dynamic motion of fluids is essential in generating aesthetically appealing effects like the oriental ink painting and fluid art. Due to the prohibitively high cost required in volumetric fluid simulations, implementing an interactive volumetric painting system in immersive virtual environments (IVEs) is challenging. We propose a framework to generate immersive fluid dynamic environments by solving the Navier-Stokes equation on the viewing sphere. With this approach, we largely reduce the complexity without losing the effective resolution. We demonstrate our method on a real-time 360-degree painting system and verify the usability of our interface prototype with examples.},
  keywords={Painting;Brushes;Dynamics;Computational modeling;Solid modeling;Paints;Ink;Fluid simulation;head-mounted display;panoramic image;spherical coordinate;dynamic painting;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Animation;Physical simulation;Human-centered computing;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/VR.2019.8798200},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798073,
  author={Duncan, Stuart and Regenbrecht, Holger and Langlotz, Tobias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Fast Multi-RGBD-Camera Calibration}, 
  year={2019},
  volume={},
  number={},
  pages={906-907},
  abstract={Calibrating multiple depth cameras to a common coordinate space can be a laborious and time-consuming task, and often relies on expensive motion capture tracking systems. However, if accuracy constraints can be relaxed then a consumer-grade virtual reality tracking system is sufficient to support the calibration process. In this paper, we present a fast and convenient camera calibration method aimed at such systems. The calibration process can be carried out in about ten minutes and the resulting calibration is sufficiently accurate to achieve spatial coherence when targeting reconstructions on an 8 mm grid over a ≈ 2.2m3 capture volume.},
  keywords={Calibration;Cameras;Three-dimensional displays;Virtual reality;Aerospace electronics;Spatial coherence;Tracking;Computer Vision;Virtual Reality;Mixed Reality;3D Reconstruction},
  doi={10.1109/VR.2019.8798073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797969,
  author={de Dinechin, Gregoire Dupont and Paljic, Alexis},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Generation of Interactive 3D Characters and Scenes for Virtual Reality from a Single-Viewpoint 360-Degree Video}, 
  year={2019},
  volume={},
  number={},
  pages={908-909},
  abstract={This work addresses the problem of using real-world data captured from a single viewpoint by a low-cost 360-degree camera to create an immersive and interactive virtual reality scene. We combine different existing state-of-the-art data enhancement methods based on pre-trained deep learning models to quickly and automatically obtain 3D scenes with animated character models from a 360-degree video. We provide details on our implementation and insight on how to adapt existing methods to 360-degree inputs. We also present the results of a user study assessing the extent to which virtual agents generated by this process are perceived as present and engaging.},
  keywords={Three-dimensional displays;Solid modeling;Adaptation models;Virtual reality;Cameras;Shape;Pose estimation;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Computing methodologies—Computer graphics—Animation—Motion capture},
  doi={10.1109/VR.2019.8797969},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798006,
  author={Durai, Varun S.I. and Arjunan, Raj and Manivannan, M.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Audio and Visual Modality Based CPR Skill Training with Haptics Feedback in VR}, 
  year={2019},
  volume={},
  number={},
  pages={910-911},
  abstract={The hypothesis of this study is to verify the sensory dominance with the combinations of three sensory modalities (Audio-Haptics (AH), Visual-Haptics (VH), Audio-Visual-Haptics (AVH)) using Virtual Reality (VR) based Cardiopulmonary Resuscitation (CPR) simulator. To test this hypothesis three experiments with three different groups of participants were conducted with the above three modes of combinations. Finally, three groups were tested for their CPR performance on an unknown linear chest stiffness of mannequin-based CPR simulator and their performance score was compared. The % mean and standard deviation of the performance score (p-value: 0.00006) in the testing phase for group A-AH, B-VH, and C-AVH is 77.95%±8.27%, 89.47%±6.19%, and 91.73%±3.14% respectively. The results show that the group who trained with the three sensory modalities have better performance than that of the other two groups. Our future work is to incorporate rescue breathing in the CPR training simulator for better skill training.},
  keywords={Visualization;Training;Haptic interfaces;Cardiac arrest;Force;Testing;Heart;Cardio Pulmonary Resuscitation (CPR);mannequin;haptics;virtual reality;sensory modalities},
  doi={10.1109/VR.2019.8798006},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797887,
  author={Englmeier, David and Schönewald, Isabel and Butz, Andreas and Höllerer, Tobias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sphere in Hand: Exploring Tangible Interaction with Immersive Spherical Visualizations}, 
  year={2019},
  volume={},
  number={},
  pages={912-913},
  abstract={The emerging possibilities of data analysis and exploration in virtual reality raise the question of how users can be best supported during such interactions. Spherical visualizations allow for convenient exploration of certain types of data. Our tangible sphere, exactly aligned with the sphere visualizations shown in VR, implements a very natural way of interaction and utilizes senses and skills trained in the real world. This work is motivated by the prospect to create in VR a low-cost, tangible, robust, handheld spherical display that would be difficult or impossible to implement as a physical display. Our concept enables it to gain insights about the impact of a fully tangible embodiment of a virtual object on task performance, comprehension of patterns, and user behavior. After a description of the implementation we discuss the advantages and disadvantages of our approach, taking into account different handheld spherical displays utilizing outside and inside projection.},
  keywords={Three-dimensional displays;Visualization;Virtual reality;Hardware;Data visualization;Prototypes;Data analysis;Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797887},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798180,
  author={Ergün, Oğuzcan and Akın, Şahin and Dino, İpek Gürsel and Surer, Elif},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Architectural Design in Virtual Reality and Mixed Reality Environments: A Comparative Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={914-915},
  abstract={Virtual reality (VR) provides a completely digital world of interaction which enables the users to modify, edit, and transform digital elements in a responsive way. Mixed reality (MR), which is the result of blending the digital world and the physical world together, brings new advancements and challenges to human, computer and environment interactions. This paper focuses on adapting the already-existing methods and tools in architecture to both VR and MR environments under sustainable architectural design domain. For this purpose, we benefit from the semantically enriched data platforms of Building information modelling (BIM) tools, the performance calculation functions of building energy simulation tools while transcending these data into VR and MR environments. In this way, we were able to merge these diverse data for the virtual design activity. Nine participants have already tested the initial prototype of MR-based only interaction environment in our previous study [1]. According to the feedbacks, the user interface and interaction mechanisms were updated and the environment was made accessible also in VR. These updates made four types of interactions possible in MR and VR: 1) MR environment using HoloLens with gestures, 2) MR environment using HoloLens with a clicker, 3) VR environment using HTC Vive with two controllers, and 4) HoloLens emulator with a mouse. All these interaction cases were tested by 21 architecture students in an in-house workshop. In this workshop, we collected data on presence, usability, and technology acceptance of these cases. Our results show that interaction in a VR environment is the most natural interaction type and the participants were eager to use both MR and VR environments instead of an emulator. To our best of knowledge, this is the first comparative study of a BIM-based architectural design medium in both VR and MR environments.},
  keywords={Virtual reality;Solid modeling;Tools;Buildings;Usability;Data models;Data visualization;Mixed Reality;Virtual Reality;Building Information Modelling;J [Computer Applications]: J.6. Computer-Aided Engineering},
  doi={10.1109/VR.2019.8798180},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797968,
  author={Farmer, Harry and Bevan, Chris and Green, David P. and Rose, Mandy and Cater, Kirsten and Stanton-Fraser, Danaë},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Did You See What I Saw?: Comparing User Synchrony When Watching 360° Video In HMD Vs Flat Screen}, 
  year={2019},
  volume={},
  number={},
  pages={916-917},
  abstract={This study examined whether the high level of immersion provided by HMDs encourages participants to synchronise their attention during viewing. 39 participants watched the 360° documentary “Clouds Over Sidra” using either a HMD or via a flat screen tablet display. We found that the HMD group showed significantly greater overall ISC did the tablet group and that this effect was strongest during transition between scenes.},
  keywords={Resists;Virtual reality;Time series analysis;Cloud computing;Visualization;Correlation;Media;360° video;synchrony;inter-subject correlation analysis;Topic Area #1 [Technologies & Applications];Topic Area #3 [Interaction]},
  doi={10.1109/VR.2019.8797968},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797967,
  author={Faure, Charles and Limballe, Annabelle and Bideau, Benoit and Perrin, Théo and Kulpa, Richard},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Acting Together, Acting Stronger? Interference between Participants during Face-to-face Cooperative Interception Task}, 
  year={2019},
  volume={},
  number={},
  pages={918-919},
  abstract={People generally coordinate their action to be more effective. However, in some cases, interference between them occur, resulting in an inefficient collaboration. The main goal of this study is to explore the way two persons regulate their actions when performing a cooperative task of ball interception, and how interference between them may occur. Starting face to face, twenty-four participants (twelve teams of two) had to physically intercept balls moving down from the roof to the floor in a virtual room. To this end, they controlled a virtual paddle attached to their hand moving along the anterior-posterior axis, and were not allowed to communicate. Results globally showed participants were often able to intercept balls without collision by dividing the interception space in two equivalent parts. However, an area of uncertainty (where many trials were not intercepted) appeared in the center of the scene highlighting the presence of interference between participants. The width of this area increased when situation became more complex and when less information was available. Moreover, participants often interpreted balls starting above them as balls they should intercept, even when these balls were in fine intercepted by their partner. Overall, results showed that team coordination emerges from between-participants interactions in this ball interception task and that interference between them depends on task complexity (uncertainty on partner's action and visual information available).},
  keywords={Task analysis;Interference;Uncertainty;Visualization;Sports;Complexity theory;Collaboration;Virtual reality;Collaborative interactions;Ball interception;Perception-action;Team interference},
  doi={10.1109/VR.2019.8797967},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798298,
  author={Fernandez-Cervantes, Victor and Stroulia, Eleni},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual-GymVR: A Virtual Reality Platform for Personalized Exergames}, 
  year={2019},
  volume={},
  number={},
  pages={920-921},
  abstract={Virtual-GymVR is a platform for serious exergames in virtual reality. Its purpose is to provide older adults with a fun experience, while, at the same time, encouraging them to complete their personalized exercise sessions. The platform takes as input a description of a prescribed exercise, in terms of a posture-transition grammar, and constructs personalized versions of its games by accordingly configuring the behavior of the interactive objects in these games. The game-configuration process essentially controls the placement and the interaction behavior of the games' objects so that they induce the user to adopt the proper postures, as described by the input exercise specification. At run time, the sequence of game events stimulate the user to move to the prescribed exercise postures and, thus, accomplish their own personalized exercise goals. Given the intended user population of older adults, we have designed three different Virtual-GymVR games with metaphors appropriate for three different types of exercises. Our initial experimentation with the Virtual-GymVR games indicates that the approach is promising.},
  keywords={Games;Biometrics (access control);Virtual environments;Psychology;Conferences;Virtual Reality;Virtual-Gym;Exergames;Silver Games;Rehabilitation},
  doi={10.1109/VR.2019.8798298},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797710,
  author={Fiederer, Lukas D.J. and Alwanni, Hisham and Völker, Martin and Schnell, Oliver and Beck, Jürgen and Ball, Tonio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Research Framework for Virtual-Reality Neurosurgery Based on Open-Source Tools}, 
  year={2019},
  volume={},
  number={},
  pages={922-924},
  abstract={Fully immersive virtual reality (VR) has the potential to improve neurosurgical planning. However, there is a lack of research tools for this area. We present a research framework for VR neurosurgery based on open-source tools. We showcase the potential of such a framework using clinical data of two patients and research data of one subject. As first step toward practical evaluations, certified neurosurgeons positively assessed the VR visualizations and interactions using head-mounted displays. Methods and findings described in our study thus provide a foundation for research and development of versatile and user-friendly VR tools for improving neurosurgical planning and training.},
  keywords={Neurosurgery;Virtual reality;Solid modeling;Three-dimensional displays;Biomedical imaging;Brain modeling;Virtual reality;Imaging;Health informatics;Image segmentation;Medical technologies;3D imaging;Human-centered computing—Interaction paradigms—Virtual reality;Human-centered computing—Interaction devices—Haptic devices},
  doi={10.1109/VR.2019.8797710},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798299,
  author={Florea, Ciprian and Alavesa, Paula and Arhippainen, Leena and Pouke, Matti and Huang, Weiping and Haukipuro, Lotta and Väinämö, Satu and Niemelä, Arttu and Orduña, Marta Cortés and Pakanen, Minna A. and Ojala, Timo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Extending a User Involvement Tool with Virtual and Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={925-926},
  abstract={Living labs are environments for acquiring user feedback on new products and services. Virtual environments can complement living labs by providing dynamic immersive setup for depicting change. This paper describes implementation of Virtual and Augmented Reality clients as an extension to a user involvement tool for an existing living lab. We conducted a user experience study with 14 participants to compare the clients. According to our study, the virtual reality client was experienced as innovative, easy to use, entertaining and fun. Whereas the augmented reality client was perceived playful and empowering.},
  keywords={Three-dimensional displays;Tools;Augmented reality;Technological innovation;Visualization;Virtual environments;living lab;user involvement;user experience;virtual reality;augmented reality;H.5.1 [Information Interfaces and Presentation] Multimedia Information Systems;Artificial, augmented and virtual realities},
  doi={10.1109/VR.2019.8798299},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797732,
  author={Fujimoto, Yuichiro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Food Appearance Optimizer: Automatic Projection Mapping System for Enhancing Perceived Deliciousness Based on Appearance}, 
  year={2019},
  volume={},
  number={},
  pages={927-928},
  abstract={This paper proposes a system to enhance the degree of subjective deliciousness of food visually perceived by a person by automatically changing its appearance in a real environment. The system is called the Food Appearance Optimizer. The proposed system analyzes the appearance of food in a camera image and projects an appropriate image onto the food using a projector. The relationship between the degree of subjective deliciousness and four simple appearance features for each food category is modeled using data gathered via a crowdsourcing-based questionnaire. Using this model, the system generates the appropriate projection image to increase the deliciousness of the food.},
  keywords={Solid modeling;Cameras;Computational modeling;Data models;Analytical models;Correlation;Image recognition;Human-centered computing—Mixed/augmented reality;Human-centered computing—User models},
  doi={10.1109/VR.2019.8797732},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797727,
  author={Fujino, Yuichi and Matsukura, Haruka and Iwai, Daisuke and Sato, Kosuke},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Odor Modulation by Warming/Cooling Nose Based on Cross-modal Effect}, 
  year={2019},
  volume={},
  number={},
  pages={929-930},
  abstract={Presentation of odors is considered to be important as a means for giving a sense of presence. However, the basic odors have not been established to generate any kinds of odors by combining them, like the three primary colors in vision, the basic tastes in gustation. In order to present various kinds of odors, in general, it is necessary to prepare each corresponding odorant. In this research, an odor modulation method is proposed based on a cross-modal effect between olfaction and thermal sensation, which might be able to decrease the number of odorants used to generate odors presented to the user. The experimental results are reported to show that sensation of odors can be modulated by presenting warm/cool air with the odors even if the same odors are presented to the subjects.},
  keywords={Olfactory;Iron;Visualization;Tires;Dogs;Temperature sensors;Modulation;Olfaction;olfactory display;cross-modality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI},
  doi={10.1109/VR.2019.8797727},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797788,
  author={Fujiwara, Eric and Wu, Yu Tzu and Gomes, Matheus K. and Silva, Willian H. A. and Suzuki, Carlos K.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Haptic Interface Based on Optical Fiber Force Myography Sensor}, 
  year={2019},
  volume={},
  number={},
  pages={931-932},
  abstract={A haptic grasp interface based on the force myography technique is reported. The hand movements and forces during the object manipulation are assessed by an optical fiber sensor attached to the forearm, so the virtual contact is computed, and the reaction forces are delivered to the subject by graphical and vibrotactile feedbacks. The system was successfully tested for different objects, providing a non-invasive and realistic approach for applications in virtual-reality environments.},
  keywords={Force;Haptic interfaces;Optical sensors;Biomedical optical imaging;Visualization;Transducers;Robot sensing systems;Human-centered computing;Interaction devices;Haptic devices},
  doi={10.1109/VR.2019.8797788},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798003,
  author={Fujiwara, Eric and Wu, Yu Tzu and Silva, Luiz E. and Freitas, Hugo E. and Aristilde, Stenio and Cordeiro, Cristiano M. B.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optical Fiber 3D Shape Sensor for Motion Capture}, 
  year={2019},
  volume={},
  number={},
  pages={933-934},
  abstract={An optical fiber 3D shape sensor for motion capture is reported. The probe is comprised of Bragg grating strain sensors embedded into a 3D -printed polymer fiber substrate, allowing for the continuous assessment of the bending magnitude and direction. Moreover, the device was tested on the measurement of the forearm movements, making it possible to correctly replicate the captured motions in an avatar, providing a simple and minimally-invasive approach for applications in virtual reality.},
  keywords={Robot sensing systems;Probes;Three-dimensional displays;Optical sensors;Shape;Fiber gratings;Human-centered computing;Interaction devices},
  doi={10.1109/VR.2019.8798003},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798156,
  author={Funahashi, Kenji and Sumida, Naoki and Mizuno, Shinji},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Self Bird's Eye View with Omnidirectional Camera on HMD}, 
  year={2019},
  volume={},
  number={},
  pages={935-936},
  abstract={The experience to see oneself from behind in real-time, that is like out-of-body experience, is refreshing, it is also expected as a trigger to improve oneself. However selfie-stick, drone, or something is necessary to take a bird's-eye view. We propose the method to make virtual self bird's-eye view using an omnidirectional camera on an HMD.},
  keywords={Resists;Cameras;Head;Sports;Three-dimensional displays;Real-time systems;Drones;Bird's eye view;omnidirectional camera;HMD;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798156},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797860,
  author={Furumoto, Takuro and Fujiwara, Masahiro and Makino, Yasutoshi and Shinoda, Hiroyuki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={BaLuna: Floating Balloon Screen Manipulated Using Ultrasound}, 
  year={2019},
  volume={},
  number={},
  pages={937-938},
  abstract={In this paper, we present BaLuna, a prototype of an externally actuated midair display for indoor use in a room-scale workspace. This system is the first battery-less midair display with a one-meter-cubic workspace. The system projects an image onto a balloon screen whose position is controlled by airborne ultrasound phased array (AUPA) devices. Users can naturally manipulate the screen position by dragging and dropping the screen directly with their hands. We adapted feedback-based acoustic manipulation technology that enables sparsely distributed AUPA devices to control the screen position. This is combined with a depth-image-based tracking and a three-dimensionally calibrated projector.},
  keywords={Three-dimensional displays;Ultrasonic imaging;Prototypes;Augmented reality;Phased arrays;Acoustics;Drones;Midair display;Airborne ultrasound;Augmented reality;[Human-centered Computing]: Interaction devices—Displays and Imagers;[Human-centered Computing]: Interaction paradigm—Mixed / augmented reality},
  doi={10.1109/VR.2019.8797860},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798262,
  author={Gattullo, Michele and Dalena, Vito and Evangelista, Alessandro and Uva, Antonio E. and Fiorentino, Michele and Boccaccio, Antonio and Ruta, Michele and Gabbard, Joseph L.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Context-Aware Technical Information Manager for Presentation in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={939-940},
  abstract={Technical information presentation is evolving from static contents presented on paper or via digital publishing to real-time context-aware contents displayed via virtual and augmented reality devices. We present a Context-Aware Technical Information Management system (CATIM), that dynamically manages (1) what information as well as (2) how information is presented in an augmented reality interface. CATIM acquires context data about activity, operator, and environment, and then based on these data, proposes a dynamic augmented reality output tailored to the current context. The system was successfully implemented and preliminarily evaluated in a case study regarding the maintenance of a hydraulic valve.},
  keywords={Augmented reality;User interfaces;Task analysis;Mathematics;Maintenance engineering;Ontologies;Layout;Industrial Augmented Reality;Technical Information Manager;Context-aware information;Augmented Reality;Context Aware;Visualization},
  doi={10.1109/VR.2019.8798262},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798348,
  author={Gonzalez-Franco, Mar and Abtahi, Parastoo and Steed, Anthony},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Individual Differences in Embodied Distance Estimation in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={941-943},
  abstract={There are important individual differences when experiencing VR setups. We ran a study with 20 participants who got a scale-matched avatar and were asked to blind-walk to a VR target placed 2.5 meters away. In such setups, people typically underestimate distances by approximately 10% when virtual environments are viewed through head mounted displays. Consistent with previous studies we found that the underestimation was significantly reduced the more embodied the participants were. However, not all participants developed the same level of embodiment when exposed to the exact same conditions.},
  keywords={Virtual environments;Avatars;Legged locomotion;Task analysis;Estimation;Foot},
  doi={10.1109/VR.2019.8798348},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797895,
  author={Grandi, Jerônimo G and Ogren, Mark and Kopper, Regis},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Approach to Designing Next Generation User Interfaces for Public-Safety Organizations}, 
  year={2019},
  volume={},
  number={},
  pages={944-945},
  abstract={High-speed broadband networks will enable public-safety organizations to handle critical situations that go beyond the common voice communication channel. In this paper, we present a user-centered approach that makes the deployment and adoption of next-generation user interfaces reflect the first responders' needs, requirements, and contexts of use. It is composed of four phases where we elicit requirements, iteratively prototype user interfaces, and evaluate our designs. In this process, public-safety organizations are always engaged, contributing through their feedback and evaluation. We will use immersive Virtual Reality to simulate the user interface designs. Within the virtual environment, it is possible to prototype several concepts before committing to a definitive interface. The solutions proposed will be instrumental for the adoption of next-generation user interfaces by the public safety community.},
  keywords={User interfaces;Safety;Next generation networking;Task analysis;Prototypes;Virtual reality;Medical services;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797895},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797971,
  author={Gunkel, Simon N.B and Dohmen, Marleen D.W. and Stokking, Hans and Niamut, Omar},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={360-Degree Photo-realistic VR Conferencing}, 
  year={2019},
  volume={},
  number={},
  pages={946-947},
  abstract={VR experiences are becoming more social, but many social VR systems represent users as artificial avatars. For use cases such as VR conferencing, photo-realistic representations may be preferred. In this paper, we present ongoing research into social VR experiences with photo-realistic representations of participants and present a web-based social VR framework that extends current video conferencing capabilities with new VR functionalities. We explain the underlying design concepts of our framework and discuss user studies to evaluate the framework in three different scenarios. We show that people are able to use VR communication in real meeting situations and outline our future research to better understand the actual benefits and limitations of our approach, to fully understand the technological gaps that need to be bridged and to better understand the user experience.},
  keywords={Cameras;Three-dimensional displays;User experience;Resists;Face;Avatars;Virtual Reality;VR;Social VR;VR Conferencing;WebRTC;WebVR;Immersive Virtual Environments;Information systems—World Wide Web—Web conferencing;Information systems—Multimedia information systems—Multimedia streaming},
  doi={10.1109/VR.2019.8797971},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797972,
  author={Guo, Jie and Weng, Dongdong and Zhang, Zhenliang and Liu, Yue and Wang, Yongtian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Maslows Hierarchy of Needs on Long-Term Use of HMDs – A Case Study of Office Environment}, 
  year={2019},
  volume={},
  number={},
  pages={948-949},
  abstract={Long-term exposure to VR will become more and more important, but what we need for long term immersion to meet users fundamental needs is still under-researched. In this paper, we apply the theory of Maslows Hierarchy of Needs to guide the design of VR for longterm immersion based on the normal biological rhythm of human beings (24 hours). An office environment is designed to verify those needs. The efficiency, the physical and the psychological effects of this VR office system are tested. The results show that the VR office environment is as comfortable as the physical environment at short-term immersion and it can support users basic immersion. It means that the Maslows Hierarchy of Needs can be a guideline for long-term immersion.},
  keywords={Image processing;Psychology;Employment;Virtual reality;Training;Solid modeling;Biology;Maslows Hierarchy of Needs;long-term immersion;virtual office;Human-centered computing—Human computer interaction—Interaction paradigms—Virtual reality;Human-centered computing—Interaction design—Interaction design theory, concepts and paradigms},
  doi={10.1109/VR.2019.8797972},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797905,
  author={Hagemann, Georg and Zhou, Qian and Stavness, Ian and Fels, Sidney},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Spherical Fish Tank Virtual Reality Displays for Establishing Realistic Eye-Contact}, 
  year={2019},
  volume={},
  number={},
  pages={950-951},
  abstract={Eye-contact is a key aspect of non-verbal human communication in everyday tasks [1]. It provides important social and emotional information that can increase the effectiveness of human communication [8]. In a conversation, eye-contact, or the lack thereof, is constantly evaluated by human brains. Conversation partners derive subjective judgment of others' credibility, focus and confidence [4] from it. Seeking another person's eye-contact is a signal for them that focus is put on that person and the main receptive senses are prepared to receive input from the other person. Likewise breaking eye-contact usually indicates distraction, loss of confidence, loss of interest or shifting focus to a different target.},
  keywords={Virtual reality;Three-dimensional displays;Face;Fish;Sensitivity;Two dimensional displays;Visualization;Fish Tank Virtual Reality;Mixed Reality;3D Displays;Telepresence;Eye-Contact},
  doi={10.1109/VR.2019.8797905},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797832,
  author={Hagimori, Daiki and Yoshimoto, Shunsuke and Sakata, Nobuchika and Kiyokawa, Kiyoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tendon Vibration Increases Vision-induced Kinesthetic Illusions in a Virtual Environment}, 
  year={2019},
  volume={},
  number={},
  pages={952-953},
  abstract={In virtual reality (VR) systems, a user avatar is typically manipulated based on actual body motion in order to present natural immersive sensations resulted from proprioceptors. It is useful to be able to feel a large body motion in a virtual environment while it is actually smaller in the real environment. As a method to achieve this, we have been working on kinesthetic illusions induced by tendon vibration. In this article, we report on a user study focusing on the effects of a combination of visual stimulus and tendon vibration. In the user study, we measured subjects' perceived elbow angles while they observed the corresponding virtual arm bending to 90 degrees, with different rotation amplification ratios between 1.0 and 1.5, with and without tendon vibration on the wrist joint. The results show that the perceived angle is increased by up to 8 degrees when tendon vibration is applied, roughly in proportion to the rotation amplification ratio. Our study suggests that the vision-induced kinesthetic illusion is further increased by tendon vibration, and that our technique can be applied to VR applications to make users feel larger body motion than that in the real environment.},
  keywords={Vibrations;Tendons;Elbow;Visualization;Muscles;Virtual environments;Wrist;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception;Hardware validation;Emerging technologies;Emerging interfaces},
  doi={10.1109/VR.2019.8797832},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798222,
  author={Hagiwara, Takayoshi and Sugimoto, Maki and Inami, Masahiko and Kitazaki, Michiteru},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shared Body by Action Integration of Two Persons: Body Ownership, Sense of Agency and Task Performance}, 
  year={2019},
  volume={},
  number={},
  pages={954-955},
  abstract={Humans have one own body. However, we can share a body with two different persons in a virtual environment. We have developed a shared body as an avatar that is controlled by two persons' actions. Movements of two subjects were continuously captured, and integrated into the avatar's motion with the ratios of 0:100,25:75, 50:50, 75:25, 100:0. They were not aware of integration ratios, and asked to reach cubes with the right hand. They felt body ownership and sense of agency to the shared avatar more when the responsible ratio was higher. The reaching path of the avatar's hand was shorter in the shared body condition (75:25) than the single body condition (100:0). These results suggest that we have some of body ownership and sense of agency to the shared body, and the task performance is improved by the shared body.},
  keywords={Avatars;Task analysis;Virtual environments;Head;Collaboration;Robots;Rubber;sense of agency;body ownership;embodiment;collaboration;augmented human;I.3.7 [Computer graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;H.1.2 [Models and Principles]: User/Machine Systems—human factors},
  doi={10.1109/VR.2019.8798222},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798360,
  author={Han, Ping-Hsuan and Tsai, Ling and Lin, Jia-Wei and Chan, Yuan-An and Hsu, Jhih-Hong and Huang, Wan-Ting and Hsieh, Chiao-En and Hung, Yi-Ping},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Chair: Exploring the Sittable Chair in Immersive Virtual Reality for Seamless Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={956-957},
  abstract={Virtual reality (VR) has been a promising technique to provide an immersive experience. Comparing to traditional multimedia, when users want to take a rest or change the position such as sitting down, the chair might not be sittable because of the inconsistency between the physical and the virtual chair. In this work, we utilized a tracker attached to a physical chair and conducted a user study to explore the sitting behavior when they interact with different forms of the virtual chair. Results indicate that visualizing each part of the chair could provide different information and affect trust and preference.},
  keywords={Legged locomotion;Visualization;Resists;Cameras;Haptic interfaces;Virtual environments;prop-based haptic;seamless interaction;immersive environment;virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms},
  doi={10.1109/VR.2019.8798360},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798027,
  author={Han, Sang Yoon and Kim, Yoonsik and Lee, Sang Hwa and Cho, Nam Ik},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Pupil Center Detection Based on the UNet for the User Interaction in VR and AR Environments}, 
  year={2019},
  volume={},
  number={},
  pages={958-959},
  abstract={Finding the location of a pupil center is important for the human-computer interaction especially for the user interface in AR/VR devices. In this paper, we propose an indirect use of the convolutional neural network (CNN) for the task, which first segments the pupil region by a CNN, and then finds the center of mass of the region. For this, we create a dataset by labeling the pupil area on 111,581 images from 29 IR video sequences. We also label the pupil region of widely used datasets to test and validate our method on a variety of inputs. Experiments show that the proposed method provides better accuracies than the conventional ones, showing robustness to the noise.},
  keywords={Image segmentation;Floors;Reflection;Training;User interfaces;Cameras;Biomedical imaging;Pupil Detection;Segmentation;Neural Networks;Human-centered computing—Human computer interaction—Interaction devices—Graphics input devices;Computing methodologies—Artificial intelligence—Computer vision—Object detection;Computing methodologies—Machine learning—Machine learning approaches—Neural networks},
  doi={10.1109/VR.2019.8798027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797857,
  author={Hartney, Josephine H. and Rosenthal, Sydney N. and Kirkpatrick, Aaron M. and Skinner, J. Maci and Hughes, Jason and Orlosky, Jason},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Revisiting Virtual Reality for Practical Use in Therapy: Patient Satisfaction in Outpatient Rehabilitation}, 
  year={2019},
  volume={},
  number={},
  pages={960-961},
  abstract={Though Virtual Reality (VR) has made its way into many commercial applications, it has only begun to gain adoption in applications for rehabilitation and therapy. Though some research exists in this area, we still need to better understand how interactive VR can be integrated into a fast-paced clinical setting. To address this challenge, we present the results of a pilot study with 25 participants, who used an interactive application for conducting upper-limb rehabilitation. Our interface consists of a VR display with two controllers that are used to clean and clear segments of a target virtual window. From observations of participant challenges with the interface, feedback from the occupational therapy staff, and a subjective questionnaire using Likert scales, we examine ways to better integrate VR into a patient's scheduled rehabilitation.},
  keywords={Virtual reality;Medical treatment;Task analysis;Solid modeling;Headphones;Stroke (medical condition);Image color analysis;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8797857},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798052,
  author={Hatsushika, Denik and Nagata, Kazuma and Hashimoto, Yuki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SCUBA VR: Submersible-Type Virtual Underwater Experience System}, 
  year={2019},
  volume={},
  number={},
  pages={962-963},
  abstract={In this paper, we propose the development of an underwater virtual reality (VR) system for scuba training. The aim is to enable scuba students to experience an arbitrary underwater environment and enable training using VR technology in limited water environments such as a pool or shallow water. Using this system, scuba training, which can currently only be done by hands-on training, can be reproduced through a VR experience in a pool. This system consists of a cable-connected PC-HMD (UWHMD: Underwater Wired Head Mounted Display) that can be used underwater and a motion capture system. According to the pilot test, it was confirmed that position tracking was successful at widths of about 2.25 m, lengths of about 3.0 m, and depths of about 1.3m.},
  keywords={Training;Tracking;Virtual reality;Resists;Head;Sports;Cameras;Head Mounted Display;Motion Capture;Underwater;Scuba Diving;Virtual Reality;Training Simulator;Human-centered computing;Human computer interaction (HCI);Interaction paradigms},
  doi={10.1109/VR.2019.8798052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798201,
  author={Hatzipanayioti, Adamantini and Pavlidou, Anastasia and Dixken, Manuel and Bülthoff, Heinrich H. and Meilinger, Tobias and Bues, Matthias and Mohler, Betty J.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative Problem Solving in Local and Remote VR Situations}, 
  year={2019},
  volume={},
  number={},
  pages={964-965},
  abstract={Virtual Reality supports collaboration among partners across departments and fields, independent of physical boundaries. Virtual reality applications can solve the time and cost consuming logistic problem that companies encounter when sending experts to remote locations. However, it is not yet clear how effective partners collaborate when in remote locations. In one experiment, we examined whether partners who are physically in the same room and interact with each other before they start collaborating affects performance compared to collaborators who meet and interact only within the virtual space. Participants had to solve a Rubik's cube type three-dimensional puzzle by arranging cubes that varied in color within a solution space in such a way, so that each side of the solution space showed a single color. Participants were immersed within a virtual environment and in one condition participants were collocated in the same room (local condition), while in the other one they were located in different rooms (remote condition). Results showed that collaborators in both conditions successfully completed the task but performance was better during the local compared to the remote condition.},
  keywords={Task analysis;Collaboration;Color;Virtual environments;Headphones;Microphones;Collaboration;problem solving;puzzle task;social interaction},
  doi={10.1109/VR.2019.8798201},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798321,
  author={Heidari, Omid and Perez-Gracia, Alba},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Synthesis of Robotic Systems for Human Upper-Limb and Hand Tasks}, 
  year={2019},
  volume={},
  number={},
  pages={966-967},
  abstract={The design of robotic systems for human-hand tasks could benefit for a more intuitive environment for task definition and prototype testing. Virtual reality with human-limb motion identification and visualization seems to be an appropriate environment both for defining the task and to visualize the solution. In this work we present the combination of a method for the kinematic design of robots with serial and tree structures, with a virtual reality and depth-sensing system that allows faithful representation and data extraction for hand and upper extremity motion. The communications and solver are embedded in the virtual reality programming, yielding a tool for design of cooperative human-robot systems.},
  keywords={Task analysis;Virtual reality;Kinematics;Robot kinematics;Three-dimensional displays;Mathematical model;Human-centered computing—Virtual reality;Computer systems organization—External interfaces for robotics},
  doi={10.1109/VR.2019.8798321},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798132,
  author={Herder, Jens and Brettschneider, Nico and de Mooij, Jeroen and Ryskeldiev, Bektur},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Avatars for Co-located Collaborations in HMD-based Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={968-969},
  abstract={Multi-user virtual reality is transforming towards a social activity that is no longer only used by remote users, but also in large-scale location-based experiences. Usage of realtime-tracked avatars in colocated business-oriented applications with a “guide-user-scenario” is examined for user-related factors of Spatial Presence, Social Presence, User Experience and Task Load. A user study was conducted in order to compare both techniques of a realtime-tracked avatar and a non-visualised guide. Results reveal that the avatar-guide enhanced and stimulated communicative processes while facilitating interaction possibilities and creating a higher sense of mental immersion for users and engagement.},
  keywords={Avatars;Task analysis;Collaboration;Virtual environments;User experience;Visualization;Virtual Reality;Co-located Collaborations;Head-mounted Display;Avatars;Social Presence},
  doi={10.1109/VR.2019.8798132},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798369,
  author={Hertweck, Stephan and Weber, Desiée and Alwanni, Hisham and Unruh, Fabian and Fischbach, Martin and Latoschik, Marc Erich and Ball, Tonio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Brain Activity in Virtual Reality: Assessing Signal Quality of High-Resolution EEG While Using Head-Mounted Displays}, 
  year={2019},
  volume={},
  number={},
  pages={970-971},
  abstract={Biometric measures such as the electroencephalogram (EEG) promise to become viable alternatives to subjective questionnaire ratings for the evaluation of psychophysical effects associated with Virtual Reality (VR) systems, as they provide objective and continuous measurements without breaking the exposure. The extent to which the EEG signal can be disturbed by the presence of VR systems, however, has been barely investigated. This study outlines how to evaluate the compatibility of a given EEG-VR setup on the example of two commercial head-mounted displays (HMDs), the Oculus Rift and the HTC Vive Pro. We use a novel experimental protocol to compare the spectral composition between conditions with and without an HMD present during an eyes-open vs. eyes-closed task. We found general artifacts at the line hum of 50 Hz, and additional HMD refresh rate artifacts (90 Hz) for the Oculus rift exclusively. Frequency components typically most interesting to non-invasive EEG research and applications , however, remained largely unaffected. We observed similar topographies of visually-induced modulation of alpha band power for both HMD conditions in all subjects. Hence, the study introduces a necessary validation test for HMDs in combination with EEG and further promotes EEG as a potential biometric measurement method for psychophysical effects in VR systems.},
  keywords={Electroencephalography;Frequency measurement;Resists;Electrodes;Virtual reality;Task analysis;Physiology},
  doi={10.1109/VR.2019.8798369},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797830,
  author={Heyse, Joris and Vega, Maria Torres and de Backere, Femke and de Turck, Filip},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Contextual Bandit Learning-Based Viewport Prediction for 360 Video}, 
  year={2019},
  volume={},
  number={},
  pages={972-973},
  abstract={Accurately predicting where the user of a Virtual Reality (VR) application will be looking at in the near future improves the perceive quality of services, such as adaptive tile-based streaming or personalized online training. However, because of the unpredictability and dissimilarity of user behavior it is still a big challenge. In this work, we propose to use reinforcement learning, in particular contextual bandits, to solve this problem. The proposed solution tackles the prediction in two stages: (1) detection of movement; (2) prediction of direction. In order to prove its potential for VR services, the method was deployed on an adaptive tile-based VR streaming testbed, for benchmarking against a 3D trajectory extrapolation approach. Our results showed a significant improvement in terms of prediction error compared to the benchmark. This reduced prediction error also resulted in an enhancement on the perceived video quality.},
  keywords={Streaming media;Extrapolation;Resists;Solid modeling;Virtual reality;Predictive models;Prediction algorithms;Adaptive 360 video streaming;contextual bandit;VR;Information system—Information systems applications—Multimedia information systems—Multimedia streaming;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8797830},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798190,
  author={Hirao, Yutaro and Kawai, Takashi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Can We Create Better Haptic Illusions by Reducing Body Information?}, 
  year={2019},
  volume={},
  number={},
  pages={974-975},
  abstract={In this paper, we propose a method of alleviating a sense of unnaturalness and individual differences in pseudo-haptic experience in VR. The method is to use a non-isomorphic manipulation of a virtual body, where the actual and virtual body movement is decoupled. As an evaluation of this approach, two manipulation methods are compared in the task of pulling a virtual object in VR. In the first one, the user physically performs a pulling gesture, and in the second one, the user pulls with a controller's analog stick (Fig. 1). A temporal delay is added to the virtual object's motion for the pseudo-haptic effect. The results suggest a tendency where the individual differences in pseudo-haptic experience and unnaturalness were smaller with the analog stick manipulation, even with intensive pseudo-haptic expressions.},
  keywords={Haptic interfaces;Delays;Visualization;Virtual reality;Task analysis;Art;Pseudo-haptics;cross-modality;virtual reality;human information processing;perception;[Computer graphics]: Graphics systems and interfaces---Perception;[Computer graphics]: Graphics systems and interfaces---virtual reality},
  doi={10.1109/VR.2019.8798190},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798118,
  author={Hirt, Christian and Zank, Markus and Kunz, Andreas},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={PReWAP: Predictive Redirected Walking Using Artificial Potential Fields}, 
  year={2019},
  volume={},
  number={},
  pages={976-977},
  abstract={In predictive redirected walking applications, planning the redirection and path prediction are crucial for a safe and effective redirection. Common predictive redirection algorithms require many simplifications and limitations concerning the real and the virtual environments to achieve a real-time performance. These limitations include for example that the tracking space needs to be convex, and only a single user is supported. In this paper, we present a novel approach called PReWAP which addresses many of these shortcomings. We introduce artificial potential fields to represent the real environment which are able to handle non-convex environments and multiple co-located users. Further, we show how this new approach can be integrated into a model predictive controller which will allow various redirection techniques and multiple gains to be applied.},
  keywords={Legged locomotion;Trajectory;Planning;Aerospace electronics;Virtual reality;Prediction algorithms;Predictive models;Human-centered computing;Virtual reality;Computing methodologies;Motion path planning},
  doi={10.1109/VR.2019.8798118},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797709,
  author={Hirt, Christian and Zank, Markus and Kunz, Andreas},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Short-term Path Prediction for Virtual Open Spaces}, 
  year={2019},
  volume={},
  number={},
  pages={978-979},
  abstract={In predictive redirected walking applications, reliable path prediction is essential for an effective redirection. So far, most predictive redirected walking algorithms introduced many restrictions to the virtual environment in order to simplify this path prediction. Path prediction is time-consuming and is also prone to errors due to potentially impulsive and even irrational walking behaviour of humans in virtual environments. Therefore, many applications confine users in narrow virtual corridors or mazes in order to minimise larger deviations from intended and predictable walking patterns. In this paper, we present a novel approach for short-term path prediction which can be applied to virtual open space predictive redirected walking. We introduce a drop-shaped trajectory prediction which is described using a Lemniscate of Bernoulli. The drop's contour is discretised and we show how this is used to determine potential user trajectories in the virtual environment.},
  keywords={Legged locomotion;Trajectory;Prediction algorithms;Planning;Virtual environments;Table lookup;Human-centered computing—Virtual reality;Computing methodologies—Motion path planning},
  doi={10.1109/VR.2019.8797709},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797993,
  author={Hoppe, Adrian H. and Marek, Felix and van de Camp, Florian and Stiefelhagen, Rainer},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VirtualTablet: Extending Movable Surfaces with Touch Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={980-981},
  abstract={Immersive output and effortless input are two core aspects of a virtual reality (VR) experience. We transfer ubiquitous touch interaction with haptic feedback into a virtual environment (VE). The movable and cheap real world object supplies an accurate touch detection equal to a ray-casting interaction with a controller. Moreover, the virtual tablet extends the functionality of a real world tablet. Additional information is displayed in mid-air around the touchable area and the tablet can be turned over to interact with both sides. It allows easy to learn and precise system interaction and can even augment the established touch metaphor with new paradigms.},
  keywords={Three-dimensional displays;Cameras;Haptic interfaces;Visualization;Virtual environments;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797993},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797759,
  author={Houzangbe, Samory and Christmann, Olivier and Gorisse, Geoffrey and Richir, Simon},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Voluntary Heart Rate Control on User Engagement in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={982-983},
  abstract={The usage of biofeedback in Virtual Reality (VR) is becoming more and more important in providing fully immersive experiences. With the rapid evolution of physiological monitoring technologies it is important to study how different modalities of biofeedback can alter user experience. While previous studies use biofeedback as an additional interaction mechanic. We created a protocol to assess heart rate control competency and used the results of said protocol to immerse our participants in a VR experience where the biofeedback mechanics are mandatory to complete a game. We observed consistent results between our competency scale and the participants' mastery of the biofeedback game mechanic in the VR experience. We also found that the biofeedback mechanic has a significant impact on engagement.},
  keywords={Heart rate;Games;Biological control systems;Virtual reality;Art;Calibration;Biomedical monitoring;Virtual Reality;Biofeedback;Engagement;User Study;Human-centered computing—Interaction paradigms—Virtual reality;Human-centered computing—Interaction design process and methods—User centered design},
  doi={10.1109/VR.2019.8797759},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798248,
  author={Hu, Luyao and Zhang, Yaorui and Wang, Rui and Gao, Zaifeng and Bao, Hujun and Hua, Wei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Human Sensitivity to Slopes of Slanted Paths}, 
  year={2019},
  volume={},
  number={},
  pages={984-985},
  abstract={Redirected walking allows users to walk naturally through a large immersive virtual environment while the physical space is limited. Previous studies have analyzed human sensitivity to redirected walking in a horizontal direction, but users also need to walk on slopes to change their height. In this work, we expand the vertical movement space by positioning users on virtual paths with slopes that are different from those of real paths. We conduct psychological experiments to explore human sensitivity to slope gains that describe the discrepancies between the slopes of paths in virtual and real environments. The investigation shows that humans can walk on virtual slopes that are higher or lower than the real position without detecting the slopes and establishes corresponding detection thresholds.},
  keywords={Legged locomotion;Sensitivity;Virtual environments;Psychology;Ions;Roads;Virtual reality;redirected walking;slope gains;detection threshold},
  doi={10.1109/VR.2019.8798248},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797804,
  author={Huynh, Brandon and Orlosky, Jason and Höllerer, Tobias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Semantic Labeling and Object Registration for Augmented Reality Language Learning}, 
  year={2019},
  volume={},
  number={},
  pages={986-987},
  abstract={We propose an Augmented Reality vocabulary learning interface in which objects in a user's environment are automatically recognized and labeled in a foreign language. Using AR for language learning in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is still a significant challenge, and interaction with arbitrary physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a system that utilizes real-time object recognition to perform semantic labeling and object registration in Augmented Reality. We discuss its implementation, our motivations in designing it, and how it can be applied to AR language learning applications.},
  keywords={Calibration;Gaze tracking;Labeling;Augmented reality;Object recognition;Three-dimensional displays;Cameras;Human-centered computing—Mixed and augmented reality;Theory and algorithms for application domains—Semi-supervised learning},
  doi={10.1109/VR.2019.8797804},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798267,
  author={Hwang, Dong-Hyun and Aso, Kohei and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={MonoEye: Monocular Fisheye Camera-based 3D Human Pose Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={988-989},
  abstract={Wearable cameras have the potential to be used in various ways in combination with egocentric views such as action recognition, gesture input method for augmented/virtual reality (AR/VR) as well as lifelogger. Particularly, the pose of the camera wearer is one of the interesting factors of the egocentric view and various eccentric view-based pose estimation systems have been proposed; however, there is no balance between recognizable poses and enough egocentric views. In this work, we propose MonoEye, a system to provide wearer's estimated 3D pose and wide egocentric view. Our system's chest-mounted camera, equipped with the ultra-wide fisheye lens, covers the wearer's limbs and wide egocentric view; our pose estimation network estimates 3D body pose of the wearer from the camera's egocentric view. The proposed system not only can be used as an input interface of AR and VR through estimation of a various pose of the wearer but also has a potential to be used for action recognition by providing a wide egocentric view.},
  keywords={Three-dimensional displays;Cameras;Prototypes;Pose estimation;Lenses;Training;Heating systems;Computing methodologies—Motion capture;Computing methodologies—Activity recognition and understanding;Computing methodologies—Gestural input},
  doi={10.1109/VR.2019.8798267},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797883,
  author={Ishiyama, Toshiyuki and Kitahara, Tetsuro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Prototype of Virtual Drum Performance System with a Head-Mounted Display}, 
  year={2019},
  volume={},
  number={},
  pages={990-991},
  abstract={The goal of our study is to develop a virtual remote performance system that enables musicians who are distant from each other to play ensemble performances using virtual reality (VR) technologies. In ensemble performances, musicians usually use gestures to communicate with each other. Therefore, as the first step, we have developed a drum performance system that displays a computer-generated character that represents a bass player. The drummer wears a head-mounted display, which displays the computer-graphics (CG) bass-player character. As the human bass player moves in front of a Kinect sensor, the CG-based character's movements are synchronized with those of the human bass player. An experiment was carried out to demonstrate the performance of the system.},
  keywords={Three-dimensional displays;Timing;Resists;Music;Prototypes;Generators;Tracking;Durms;3D Caracter;HMD;Motion Tracker},
  doi={10.1109/VR.2019.8797883},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798045,
  author={Jang, Minsu and Kim, Jun Sik and Kang, Kyumin and Um, Soong Ho and Yang, Sungwook and Kim, Jinseok},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Development of Wearable Motion Capture System Using Fiber Bragg Grating Sensors for Measuring Arm Motion}, 
  year={2019},
  volume={},
  number={},
  pages={994-995},
  abstract={Motion capture systems are gaining much attention in various fields, including entertainment, medical and sports fields. Although many types of motion capture sensor have been emerging, they have limitations and disadvantages such as occlusion, drift and interference by electromagnetic fields. Here, we introduce the novel wearable motion capture system using fiber Bragg gratings (FBGs) sensors. Since the human joints have different degrees of freedom (DOF), we developed three types of sensors to reconstruct the human body motion from the strains induced on the FBGs. First, a shape sensor using three fibers provides the position and orientation of joints in three dimensional space. Second, we introduce the angle sensor which is capable of measuring bending angle with high curvature using single fiber. Lastly, to detect the twisting of joints, a sensor with fiber attached on a soft material spirally is used. With the optical fiber based motion capture sensors, we reconstruct the motion of arm in realtime. In detail, the joints of the arm include the sternoclavicular, acromioclavicular, shoulder and elbow. By arranging the three types of sensors on the joints in accordance with the DOF, the accuracy of the reconstructed motion is evaluated, resulting in an average error below 2.42°. Finally, to prove the feasibility of applying in virtual reality, we successfully manipulate the virtual avatar in real-time.},
  keywords={Fiber Bragg gratings;Motion capture;Virtual reality;FBG [Fiber Bragg grating]: DOF [Degree of freedom]},
  doi={10.1109/VR.2019.8798045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797745,
  author={Jin, Guojing and Chen, Jing and Wang, Jingyao and Wang, Yongtian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Neural Motion Deblurring Approach to Restore Rich Textures for Visual SLAM}, 
  year={2019},
  volume={},
  number={},
  pages={996-997},
  abstract={In this paper, we present a sequential video deblurring method based on a spatio-temporal recurrent network for visual SLAM. The method can be applied to any SLAM systems to make sure continuous localization even with blurred images. The quality of the deblurring method is evaluated on real-world problems: feature points extraction and SLAM, which prove the method can significantly improve the performance of tracking accuracy especially in some severe cases containing strong camera shake or fast motion.},
  keywords={Feature extraction;Simultaneous localization and mapping;Image restoration;Computer vision;Tracking;Real-time systems;Mathematical model;SLAM;Motion Blur;Deblur},
  doi={10.1109/VR.2019.8797745},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797764,
  author={Jin, Xinpei and Bian, Yulong and Geng, Wenxiu and Chen, Yeqing and Chu, Ke and Hu, Hao and Liu, Juan and Shi, Yuliang and Yang, Chenglei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Developing an Agent-based Virtual Interview Training System for College Students with High Shyness Level}, 
  year={2019},
  volume={},
  number={},
  pages={998-999},
  abstract={In this paper, we developed an agent-based virtual interview training system which can help college students with high shyness level to improve interview skills and reduce their anxiety by themselves before they take a real interview. The system includes three main contents: three virtual agents with different types of personality, three kinds of interview training contents and a multidimensional evaluation method, so that it is able to meet common demands of preparing for interviews. User study indicates the system can help shy college students cope with interview anxiety and improve their interview training performance effectively.},
  keywords={Interviews;Training;Stereo image processing;Heart rate;Visualization;Virtual reality;Three-dimensional displays;Virtual interview;agent-based interaction;self-training;persona effect;shyness;I.3.7 Three-Dimensional Graphics and Realism—Virtual Reality;J.4 Social and Behavioral Sciences—Psychology},
  doi={10.1109/VR.2019.8797764},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798361,
  author={Jones, J. Adam and Luckett, Ethan and Key, Tykeyah and Newsome, Nathan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Latency Measurement in Head-Mounted Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={1000-1001},
  abstract={In this paper, we discuss a generalizable method to measure end-to-end latency. This is the length of time that elapses between when a real-world movement occurs and when the pixels within a head-mounted display are updated to reflect this movement. The method described here utilizes components commonly available at electronics and hobby shops. We demonstrate this measurement method using an HTC Vive and discuss the influence of its low-persistence display on latency measurement.},
  keywords={Photodiodes;Resists;Oscilloscopes;Liquid crystal displays;Virtual environments;Cameras;Human-centered computing;Interaction paradigms;Virtual reality;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798361},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798268,
  author={Kaluschke, Maximilian and Weller, René and Zachmann, Gabriel and Lorenz, Mario},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Continuous Material Cutting Model with Haptic Feedback for Medical Simulations}, 
  year={2019},
  volume={},
  number={},
  pages={1002-1003},
  abstract={We present a novel haptic rendering approach to simulate material removal in medical simulations at haptic rates. The core of our method is a new massively-parallel continuous collision detection algorithm in combination with a stable and flexible 6-DOF collision response scheme that combines penalty-based and constraint-based force computation.},
  keywords={Haptic interfaces;Bones;Tools;Solid modeling;Collision avoidance;Teeth;Rendering (computer graphics);Computing methodologies;Modeling and simulation;Simulation types and techniques;Massively parallel and high-performance simulations},
  doi={10.1109/VR.2019.8798268},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797714,
  author={Kamei, Ikuo and Hiraki, Takefumi and Fukushima, Shogo and Naemura, Takeshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={PILC Projector: RGB-IR Projector for Pixel-level Infrared Light Communication}, 
  year={2019},
  volume={},
  number={},
  pages={1004-1005},
  abstract={The projection of invisible data on visible images can facilitate seamless interactive projection, since data embedded in regular images is unobtrusive to human viewers. However, the previous techniques sacrificed one of the following key goals: 1) calibration-free setup; 2) full-color projection; or 3) high contrast image. In this paper, we propose a Pixel-level Infrared Light Communication (PILC) projector that achieves all these requirements by adding an infrared light source to the full-color projector. To provide a proof of concept, we built a functional prototype, evaluated its performance, and presented a basic application.},
  keywords={Light emitting diodes;Brightness;Image color analysis;Decoding;Light sources;Color;Sensors;Visible light communication;infrared data embedding;digital micromirror device;augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction devices;Displays and imagers},
  doi={10.1109/VR.2019.8797714},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797856,
  author={Kang, Ning and Bai, Junxuan and Pan, Junjun and Qin, Hong},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-time Animation and Motion Retargeting of Virtual Characters Based on Single RGB-D Camera}, 
  year={2019},
  volume={},
  number={},
  pages={1006-1007},
  abstract={The rapid generation and flexible reuse of characters animation by commodity devices are of significant importance to rich digital content production in virtual reality. This paper aims to handle the challenges of current motion imitation for human body in several indoor scenes (e.g., fitness training). We develop a real-time system based on single Kinect device, which is able to capture stable human motions and retarget to virtual characters. A large variety of motions and characters are tested to validate the efficiency and effectiveness of our system.},
  keywords={Animation;Turning;Cameras;Joints;Bones;Quaternions;Character Animation;Motion Capture;Retargeting;RGB-D Camera;Adaptive filter;Inverse Kinematics},
  doi={10.1109/VR.2019.8797856},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798151,
  author={Kang, Sinhwa and Chanenson, Jake and Ghate, Pranav and Cowal, Peter and Weaver, Madeleine and Krum, David M.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Advancing Ethical Decision Making in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1008-1009},
  abstract={Virtual reality (VR) has been widely utilized for training and education purposes because of pedagogical, safety, and economic benefits. The investigation of moral judgment is a particularly interesting VR application, related to training. For this study, we designed a within-subject experiment manipulating the role of study participants in a Trolley Dilemma scenario: either victim or driver. We conducted a pilot study with four participants and describe preliminary results and implications in this poster.},
  keywords={Ethics;Virtual reality;Vehicles;Decision making;Training;Switches;Three-dimensional displays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798151},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797809,
  author={Katzakis, Nikolaos and Chen, Lihan and Mostajeran, Fariba and Steinicke, Frank},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Peripersonal Visual-Haptic Size Estimation in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1010-1011},
  abstract={We report an experiment in which participants compared the size of a visual sphere to a haptic sphere in VR. The sizes from two modalities were either congruent or conflicting (with different disparities). Importantly, three standard references (small, medium and large) for haptic sizes were used with the method of constant stimuli. Results show a dominant functional priority of the visual size perception. Moreover, observers demonstrated a central tendency effect: over-estimation for smaller haptic sizes but under-estimation for larger haptic sizes. The results are discussed in the framework of adaptation level theory for haptic size reference, and provide important implications for the design of 3D visuo-haptic human-computer interaction.},
  keywords={Haptic interfaces;Visualization;Task analysis;Phantoms;Virtual reality;Informatics;Standards;Haptics;Integration;Somatosensation;Vision;Cross-Modal;Perception;Proprioception;Virtual Reality;Graphics;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Haptic I/O;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality},
  doi={10.1109/VR.2019.8797809},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798260,
  author={Keller, Marilyn and Tchilinguirian, Tristan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Obstacles Awareness Methods from Occupancy Map for Free Walking in VR}, 
  year={2019},
  volume={},
  number={},
  pages={1012-1013},
  abstract={With Head Mounted Displays (HMD) equipped with extended tracking features, users can now walk in a room scale space while being immersed in a virtual world. However, to fully exploit this feature and enable free-walking, these devices still require a large physical space, cleared of obstacles. This is an essential requirement that not any user can meet, especially at home, thus this constraint limits the use of free-walking in Virtual Reality (VR) applications. In this poster, we propose ways of representing the physical obstacles surrounding the user. There are generated from an occupancy map and compared to the representation as a point cloud. We propose three visualisation modes: integrating an occupancy map into the virtual floor, generating lava lakes where obstacles are and building a semi-transparent wall along the obstacles boundaries. We found that although showing the obstacles on the floor only impacts lightly the navigation, the preferred visualization mode remains the point cloud.},
  keywords={Three-dimensional displays;Visualization;Legged locomotion;Trajectory;Resists;Navigation;Google;Human-centered computing;Visualization;Visualization techniques;Empirical studies in visualization},
  doi={10.1109/VR.2019.8798260},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798250,
  author={Kent, Lee and Snider, Chris and Hicks, Ben},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Early Stage Digital-Physical Twinning to Engage Citizens with City Planning and Design}, 
  year={2019},
  volume={},
  number={},
  pages={1014-1015},
  abstract={The pairing of physical objects with a digital counterpart, referred to as a digital-physical twin, provides the capability to leverage the affordances of each. This can be applied to city scale visualisation during concept generation to engage citizens with city planning. A Virtual Reality based platform is proposed, tested with members of the public and its suitability as an engagement tool is discussed. The platform, City Blocks, allows later design stage visualisation and analysis of ideas to be brought to concept development with citizens able to reason and refine designs via an abstracted physical twin.},
  keywords={Virtual reality;Solid modeling;Visualization;Color;Urban planning},
  doi={10.1109/VR.2019.8798250},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797801,
  author={Khenak, Nawel and Vézien, Jean-Marc and Thery, David and Bourdot, Patrick},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatial Presence in Real and Remote Immersive Environments}, 
  year={2019},
  volume={},
  number={},
  pages={1016-1017},
  abstract={This paper presents an experiment assessing the feeling of spatial presence in both real and remote environments (respectively the socalled “natural presence” and “telepresence”). Twenty-eight (28) participants performed a 3D-pointing task while being located in a real office and the same office remotely rendered over HMD. The spatial presence was evaluated by means of the ITC-SOPI questionnaire and users' behaviour analysis (trajectories of head during the task). The analysis also included the effect of different levels of immersion of the system - visual-only versus visual and audio - rendering in such environments. The results show a higher sense of spatial presence for the remote condition, regardless of the degree of immersion, and for the “visual and audio” condition regardless of the environment. Additionally, trajectory analysis of users' heads reveals that participants behaved similarly in both environments.},
  keywords={Telepresence;Task analysis;Visualization;Headphones;Rendering (computer graphics);Virtual environments;Trajectory;Telepresence;Spatial Presence;Immersion;User Evaluation;Reality-Test;H.5.1 [Information Interfaces and Presentation]: Multimedia Information System—Virtual reality, Telepresence System;[Information Interfaces and Presentation]: Multimedia Information System—Evaluation},
  doi={10.1109/VR.2019.8797801},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797896,
  author={Khokhar, Adil and Yoshimura, Andrew and Borst, Christoph W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Pedagogical Agent Responsive to Eye Tracking in Educational VR}, 
  year={2019},
  volume={},
  number={},
  pages={1018-1019},
  abstract={We present an architecture to make a VR pedagogical agent responsive to shifts in user attention monitored by eye tracking. The behavior-based AI includes low-level sensor elements, sensor combiners that compute attention metrics for higher-level sensors called generalized hotspots, an annotation system for arranging scene elements and responses, and its response selection system. We show that the techniques can control the playback of teacher avatar clips that point out and explain objects in a VR oil rig for training.},
  keywords={Gaze tracking;History;Computer architecture;Artificial intelligence;Monitoring;Conferences;Virtual reality;Human-centered computing-Visualization},
  doi={10.1109/VR.2019.8797896},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797870,
  author={Kim, Aelee and Kwon, Minyoung and Chang, Minha and Kim, Sunkyue and Jung, Dahei and Lee, Kyoungmin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Influence of Body Position on Presence When Playing a Virtual Reality Game}, 
  year={2019},
  volume={},
  number={},
  pages={1020-1021},
  abstract={In this study, we explore how proprioception relates to the sense of presence in a virtual environment. To investigate this objective, we compared three body conditions (Standing vs. Sitting vs. Half-Sitting) to examine whether body position influences the level of presence when playing a virtual reality (VR) game. The results showed that participants who played the game in a standing position felt greater presence than those who were sitting or half-sitting, while the degree of presence was higher for sitting than for half-sitting participants. In addition, a correlation analysis of the control, sensory, and attention factor associated with presence revealed a strong connection between control and sensory factor, whereas attention factor was moderately linked to control and sensory factor. Based on these results, we may assume that variations in movement related to body position influence proprioception, which consequently affects the sense of presence. Overall, this study has confirmed the important association between body position and presence in VR.},
  keywords={Games;Virtual environments;Correlation;Visualization;Atmospheric measurements;Particle measurements;H.1.2 [Models and Principles]: User/Machine Systems;Human Factors;1.3.7 [Computing Graphics]: Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VR.2019.8797870},
  ISSN={2642-5254},
  month={March},}
@INPROCEEDINGS{8797897,
  author={Klippel, Alexander and Wallgrün, Jan Oliver and Masrur, Arif and Zhao, Jiayan and LaFemina, Peter},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Warping Space and Time - xR Reviving Educational Tools of the 19th Century}, 
  year={2019},
  volume={},
  number={},
  pages={1022-1023},
  abstract={xR has the potential to warp both space and time. We demonstrate this potential by designing a mixed reality application for mobile devices for the Penn State's Obelisk, a historic landmark on the main Penn State campus that artistically reveals the geological history of Pennsylvania. Our AR application allows for placing a model of the Obelisk on any surface, interacting with the individual stones to reveal their geological characteristics and location of excavation, and changing to an immersive VR experience of this location based on 360° imagery. Originally conceptualized as a teaching tool for the School of Mines, our xR application revives the Obelisk's long forgotten mission and allows educators to integrate it once more into the curriculum as well as creatively expand its potential.},
  keywords={Solid modeling;Geology;Three-dimensional displays;Augmented reality;Media;Tools;Augmented reality;mixed reality;interactive learning;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;K.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology},
  doi={10.1109/VR.2019.8797897},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798084,
  author={Koilias, Alexandros and Mousas, Christos and Rekabdar, Banafsheh and Anagnostopoulos, Christos-Nikolaos},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Passenger Anxiety when Seated in a Virtual Reality Self-Driving Car}, 
  year={2019},
  volume={},
  number={},
  pages={1024-1025},
  abstract={A virtual reality study was conducted to understand participants' anxiety when immersed in a virtual reality trip with a self-driving car. Participants were placed as passengers in a virtual car, and they were seated in the co-driver seat. Five different conditions were developed and examined. For this experiment, the Anxiety Modality Questionnaire that captures the cognitive anxiety of participants was used. The obtained results indicated that the participants' level of anxiety for the partial awareness of the driver condition is influenced less than expected. Specifically, lower levels of anxiety were found when the driver is either fully or partially aware of the traffic and the behavior of the car, and higher anxiety levels were found when the driver is completely unaware.},
  keywords={Autonomous automobiles;Automobiles;Roads;Urban areas;Virtual environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798084},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798033,
  author={Komura, Hiraku and Ohka, Masahiro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effects of Tactile Gestalt on Generating Velvet Hand Illusion}, 
  year={2019},
  volume={},
  number={},
  pages={1026-1027},
  abstract={Smoothness is one of the important factors in controlling texture sensation in tactile VR. In previous research, the authors utilized Velvet Hand Illusion (VHI), which is a tactile illusion phenomena, to generate a smooth texture on a dot-matrix display. Since the dot-matrix display can generate arbitrary figures, we are attempting to evaluate our new hypothesis of the VHI mechanism using this display. In our hypothesis, a cognitive mechanism called Gestalt grouping [3] is related to VHI occurrence. If tactile Gestalt were elucidated, we could understand the tactile sensation mechanism deeply and the various devices would be developed in the VR research field. In this paper, we investigate the relationship between tactile Gestalt and VHI through psychophysical experiments. In these experiments, the appearance intensity of tactile Gestalt is controlled by means of density and length of the lines formed on the dot-matrix display. It is concluded that the law of closure stated in the principle of Prägnanz is an essential condition related to tactile Gestalt to generate VHI.},
  keywords={Virtual reality;Haptic interfaces;Shape;Information processing;Tactile sensors;haptic display;tactile Gestalt;psychophysics;Velvet Hand Illusion;law of closure;H.1.1 [Systems and Information Theory]: System and Information Theory;Human information processing},
  doi={10.1109/VR.2019.8798033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798346,
  author={Kondo, Ryota and Sugimoto, Maki and Inami, Masahiko and Kitazaki, Michiteru},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Scrambled Body: A Method to Compare Full Body Illusion and Illusory Body Ownership of Body Parts}, 
  year={2019},
  volume={},
  number={},
  pages={1028-1029},
  abstract={Humans can feel as if a fake body is their own body in the illusory body ownership. The illusion can be induced to a full body by a visual-tactile synchronicity or visual-motor synchronicity. In our previous study, illusory full body ownership of invisible body was elicited by synchronous movements of virtual gloves and socks. In this study, we aimed to investigate whether the spatial relationship is necessary for the full body illusion using a stimulus of scrambled body. In the scrambled body, positions of gloves and socks were scrambled. The results suggest that spatial relationship of body parts is necessary for the full body illusion.},
  keywords={Rubber;Synchronization;Virtual reality;Footwear;Task analysis;Color;Floors;Body ownership;full body illusion;body-part ownership;I.3.7 [Computer graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;H.1.2 [Models and Principles]: User/Machine Systems—human factors},
  doi={10.1109/VR.2019.8798346},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798194,
  author={Koshi, Masaki and Sakata, Nobuchika and Kiyokawa, Kiyoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Concentration: Concentration Improvement by Visual Noise Reduction with a Video See-Through HMD}, 
  year={2019},
  volume={},
  number={},
  pages={1030-1031},
  abstract={We propose a concentration improvement technique by using a video see-through head mounted display (HMD). Our technique reduces the visual noise or disturbing visual stimulus, such as moving objects near the central visual field, by lowering its visual saliency in real-time. Earphones and noise cancelling headphones are often used to shutdown auditory noise from surroundings when we need to concentrate on the job. Several studies have proven the effectiveness of such noise reduction on improving the concentration and learning efficiencies. We apply this analogy to the visual noise and propose a visual noise reduction HMD to improve concentration. In this article, we report on two preliminary user studies we conducted to investigate the effectiveness of our method. In the prototype system, we manually specify a part of the visual field as the working area and apply grayscale and blur filters outside it in real-time. We compared two conditions “no effect” and “strong blur” (for Exp 1) or “weak blur” (for Exp 2), using a simple math task. Our preliminary results show that visual noise reduction improves the task completion time by 10% for Exp 1 and 7.9% for Exp 2 on average.},
  keywords={Visualization;Streaming media;Task analysis;Resists;Gray-scale;Noise reduction;Noise measurement;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception},
  doi={10.1109/VR.2019.8798194},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798139,
  author={Krogmeier, Claudia and Mousas, Christos and Whittinghill, David},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Human, Virtual Human, Bump! A Preliminary Study on Haptic Feedback}, 
  year={2019},
  volume={},
  number={},
  pages={1032-1033},
  abstract={How does haptic feedback during a human-virtual human interaction affect emotional arousal in virtual reality? In this between-subjects study, we compare haptic feedback and no haptic feedback conditions in which a virtual human “bumps” into the participant in order to determine the influence of haptic feedback on emotional arousal, sense of presence, and embodiment in virtual reality, as well as compare self-report measures of emotional arousal to those objectively collected via event-related galvanic skin response (GSR) recordings. We plan to extend the current preliminary study by adding three more conditions as described in the future work section. Participants are students age 18-32 with at least moderate experience in virtual reality. Preliminary results indicate significant differences in presence and embodiment between haptic feedback and no haptic feedback groups. With our small sample size at the current time, GSR does not show significant differences between haptic and no haptic feedback conditions.},
  keywords={Haptic interfaces;Virtual environments;Skin;Sports equipment;Indexes;Games;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798139},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798220,
  author={Krzanich, Kylee M. and Whitmire, Eric and Stengel, Michael and Kass, Michael and Akşit, Kaan and Luebke, David},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={RetroTracker: Upgrading Existing Virtual Reality Tracking Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1034-1035},
  abstract={Virtual reality systems often make use of spatially tracked handheld props in the form of controllers or specialized objects to add realism and interaction. Tracking these objects today relies on the use of expensive, bulky, and power-consuming trackers that must be attached to an object. We propose a passive tracking technique that works with existing low-cost, off-the-shelf optical tracking components and is capable of turning any object into a tracked virtual reality prop. Our method utilizes paper-thin retro-reflective markers that can be placed in any free-form on everyday objects. The proof-of-concept prototype acts as a simple add-on for an existing tracking system and requires only a minimal amount of compute overhead. We demonstrate that our method allows bringing physical real-world objects to virtual worlds with ease, and provides an object identification technique using patterned retro-reflective markers.},
  keywords={Virtual reality;Haptic interfaces;Base stations;Tracking;Prototypes;Synchronization;Human factors;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798220},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798087,
  author={Lages, Wallace S and Li, Yuan and Lisle, Lee and Lu, Feiyu and Höllerer, Tobias and Bowman, Doug A.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enhanced Geometric Techniques for Point Marking in Model-Free Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1036-1037},
  abstract={Specifying points in three-dimensional space is essential in AR applications. Geometric triangulation is a straightforward way to specify points, but its naïve implementation has low precision. We designed two enhanced geometric techniques for 3D point marking: VectorCloud, which uses multiple rays to reduce jittering, and ImageRefinement, which allows 3D ray refinement to improve precision. Our experiments, conducted in both simulated and real AR, demonstrate that both techniques improve the precision of 3D point marking, and that ImageRefinement is superior to VectorCloud overall. These results are particularly relevant in the design of mobile AR systems for large outdoor areas.},
  keywords={Three-dimensional displays;Head;Task analysis;Solid modeling;Augmented reality;Human computer interaction;Human-centered computing;Mixed / augmented reality;Pointing;User interface design},
  doi={10.1109/VR.2019.8798087},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798331,
  author={Lin, Chengyuan and Rojas-Muñoz, Edgar and Cabrera, Maria Eugenia and Sanchez-Tamayo, Natalia and Andersen, Daniel and Popescu, Voicu and Noguera, Juan Antonio Barragan and Zarzaur, Ben and Murphy, Pat and Anderson, Kathryn and Douglas, Thomas and Griffis, Clare and Wachs, Juan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Robust High-Level Video Stabilization for Effective AR Telementoring}, 
  year={2019},
  volume={},
  number={},
  pages={1038-1039},
  abstract={This poster presents the design, implementation, and evaluation of a method for robust high-level stabilization of mentees first-person video in augmented reality (AR) telementoring. This video is captured by the front-facing built-in camera of an AR headset and stabilized by rendering from a stationary view a planar proxy of the workspace projectively texture mapped with the video feed. The result is stable, complete, up to date, continuous, distortion free, and rendered from the mentee's default viewpoint. The stabilization method was evaluated in two user studies, in the context of number matching and for cricothyroidotomy training, respectively. Both showed a significant advantage of our method compared with unstabilized visualization.},
  keywords={Visualization;Cameras;Feeds;Task analysis;Augmented reality;Resists;Three-dimensional displays;Human-centered computing;Visualization;Visualization design and evaluation methods Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Collaborative interaction Human-centered computing;Mixed / augmented reality},
  doi={10.1109/VR.2019.8798331},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798069,
  author={Lindemann, Patrick and Eisl, Dominik and Rigoll, Gerhard},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Acceptance and User Experience of Driving with a See-Through Cockpit in a Narrow-Space Overtaking Scenario}, 
  year={2019},
  volume={},
  number={},
  pages={1040-1041},
  abstract={In this work, we examine the implications of driving with transparent cockpits (TCs) in a narrow-space overtaking scenario. We utilize a virtual environment to simulate two possible manifestations: a user-controlled head-mounted system and a static projection-based system. We conducted a user study with an overtaking task and present results for acceptance and a comparison of both systems regarding user experience. Participants preferred the static TC and evaluated it as the solution with higher pragmatic quality and attractiveness. The TC generally scored highly in hedonic quality and was rated positively regarding perceived safety and ease of use.},
  keywords={Resists;Vehicles;Solid modeling;Task analysis;Virtual reality;User experience;Safety;Human-centered computing;Human computer interaction (HCI);Mixed / augmented reality, Virtual reality},
  doi={10.1109/VR.2019.8798069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797957,
  author={Lindemann, Patrick and Zhu, Rui and Rigoll, Gerhard},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Simulation for Examining the Effects of Inaccurate Head Tracking on Drivers of Vehicles with Transparent Cockpit Projections}, 
  year={2019},
  volume={},
  number={},
  pages={1042-1043},
  abstract={The transparent cockpit (TC) is a driver-car interface concept in which processed camera images of the surrounding environment are superimposed onto the car interior to give the driver the ability to see through the cockpit. For a perspectively accurate experience from the driver's point of view, head tracking is required. However, a real-world system may be prone to typical tracking errors, especially while driving. In this work, we present a TC simulation using artificial errors of various types to examine how much each type affects driver performance and experience. First results of an initial study show that there is generally no significant deterioration of lateral performance compared to a perfectly accurate TC. Repeated loss of tracking was least noticed by participants. Accuracy (miscalibration) and precision (jitter) errors were noticed the most.},
  keywords={Solid modeling;Head;Automobiles;Virtual reality;Visualization;Prototypes;Human-centered computing;Human computer interaction (HCI);Mixed / augmented reality, Virtual reality},
  doi={10.1109/VR.2019.8797957},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798315,
  author={Ling, Frank Fong and Elvezio, Carmine and Bullock, Jacob and Henderson, Steve and Feiner, Steven},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Hybrid RTK GNSS and SLAM Outdoor Augmented Reality System}, 
  year={2019},
  volume={},
  number={},
  pages={1044-1045},
  abstract={In the real world, we are surrounded by potentially important data. For example, military personnel and first responders may need to understand the layout of an environment, including the locations of designated assets, specified in latitude and longitude. However, many augmented reality (AR) systems cannot associate absolute geographic coordinates with the coordinate system in which they track. We describe a simple approach for developing a wide-area outdoor wearable AR system that uses RTK GNSS position tracking to align together and georegister multiple smaller maps from an existing SLAM tracking system.},
  keywords={Global navigation satellite system;Simultaneous localization and mapping;Augmented reality;Servers;Cameras;Global Positioning System;Base stations;Human-centered computing;Human-computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer Graphics;Graphics systems and interfaces},
  doi={10.1109/VR.2019.8798315},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797836,
  author={Liu, Angelina Chang and Lee, Brian Hyun-jong and Kopper, Regis},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards a Virtual Memory Palace}, 
  year={2019},
  volume={},
  number={},
  pages={1046-1047},
  abstract={Mnemonics is a powerful tool to assist memorization of a large amount of complex information. Amongst the different techniques, memory palace, also known as the method of Loci, presents significant results in enhancing memory performance for massive lists of numbers, objects and even texts. Substantial research has been conducted to determine both the validity of applying this complex technique in education, as well as that of applying VR to ease the process of learning this technique. Experiments have shown auspicious results suggesting that a virtual environment helps with spatial memory enhancements. However, previous studies have not investigated factors such as training participants over several stages of learning or including interactivity in the virtual environment. The objective of this pilot project is to assess such effects systematically and suggest several guidelines for improving future studies on virtual memory palace.},
  keywords={Memory management;Task analysis;Virtual environments;Visualization;Tools;Training;Memory palace;Education;Phased learning},
  doi={10.1109/VR.2019.8797836},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798224,
  author={Liu, Sinuo and Wang, Xiaokun and Ban, Xiaojuan and Xu, Yanrui and Zhou, Jing and Zhang, Yalan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Viscosity-based Vorticity Correction for Turbulent SPH Fluids}, 
  year={2019},
  volume={},
  number={},
  pages={1048-1049},
  abstract={A critical problem of Smooth Particle Hydrodynamics (SPH) methods is the numerical dissipation in viscosity computation. This leads to unrealistic results where high frequency details, like turbulence, are smoothed out. To address this issue, we introduce a viscosity-based vorticity correction scheme for SPH fluids, without complex time integration or limited time steps. In our method, the energy difference in viscosity computation is used to correct the vorticity field. Instead of solving Biot-Savart integrals, we adopt stream function, which is easier to solve and more efficient, to recover the velocity field from the vorticity difference. Our method can increase the existing vortex significantly and generate additional turbulence at potential position. Moreover, it is simple to implement and can be easily integrated with other SPH methods.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Smooth Particle Hydrodynamics;turbulence;Lagrangian vortex method;fluid simulation;I.3.7: Three-Dimensional Graphics and Realism;H.5.1: Multimedia Information Systems-Artificial, Augmented, and Virtual Realities},
  doi={10.1109/VR.2019.8798224},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797821,
  author={Liu, Weiquan and Wang, Cheng and Zang, Yu and Lai, Shang-Hong and Weng, Dongdong and Bian, Xuesheng and Lin, Xiuhong and Shen, Xuelun and Li, Jonathan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Ground Camera Images and UAV 3D Model Registration for Outdoor Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1050-1051},
  abstract={This paper presents a novel virtual-real registration approach for augmented reality (AR) in large-scale outdoor environments. Essentially, it is a pose estimation for the mobile camera images (ground camera images) in 3D model recovered by Unmanned Aerial Vehicle (UAV) image sequence via Structure-From-Motion (SFM) technology. The approach considers to indirectly establish the spatial relationship between 2D and 3D space by inferring the transformation relationship between the ground camera images and the UAV 3D model rendered images. Specifically, the proposed approach can overcome the positioning errors, which are deterioration and drift in the GPS, and deviation of orientation. The experimental results demonstrate the possibility of the proposed virtual-real registration approach, and show that the approach is robust, efficient and intuitive for AR in large-scale outdoor environments.},
  keywords={Solid modeling;Three-dimensional displays;Cameras;Unmanned aerial vehicles;Mobile handsets;Global Positioning System;Augmented reality;Virtual-real registration;outdoor AR;cross-domain image matching;Human-centered computing;Visualization;Visualization techniques;Augmented reality},
  doi={10.1109/VR.2019.8797821},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797782,
  author={Liu, Xiaoxu and Liu, Yue and Wang, Yongtian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real Time 3D Magnetic Field Visualization Based on Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1052-1053},
  abstract={In physics teaching, electromagnetism is one of the most difficult concepts for students to understand. This paper proposes a real time visualization method for 3-D magnetic field based on the augmented reality technology, which can not only visualize magnetic flux lines in real time, but also simulates the approximate sparse distribution of magnetic flux lines in space. An application utilizing this method is also presented. It permits leaners to freely and interactively move the magnets in 3-D space and to observe the magnetic flux lines in real time. As a result, the proposed method visualizes the invisible factors in 3-D magnetic field, with which students will have real-life reference when studying electromagnetic.},
  keywords={Real-time systems;Magnetic fields;Visualization;Magnetic domains;Augmented reality;Magnetic flux density;augmented reality;real time visualization;magnetic flux lines;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Visualization;Visualization application domains;Scientific visualization},
  doi={10.1109/VR.2019.8797782},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798159,
  author={Liyanage, Samali U. and Jayaratne, Lakshman and Wickramasinghe, Manjusri and Munasinghe, Aruna},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards an Affordable Virtual Reality Solution for Cardiopulmonary Resuscitation Training}, 
  year={2019},
  volume={},
  number={},
  pages={1054-1055},
  abstract={Cardiopulmonary Resuscitation (CPR) is an essential skill for medical professionals. Currently, CPR training is carried out using mechanical manikins which have drawbacks when it comes to the realism of the simulation, cost and durability. In the meantime, Virtual Reality (VR) has entered a new phase of wider availability and affordability in the consumer market. Therefore, a low-cost VR-based solution can now be proposed to address some of these issues with the mechanical manikin. Hence, this study presents an approach where the mechanical manikin has been augmented with VR. To test the viability and acceptance of this solution, a user based evaluation was carried out with a group of experts and novices in CPR. Despite their contrasting experience and knowledge of CPR, both groups have expressed favourable views towards the basic solution presented in this paper.},
  keywords={Training;Avatars;Tracking;Hardware;Haptic interfaces;Headphones;Virtual Reality;Applications;Modeling and Simulation;Cardiopulmonary Resuscitation Training},
  doi={10.1109/VR.2019.8798159},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798144,
  author={Lorenz, Mario and Neupetsch, Constanze and Rotsch, Christian and Klimant, Philipp and Hammer, Niels},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Early Virtual Reality User Experience and Usability Assessment of a Surgical Shape Memory Alloy Aspiration/Irrigation Instrument}, 
  year={2019},
  volume={},
  number={},
  pages={1056-1057},
  abstract={Performing user experience (UX) and usability studies in VR to assess early product prototypes is common in the automotive and other sectors but has not yet been applied for the development of surgical instruments. For the development of an aspiration/irrigation instrument using a shape memory alloy (SMA) tube, we wanted to get an early user feedback of our prototype from medical staff. We conducted a study with 22 participants with a medical background and considered a potential presence bias of the UX and usability ratings. The results confirm the novelty and usefulness of a SMA tube and gave us valuable feedback for optimizing the shapeshifting mechanism.},
  keywords={Instruments;Usability;Surgery;Virtual reality;Electron tubes;Shape;Prototypes;User experience;Usability;Presence;Virtual reality;Surgery;Applied computing → Life and medical sciences;Human-centered computing → Virtual reality;Human-centered computing → User studies},
  doi={10.1109/VR.2019.8798144},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798164,
  author={Lou, Ruding and Chardonnet, Jean-Rémy},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reducing Cybersickness by Geometry Deformation}, 
  year={2019},
  volume={},
  number={},
  pages={1058-1059},
  abstract={One major and well-known issue that occurs during VR experience is the appearance of cybersickness, which refrains users from accepting VR technologies. The induced cybersickness is due to a self-motion feeling that is produced when users see objects moving in the virtual world. To reduce cybersickness several methods have been proposed in the literature, however they do not guarantee immersion and navigation quality. In this paper, a new method to reduce cybersickness is proposed. The geometric deformation of the virtual model displayed in the peripheral field of view allows reducing the self-motion perceived by the user. Pilot test results show that visually induced self-motion is reduced with a guaranteed immersion quality while the user navigation parameters are kept.},
  keywords={Navigation;Strain;Legged locomotion;Observers;Lattices;Geometry;Buildings;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Computer graphics;Shape modeling;Mesh geometry models},
  doi={10.1109/VR.2019.8798164},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797901,
  author={Lu, Xueshi and Yu, Difeng and Liang, Hai-Ning and Feng, Xiyu and Xu, Wenge},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={DepthText: Leveraging Head Movements towards the Depth Dimension for Hands-free Text Entry in Mobile Virtual Reality Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1060-1061},
  abstract={Text entry is a common activity in virtual reality (VR) systems. However, there is a limited number of approaches available for mobile VR systems, where it might be inconvenient for users to carry an input device. We propose a novel hands-free text entry technique we call DepthText which leverages the acceleration sensing abilities of built-in IMU sensors of mobile VR systems. Users are able to enter text by moving their head forward. The results of a 5-day study indicate that users can achieve an average of 10.76 words per minute (wpm) on the last day with low errors. This performance is comparable to the dwell-based technique which is the most common way of entering text that is hands-free. One advantage of DepthText over the dwell-based technique is that users can have more control of the pace of selecting characters, rather than being pushed by a pre-set dwell time.},
  keywords={Acceleration;Sensors;Virtual reality;Input devices;Error analysis;Mobile handsets;Training},
  doi={10.1109/VR.2019.8797901},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798017,
  author={Lugrin, Jean-Luc and Landeck, Maximilian and Latoschlk, Marc Erich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulated Reference Frame Effects on Steering, Jumping and Sliding}, 
  year={2019},
  volume={},
  number={},
  pages={1062-1063},
  abstract={In this paper, we investigated the impact of an egocentric simulated frame of reference, the so-called simulated CAVE, on three type of travel techniques: Steering, Jumping and Sliding. Contrary to suggestions from previous work, no significant differences were found regarding spatial awareness between all techniques with or without the simulated CAVE. Our first results also showed a negative effect of the simulated CAVE on participants' motion sickness for every technique, while confirming that the Jumping is eliciting less motion sickness with or without it.},
  keywords={Virtual environments;Task analysis;Dynamics;User interfaces;Tracking;Legged locomotion;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality H.5.2 [Interfaces and Presentation]: User Interfaces},
  doi={10.1109/VR.2019.8798017},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798126,
  author={Palmerius, Karljohan E. Lundin and Lundberg, Jonas},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interaction Design for Selection and Manipulation on Immersive Touch Table Display Systems for 3D Geographic Visualization}, 
  year={2019},
  volume={},
  number={},
  pages={1064-1065},
  abstract={Geographic visualizations are, due to the limited need for vertical navigation, suitable for touch tables. In this poster we consider the design of interaction design for selection and manipulation through touch on the screen used for the display of 3D geographic visualization-in our case the visualization of and interaction with drone traffic over rural and urban areas-focusing on moving from a monoscopic to a more immersive, stereoscopic touch table, and how this move affects the interaction design. With a monoscopic display our stereoscopic vision uses the graphics to perceive the location of the surface, and touch interaction can naturally and intuitively be performed on top of 3D objects. Moving to stereocopic display, for increased sense of immersion, the graphics no longer provide visual cues about the location of the screen. We argue that this motivates modification of the design principles, with an alternative interaction design as a result.},
  keywords={Three-dimensional displays;Stereo image processing;Visualization;Touch sensitive screens;Rendering (computer graphics);Software},
  doi={10.1109/VR.2019.8798126},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797772,
  author={Luo, Siqi and Teather, Robert J.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Camera-Based Selection with Cardboard HMDs}, 
  year={2019},
  volume={},
  number={},
  pages={1066-1067},
  abstract={We present a study of selection techniques for low-cost mobile VR devices, such as Google Cardboard, using the outward facing camera on modern smartphones. We compared three selection techniques, air touch, head ray, and finger ray. Initial evaluation indicates that hand-based selection (air touch) was the worst. A ray cast using the tracked finger position offered much higher selection performance. Our results suggest that camera-based mobile tracking is feasible with ray-based techniques.},
  keywords={Three-dimensional displays;Cameras;Mobile handsets;Task analysis;Performance evaluation;Virtual environments;Two dimensional displays;Mobile VR;selection;Google Cardboard;Human-centered computing → Virtual Reality;Human-centered computing → Pointing},
  doi={10.1109/VR.2019.8797772},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797811,
  author={Luo, Tianren and Liu, Zehao and Pan, Zhigeng and Zhang, Mingmin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Virtual-real Occlusion Method Based on GPU Acceleration for MR}, 
  year={2019},
  volume={},
  number={},
  pages={1068-1069},
  abstract={The mixed reality makes the user's realism strong or weak, depending on the fusion effect of the virtual object and the real scene, and the key factor affecting the virtual-real fusion effect is whether the virtual and real object in the synthetic scene has occlusion consistency. In this paper, a method is proposed that utilizes color and depth information provided by an RGB-D camera to improved occlusion. In this method, based on the GPU, extracting the ROI area where the virtual object is located (the area where the virtual-real occlusion occurs),then introducing the real scence color image as the guide image to modified joint bilateral filtering the depth image to repair the wrong edge information and “black hole” of the depth image;pixel-by-pixel point to determine the depth relationship between the virtual object and real object, to achieve virtual-real occlusion rendering blend;for the still existing “sawtooth artifacts” using delay-coloring only fuzzy local boundary algorithm, to achieve accurate and smoothing edges virtual-real occlusion to enhance visual effects.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Erbium;Mixed Reality;Augmented Reality;Occlusion Handling;RGB-D Camera},
  doi={10.1109/VR.2019.8797811},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797748,
  author={Magaki, Takurou and Vallance, Michael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Developing an Accessible Evaluation Method of VR Cybersickness}, 
  year={2019},
  volume={},
  number={},
  pages={1072-1073},
  abstract={Despite its growing popularity, Virtual Reality (VR) has yet to make a significant impact in conventional education due to its high cost, unconvincing learning data, complexity of the technologies and, persistently, cybersickness. To alleviate this dilemma, it is necessary to develop a straightforward and reliable measurement of cybersickness for VR application developers and mainstream educators. The Empatica E4 wearable device and its eco-system were utilized to record Heart Rate Variability (HRV) and Electrodermal Activity (EDA) during customized computer-based and VR tasks with 16 participants. The metrics of S, NNMean, SDNN, RMSSD, and Poincaré Plot in HRV data and SCR width in EDA data were found to be potential indicators of cybersickness. Further research aims to determine a specific cybersickness index.},
  keywords={Task analysis;Heart rate variability;Physiology;Temperature measurement;Education;Indexes;Cybersickness;Education;Physiological data;H.5.2 [Information Interfaces and Presentation]: User Interfaces;K.3.1 [Computers and Education]: Computer Uses in Education},
  doi={10.1109/VR.2019.8797748},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798122,
  author={Maloney, Divine and Robb, Andrew},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Initial Investigation into Stereotypical Influences on Implicit Racial Bias and Embodied Avatars}, 
  year={2019},
  volume={},
  number={},
  pages={1074-1075},
  abstract={In this paper, we present an initial study to investigate the effects stereotypical settings and avatar appearance of embodied avatars on a user's implicit racial bias. Literature demonstrates the effects embodied avatars can have on a users biases, both implicit and explicit. These shifts in bias and behavior could be caused by the avatars appearance or the stereotypical environment. Few studies have investigated the presence of stereotypical triggers and avatar representation in a learning, game-like environment. With virtual reality entertainment and training simulations becoming popular it is necessary to better understand the effects avatars can have on our behavior, perception, and biases. This study will investigate the potential effects of embodied avatars reinforcing a user's implicit racial biases.},
  keywords={Avatars;Atmospheric measurements;Particle measurements;Clothing;Task analysis;Training;embodied virtual avatars;implicit racial bias;social good},
  doi={10.1109/VR.2019.8798122},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798077,
  author={Maruya, Kazushi and Ohtani, Tomoko},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shadows Can Change the Shape Appearances of Real and Virtual Objects}, 
  year={2019},
  volume={},
  number={},
  pages={1076-1077},
  abstract={The human visual system can estimate the shape of an object casting a shadow. The principle has been widely studied, and utilized in 3D scanners. However, estimating the shape of an object on which a shadow is casted (“screen object”) was rarely investigated. In this study, we show that the casted shadow distorts the perceived shape of a screen object in a class of virtual and physical scenes. In addition, we utilized the principle to create variations of Cafe-wall illusion. The principles can be utilized to control the perceived shapes without changing the structure of the screen object.},
  keywords={Shape;Three-dimensional displays;Optical distortion;Distortion;Casting;Two dimensional displays;Optical imaging;Projection;illumination and shadow;optical illusion;Shape representation;User-centered design;Appearance and texture representation;Psychology},
  doi={10.1109/VR.2019.8798077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797910,
  author={McNamara, Ann and Boyd, Katherine and George, Joanne and Suther, Annie and Jones, Weston and Oh, Somyung},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Information Placement in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1078-1079},
  abstract={In this poster, we develop a technique for placing informational labels in complex Virtual Environments (VEs). The ability to effectively and efficiently present labels in VEs is valuable in Virtual Reality (VR) for many reasons, but the motivation is to bring us closer to a system that delivers information in VR in an optimal way without causing information overload. The novelty of this technique lies in the use of eye tracking as an accurate indicator of attention to identifying objects of interest. Labels associated with such objects of interest are revealed when the user attends to them. We conduct a series of experiments to evaluate label placement based on user attention. This attention is measured using eye tracking. Results show that one method in particular results in 100% accuracy in some scenarios in a search task with little difference in time taken to complete the task.},
  keywords={Task analysis;Gaze tracking;Virtual reality;Information retrieval;Visualization;Three-dimensional displays;Resists;Human-centered computing;Virtual Reality;Information Placement;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797910},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797754,
  author={Menzner, Tim and Otte, Alexander and Gesslein, Travis and Grubert, Jens and Gagel, Philipp and Schneider, Daniel},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Capacitive-sensing Physical Keyboard for VR Text Entry}, 
  year={2019},
  volume={},
  number={},
  pages={1080-1081},
  abstract={In the context of immersive VR Head-Mounted Displays, physical keyboards have been proven to be an efficient typing interface. However, text entry using physical keyboards typically requires external camera-based tracking systems. Touch-sensitive physical keyboards allow for on-surface interaction, with sensing integrated into the keyboard itself, but have not been utilized for VR. We propose to utilize touch-sensitive physical keyboards for text entry as an alternative sensing mechanism for tracking user's fingertips and present a first prototype for VR.},
  keywords={Keyboards;Sensors;Virtual reality;Pins;User interfaces;Standards;Conferences;text entry;touch;physical keyboards;virtual reality;capacitive sensing;H.5. 2: [User Interfaces - Input devices and strategies.]},
  doi={10.1109/VR.2019.8797754},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798009,
  author={Mikawa, Yuri and Sueishi, Tomohiro and Hayakawa, Tomohiko and Ishikawa, Masatoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Laser-based Photochromic Drawing Method for Rotating Objects with High-speed Visual Feedback}, 
  year={2019},
  volume={},
  number={},
  pages={1082-1083},
  abstract={Color-forming display can present digital vision by repeatedly changing its color; this display has been studied in recent research on augmented reality (AR) displays. Previous methods exhibit large-latency and low-speed problems, leading to the lack of interaction in immersive AR experiences. This study proposes a highspeed and low-latency drawing method on dynamically moving objects, using photochromic material colored by ultraviolet light laser, high-speed mirror control, and visual feedback. The experiment evaluated the method's drawing accuracy and we confirmed that this method has an adequate tracking ability against random rotation to produce accurate drawings.},
  keywords={Visualization;Cameras;Image color analysis;Mirrors;Printers;Lasers;Acceleration;Hardware;Communication hardware, interfaces, and storage;Displays and imagers;Human-centered computing;Human computer interaction (HCI);Interaction devices},
  doi={10.1109/VR.2019.8798009},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798320,
  author={Miyamoto, Eisaku and Kijima, Ryugo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Dynamic Characteristics of Head Mounted Display in Parallel Movement with Simultaneous Subjective Observation Method}, 
  year={2019},
  volume={},
  number={},
  pages={1084-1085},
  abstract={In the Head Mounted Display(HMD) system, there is an item called dynamic characteristics such as latency takes for the motion to be reflected on display and motion wave distortion caused by motion estimation. It is said that the difference between the user's sense of motion and visual information due to this deterioration of dynamic characteristics is the factor of virtual reality sickness. Therefore, the dynamic characteristics are an important item of HMD. The final purpose of this research is to establish a method for measuring the dynamic characteristics of HMD. In this paper, we examined whether our proposed simultaneous subjective observation method is valid for the parallel movement and measured dynamic characteristics under various conditions. In conclusion, the method had enough precision to measure the dynamic characteristics of an HMD in the parallel movement and could be applied to various usage situation.},
  keywords={Resists;Cameras;Tracking;Virtual reality;Actuators;Dynamics;Visualization;Dynamic Characteristics;Latency;simultaneous subjective observation method},
  doi={10.1109/VR.2019.8798320},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797726,
  author={Miyata, Akihiro and Uno, Hironobu and Go, Kenro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation on a Wheelchair Simulator Using Limited-Motion Patterns and Vection-Inducing Movies}, 
  year={2019},
  volume={},
  number={},
  pages={1086-1087},
  abstract={Existing virtual reality (VR) based wheelchair simulators have difficulty providing both visual and motion feedback at low cost. To address this issue, we propose a VR-based wheelchair simulator using a combination of motions attainable by an electric-powered wheelchair and vection-inducing movies displayed on a head-mounted display. This approach enables the user to have a richer simulation experience, because the scenes of the movie change as if the wheelchair performs motions that are not actually performable. We developed a proof of concept using only consumer products and conducted evaluation tasks, confirming that our approach can provide a richer experience for barrier simulations.},
  keywords={Wheelchairs;Resists;Motion pictures;Roads;Visualization;Solid modeling;Computational modeling;HMD;Wheelchair;Simulator;Vection;Human-centered computing;Accessibility;Accessibility technologies;Human computer interaction (HCI);Interactive systems and tools},
  doi={10.1109/VR.2019.8797726},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797976,
  author={Mizutani, Junya and Matsumoto, Keigo and Nagao, Ryohei and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Estimation of Detection Thresholds for Redirected Turning}, 
  year={2019},
  volume={},
  number={},
  pages={1090-1091},
  abstract={Redirection makes it possible to walk around a vast virtual space in a limited real space while providing a natural walking sensation by applying a gain to the amount of movement in a real space. However, manipulating the walking path while keeping it and maintaining the naturalness of walking when turning at a corner cannot be achieved by the existing methods. To realize natural manipulation for turning at a corner, this study proposes novel “turning gains”, which refer to the increase in real and virtual turning degrees. The result of an experiment which aims to estimate the detection thresholds of turning gains indicated that when the turning radius is 0.5 m, discrimination is more difficult compared with the rotation gains (r = 0.0m).},
  keywords={Turning;Legged locomotion;Resists;Virtual environments;Three-dimensional displays;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797976},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798186,
  author={Murnane, Mark and Breitmeyer, Max and Matuszek, Cynthia and Engel, Don},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality and Photogrammetry for Improved Reproducibility of Human-Robot Interaction Studies}, 
  year={2019},
  volume={},
  number={},
  pages={1092-1093},
  abstract={Collecting data in robotics, especially human-robot interactions, traditionally requires a physical robot in a prepared environment, that presents substantial scalability challenges. First, robots provide many possible points of system failure, while the availability of human participants is limited. Second, for tasks such as language learning, it is important to create environments that provide interesting' varied use cases. Traditionally, this requires prepared physical spaces for each scenario being studied. Finally, the expense associated with acquiring robots and preparing spaces places serious limitations on the reproducible quality of experiments. We therefore propose a novel mechanism for using virtual reality to simulate robotic sensor data in a series of prepared scenarios. This allows for a reproducible dataset that other labs can recreate using commodity VR hardware. We demonstrate the effectiveness of this approach with an implementation that includes a simulated physical context, a reconstruction of a human actor, and a reconstruction of a robot. This evaluation shows that even a simple “sandbox” environment allows us to simulate robot sensor data, as well as the movement (e.g., view-port) and speech of humans interacting with the robot in a prescribed scenario.},
  keywords={Robot sensing systems;Solid modeling;Virtual reality;Human-robot interaction;Three-dimensional displays;Data models;Virtual Reality;VR;Photogrammetry;Human-Robot Interaction;Virtual Presence;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented and Virtual Realities;I.2.9 [Artificial Intelligence]: Robotics—Operator Interfaces;I.2.9 [Artificial Intelligence]: Robotics—Sensors},
  doi={10.1109/VR.2019.8798186},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798206,
  author={Nakamura, Akihiro and Sakai, Shinji and Shidoji, Kazunori},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Rotation with Visuo-Haptics}, 
  year={2019},
  volume={},
  number={},
  pages={1094-1095},
  abstract={Redirected walking has been proposed as a means of making a narrow space feel wide in a virtual space. We examined the effect of visuo-haptics on the detection threshold of rotation gain when participants walked around a wall. We found that the threshold was affected by visuo-haptics only when they walked around the outside of the wall but was not affected when they walked around the inside of the wall.},
  keywords={Haptic interfaces;Legged locomotion;Virtual reality;Three-dimensional displays;Analysis of variance;Training;Estimation;Virtual reality;virtual rotation;visuo-haptics},
  doi={10.1109/VR.2019.8798206},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798336,
  author={Nakano, Kizashi and Horita, Daichi and Sakata, Nobuchika and Kiyokawa, Kiyoshi and Yanai, Keiji and Narumi, Takuji},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enchanting Your Noodles: GAN-based Real-time Food-to-Food Translation and Its Impact on Vision-induced Gustatory Manipulation}, 
  year={2019},
  volume={},
  number={},
  pages={1096-1097},
  abstract={We propose a novel gustatory manipulation interface which utilizes the cross-modal effect of vision on taste elicited with augmented reality (AR)-based real-time food appearance modulation using a generative adversarial network (GAN). Unlike existing systems which only change color or texture pattern of a particular type of food in an inflexible manner, our system changes the appearance of food into multiple types of food in real-time flexibly, dynamically and interactively in accordance with the deformation of the food that the user is actually eating by using GAN-based image-to-image translation. The experimental results reveal that our system successfully manipulates gustatory sensations to some extent and that the effectiveness depends on the original and target types of food as well as each user's food experience.},
  keywords={Real-time systems;Visualization;Resists;Generative adversarial networks;Gallium nitride;Olfactory;Servers;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception;Machine learning;Machine learning approaches;Neural networks},
  doi={10.1109/VR.2019.8798336},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797845,
  author={Nguyen, Huyen and Ward, Benjamin and Engelke, Ulrich and Thomas, Bruce and Bednarz, Tomasz},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative Data Analytics Using Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1098-1099},
  abstract={Immersive analytics allows a large amount of data and complex structures to be concurrently investigated. We propose a collaborative analytics system that benefits from new advances in immersive technologies for collaborators working in the early stages of data exploration. We implemented a combination of Star Coordinates and Star Plot visualisation techniques to support the visualisation of multidimensional data and the encoding of datasets using simple and compact visual representations. To support data analytics tasks, we propose tools and interaction techniques for users to build decision trees for visualising and analysing data in a top-down method.},
  keywords={Data visualization;Collaboration;Data analysis;Task analysis;Decision trees;Extremities;Virtual reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797845},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797778,
  author={Nie, Guang-Yu and Liu, Yun and Wang, Cong and Liu, Yue and Wang, Yongtian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Stereovision-Based 3-D Scene Reconstruction for Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1100-1101},
  abstract={Three-dimensional (3-D) scene reconstruction is one of the key techniques in Augmented Reality (AR), which is related to the integration of image processing and display systems of complex information. Stereo matching is a computer vision based approach for 3-D scene reconstruction. In this paper, we explore an improved stereo matching network, SLED-Net, in which a Single Long Encoder-Decoder is proposed to replace the stacked hourglass network in PSM-Net for better contextual information learning. We compare SLED-Net to state-of-the-art methods recently published, and demonstrate its superior performance on Scene Flow and KITTI2015 test sets.},
  keywords={Image reconstruction;Convolution;Training;Three-dimensional displays;Mercury (metals);Augmented reality;Image color analysis;Computing methodologies;Scene understanding;Reconstruction;Mixed/augmented reality},
  doi={10.1109/VR.2019.8797778},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798145,
  author={Nishimura, Ryosuke and Sakata, Nobuchika and Tominaga, Tomu and Hijikata, Yoshinori and Harada, Kensuke and Kiyokawa, Kiyoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Speech-Driven Facial Animation by LSTM-RNN for Communication Use}, 
  year={2019},
  volume={},
  number={},
  pages={1102-1103},
  abstract={The goal of this research is developing a system that a rich facial animation can be used in communication is generated from only speech. Generally, a source of the generating facial animation is a camera. Using cameras as an input source, it causes limitations of the angle of view of the camera or problems that cannot be aware of the human face, depending on the orientation of the face. Therefore, it is reasonable for developing a system for generating a facial animation using only voice. In this study, we generate facial expressions from only speech using LSTM-RNN. Comparing 3 patterns of speech analysis data, we showed that the proposed method using A-weighting is effective for facial expression estimation.},
  keywords={Facial animation;Cameras;Face;Shape;Deep learning;Spectrogram;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798145},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797912,
  author={Nomura, Miyu and Oku, Hiromasa},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Edible Lens Made of Agar}, 
  year={2019},
  volume={},
  number={},
  pages={1104-1105},
  abstract={In this paper, we propose an edible lens made from foodstuffs. The optical lens is used for forming an optical system. It is expected that it will be possible to make optical systems edible by preparing an edible lens that has the same function as the conventional optical lens. In order to realize this, prototypes of edible lens made of agar were developed, because the agar had been reported as a material to form edible retroreflector materials. Furthermore, we investigated its optical performance.},
  keywords={Lenses;Prototypes;Biomedical optical imaging;Optical refraction;Optical variables control;Image resolution;Optical imaging;Edible;Lens;Agar;Hardware;Emerging technologies;Analysis and design of emerging devices and systems;Emerging tools and methodologies},
  doi={10.1109/VR.2019.8797912},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797966,
  author={Norman, Mitchell and Lee, Gun and Smith, Ross T. and Billinqhurs, Mark},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Mixed Presence Collaborative Mixed Reality System}, 
  year={2019},
  volume={},
  number={},
  pages={1106-1107},
  abstract={Research has shown that Mixed Presence Groupware (MPG) systems are a valuable collaboration tool. However research into MPG systems is limited to a handful of tabletop and Virtual Reality (VR) systems with no exploration of Head-Mounted Display (HMD) based Augmented Reality (AR) solutions. We present a new system with two local users and one remote user using HMD based AR interfaces. Our system provides tools allowing users to layout a room with the help of a remote user. The remote user has access to a marker and pointer tools to assist in directing the local users. Feedback collected from several groups of users showed that our system is easy to learn but could have increased accuracy and consistency.},
  keywords={Collaboration;Resists;Prototypes;Tools;Augmented reality;Webcams;Augmented Reality;remote collaboration;mixed presence;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces;Collaborative computing;H.l.2 [Models and Principles]: User/Machine Systems;Human Factors},
  doi={10.1109/VR.2019.8797966},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798120,
  author={Onuki, Yoshikazu and Kurnazawa, Itsuo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reorient the Gazed Scene Towards the Center: Novel Virtual Turning Using Head and Gaze Motions and Blink}, 
  year={2019},
  volume={},
  number={},
  pages={1108-1109},
  abstract={Novel virtual turning for stationary VR environments, accomplishing to reorient the gazed view towards the center, is proposed. Prompt reorientation during rapid head motion and blinking performed unnoticeable scene switching that achieved the seamless user experience, especially for the wide-angle turning. Whereas, continuous narrow-angle turning by horizontally rotating the virtual world corresponding to the face orientation achieved enhanced sense of reality. The proposal comprises a hybrid of these two turning schemes. Experiments using simulator sickness and presence questionnaires revealed that our methods achieved comparable or lower sickness scores and higher presence scores than conventional smooth and snap turns.},
  keywords={Turning;Face;Task analysis;Switches;Proposals;User experience;Games;Human-centered computing;Interaction paradigms;Virtual reality;Interaction design;Interaction design process and methods;Interface design prototyping},
  doi={10.1109/VR.2019.8798120},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798182,
  author={Ortega, Francisco R. and Tarre, Katherine and Kress, Mathew and Williams, Adam S. and Barreto, Armando B. and Rishe, Naphtali D.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Selection and Manipulation Whole-Body Gesture Elicitation Study in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1110-1111},
  abstract={We present a whole-body gesture elicitation study using Head Mounted Displays, including a legacy bias reduction technique. The motivation for this study was to understand the type of gesture agreement rates for selection and manipulation interactions and to improve the user experience for whole-body interactions. We found that regardless of the production technique used to remove legacy bias, legacy bias was still found in some of the produced gestures.},
  keywords={Wheels;Three-dimensional displays;Virtual reality;User interfaces;Resists;Magnetic resonance imaging;Gesture Elicitation;Gestures;Virtual Reality},
  doi={10.1109/VR.2019.8798182},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798235,
  author={Osato, Yui and Koizurni, Naoya},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optical System That Forms a Mid-Air Image Moving at High Speed in the Depth Direction}, 
  year={2019},
  volume={},
  number={},
  pages={1112-1113},
  abstract={Mid-air imaging technology expresses how virtual images move about in the real world. A conventional mid-air image display using a retro-transmissive optical element moves a light source the distance a mid-air image is moved. In conventional mid-air image displays, the linear actuator that moves a display as a light source makes the system large. In order to solve this problem, we designed an optical system that realizes high-speed movement of mid-air images without a linear actuator. We propose an optical system that moves the virtual image of the light source at a high speed by generating the virtual image of the light source with a rotating mirror and light source by the motor.},
  keywords={Augmented reality;mid-air imaging;high-speed movement in depth direction;Hardware;Emerging technologies;Emerging optical and photonic technologies},
  doi={10.1109/VR.2019.8798235},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798345,
  author={Park, Chanho and Jang, Kyungho},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigation of Visual Self-Representation for a Walking-in-Place Navigation System in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1114-1115},
  abstract={Walking-in-place (WIP) is one of the techniques for navigation in virtual reality (VR), and it can be configured in a limited space with a simple algorithm. Although WIP systems provide a sense of movement, it is important to deliver immersive VR experiences by providing information as similar as possible to walking in the real world. There have been many studies on the WIP technology, but it has rarely been done on visual self-representation of WIP in the virtual environment (VE). In this paper, we describe our investigation of virtual self-representation for application to a WIP navigation system using a HMD and full body motion capture system. Our system is designed to move in the pelvis direction by calculating the inertial sensor data, and a virtual body that is linked to the user's movement is seen from the first-person perspective (1PP) in two ways: (i) full body, and (ii) full body with natural walking. In (ii), when a step is detected, the motion of the lower part of the avatar is manipulated as if the user is performing real walking. We discuss the possibility of visual self-representation for the WIP system.},
  keywords={Foot;Legged locomotion;Avatars;Visualization;Navigation;Virtual environments;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/VR.2019.8798345},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798168,
  author={Passalenti, Andrea and Paisa, Razvan and Nilsson, Niels C. and Andersson, Nikolaj S. and Fontana, Federico and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={No Strings Attached: Force and Vibrotactile Feedback in a Virtual Guitar Simulation}, 
  year={2019},
  volume={},
  number={},
  pages={1116-1117},
  abstract={The poster describes a multisensory simulation of plucking guitar strings in virtual reality and a user study evaluating the simulation. Auditory feedback is generated by a physics-based simulation of guitar strings, and haptic feedback is provided by a combination of high fidelity vibrotactile actuators and a Phantom Omni. The study compared four conditions: no haptic feedback, vibrotactile feedback, force feedback, and a combination of force and vibrotactile feedback. The results indicate that the combination of vibrotactile and force feedback elicits the most realistic experience, and during this condition, participants were less likely to inadvertently hit strings. Notably, no significant differences were found between the conditions involving either vibrotactile or force feedback.},
  keywords={Force;Solid modeling;Force feedback;Instruments;Phantoms;Music;I.3.7 [Computer Graphics]: Three-Dimenshional Graphics and Realism—Virtual Reality},
  doi={10.1109/VR.2019.8798168},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797959,
  author={Peng, Zhengfu and Lu, Ting and Chen, Zhaowen and Xu, Xiangmin and Lin, Shu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Human Face Reconstruction under a HMD Occlusion}, 
  year={2019},
  volume={},
  number={},
  pages={1118-1119},
  abstract={With the help of existing augmented vision perception motion capture technologies, virtual reality (VR) can make users immerse in virtual environments. But users are difficult to convey their actual emotions to others in virtual environments. Since the head-mounted displays (HMDs) significantly obstruct users face, it is hard to recover the full face directly with traditional techniques. In this paper, we introduce a novel method to address this problem by only using the RGB image of a person, without the need of any other sensors or devices. Firstly, we utilize the facial landmark points to estimate the face shape, expression and pose of the user. Then with the information of the Non occlusion face area, we could recover the face texture and the illumination of the current scene.},
  keywords={Face;Image reconstruction;Three-dimensional displays;Shape;Solid modeling;Two dimensional displays;Resists;H.5.1—Information Systems—Artificial-Augmented and Virtual Realities},
  doi={10.1109/VR.2019.8797959},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798076,
  author={Pimentel, Daniel and Amaya, Ricardo and Halan, Shiva and Kalyanaraman, Sri and Bailenson, Jeremy},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Climate Change on Your Plate: A VR Seafood Buffet Experience}, 
  year={2019},
  volume={},
  number={},
  pages={1120-1121},
  abstract={The use of virtual reality (VR) to depict climate change impacts is a popular strategy used by environmental, news, and political organizations to encourage pro-environmental outcomes. However, despite widespread dissemination of immersive content, climate change mitigation efforts remain tepid. In response, we present a VR simulation conveying the adverse effects of climate change in a personally-relevant fashion. The “Virtual Seafood Buffet” experience allows users to select from dozens of lifelike virtual seafood items and experience the degradation of that particular species based on projected climate change impacts. The developed simulation is proposed as an intervention designed to encourage climate change mitigation efforts. An overview of the simulation, its purpose, and directions for future research are outlined herein.},
  keywords={Degradation;Virtual reality;Oceans;Psychology;Visualization;Solid modeling;Climate change;Climate change;food;virtual reality;Human Computer Interaction (HCI)},
  doi={10.1109/VR.2019.8798076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797681,
  author={Pimentel, Daniel and Kalyanaraman, Sri and Fillingim, Roger and Halan, Shiva},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR and Volitional Pain: Testing Immersive Interventions During a Tattoo}, 
  year={2019},
  volume={},
  number={},
  pages={1122-1123},
  abstract={The efficacy of Virtual reality (VR) as a non-pharmacological pain remedy remains widely supported. Yet, despite robust findings establishing VR as a pain reducer, the strength of this claim is weakened by several factors, namely restricted accessibility to vulnerable subjects and variability in pain type/severity. To address these issues, we propose testing the analgesic effects of VR on volitional pain, namely pain experienced during a tattoo. Leveraging qualitative interviews with tattoo artists and customers, a 3-month on-site field experiment at a tattoo parlor was conducted. Preliminary findings support the efficacy of volitional pain as a means by which to test the analgesic effects of VR interventions. Methodological considerations and guidelines for follow-up investigations using this experimental approach are discussed.},
  keywords={Pain;Virtual reality;Testing;Interviews;Media;Wounds;Limiting;Tattoo;pain;VR;methodology;Human Computer Interaction (HCI);Virtual reality},
  doi={10.1109/VR.2019.8797681},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798174,
  author={Ping, Jiamin and Liu, Yue and Weng, Dongdong},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparison in Depth Perception between Virtual Reality and Augmented Reality Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1124-1125},
  abstract={The virtual reality (VR) and augmented reality (AR) applications have been widely used in a variety of fields; one of the key requirements in a VR or AR system is to understand how users perceive depth in the virtual environment and augmented reality. This paper conducts an experiment to compare users' performance of depth perception in VR and AR system using an optical see-through head-mounted display (HMD). The result shows that the accuracy of depth estimation in AR is higher than in VR. Besides, the matching error increases as the distance becomes farther.},
  keywords={Task analysis;Augmented reality;Resists;Adaptive optics;Bars;Virtual environments;Human-centered computing;Interaction paradigms;Virtual reality;Mixed/augmented reality;Computing methodologies;Computer Graphics;Graphics systems and interface;Perception},
  doi={10.1109/VR.2019.8798174},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797908,
  author={Pittman, Corey and LaViola, Joseph J.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Determining Design Requirements for AR Physics Education Applications}, 
  year={2019},
  volume={},
  number={},
  pages={1126-1127},
  abstract={While we are in the midst of a renaissance of interest in augmented reality (AR), there remain a small number of application domains which have seen significant development. Education is a domain that often drives innovation with emerging technologies. One particular subject which benefits from additional visualization capabilities is physics. In this paper, we present the results of a series of interviews with secondary school teachers about their experience with AR and the features which would be most beneficial to them from a pedagogical perspective. To gather meaningful information, a prototype application was developed and presented to the teachers. Based on the feedback collected from the teachers, we present a set of design recommendations for AR physics education tools, as well as other useful collects comments.},
  keywords={Interviews;Prototypes;Augmented reality;Physics education;Headphones;Augmented Reality;Physics Education;Qualitative Interview;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797908},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797773,
  author={Pohl, Daniel and Achtelik, Markus},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Personalized Personal Spaces for Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1128-1129},
  abstract={An important criterion for virtual reality experiences is that they are very immersive. The person inside the head-mounted display feels like really being in the virtual environment. While this can be a very pleasant experience, the opposite can happen as well. The concepts of personal spaces and people or unfriendly avatars entering them, can lead to the same discomfort as if it would happen in real life. In this work, we propose to define multi-level artificial barriers for other avatars and objects, respecting the personal spaces as defined by users. We apply this to both interactive rendered environments and as much as possible also to 360 degree photo and video content.},
  keywords={Avatars;Games;Engines;Meters;Virtual environments;Head-mounted displays;Social and professional topics;User characteristics},
  doi={10.1109/VR.2019.8797773},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798249,
  author={Rajeswaran, Pavithra and Kesavadas, Thenkurussi and Jani, Priti and Kumar, Praveen},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={AirwayVR: Virtual Reality Trainer for Endotracheal Intubation-Design Considerations and Challenges}, 
  year={2019},
  volume={},
  number={},
  pages={1130-1131},
  abstract={Endotracheal Intubation is a lifesaving procedure in which a tube is passed through the mouth into the trachea (windpipe) to maintain an open airway and facilitate artificial respiration. It is a complex psychomotor skill, which requires significant training and experience to prevent complications. The current methods of training, including manikins and cadaver, have limitations in terms of their availability for early medical professionals to learn and practice. These training options also have limitations in terms of presenting high risk/ difficult intubation cases for experts to mentally plan their approach in high-risk scenarios prior to the procedure. In this paper, we present the design considerations and challenges of AirwayVR: virtual reality-based simulation trainer for intubation training for two different target audience (medical professionals) with two different objectives. The first one is to use AirwayVR as an introductory platform to learn and practice intubation in virtual reality for novice learners (Medical students and residents). The second objective is to utilize this technology as a Just-in-time training platform for experts to mentally prepare for a complex case prior to the procedure.},
  keywords={Training;Virtual reality;Biomedical imaging;Three-dimensional displays;Medical services;Atmospheric modeling;Solid modeling;Endotracheal Intubation;Virtual reality;Medical simulation;Medical skills training;Application of Virtual reality for skill training in medical simulation;Using Virtual reality for Intubation training;Just-in-time training using virtual reality in medicine},
  doi={10.1109/VR.2019.8798249},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798349,
  author={Rasmussen, Loki and Basinger, Jay and Milanova, Mariofanna},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Networking Consumer Systems to Provide a Development Environment for Inside-Out Marker-Less Tracking for Virtual Reality Headsets}, 
  year={2019},
  volume={},
  number={},
  pages={1132-1133},
  abstract={Many companies are working to create inside-out marker-less tracking for virtual reality headsets. Inside-out marker-less tracking can be found on consumer augmented reality devices, but currently there is no system available to researchers, developers, or consumers that provides this feature without custom hardware and software. Our research provides a development environment for testing content that would take advantage of this feature before consumer level inside-out marker-less virtual reality systems hit the market. The solution utilizes current commercial off-the-shelf hardware systems, and by networking them together, allows the user to move through a captured environment without needing tracking towers. Ultimately, it provides researchers and developers the opportunity to test content that both takes advantage of the free movement through rooms provided by inside-out tracking and is yet constrained by real-world boundaries such as walls within the virtual environment.},
  keywords={Virtual environments;Tracking;Calibration;Headphones;Augmented reality;Testing;Inside-Out-Tracking;virtual reality;procedural generation;marker-less},
  doi={10.1109/VR.2019.8798349},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797873,
  author={Rauter, Michael and Abseher, Christoph and Safar, Markus},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmenting Virtual Reality with Near Real World Objects}, 
  year={2019},
  volume={},
  number={},
  pages={1134-1135},
  abstract={Pure virtual reality headsets lack support for user awareness of real world objects. We show how to augment a virtual environment with real world objects by incorporating color and stereo information from the front-mounted stereo camera system of the HTC Vive Pro. With an adjustable amount of virtual and real elements by specifying the depth range of interest for real objects these can be either always augmented on top of the virtual reality or embedded in the virtual world with correct occlusion. Possible use cases are training with appliances that require you to see both the appliance as well as your hands, collaborating in virtual reality with the real users instead of avatars, as well as static and moving obstacle detection. Objects of interest are cut out depending on depth values of the stereo matching system's depth image. The depth image matches only one of both camera sensors so a second depth image is derived to match the other camera sensor as well. We overlay the rendered virtual reality images for both eyes with the corresponding cut-out camera images. Furthermore we address the problem of missing depth data especially for very near objects with an integral image based depth-dependent foreground mask expansion algorithm. Since a comfortable mixed reality experience requires high framerates of optimally 90 frames per second we utilize the GPU for optimized implementations of the algorithms and computations involved.},
  keywords={Cameras;Sensors;Image sensors;Runtime;Headphones;Virtual environments},
  doi={10.1109/VR.2019.8797873},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798253,
  author={Ferrer, César Daniel Rojas and Shishido, Hidehiko and Kitahara, Itaru and Kameda, Yoshinari},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Exploratory Activity under Microgravity Conditions in VR: An Exploratory Study during a Parabolic Flight}, 
  year={2019},
  volume={},
  number={},
  pages={1136-1137},
  abstract={This work explores the human visual exploratory activity (VEA) in a microgravity environment compared to one-G. Parabolic flights are the only way to experience microgravity without astronaut training, and the duration of each microgravity segment is less than 20 seconds. Under such special conditions, the test subject visually searches a virtual representation of the International Space Station located in his Field of Regard (FOR). The task was repeated in two different postural positions. Interestingly, the test subject reported a significant reduction of microgravity-related motion sickness while experiencing the VR simulation, in comparison to his previous parabolic flights without VR.},
  keywords={Visualization;Task analysis;Extraterrestrial measurements;Gravity;Resists;Three-dimensional displays;International Space Station;Microgravity;Visual Exploratory Activity;Presence;Motion Sickness;Human-centered computing;Virtual reality;Computing methodologies;Perception},
  doi={10.1109/VR.2019.8798253},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798055,
  author={Rosa, Nina and van Bommel, Jean-Paul and Hürst, Wolfgang and Nijboer, Tanja and Veltkamp, Remco C. and Werkhoven, Peter},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Embodying an Extra Virtual Body in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1138-1139},
  abstract={Presence and the sense of embodiment are essential concepts for the experience of our self and virtual bodies, but there is little quantitative evidence for a relation between these, and this relation becomes more complicated when there are real and virtual bodies in augmented reality (AR). We investigate the experience of body ownership, agency, self-location and self-presence in AR where users can see their real body and a virtual body from behind. Active arm movement congruency and virtual anthropomorphism are varied. We found significant effects of movement congruency but not anthropomorphism, a strong correlation between self-presence and body ownership, and a moderate correlation between self-presence and agency and self-location.},
  keywords={Anthropomorphism;Avatars;Augmented reality;Correlation;Conferences;Cameras;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Empirical studies in HCI},
  doi={10.1109/VR.2019.8798055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797909,
  author={Rossa, Pedro and Horota, Rafael Kenji and Marques Junior, Ademir and Aires, Alysson Soares and De Souza, Eniuce Menezes and Kannenberg, Gabriel Lanzer and De Fraga, Jean Luca and Santana, Leonardo and Alves, Demetrius Nunes and Boesing, Julia and Gonzaga, Luiz and Veronez, Maurício Roberto and Cazarin, Caroline Lessio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={MOSIS: Immersive Virtual Field Environments for Earth Sciences}, 
  year={2019},
  volume={},
  number={},
  pages={1140-1141},
  abstract={For the past decades, environmental studies have been mostly a field activity, especially when concerning geosciences, where rock exposures could not be represented or taken into laboratories. Besides that, VR (Virtual Reality) is growing in many academic areas as an important technology to represent 3D objects, bringing immersion to the most simple tasks. Following that trend, MOSIS (Multi Outcrop Sharing and Interpretation System) was created to help earth scientists and other users to visualize and study VFEs (Virtual Field Environments) from all over the world in immersive virtual reality.},
  keywords={Tools;Geology;Solid modeling;Three-dimensional displays;Software;Virtual reality;Oils;Immersive visualization;Digital outcop model;Virtual outcrop model;Geological Interpretation;Virtual reality;Reservoir;Oil;Gas},
  doi={10.1109/VR.2019.8797909},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798372,
  author={Sano, Yuji and Sato, Koya and Shiraishi, Ryoichiro and Otsuki, Mai},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Player Perception Augmentation for Beginners Using Visual and Haptic Feedback in Ball Game}, 
  year={2019},
  volume={},
  number={},
  pages={1142-1143},
  abstract={We developed Sports Support System that augments the perception of beginner players and supports situation awareness to motivate beginners in multiplayer sports through visual and haptic feedback. Our system provides positional relationship of the opponents using visual feedback. In addition, the position of the opponent beyond the field of view was provided using haptic feedback. An experiment of pass interception as used to compare the visual and haptic feedback. The results confirmed the effectiveness and characteristics of these feedback processes in a multiplayer ball game.},
  keywords={Haptic interfaces;Visualization;Sports;Games;Vibrations;Three-dimensional displays;Actuators;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR.2019.8798372},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798364,
  author={Sasaki, Hisayuki and Watanabe, Hayato and Okaichi, Naoto and Hisatomi, Kensuke and Kawakita, Masahiro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Color Moiré Reduction Method for Thin Integral 3D Displays}, 
  year={2019},
  volume={},
  number={},
  pages={1144-1145},
  abstract={The integral three-dimensional (3D) display is an ideal visual 3D user interface. It is a display method that fulfills many of the physiological factors of human vision. However, in integral 3D displays for mobile applications that use a direct viewing flat panel to display elemental images, the occurrence of color moiré is a problem owing to the sampling of subpixels by the elemental lenses and the insufficient resolution and depth reproduction performance of the reconstructed 3D image. We propose a method to solve these problems that utilizes optical wobbling spatiotemporal multiplexing using a birefringent element and a polarization controller. With the conventional moiré reduction method, the degree of defocus of el-ementallenses has to be set to a large value, which has also been a factor that reduces the depth reproduction performance. We show that effective color moiré reduction can be achieved with a slight defocus of elemental lenses and without deteriorating depth reproducibility by using the proposed optical wobbling method.},
  keywords={Three-dimensional displays;Image color analysis;Lenses;Optical imaging;Optical polarization;Organic light emitting diodes;Optical modulation;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers;Hardware;Communication hardware;interfaces and storage},
  doi={10.1109/VR.2019.8798364},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798075,
  author={Sato, Miko and Funato, Yuki and Oku, Hiromasa},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Edible Retroreflector Made of Candy}, 
  year={2019},
  volume={},
  number={},
  pages={1146-1147},
  abstract={In this research, we propose an edible retroreflector made of candy. Previously proposed edible retroreflector was made of agar, granulated sugar, and water, so it was weak against drying and lost its function in a short period of time. However, solid foodstuffs like candy are stable against drying. Thus, if it was possible to create a retroreflector from candy, longer lifetime is expected to be achieved. Therefore, edible retroreflectors made of candy was developed, and the performances as an optical device were evaluated.},
  keywords={Prototypes;Light sources;Sugar;Optical reflection;Cameras;Optical imaging;Glass;Optical marker;Retroreflector;Edible;Food;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems;Object detection;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality},
  doi={10.1109/VR.2019.8798075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798100,
  author={Scavarelli, Anthony and Arya, Ali and Teather, Robert J.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards a Framework on Accessible and Social VR in Education}, 
  year={2019},
  volume={},
  number={},
  pages={1148-1149},
  abstract={In this extended abstract, we argue that for virtual reality to be a successful tool in social learning spaces (e.g. classrooms or museums) we must also look outside the virtual reality literature to provide greater focus on accessible and social collaborative content. We explore work within Computer Supported Collaborative Learning (CSCL) and social VR domains to move towards developing a design framework for socio-educational VR. We also briefly describe our work-in-progress application framework, Circles, including these features in WebVR.},
  keywords={Collaboration;Resists;Education;Avatars;Prototypes;Virtual Reality;multi-device;multi-user;WebVR;interaction techniques;education;collaboration;CLSL;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Applied computing;Interactive learning environments},
  doi={10.1109/VR.2019.8798100},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797760,
  author={Schütz, Markus and Wimmer, Michael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Live Coding of a VR Render Engine in VR}, 
  year={2019},
  volume={},
  number={},
  pages={1150-1151},
  abstract={Live coding in virtual reality allows users to create and modify their surroundings through code without the need to leave the virtual reality environment. Previous work focuses on modifying the scene. We propose an application that allows developers to modify virtually everything at runtime, including the scene but also the render engine, shader code and input handling, using standard desktop IDEs through a desktop mirror.},
  keywords={Encoding;Runtime;Engines;Rendering (computer graphics);Mirrors;Virtual reality;C++ languages;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality},
  doi={10.1109/VR.2019.8797760},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797917,
  author={Shen, Songjia and Chen, Hsiang-Ting and Leong, Tuck Wah},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Training Transfer of Bimanual Assembly Tasks in Cost-Differentiated Virtual Reality Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1152-1153},
  abstract={Recent advances of the affordable virtual reality headsets make virtual reality training an economical choice when compared to traditional training. However, these virtual reality devices present a range of different levels of virtual reality fidelity and interactions. Few works have evaluated their validity against the traditional training formats. This paper presents a study that compares the learning efficiency of a bimanual gearbox assembly task among traditional training, virtual reality training with direct 3D inputs (HTC VIVE), and virtual reality training without 3D inputs (Google Cardboard). A pilot study was conducted and the result shows that HTC VIVE brings the best learning outcomes.},
  keywords={Training;Task analysis;Virtual reality;Three-dimensional displays;Headphones;Google;Software;Assistive systems;Head-mounted Display;Virtual Reality;Learning Transfer},
  doi={10.1109/VR.2019.8797917},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797960,
  author={Shimamura, Ryo and Kayukawa, Seita and Nakatsuka, Takayuki and Miyakawa, Shoki and Morishima, Shigeo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Study on the Sense of Burden and Body Ownership on Virtual Slope}, 
  year={2019},
  volume={},
  number={},
  pages={1154-1155},
  abstract={This paper provides insight into the burden when people are walking up and down slopes in a virtual environment (VE) while actually walking on a flat floor in the real environment (RE). In RE, we feel a physical load during walking uphill or downhill. To reproduce such physical load in the VE, we provided visual stimuli to users by changing their step length. In order to investigate how the stimuli affect a sense of burden and body ownership, we performed a user study where participants walked on slopes in the VE. We found that changing the step length has a significant impact on a burden on the user and less correlation between body ownership and step length.},
  keywords={Legged locomotion;Visualization;Virtual environments;Correlation;Resists;Task analysis;Human-centered computing;Virtual Reality},
  doi={10.1109/VR.2019.8797960},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798227,
  author={Shimizu, Shunki and Sumi, Kaoru},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sports Training System for Visualizing Bird's-Eye View from First-Person View}, 
  year={2019},
  volume={},
  number={},
  pages={1156-1158},
  abstract={In ball games, it is important that the players are able to estimate the position of the other players from a bird's-eye view based on the information obtained from their first-person view. We have developed a training system for improving this ability. The user wears a head-mounted display and can simulate ball games in 360° from the first-person view. The system allows the user to rearrange all players, and a ball from the bird's-eye view. The user can then track the other players from the first-person viewpoint and perform actions specific to the ball game such as passing, receiving a ball, and (if a defense player) following offense players.},
  keywords={Sports;Training;Games;Visualization;Cognition;Tracking;Task analysis;Sports training system;Situational judgment skill;Spatial cognition ability;Bird's-eye view;[Software and its engineering]: Virtual worlds software-Interactive games;K.3 [COMPUTER AND EDUCATION]: Miscellaneous-Computer literacy},
  doi={10.1109/VR.2019.8798227},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797855,
  author={Simon, Gwendal and Petrangeli, Stefano and Carr, Nathan and Swaminathan, Viswanathan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Streaming a Sequence of Textures for Adaptive 3D Scene Delivery}, 
  year={2019},
  volume={},
  number={},
  pages={1159-1160},
  abstract={Delivering rich, high quality 3D scenes over the internet is challenged by the size of the 3D objects in terms of geometry and textures. This paper proposes a new method for the delivery of textures, which are encoded and delivered as a video sequence, rather than independently. Implemented on the existing video delivery infrastructure, our method provides a fine-grained control on the quality of the resulting video sequence.},
  keywords={Streaming media;Three-dimensional displays;Image coding;Image reconstruction;Video sequences;Delays;Encoding;streaming;textures;3D scene;H.5.1 [Information Systems]: Information Presentation and Interfaces;Multimedia Information Systems},
  doi={10.1109/VR.2019.8797855},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798342,
  author={Simon, Gwendal and Swaminathan, Viswanathan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Open Initiative for the Delivery of Infinitely Scalable and Animated 3D Scenes}, 
  year={2019},
  volume={},
  number={},
  pages={1161-1162},
  abstract={Planet-scale Augmented Reality (AR) and photorealistic Virtual Reality (VR) are two examples of applications that require the delivery of a rich, wide, and animated 3D scene. We extend this concept to infinitely-scalable and animated 3D scenes and we propose key concepts to launch an open initiative for the delivery of these 3D scenes. We draw on the lessons learned from large-scale implementation of video streaming to design a pull-based mechanism with segmented representations of objects and periodic map updates.},
  keywords={Three-dimensional displays;Servers;Cameras;Animation;Games;Streaming media;Virtual reality;streaming;textures;3D scene;H.5.1 [Information Systems]: Information Presentation and Interfaces;Multimedia Information Systems},
  doi={10.1109/VR.2019.8798342},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798291,
  author={Singla, Ashutosh and Rao, Rakesh Rao Ramachandra and Göring, Steve and Raake, Alexander},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Assessing Media QoE, Simulator Sickness and Presence for Omnidirectional Videos with Different Test Protocols}, 
  year={2019},
  volume={},
  number={},
  pages={1163-1164},
  abstract={QoE for omnidirectional videos comprises additional components such as simulator sickness and presence. In this paper, a series of tests is presented comparing different test protocols to assess integral quality, simulator sickness and presence for omnidirectional videos in one test run, using the HTC Vive Pro as head-mounted display. For quality ratings, the five-point ACR scale was used. In addition, the well-established Simulator Sickness Questionnaire and Presence Questionnaire methods were used, once in a full version, and once with only one single integral scale, to analyze how well presence and simulator sickness can be captured using only a single scale.},
  keywords={Videos;Video sequences;Quality of experience;Correlation;Protocols;Media;Head-mounted displays;Simulator Sickness;Presence;QoE;360° Videos},
  doi={10.1109/VR.2019.8798291},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798193,
  author={Soccini, Agata Marta and Grangetto, Marco and Inamura, Tetsunari and Shimada, Sotaro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Hand Illusion: The Alien Finger Motion Experiment}, 
  year={2019},
  volume={},
  number={},
  pages={1165-1166},
  abstract={In Virtual Reality, the need to understand how subjects perceive their representation is gaining attention and importance. We present a contribution to a better understanding of the sense of embodiment by assessing two of its main components, body ownership and agency, through an experiment involving alien motion. The key aspect of the experimental protocol is to integrate a condition with some personalized alien finger movement while the subject is asked to remain still. Body ownership appears to be significantly reduced, but not agency. We also propose a metric to assess quantitatively that the view of the alien movement induces more finger posture variation compared to the reference context in the still condition.},
  keywords={Visualization;Correlation;Avatars;Measurement;Standards;Virtual Embodiment;Avatar;Alien Motion;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;I.2.0 [Artificial Intelligence]: General—Cognitive simulation},
  doi={10.1109/VR.2019.8798193},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798114,
  author={Song, Yingjie and Zhou, Nianmei and Sun, Qianhui and Gai, Wei and Liu, Juan and Bian, Yulong and Liu, Shijun and Cui, Lizhen and Yang, Chenglei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality Storytelling Environments Based on Tangible User Interface: Take Origami as an Example}, 
  year={2019},
  volume={},
  number={},
  pages={1167-1168},
  abstract={This paper presents a mixed reality storytelling system, which takes handicrafts as tangible interaction tools. Via the system, users can learn handicraft and then use the handicraft pieces to design, create and tell stories with HoloLens iteratively. In order to overcome the limitations of HoloLens gesture interaction, the system uses hand tracking with Kinect to implement a touch-like effect on the desktop. User study shows that our system has good usability, and it is welcomed by users. In addition, it can stimulate users interest in handicraft and storytelling, and even promote parent-child interaction effectively.},
  keywords={Virtual reality;Usability;Tools;Education;Three-dimensional displays;User interfaces;Systems architecture;Handicraft;Storytelling;Tangible User Interface;Mixed Reality;Parent-child Interaction;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction design;Interaction design theory, concepts and paradigms},
  doi={10.1109/VR.2019.8798114},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797750,
  author={Stadler, Sebastian and Cornet, Henriette and Frenkler, Fritz},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Study in Virtual Reality on (Non-)Gamers' Attitudes and Behaviors}, 
  year={2019},
  volume={},
  number={},
  pages={1169-1170},
  abstract={Virtual Reality (VR) constitutes an advantageous alternative for research considering scenarios that are not feasible in real-life conditions. Thus, this technology was used in the presented study for the behavioral observation of participants when being exposed to autonomous vehicles (AVs). Further data was collected via questionnaires before, directly after the experience and one month later to measure the impact that the experience had on participants' general attitude towards AVs. Despite a nonsignificance of the results, first insights suggest that participants with low prior gaming experience were more impacted than gamers. Future work will involve bigger sample size and refined questionnaires.},
  keywords={Autonomous vehicles;Virtual environments;Atmospheric measurements;Particle measurements;Data collection;User interfaces;Virtual Reality;Autonomous vehicles;Human-centered design;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems – Artificial, augmented and virtual realities;H.1.2 [User/Machine Systems]: Human factors;H.5.2 [User Interfaces]: User-centered design},
  doi={10.1109/VR.2019.8797750},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797956,
  author={Sun, Mengmeng and He, Weiping and Zhang, Li and Wang, Peng and Wang, Shuxia and Bai, Xiaoliang},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Haptic Compass: Active Vibrotactile Feedback of Physical Object for Path Guidance}, 
  year={2019},
  volume={},
  number={},
  pages={1171-1172},
  abstract={Haptic guidance is an essential research topic of haptic interaction technology. In some conditions, haptic guidance devices could be better at assisting navigation. We developed Haptic Compass, a prototype with active vibrotactile feedback by attaching vibration motors to a cylinder-shaped physical object, to provide directional cues for haptic guidance. To validate the prototype, we conducted two user studies. The first study investigated if users could identify the vibrotactile feedback from the prototype. The results showed that participants could effectively recognize and judge the vibration direction. In the second study, we evaluated the task completion time and the movement error among the Visual-only, Haptic-Only and Haptic-Visual feedback conditions in a typical path-guiding task. We found that, together with visual feedback, haptic feedback could enhance task performance and user experience for direct path guidance. Performance of the two studies demonstrated that our prototype has the potential to be an alternative for applications where non-wearable haptic feedback is desired.},
  keywords={Vibrations;Visualization;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;User experience;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2019.8797956},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798011,
  author={Syawaludin, Muhammad Firdaus and Kim, Chanho and Hwana, Jae-In},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hybrid Camera System for Telepresence with Foveated Imaging}, 
  year={2019},
  volume={},
  number={},
  pages={1173-1174},
  abstract={To improve the telepresence sense of a local HMD user, a high-resolution view of the remote environment is necessary. However, current commodity omnidirectional camera could not support enough resolution for the human eye. Using a higher resolution omnidirectional camera is also infeasible because it will increase the streaming bandwidth. We propose a hybrid camera system that can convey a higher resolution for the HMD user viewport ROI region in available bandwidth range. The hybrid camera consists of a pair of omnidirectional and PTZ camera which is close to each other. The HMD user head orientation controls the PTZ camera orientation. The HMD user also controls the zooming level of the PTZ camera to achieve higher resolution up to PTZ camera maximum optical zoom level. The remote environment view obtained from each camera is streamed to the HMD user and then stitched into one combined view. This combined view simulates human visual system (HVS) phenomenon called foveation, where only a small part in the human view is in high resolution, and the rests are in low resolution.},
  keywords={Cameras;Resists;Telepresence;Bandwidth;Visualization;Servomotors;Integrated optics;Telepresence-Hybrid Camera-Foveation-HMD},
  doi={10.1109/VR.2019.8798011},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798351,
  author={Takizawa, Ryo and Verhulst, Adrien and Seaborn, Katie and Fukuoka, Masaaki and Hiyama, Atsushi and Kitazaki, Michiteru and Inami, Masahiko and Sugimoto, Maki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Parasitic Body: Exploring Perspective Dependency in a Shared Body with a Third Arm}, 
  year={2019},
  volume={},
  number={},
  pages={1175-1176},
  abstract={With advancements in robotics, systems featuring wearable robotic arms teleoperated by a third party are appearing. An important aspect of these systems is the visual feedback provided to the third party operator. This can be achieved by placing a wearable camera on the robotic arm's “host,” or Main Body Operator (MBO), but such a setup makes the visual feedback dependant on the movements of the main body. Here we introduce a VR system called Parasitic Body to explore a VR shared body concept representative of the wearable robotic arms “host” (the MBO) and of the teleoperator (here called the Parasite Body Operator (PBO)). 2 users jointly operate a shared virtual body with a third arm: The MBO controls the main body and the PBO controls a third arm sticking out from the left shoulder of the main body. We focused here on the perspective dependency of the PBO (indeed, the PBO view is dependant of the movement of the MBO) in a “finding and reaching” task.},
  keywords={Task analysis;Manipulators;Visualization;Cameras;Teleoperators;Avatars;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Shared body;Supernumerary Limbs;Third Arm},
  doi={10.1109/VR.2019.8798351},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798067,
  author={Tarko, Joanna and Tompkin, James and Richardt, Christian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={OmniMR: Omnidirectional Mixed Reality with Spatially-Varying Environment Reflections from Moving 360° Video Cameras}, 
  year={2019},
  volume={},
  number={},
  pages={1177-1178},
  abstract={We propose a new approach for creating omnidirectional mixed reality (OmniMR) from moving-camera 360° video. To insert virtual computer-generated elements into a moving-camera 360° video, we reconstruct camera motion and sparse scene content via structure from motion on stitched equirectangular video (the default output format of current 360° cameras). Then, to plausibly reproduce realworld lighting conditions for these inserted elements, we employ inverse tone mapping to recover high dynamic range environment maps which vary spatially along the camera path. We implement our approach into the Unity rendering engine for real-time object rendering with dynamic lighting and user interaction. This expands the use and flexibility of 360° video for mixed reality.},
  keywords={Cameras;Streaming media;Virtual reality;Lighting;Rendering (computer graphics);Image reconstruction;Real-time systems;Omnidirectional video;mixed reality;structure from motion;3D reconstruction;lighting estimation;environment maps},
  doi={10.1109/VR.2019.8798067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798266,
  author={Tarng, Stanley and Wang, Deng and Hu, Yaoping and Merienne, Frédéric},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards EEG-Based Haptic Interaction within Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={1179-1180},
  abstract={Current virtual environments (VE) enable perceiving haptic stimuli to facilitate 3D user interaction, but lack brain-interfacial contents. Using electroencephalography (EEG), we undertook a feasibility study on exploring event-related potential (ERP) patterns of the user's brain responses during haptic interaction within a VE. The interaction was flying a virtual drone along a curved transmission line to detect defects under the stimuli (e.g., force increase and/or vibrotactile cues). We found that there were variations in the peak amplitudes and latencies (as ERP patterns) of the responses at about 200 ms post the onset of the stimuli. The largest negative peak occurred during 200~400 ms after the onset in all vibration-related blocks. Moreover, the amplitudes and latencies of the peak were differentiable among the vibration-related blocks. These findings imply feasible decoding of the brain responses during haptic interaction within VEs.},
  keywords={Haptic interfaces;Electroencephalography;Force;Electrodes;Three-dimensional displays;Vibrations;Testing;virtual reality (VR);haptic stimuli;force;vibration;electroencephalography (EEG);event-related potential (ERP);H.1.2 [User/Machine Systems]: Human information processing;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2019.8798266},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797979,
  author={Tashiro, So and Uchiyama, Hideaki and Thomas, Diego and Taniguchi, Rin-ichiro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={3D Positioning System Based on One-handed Thumb Interactions for 3D Annotation Placement}, 
  year={2019},
  volume={},
  number={},
  pages={1181-1182},
  abstract={This paper presents a 3D positioning system based on one-handed thumb interactions for simple 3D annotation placement with a smart-phone. To place an annotation at a target point in the real environment, the 3D coordinate of the point is computed by interactively selecting the corresponding points in multiple views by users while performing SLAM. Generally, it is difficult for users to precisely select an intended pixel on the touchscreen. Therefore, we propose to compute the 3D coordinate from multiple observations with a robust estimator to have the tolerance to the inaccurate user inputs. In addition, we developed three pixel selection methods based on one-handed thumb interactions. A pixel is selected at the thumb position at a live view in FingAR, the position of a reticle marker at a live view in SnipAR, or that of a movable reticle marker at a freezed view in FreezAR. In the preliminary evaluation, we investigated the 3D positioning accuracy of each method.},
  keywords={Three-dimensional displays;Thumb;Augmented reality;Cameras;Simultaneous localization and mapping;Task analysis;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computer vision;Computer vision problems;Reconstruction;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Pointing},
  doi={10.1109/VR.2019.8797979},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798317,
  author={Taupiac, Jean-Daniel and Rodriguez, Nancy and Strauss, Olivier and Beney, Pierre},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Social Skills Training Tool in Virtual Reality, Intended for Managers and Sales Representatives}, 
  year={2019},
  volume={},
  number={},
  pages={1183-1184},
  abstract={Social skills training courses for managers and sales representatives are mainly realized today through role-playing sessions, which show several limits: realism, contextualization, evaluation objectivity. This paper describes a prototype of a Virtual Reality tool intended to answer these issues. This tool allows living role-playing sessions with virtual characters. We present the results of the first user tests we carried out. Users felt spatially present, involved, and socially present. Results also underline areas of improvement, such as nonverbal behaviors, scenario content and environment realism.},
  keywords={Training;Virtual reality;Prototypes;Tools;Interviews;Face;Resists},
  doi={10.1109/VR.2019.8798317},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798061,
  author={Techasarntikul, Nattaon and Mashita, Tomohiro and Ratsamee, Photchara and Uranishi, Yuki and Takemura, Haruo and Orlosky, Jason and Kiyokawa, Kiyoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Pointing Interfaces with an AR Agent for Multi-section Information Guidance}, 
  year={2019},
  volume={},
  number={},
  pages={1185-1186},
  abstract={In educational settings such as art galleries or museums, Augmented Reality (AR) has the potential to provide detailed information about exhibits. However, dealing with items that contain information in multiple sections or areas is still a significant challenge. For example, a large painting may contain many minute details, which requires a system that can explain its broader features rather than just a generic description. To address this challenge, we introduce an AR guidance system that uses an embodied agent to point out items and explain each piece and part of exhibit items in detail. We also designed and tested 3 different pointing interfaces for the embodied agent: gesture only, gesture with a dot laser, and gesture with line laser. To evaluate this interface, we conducted a user experiment simulating painting guidance to test interest and exhibit memory. During the experiment, the agent pointed to various areas of interest in the painting and provided a detailed description to participants. The result shows that the search times for target positions were the fastest with the line laser. However, no particular interface outperformed others in memory recall of exhibit content.},
  keywords={Painting;Tracking;Lasers;Augmented reality;Art;Conferences;Human-centered computing;User interface design;Computing methodologies;Mixed / augmented reality;Information systems;Retrieval effectiveness},
  doi={10.1109/VR.2019.8798061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798128,
  author={Teo, Theophilus and Lee, Gun A and Billinghurst, Mark and Adcock, Matt},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Supporting Visual Annotation Cues in a Live 360 Panorama-based Mixed Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={1187-1188},
  abstract={We propose enhancing live 360 panorama-based Mixed Reality (MR) remote collaboration through supporting visual annotation cues. Prior work on live 360 panorama-based collaboration used MR visualization to overlay visual cues, such as view frames and virtual hands, yet they were not registered onto the shared physical workspace, hence had limitations in accuracy for pointing or marking objects. Our prototype system uses spatial mapping and tracking feature of an Augmented Reality head-mounted display to show visual annotation cues accurately registered onto the physical environment. We describe the design and implementation details of our prototype system, and discuss on how such feature could help improve MR remote collaboration.},
  keywords={Visualization;Collaboration;Resists;Three-dimensional displays;Prototypes;Augmented reality;Mixed Reality;remote collaboration;360 panorama;annotation;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Collaborative computing;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798128},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798123,
  author={Thanyadit, Santawat and Punpongsanon, Parinya and Pong, Ting-Chuen},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Visualization Techniques for Observing a Group of Virtual Reality Users Using Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1189-1190},
  abstract={As an emerging technology, virtual reality (VR) has been used increasingly as a learning tool to explore “outside the classroom experiences” inside the classroom. While VR provides an immersive experience to the students, it is difficult for the instructor to monitor the students' activities in the VR. Thus, it hinders interactions between the instructor and students. To solve this challenge, we investigated a technique that allows the instructor to observe VR users at scale using Augmented Reality. Augmented Reality techniques are used to visualize the gazes of the VR users in the virtual environment, and improve the instructor's awareness.},
  keywords={Visualization;Virtual environments;Augmented reality;Three-dimensional displays;Tools;Monitoring;Visualization;Virtual Reality;Education;User Experience;Human-centered computing;Visualization design and evaluation methods;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR.2019.8798123},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798002,
  author={Thompson, Stephen and Chalmers, Andrew and Rhee, Taehyun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-time Underwater Caustics for Mixed Reality 360° Videos}, 
  year={2019},
  volume={},
  number={},
  pages={1191-1192},
  abstract={We present a novel mixed reality (MR) rendering solution that illuminates and blends virtual objects into underwater 360° video with real-time underwater caustic effects. Image-based lighting is used in conjunction with underwater caustics to provide automatic ambient and high frequency underwater lighting. This ensures that the caustics and virtual objects are lit and blend into each frame of the video semi-automatically and in real-time. We provide an interactive interface with intuitive parameter controls to fine tune caustics to match with the background video.},
  keywords={Lighting;Real-time systems;Streaming media;Rendering (computer graphics);Sea surface;Virtual reality;Surface waves;Computing methodologies;Graphics systems and interfaces;Mixed / augmented reality},
  doi={10.1109/VR.2019.8798002},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798309,
  author={Tsai, Wan-Lun and Su, Li-wen and Ko, Tsai-Yen and Yang, Cheng-Ta and Hu, Min-Chun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Improve the Decision-making Skill of Basketball Players by an Action-aware VR Training System}, 
  year={2019},
  volume={},
  number={},
  pages={1193-1194},
  abstract={Decision-making is an essential part in basketball offenses. In this paper, we proposed a basketball offensive decision-making VR training system. During the training, the trainee can intuitively interact with the system by wearing a motion capture suit and be trained in different virtual defensive scenarios designed by professional coaches. The system will recognize the offensive action performed by the user and provide correct suggestions when he/she makes a bad offensive decision. We compared the effectiveness of the training protocols by using conventional tactics board and the proposed VR system. Furthermore, we investigated the influence of using prerecorded 360-degree panorama video and computer simulated virtual content to create immersive training environment.},
  keywords={Training;Decision making;Sports;Resists;Three-dimensional displays;Virtual environments;decision-making;virtual reality;sports;training;H.5.2 [User Interfaces]: User Interfaces—Graphical user interfaces (GUI)},
  doi={10.1109/VR.2019.8798309},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797847,
  author={Tsuji, Amato and Ushida, Keita and Yamaguchi, Saneyasu and Chen, Qiu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Collaborative Animation of 3D Models with Finger Play and Hand Shadow}, 
  year={2019},
  volume={},
  number={},
  pages={1195-1196},
  abstract={The authors propose a method for real-time collaborative animation for 3D models with finger play and hand shadow. Two or more users have the model animated more complexly and expressively than one user. For instance, a model of a crocodile is animated by two users; one manipulates its mouth, neck and tail, and the other manipulates its four legs. With the implemented system up to five users can manipulate one model collaboratively. From evaluations following points were found: 1) users could manipulate models without detailed instruction; 2) most participants felt the system operable and enjoyable; 3) the motions made with the system were not less cute, amusing and lively than those by animators with conventional methods.},
  keywords={Solid modeling;Three-dimensional displays;Animation;Collaboration;Dinosaurs;Legged locomotion;Indexes;Multi-user interaction;finger play;hand shadow;animation;3D model;Human-centered computing;Human computer interaction;Collaborative and social computing},
  doi={10.1109/VR.2019.8797847},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797797,
  author={Tutar, Altan and Peck, Tabitha},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Working Memory Load Performance Based on Collocation of Virtual and Physical Hands}, 
  year={2019},
  volume={},
  number={},
  pages={1197-1198},
  abstract={The use of real-like hands in virtual reality simulators is common; however, research into understanding how the human brain perceives hands in virtual environments is limited. Self avatars are a great way to improve the users presence and perception of space [6], but the precise implementation of avatars is arduous, and including only hands is an attractive alternative. Earlier psychology research reported that the closer the hands to the studied object, the lower the working memory load. We hypothesized that in virtual environments, virtual hands that are collocated with the users hands should improve the users working memory load, and we tested our hypothesis with a between-participant study (n=30) measuring working memory load with the Stroop Interference Task.},
  keywords={Task analysis;Interference;Avatars;Psychology;Color;Virtual environments;Virtual reality;self-avatars;virtual hands;working memory;Stroop Interference Task},
  doi={10.1109/VR.2019.8797797},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797973,
  author={González, Andrés N. Vargas and Kapalo, Katelynn and Koh, Senglee and Sottilare, Robert and Garrity, Pat and Laviola, Joseph J.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparison of Desktop and Augmented Reality Scenario Based Training Authoring Tools}, 
  year={2019},
  volume={},
  number={},
  pages={1199-1200},
  abstract={This work presents a comparison of two applications (Augmented Reality (AR) and desktop) to author Scenario-Based Training (SBT) simulations. Through an iterative design process two interface conditions are developed and then evaluated qualitatively and quantitatively. A graph based authoring visualization help designers understand the scenario learning artifacts and relationships. No significant difference was found on time taken to complete tasks nor on the perceived usability of the systems. However, Desktop was perceived as more efficient, corroborated by the significantly higher number of mistakes made in AR. Findings are presented towards building better AR immersive authoring tools.},
  keywords={Task analysis;Tools;Augmented reality;Training;Authoring systems;Visualization;Solid modeling;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Modeling and simulation;Simulation support systems;Simulation environments},
  doi={10.1109/VR.2019.8797973},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797728,
  author={Venkatakrishnan, Rohith and Bhargava, Ayush and Venkatakrishnan, Roshan and Lucaites, Kathryn M. and Volonte, Matias and Solini, Hannah and Robb, Andrew C. and Pagano, Christopher and Babu, Sabarish V.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards an Immersive Driving Simulator to Study Factors Related to Cybersickness}, 
  year={2019},
  volume={},
  number={},
  pages={1201-1202},
  abstract={The commercialization of Virtual Reality (VR) devices has made it easier for everyday users to experience VR from the comfort of their living rooms. This recent uptake in VR has also increased reported incidents of cybersickness. Cybersickness refers to the discomfort experienced by an individual while experiencing virtual environments. The symptoms are similar to those of motion sickness but are more disorienting in nature resulting in dizziness, blurred vision, etc. Cybersickness is currently one of the biggest hurdles to the widespread adoption of VR, and it is therefore critical to explore the factors that influence its onset. Towards this end, we present a proof of concept simulation to study cybersickness in highly realistic immersive virtual environments.},
  keywords={Virtual environments;Automobiles;Solid modeling;Urban areas;Three-dimensional displays;Wheels;Human-centered computing;Empirical studies in HCI;User Studies},
  doi={10.1109/VR.2019.8797728},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798295,
  author={Veronez, Mauricio Roberto and Gonzaga, Luiz and Bordin, Fabiane and Inocencio, Leonardo Campos and Eliane, Graciela and Racolte, Dos Reis and Kupssinskü, Lucas Silveira and Rossa, Pedro and Scalco, Leonardo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Imspector: Immersive System of Inspection of Bridges/Viaducts}, 
  year={2019},
  volume={},
  number={},
  pages={1203-1204},
  abstract={One of the main difficulties in the inspection of Bridges/Viaducts by observation is inaccessibility or lack of access throughout the structure. Mapping using remote sensors on Unmanned Aerial Vehicles (UAVs) or by means of laser scanning can be an interesting alternative to the engineer as it can enable more detailed analysis and diagnostics. Such mapping techniques also allow the generation of realistic 3D models that can be integrated in Virtual Reality (VR) environments. In this sense, we present the ImSpector, a system that uses realistic 3D models generated by remote sensors embedded in UAVs that implements a virtual and immersive environment for inspections. As a result, the system provides the engineer a tool to carry out field tests directly at the office, ensuring agility, accuracy and safety in bridge and viaduct inspections.},
  keywords={Inspection;Three-dimensional displays;Solid modeling;Bridges;Tools;Cameras;Remote sensing;UAV;Virtual Reality;Bridges;Viaducts;I.3.8 [Computer graphics]: Application-Virtual Reality},
  doi={10.1109/VR.2019.8798295},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798191,
  author={Voinescu, Alexandra and Fodor, Liviu-Andrei and Fraser, Danaë Stanton and Mejías, Miguel and David, Daniel},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Usability of Nesplora Aquarium, a Virtual Reality System for Neuropsychological Assessment of Attention and Executive Functioning}, 
  year={2019},
  volume={},
  number={},
  pages={1207-1208},
  abstract={Virtual reality (VR) has proved to be an efficient alternative to traditional neuropsychological assessment. As VR has become more affordable, it is ready to break out of the laboratory and enter homes and clinical practices. We present preliminary findings from a study designed to evaluate self-reported usability of a VR test for neuropsychological assessment of attention and executive function.},
  keywords={Usability;Task analysis;Fish;Virtual reality;Human computer interaction;Visualization;Virtual reality;neuropsychological assessment;usability;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;HCI design and evaluation methods;Usability testing},
  doi={10.1109/VR.2019.8798191},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797823,
  author={Volmer, Benjamin and Verhulst, Adrien and Drogemuller, Adam and Thomas, Bruce H. and Inami, Masahiko and Sugimoto, Maki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Robot Arm Training in Virtual Reality Using Partial Least Squares Regression}, 
  year={2019},
  volume={},
  number={},
  pages={1209-1210},
  abstract={Robot assistance can reduce the user's workload of a task. However, the robot needs to be programmed or trained on how to assist the user. Virtual Reality (VR) can be used to train and validate the actions of the robot in a safer and cheaper environment. In this paper, we examine how a robotic arm can be trained using Coloured Petri Nets (CPN) and Partial Least Squares Regression (PLSR). Based upon these algorithms, we discuss the concept of using the user's acceleration and rotation as a sufficient means to train a robotic arm for a procedural task in VR. We present a work-in-progress system for training robotic limbs using VR as a cost effective and safe medium for experimentation. Additionally, we propose PLSR data that could be considered for training data analysis.},
  keywords={Manipulators;Task analysis;Training;Robot kinematics;Training data;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Robot arm},
  doi={10.1109/VR.2019.8797823},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798218,
  author={Voong, Tray Minh and Oehler, Michael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Auditory Spatial Perception Using Bone Conduction Headphones along with Fitted Head Related Transfer Functions}, 
  year={2019},
  volume={},
  number={},
  pages={1211-1212},
  abstract={An approach is presented how to practically determine which head-related transfer function (HRTF) profiles fit best for individuals wearing a bone conduction headphone. Such headphones may be particularly useful for visually impaired people (e.g., for navigation applications) as they do not obstruct the outer ear. Hence, it is still possible to perceive environmental sounds without restraints while wearing such headphones. For a fast and user-friendly identification of fitting HRTF profiles, an adapted tournament system is proposed. It could be shown that the results of the tournament method, where participants had to rate overall preference, externalization and envelopment, correlated well with the results of the localization task. The correlation was higher for the conventional headphones condition than for the bone conduction headphones condition. Analyses of the transmission characteristics show an uneven frequency response of bone conduction headphones compared to conventional headphones or speakers. In future research it will be investigated whether these findings are relevant for the auditory spatial perception at all and to what extent best fitting HRTFs may compensate for these phenomena.},
  keywords={Headphones;Bones;Databases;Transfer functions;Navigation;Auditory displays;Ear;Head-related-transfer-functions;bone-conduction-headphone;tournament methods},
  doi={10.1109/VR.2019.8798218},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798305,
  author={Wan, Feifei and Yin, Yong and Zhang, Xiaoxi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Simulation of Deep-Sea Hydrothermal Fluid}, 
  year={2019},
  volume={},
  number={},
  pages={1213-1214},
  abstract={We perform a virtual simulation of deep-sea hydrothermal fluids by using a combination of physical-based particles and grids. We use Vortex-In-Cell method to simulate vorticity and generate induced velocity fields, and the semi-Lagrangian method to simulate the advection and diffusion of temperature and density fields. Finally, we show the rich details of hydrothermal fluids by comparing simulations with real fluids.},
  keywords={Computer Graphics;Three-Dimensional Graphics and Realism;Animation},
  doi={10.1109/VR.2019.8798305},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797789,
  author={Wang, Cheng Yao and Drumm, Logan and Troup, Christopher and Ding, Yingjie and Stevenson Won, Andrea},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-Replay: Capturing and Replaying Avatars in VR for Asynchronous 3D Collaborative Design}, 
  year={2019},
  volume={},
  number={},
  pages={1215-1216},
  abstract={Distributed teams rely on asynchronous CMC tools to complete collaborative tasks due to the difficulties and costs surrounding scheduling synchronous communications. In this paper, we present VR-Replay, a new communication tool that records and replays avatars with both nonverbal behavior and verbal communication in VR asynchronous collaboration. We describe a study comparing VR-Replay with a desktop-based CVE with audio annotation and a VR immersive CVE with audio annotation. Our results suggest that viewing the replay avatar in VR-Replay improves teamwork, causing people to view their partners as more likable, warm, and friendly. 75% of the users chose VR-Replay as the preferred communication tool in our study.},
  keywords={Avatars;Tools;Three-dimensional displays;Task analysis;Teamwork;Prototypes;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism;Virtual reality},
  doi={10.1109/VR.2019.8797789},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798363,
  author={Wang, Cheng Yao and Sakashita, Mose and Ehsan, Upol and Li, Jingjin and Won, Andrea Stevenson},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={RelivelnVR: Capturing and Reliving Virtual Reality Experiences Together}, 
  year={2019},
  volume={},
  number={},
  pages={1217-1218},
  abstract={We present a new type of sharing VR experience over distance which allows people to relive their recorded experience in VR together. We describe a pilot study examining the user experience when people share their VR experience together remotely. Finally, we discuss the implications for sharing VR experiences over time and space.},
  keywords={Prototypes;Videos;Games;Avatars;Collaboration;User experience;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism;Virtual reality},
  doi={10.1109/VR.2019.8798363},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798024,
  author={Wang, Peng and Zhang, Shusheng and Bai, Xiaoliang and Billinghurst, Mark and He, Weiping and Wang, Shuxia and Zhang, Xiaokun and Du, Jiaxiang and Chen, Yongxing},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Head Pointer or Eye Gaze: Which Helps More in MR Remote Collaboration?}, 
  year={2019},
  volume={},
  number={},
  pages={1219-1220},
  abstract={This paper investigates how two different unique gaze visualizations (the head pointer(HP), eye gaze(EG)) affect table-size physical tasks in Mixed Reality (MR) remote collaboration. We developed a remote collaborative MR Platform which supports sharing of the remote expert's HP and EG. The prototype was evaluated with a user study comparing two conditions: sharing HP and EG with respect to their effectiveness in the performance and quality of cooperation. There was a statistically significant difference between two conditions on the performance time, and HP is a good proxy for EG in remote collaboration.},
  keywords={Collaboration;Task analysis;Virtual reality;Prototypes;Cameras;Visualization;Human factors;Remote collaboration;Augmented Reality;Mixed Reality;Eye gaze;Head Pointer;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Computer-supported cooperative work},
  doi={10.1109/VR.2019.8798024},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798044,
  author={Wang, Tzu-Yang and Sato, Yuji and Otsuki, Mai and Kuzuoka, Hideaki and Suzuki, Yusuke},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Full Body Avatar in Augmented Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={1221-1222},
  abstract={In this paper, we compared three different types of avatar design (“Body”, “Hand + Arm”, and “Hand only”) for the augmented reality remote instruction system in terms of usability. The result showed that the usability of the remote instruction system with full body avatar has a higher usability. In addition, participants felt more easily to track the full body avatar than the avatar with hand only. However, concerning the understandability of the instruction, there was no difference between three designs.},
  keywords={Avatars;Usability;Task analysis;Collaboration;Augmented reality;Resists;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798004,
  author={Wang, Yuyang and Chardonnet, Jean-Rémy and Merienne, Frédéric},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design of a Semiautomatic Travel Technique in VR Environments}, 
  year={2019},
  volume={},
  number={},
  pages={1223-1224},
  abstract={Travel in a real environment is a common task that human beings conduct easily and subconsciously. However transposing this task in virtual environments (VEs) remains challenging due to input devices and techniques. Considering the well-described sensory conflict theory, we present a semiautomatic travel method based on path planning algorithms and gaze-directed control, aiming at reducing the generation of conflicted signals that may confuse the central nervous system. Since gaze-directed control is user-centered and path planning is goal-oriented, our semiautomatic technique makes up for the deficiencies of each with smoother and less jerky trajectories.},
  keywords={Navigation;Trajectory;Task analysis;Three-dimensional displays;Virtual environments;Human-centered computing;Human computer interaction(HCI);Interaction paradigms;User interface design;Interaction devices},
  doi={10.1109/VR.2019.8798004},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797799,
  author={Wei, Huan and Liu, Yue and Wang, Yongtian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Building AR-based Optical Experiment Applications in a VR Course}, 
  year={2019},
  volume={},
  number={},
  pages={1225-1226},
  abstract={The demand for VR courses in universities is growing since VR technology is widely used in industry, entertainment, and education today. Traditional lectures and exercises have problems in motivating and engaging students, especially non-CS-majors. We design a VR course with a teamwork project assignment for Opto-Electronic Engineering undergraduates, providing them with project-based learning (PBL) experience. The task requires three students to group a team to build an AR-based optical experiment application over the course, aiming to develop students' practical engineering ability. The process of the project consists of three main stages: preparation, designing and implementing. We also evaluate students' work from different aspects and survey to analyze the students' attitude toward the project.},
  keywords={Education;Three-dimensional displays;User experience;Buildings;Virtual reality;Programming profession;Augmented Reality;optical experiment;higher education;teamwork;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;K.3.1 [Computers and Education]: Computer Uses in Education},
  doi={10.1109/VR.2019.8797799},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798287,
  author={Weik, David and Lorenz, Mario and Knopp, Sebastian and Pelliccia, Luigi and Feierabend, Stefanie and Rotsch, Christian and Klimant, Philipp},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Integrating Tactile Feedback in an Acetabular Reamer for Surgical VR-Training}, 
  year={2019},
  volume={},
  number={},
  pages={1227-1228},
  abstract={Surgical VR-simulators including haptic and tactile feedback can severely improve the training of surgeons. However, so far only minimally invasive surgery benefited from the capabilities of VR. For non-minimally invasive surgery, only one prototype to simulate the reaming of the acetabula during hip joint replacement surgery exists, which is unable to simulate the vibrations of the surgical reamer. By integrating electronic components, a tactile reamer, able to simulate the vibrations, could be developed. A qualitative assessment with orthopedic surgeons revealed that the simulated sound of the vibrations is realistic but the intensity of the vibrations needs to be improved.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Haptics;Microcontroller;Virtual reality;Surgery;Medicine;Training;Applied computing → Life and medical sciences;Human-centered computing → Virtual reality;Human-centered computing → Haptic devices},
  doi={10.1109/VR.2019.8798287},
  ISSN={2642-5254},
  month={March},}
@INPROCEEDINGS{8798117,
  author={Williams, Niall and Peck, Tabitha C.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Estimation of Rotation Gain Thresholds for Redirected Walking Considering FOV and Gender}, 
  year={2019},
  volume={},
  number={},
  pages={1229-1230},
  abstract={Redirected walking techniques enable users to naturally locomote in virtual environments (VEs) that are larger than the tracked space. Redirected walking imperceptibly transforms the VE around the user with predefined estimated threshold gains. Previously estimated gains were evaluated with a 40° field of view (FOV). We conducted a within-participant user study to estimate and compare thresholds for rotation gains. Significant differences in detection thresholds were found between FOVs. When using a 110° FOV, rotations can be decreased 31% and increased 49% compared to decreased 18% and increased 47% with a 40° FOV. Significant differences were found between female and male gains with a 110° FOV.},
  keywords={Legged locomotion;Visualization;Distance measurement;Estimation;Image edge detection;Virtual environments;Virtual reality;Locomotion;Perception;Detection thresholds;Redirected walking;Gender differences},
  doi={10.1109/VR.2019.8798117},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798007,
  author={Williamson, Brian M. and Taranta, Eugene M. and Garrity, Pat and Sottilare, Robert and LaViola, Joseph J.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Systematic Evaluation of Multi-Sensor Array Configurations for SLAM Tracking with Agile Movements}, 
  year={2019},
  volume={},
  number={},
  pages={1231-1232},
  abstract={Accurate tracking of a user in a marker-less environment can be difficult, even more so when agile head or hand movements are expected. When relying on feature detection as part of a SLAM algorithm the issue arises that a large rotational delta causes previously tracked features to become lost. One approach to overcome this problem is with multiple sensors increasing the horizontal field of view. In this paper, we perform a systematic evaluation of tracking accuracy by recording several agile movements and providing different camera configurations to evaluate against. We begin with four sensors in a square configuration and test the resulting output from a chosen SLAM algorithm. We then systematically remove a camera from the feed covering all permutations to determine the level of accuracy and tracking loss. We cover some of the lessons learned in this preliminary experiment and how it may guide researchers in tracking extremely agile movements.},
  keywords={Cameras;Tracking;Simultaneous localization and mapping;Sensor arrays;Augmented reality;Systematics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis;Tracking},
  doi={10.1109/VR.2019.8798007},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798184,
  author={Won, Andrea Stevenson and Aitamurto, Tanja and Kim, Byungdoo and Sakshuwong, Sukolsak and Kircos, Catherine and Sadeghi, Yasamin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Motivation to Select Point of View in Cinematic Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1233-1234},
  abstract={This paper examines the effects of participants' preferred point of view of two protagonists, and their motivation for this preference, on two viewings of a cinematic 360-degree video filmed from the first person perspective. Before watching the film, which dramatized gender bias in a STEM workplace, participants were asked to state whether they preferred to view the film from the point of view (POV) of a male protagonist, a female protagonist, or make no selection. They were then asked why they held this preference. Their answers were predictive. Participants' tracked head movements, and the events participants recalled from the film, differed according to their pre-stated preference and motivation.},
  keywords={Avatars;Ethics;Computers;Virtual environments;Companies;Behavioral sciences;K.3.1 [Computers and Education]: Computer Uses in Education;J.4. [Computer Applications]: Social and Behavioral Sciences},
  doi={10.1109/VR.2019.8798184},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798318,
  author={Woodworth, Jason W. and Lipari, Nicholas G. and Borst, Christoph W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Teacher Avatar Appearances in Educational VR}, 
  year={2019},
  volume={},
  number={},
  pages={1235-1236},
  abstract={We present a pilot study of four teacher avatars for an educational virtual field trip. The avatars consist of a depth-video-based mesh of a real person, a game-style human model, a robot model, and the robot with its head replaced by a video feed of the teacher's face. Multiple avatars were developed to consider alternatives to the mesh representation that required high-bandwidth networks and a non-immersive teacher interface. The pilot study presents a random avatar to the participant at each of 4 educational stations, and follows up with a subjective questionnaire. Most notably, we find positive affinity for the plain robot model to be similar to that of the video mesh, which was previously shown to provide high co-presence and good results for education. Results are guiding a larger study that will measure the educational efficacy of revised avatars.},
  keywords={Avatars;Robots;Solid modeling;Face;Three-dimensional displays;Manganese;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798318},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798063,
  author={Wu, Fan and Zhou, Qian and Seo, Kyoungwon and Kashiwagi, Toshiro and Fels, Sidney},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={I Got Your Point: An Investigation of Pointing Cues in a Spherical Fish Tank Virtual Reality Display}, 
  year={2019},
  volume={},
  number={},
  pages={1237-1238},
  abstract={Pointing is a fundamental building block in human communication. While it is ubiquitous in our daily interactions within the real world, it is difficult to precisely interpret a virtual agent's pointing direction to the physical world, considering its complex and subtle gesture cues, such as the movements of the human hand and head. Fish Tank Virtual Reality (FTVR) display has the potential to provide accurate pointing cues as it creates a compelling 3D spatial effect by rendering perspective-corrected vision. In this paper, we conducted a study with pointing cues of three levels (Head-only, Hand-only, and Hand+Head) to evaluate how the head and hand gesture cues affect observers' performance in interpretation of where a virtual agent is pointing in a spherical FTVR display. The results showed that the hand gesture significantly helps people interpret the pointing both accurately and quickly for fine pointing (15°), with 19.4% higher accuracy and 1.42 seconds faster than the head cue. The combination of the head and hand yielded a small improvement on the accuracy (4.4%) with even slightly longer time (0.38 seconds) compared to the hand-only cue. However, for coarse pointing (30°), head cue appears to be sufficient with the accuracy of 90.2%. The result of this study provides guidelines on cues selection for designing pointing in the virtual environment.},
  keywords={Three-dimensional displays;Task analysis;Virtual environments;Fish;Rendering (computer graphics);Observers;Human-Centered Computing;Human computer interaction;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR.2019.8798063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797984,
  author={Wu, Xinli and Luo, Jiali and Zhang, Minxiong and Yang, Wenzhen and Pan, Zhigeng},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Haptic Rendering for Chinese Characters Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={1239-1240},
  abstract={This paper explores new approaches to recognize Chinese characters in images through touch sense. We extract the gradient image fused with brightness and chrominance information by the flow-based Gauss difference algorithm, and use the edge tangential flow method to smooth image contour, to obtain smooth character contours with significant edge features. Then, we use a tactile generation algorithm with five mechanical elements to generate stable and continuous tactile information based on the normal vector of triangular patches of the surface. To verify the feasibility and effectiveness of the proposed methods, we developed a Chinese character tactile sensing system. Experiments show that Chinese characters in images can be recognized by touch sense with higher recognition accuracy rate.},
  keywords={Feature extraction;Character recognition;Brightness;Image edge detection;Force;Filtering;Haptic interfaces;Character contour;Contour extraction;Haptic rendering;HCI},
  doi={10.1109/VR.2019.8797984},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797791,
  author={Xia, Xingyu and Pun, Chi-Man and Zhang, Di and Yang, Yang and Lu, Huimin and Gao, Hao and Xu, Feng},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A 6-DOF Telexistence Drone Controlled by a Head Mounted Display}, 
  year={2019},
  volume={},
  number={},
  pages={1241-1242},
  abstract={Recently, a new form of telexistence is achieved by recording images with cameras on an unmanned aerial vehicle (UAV) and displaying them to the user via a head mounted display (HMD). A key problem here is how to provide a free and natural mechanism for the user to control the viewpoint and watch a scene. To this end, we propose an improved rate-control method with an adaptive origin update (AOU) scheme. Without the aid of any auxiliary equipment, our scheme handles the self-centering problem. In addition, we present a full 6-DOF viewpoint control method to manipulate the motion of a stereo camera, and we build a real prototype to realize this by utilizing a pan-tilt-zoom (PTZ) which not only provides 2-DOF to the camera but also compensates the jittering motion of the UAV to record more stable image streams.},
  keywords={Telexistence;Cameras;Resists;Drones;Three-dimensional displays;Head;Telexistence;Head Mounted Display;Unmanned Aerial Vehicle;3D interaction;Virtual Reality},
  doi={10.1109/VR.2019.8797791},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798231,
  author={Xie, Chun and Tan, Chun Kwang and Sugiyama, Taisei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Hanger Reflex on Virtual Reality Redirected Walking}, 
  year={2019},
  volume={},
  number={},
  pages={1243-1244},
  abstract={One of the major challenges in virtual reality (VR) is to create a perception of a large virtual space within a limited physical space. Here, we explore the effect of haptic-based navigation by Hanger Reflex (HR) on the perception in redirected walking (RDW) with visual manipulation. Seven individuals walked along a straight path in VR while, unbeknown to them, the visual scene was rotated with a curvature gain of π/36, forcing them to walk in a circular path in real space. HR rotation (in the left, right, and neutral direction) was induced by a wearable haptic device during each of the walking trials, and they reported their perceived walking direction and effort to walk along the path on visual analog scale. Experiment results showed that HR can influence the perception in RDW, but the effects may be complex and therefore require further investigation.},
  keywords={Legged locomotion;Visualization;Haptic interfaces;Trajectory;Virtual reality;Resists;Face;Virtual reality;redirected walking;Hanger Reflex;visual manipulation;Human-centered computing~User studies;Human-centered computing~Virtual reality;Human-centered computing~Haptic devices},
  doi={10.1109/VR.2019.8798231},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798050,
  author={Xing, Huan and Bao, Xiyu and Zhang, Fan and Gai, Wei and Qi, Meng and Liu, Juan and Shi, Yuliang and de Melo, Gerard and Yang, Chenglei and Meng, Xiangxu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Rotbav: A Toolkit for Constructing Mixed Reality Apps with Real-Time Roaming in Large Indoor Physical Spaces}, 
  year={2019},
  volume={},
  number={},
  pages={1245-1246},
  abstract={This paper presents a toolkit called Rotbav for easily constructing mixed reality (MR) apps that can be experienced in real time in large indoor physical space via HoloLens. It resolves the problem that existing MR devices, e.g. HoloLens, are unable to scan and model an entire large scene with several rooms at once. We introduce a custom data structure called VorPa, based on the Voronoi diagram, to implement path editing, accelerated rendering and location effectively. Our experiments and applications show that the toolkit is convenient and easy to use for constructing MR apps targeting large indoor physical spaces, in which users can roam in real time.},
  keywords={Virtual reality;Rendering (computer graphics);Real-time systems;Layout;Acceleration;Two dimensional displays;Three-dimensional displays;Mixed reality;Toolkit;HoloLens;Real-Time Roaming;Voronoi;Human-centered computing;Human computer interaction (HCI);Interactive systems and tools;User interface toolkits;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR.2019.8798050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797810,
  author={Xu, Yi and Yang, Shanglin and Sun, Wei and Tan, Li and Li, Kefeng and Zhou, Hui},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Garment Using Joint Landmark Prediction and Part Segmentation}, 
  year={2019},
  volume={},
  number={},
  pages={1247-1248},
  abstract={We present a novel approach that constructs 3D virtual garment models from photos. Our approach only requires two images as input, one front and one back. We first apply a multi-task learning network that jointly predicts fashion landmarks and parses a garment image into semantic parts. The predicted landmarks are used for deforming a template mesh to generate 3D garment model. The semantic parts are utilized for extracting color textures for the model.},
  keywords={Clothing;Three-dimensional displays;Solid modeling;Computational modeling;Image segmentation;Semantics;Strain;landmark prediction;part segmentation;3D modeling;Computing methodologies;Computer graphics;Graphics systems and interfaces;Artificial intelligence;Computer vision},
  doi={10.1109/VR.2019.8797810},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798294,
  author={Yi, Da-Chung and Chen, Yang-Sheng and Han, Ping-Hsuan and Wang, Hao-Cheng and Hung, Yi-Ping},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Archaeological Excavation Simulation for Interaction in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1249-1250},
  abstract={We propose a real-time excavation simulation system for interactive gameplay in Virtual Reality. In order to increase the player's immersion, our simulation system will produce actual potholes and clods according to the depth and angle of the players excavation. We divide the process into three phases: ground deformation, clod generation and clod fragmentation. In ground deformation, we describe how to simulate the topographic changes before and after excavation. In clod generation, we describe how to generate the clod which mesh matches the depth and angle of the players excavation action. In clod fragmentation, the clods are broken and fall as the shovel lifts. This simulation system can create excavation effects on different geologies by changing the material of the ground and clods.},
  keywords={Solid modeling;Strain;Shape;Virtual reality;Real-time systems;Visualization;Cultural differences;Human-centered computing;Virtual Reality},
  doi={10.1109/VR.2019.8798294},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797736,
  author={Yong, Hwanmoo and Lee, Jisuk and Choi, Jongeun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Emotion Recognition in Gamers Wearing Head-mounted Display}, 
  year={2019},
  volume={},
  number={},
  pages={1251-1252},
  abstract={Wearing head-mounted display (HMD) makes previous research regarding emotion recognition using machine vision ineffective since they utilized entire face images for training. In this paper, we trained the convolutional neural networks (CNNs) which are capable of estimating the emotions from the images of a face wearing a HMD by hiding eyes and eyebrows from existing face-emotion dataset. Our analysis based on the class activation maps show that it is capable of classifying emotions without the eyes and the eyebrows which ar to serve useful information in recognizing emotions. This implies the possibility of estimating the emotions from the images of humans wearing HMDs using machine vision.},
  keywords={Face;Emotion recognition;Resists;Feature extraction;Eyebrows;Training;Cams;Emotion Recognition;Deep Neural Network},
  doi={10.1109/VR.2019.8797736},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798355,
  author={Yoshida, Shunsuke and Sugawara, Ryo and Huang, Jiawei and Kitamura, Yoshifumi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interacting with 3D Images on a Rear-projection Tabletop 3D Display Using Wireless Magnetic Markers and an Annular Coil Array}, 
  year={2019},
  volume={},
  number={},
  pages={1253-1254},
  abstract={This paper proposes an interactive rear-projection tabletop glasses-free 3D display using a novel wireless magnetic motion capture system. Our tracking system employs an electromagnetic field generator and 16 magnetic detectors. It detects the 3D positions of several small markers in the generated electromagnetic field. The detectors are arranged in a ring around the rim of the conical screen of the 3D display to avoid occluding the reproduced 360-degree-viewable 3D images. For the proposed configuration, our experimental results reveal that a toroidal area around a hemispherical 3D image display area allows the 3D position to be measured with sufficient accuracy. We implemented an application to demonstrate real-time interaction with virtual 3D objects displayed on the table using markers attached to a physical object like a stick or finger.},
  keywords={Three-dimensional displays;Detectors;Tracking;Magnetic resonance imaging;Toroidal magnetic fields;Wireless communication;Electromagnetics;Finger tracking;wireless magnetic motion capture;360-degree glasses-free 3D display;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798355},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798327,
  author={Yoshimura, Andrew and Khokhar, Adil and Borst, Christoph W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Eye-gaze-triggered Visual Cues to Restore Attention in Educational VR}, 
  year={2019},
  volume={},
  number={},
  pages={1255-1256},
  abstract={In educational virtual reality, it is important to deal with problems of student inattention to presented content. We are developing attention-restoring visual cues for display when gaze tracking detects that student focus shifts away from critical objects. These cues include novel aspects and variations of standard cues that performed well in prior work on visual guidance. For the longer term, we propose experiments to compare various cues and their parameters to assess effectiveness and tradeoffs, and to assess the impact of eye tracking. Eye tracking is used to both detect inattention and to control the appearance and location of cues.},
  keywords={Visualization;Gaze tracking;Three-dimensional displays;Virtual reality;Standards;Conferences;Educational VR;Attention;Eye Tracking;Visual Cues;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism ‐‐‐ Virtual Reality;K.3.0 [Computers and Education]: General},
  doi={10.1109/VR.2019.8798327},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797916,
  author={Yu, Difeng and Liang, Hai-Ning and Zhang, Tianyu and Xu, Wenge},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={DepthMove: Hands-free Interaction in Virtual Reality Using Head Motions in the Depth Dimension}, 
  year={2019},
  volume={},
  number={},
  pages={1257-1258},
  abstract={Hands-free interactions are very handy for virtual reality (VR) head-worn display (HWD) systems because they allow users to interact with VR environments without the need for a hand-held device. This paper explores the potential of a new approach that we call DepthMove to allow hands-free interactions that are based on head motions towards the depth dimension. With DepthMove, a user can interact in a VR system proactively by moving towards the depth dimension with an HWD. We present the concept and implementation of DepthMove in VR HWD systems and demonstrate its potential applications. We further discuss the advantages and limitations of using DepthMove.},
  keywords={Switches;Motion pictures;Virtual reality;Headphones;Sensors;Human factors;Pediatrics},
  doi={10.1109/VR.2019.8797916},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798288,
  author={Yu, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Real-Time Music VR System for 3D External and Internal Articulators}, 
  year={2019},
  volume={},
  number={},
  pages={1259-1260},
  abstract={Both external and internal articulators are crucial to generating avatars in VR. Compared to traditional talking head with only appearance, we enhance it to 3D singing head with music signal as input, and focus on the entire head. For the results to look natural, a completed head model is first obtained by integrating matched multi-view visible images, face prior, and generic internal articulatory meshes. Then, the keyframes of song animation are substantially generated from real articulation data and physiology. Finally, the song synchronicity of articulators is learned from parallel audio-visual data using deep neural network. Experiments shows our system can produce realistic animation.},
  keywords={Animation;Three-dimensional displays;Solid modeling;Image reconstruction;Feature extraction;Mouth;Virtual reality;Music visualization;Articulatory animation;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798288},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798330,
  author={Yukawa, Hikari and Sato, Katsunari},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Crafting Experience: Hand Motion and Scent Stimulation in Conjunction with a Promotional Video for Improving Interest}, 
  year={2019},
  volume={},
  number={},
  pages={1261-1262},
  abstract={Crafting workshops are useful methods for the promotion of products because it can let customers know about the technical and social appeal of the product. In this study, we have developed simple, portable, and active multisensory VR systems of crafting experiences for product promotion. It comprises a video of crafting a wooden object with scent and hand motion synchronized with the video. The result of the product evaluations demonstrated positive effects, wherein the participants were found to be more attracted to the product in the video, where only scent or haptic stimulations were presented. However, the result also indicated that there is a negative synergetic effect when two stimulations were simultaneously presented.},
  keywords={Synchronization;Conferences;Haptic interfaces;Media;Prototypes;Virtual reality;Analysis of variance;Scent;haptics;promotion;[Human-computer interaction (HCI)]: interaction paradigm;Virtual reality},
  doi={10.1109/VR.2019.8798330},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798000,
  author={Zhang, Haoyang and Orlosky, Jason},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Thermal HDR: Applying High Dynamic Range Rendering for Fusion of Thermal Augmentations with Visible Light}, 
  year={2019},
  volume={},
  number={},
  pages={1263-1264},
  abstract={In safety applications for fields such as navigation or industrial manufacturing, thermal augmentations have often been used to help users safely navigate low-light environments and detect obstacles. However, one problem with thermal imaging is that it often occludes the environment and coloration that would otherwise be useful, for example traffic sign text or warning labels on industrial equipment. In this paper, we explore a new algorithm that takes advantage of high dynamic range (HDR) rendering techniques in order to more effectively present thermal information. Unlike previous fusion algorithms or conventional blending techniques, our setup makes use of both a thermal camera and HDR frames to synthesize a final overlay. Moreover, we set up a series of two experiments with a simulated heads up display (HUD) to 1) measure reaction times to the sudden appearance of pedestrians, and 2) conduct circuit repair. Results showed that the HDR algorithm was subjectively preferred to other approaches, and that performance on average could match other conventional algorithms on most occasions.},
  keywords={Image color analysis;Cameras;Maintenance engineering;Thermal stresses;Stress;Hazards},
  doi={10.1109/VR.2019.8798000},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797819,
  author={Zhang, Jingxin and Katzakis, Nikolaos and Mostajeran, Fariba and Steinicke, Frank},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Localizing Teleoperator Gaze in 360° Hosted Telepresence}, 
  year={2019},
  volume={},
  number={},
  pages={1265-1266},
  abstract={We evaluate the ability of locally present participants to localize an avatar head's gaze direction in 360° hosted telepresence. We performed a controlled user study to test two potential solutions to indicate a remote user's gaze. We analyze the influence of the user's distance to the avatar and display technique on localization accuracy. Our experimental results suggest that all these factors have a significant effect on the localization accuracy with varying effect sizes.},
  keywords={Avatars;Telepresence;Teleoperators;Resists;Face},
  doi={10.1109/VR.2019.8797819},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797878,
  author={Zhang, Li and He, Weiping and Sun, Mengmeng and Bai, Xiaoliang and Wang, Shuxia},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Multidirectional Haptic Feedback Prototype for Experiencing Collisions Between Virtual and Real Objects}, 
  year={2019},
  volume={},
  number={},
  pages={1267-1268},
  abstract={Haptic feedback has shown its great value in HCI research and applications for enhancing user experiences. Simulating people's tactile sensations of virtual objects is currently a primary research target. Different from wearing motors on fingers or hands, we attached vibration motors on a physical object to simulate the augmented sense of collision with virtual objects to bare hands. We developed a novel sensor-based proof-of-concept prototype that distributes multiple vibration motors around a physical object and provides vibrational sensations from the collision direction through combinations of motors. Users can obtain augmented haptic feedback when manipulating the augmented physical object to interact with virtual objects in an AR environment. We first studied the influence of the number and input voltage of motors for a correct judgment of different directions to identify the design parameters of the prototype. Then we investigated the effect of introducing the prototype in a typical manipulation task in AR. We found the prototype was efficient for enhancing human performance and collision experience with virtual objects together with visual feedback.},
  keywords={Haptic interfaces;Vibrations;Prototypes;Task analysis;Visualization;Force;User interfaces;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2019.8797878},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798136,
  author={Zhang, Sinan and Kurogi, Akiyoshi and Ono, Yumie},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Sickness in Continuous Exposure to Live-action 180°Video}, 
  year={2019},
  volume={},
  number={},
  pages={1269-1270},
  abstract={The goal of this study was to determine the factors that determine the degree of VR sickness in order to improve the audiovisual experience of VR videos or games. We used a simulator sickness questionnaire to evaluate the degree of VR sickness for nine types of live-action 180-degree videos, with a combination of different movement speeds and fields of view (FOV). Among the 40 participants we tested, those suffering from motion sickness had more serious symptoms than those without motion sickness. Although statistical tests failed to show significant differences related to the movement speeds or fields of view, our results suggested that VR exposure time was the most important factor influencing VR sickness.},
  keywords={Virtual reality;simulator sickness questionnaire;motion sickness;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers;Interaction paradigms},
  doi={10.1109/VR.2019.8798136},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797838,
  author={Zhang, Xiaoxi and Yin, Yong and Wan, Feifei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Underwater Manipulation Training Simulation System for Manned DeepSubmarine Vehicle}, 
  year={2019},
  volume={},
  number={},
  pages={1271-1272},
  abstract={In order to solve the problems of high safety risk and low training efficiency in underwater operation of deep submersible vehicle. Taking China's first manned deep submarine vehicle “jiaolong” as the simulation prototype, an underwater training platform is developed, which is not limited by time and place. Using three-dimensional modeling technology to build the three-dimensional model of “jiaolong” A mathematical model of the motion of a manipulator is established to simulate the associated motion between the joints of the manipulator. The collision detection technique is used to determine whether there is interaction between the manipulator and the operated object, between the object and the sampling basket, and between the equipment and the scene. The system is based on 3D development engine, which includes cobalt-rich crust mining area, polymetallic sulphide area, shallow sea area and cold spring area and so on. With this system the operator can train the underwater operation process. The system uses virtual reality helmet as the final visual display mode and the line of sight collision detection based on helmet instead of mouse click to simulate the whole process of underwater operation of deep submersible.},
  keywords={Solid modeling;Manipulators;Underwater vehicles;Training;Mathematical model;Biological system modeling;Three-dimensional displays;manned deep submersible vehicle;immersive;underwater operation;motion model;interaction},
  doi={10.1109/VR.2019.8797838},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798226,
  author={Zhang, Xinyu and Zhao, Yao and Mitchell, Nikk and Li, Wensong},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A New 360 Camera Design for Multi Format VR Experiences}, 
  year={2019},
  volume={},
  number={},
  pages={1273-1274},
  abstract={We present a new 360 camera design for creating 360 videos for immersive VR experiences. We place eight fish-eye lenses on a circle. Four interlaced fish-eye lenses are slightly re-oriented up in order to cover the scene above. To the best of our knowledge, our camera has the smallest diameter of any existing stereo multi-lens rig on the market. Our camera can be used to create 2D, 3D and 6DoF multi-format 360 videos. Due to its compact design, the minimum safe distance of our new camera is very short (approximately 30cm). This allows users to create special intimate immersive experiences. We also propose to characterize the camera design using the fractal ratio of the distance of adjacent view points and interpupillary distance. While most early camera designs have fractal ratio or =1, our camera has the fractal ratio . Moreover, with adjustable rendering interpupillary distance, our camera can be used to flexibly control the interpupillary distance for creating 3D 360 videos. Our camera design has high fault tolerance and it can continue operating properly even in the event of the failure of some individual lenses.},
  keywords={Cameras;Lenses;Three-dimensional displays;Fractals;Virtual reality;Stereo image processing;Adaptive optics;Multi-camera System-360 VR Camera-3D VR Stitching-6DoF VR},
  doi={10.1109/VR.2019.8798226},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797970,
  author={Zhang, Zhenliang and Wang, Cong and Weng, Dongdong and Liu, Yue and Wang, Yongtian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Symmetrical Reality: Toward a Unified Framework for Physical and Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1275-1276},
  abstract={In this paper, we review the background of physical reality, virtual reality, and some traditional mixed forms of them. Based on the current knowledge, we propose a new unified concept called symmetrical reality to describe the physical and virtual world in a unified perspective. Under the framework of symmetrical reality, the traditional virtual reality, augmented reality, inverse virtual reality, and inverse augmented reality can be interpreted using a unified presentation. We analyze the characteristics of symmetrical reality from two different observation locations (i.e., from the physical world and from the virtual world), where all other forms of physical and virtual reality can be treated as special cases of symmetrical reality.},
  keywords={Augmented reality;Virtual environments;User interfaces;Conferences;Three-dimensional displays;Solids;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/VR.2019.8797970},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797713,
  author={Zhao, Dan and Liu, Yue and Wang, Yongtian and Liu, Tong},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analyzing the Usability of Gesture Interaction in Virtual Driving System}, 
  year={2019},
  volume={},
  number={},
  pages={1277-1278},
  abstract={In this study, an experiment is presented aiming at verifying the applicability of gesture interaction in the virtual driving environment. 30 participants are recruited to perform the secondary tasks with gesture and touch interaction. The task completion rate and reaction time of two interaction modalities under different road conditions are adopted as evaluation indexes. In addition, visual attention, NASA-TLX, and subjective questionnaire are collected as evaluation factors for fuzzy comprehensive evaluation based on entropy to evaluate the usability gestures. The research results show that gesture interaction not only shows excellence in safety, but also favors more than 90% of users.},
  keywords={Roads;Task analysis;Usability;Indexes;Visualization;Human computer interaction;Switches;Virtual driving system;Gesture interaction;Usability;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Interaction techniques;Gestural input},
  doi={10.1109/VR.2019.8797713},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797715,
  author={Zhao, Wanqi and Devine, Kit and Gardner, Henry},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Fantasy Gaming and Virtual Heritage}, 
  year={2019},
  volume={},
  number={},
  pages={1279-1280},
  abstract={Virtual worlds are increasingly being developed to provide a navigable space for museum collections. The Virtual Sydney Rocks (VSR) [3] is one such virtual world that has been constructed as an authentic representation of the area of first European settlement in Australia 230 years ago - from a time just before its settlement until the present day. We constructed two versions of a game for children in order to motivate interaction with, and learning of, the historical content of the VSR. One of these game versions contained a number of fantasy design elements with the idea that their inclusion would motivate children to engage with VSR content in a pleasurable way. A preliminary study with a small number of school-aged children was not able to show that the inclusion of fantasy elements affected either the comprehension of historical facts or the quality of moral judgements made by the children. This study did, however, provide some evidence that interactivity with the relevant parts of the virtual world may have affected the retention of historical facts.},
  keywords={Games;Rocks;Ethics;Australia;Virtual reality;Europe;Computer science;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR.2019.8797715},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798362,
  author={Zhou, Qian and Wu, Fan and Stavness, Ian and Fels, Sidney},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Match the Cube: Investigation of the Head-coupled Input with a Spherical Fish Tank Virtual Reality Display}, 
  year={2019},
  volume={},
  number={},
  pages={1281-1282},
  abstract={Fish Tank Virtual Reality (FTVR) displays create a compelling 3D effect with the motion parallax cue using the head-coupled perspective. While the head-coupled viewpoint control provides natural visuomotor coupling, the motion parallax cue has been found to be underutilized with minimal head motion detected when manual input becomes available to users. We investigate whether users can effectively use head-coupling in conjunction with manual input in a mental rotation task involving inspection and comparison of a pair of 3D cubes. We found that participants managed to incorporate the head-coupled viewpoint control with the manual touch input in the task. They used the touch input as the primary input and the head as the secondary input with the input ratio of 4.2:1. The combined input approach appears to be sequential with only 8.63% duration when the head and manual input are co-activated. The result of this study provides insights for designing head-coupled interactions in many 3D interactive applications.},
  keywords={Three-dimensional displays;Task analysis;Manuals;Training;Head;Virtual reality;Two dimensional displays;Human-Centered Computing;Human computer interaction;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR.2019.8798362},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797735,
  author={Zhu, Lifeng and Liu, Xijing},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Mobile Augmented Reality Approach for Creating Dynamic Effects with Controlled Vector Fields}, 
  year={2019},
  volume={},
  number={},
  pages={1283-1284},
  abstract={Dynamic effects are commonly added offline to pre-recorded videos. In this work, we propose to synthesize online dynamic effects with controlled vector fields by using mobile augmented reality. By modelling typical primitives of a flow field to oriented markers, we use image detection and tracking techniques to build and control a virtual vector field in the real world. Virtual objects are then added and animated in real time. We prototype our system and the results show the possibility that dynamic effects can be created in mobile augmented reality to enhance the visual communication with the real world.},
  keywords={Mobile handsets;Aerodynamics;Augmented reality;Three-dimensional displays;Real-time systems;Videos;User interfaces;[Graphics systems and interfaces]: Mixed / augmented reality;[Human-centered computing]: Human computer interaction (HCI)},
  doi={10.1109/VR.2019.8797735},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798068,
  author={Zielasko, Daniel and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Travel Your Desk? An Office Desk Substitution and its Effects on Cybersickness, Presence and Performance in an HMD-based Exploratory Analysis Task}, 
  year={2019},
  volume={},
  number={},
  pages={1285-1286},
  abstract={In this work, we evaluate the feasibility of an office desk substitution in the context of a visual data analysis task involving travel. We measure the impact on cybersickness as well as the general task performance and presence. In the conducted user study (n=52), surprisingly, and partially in contradiction to existing work, we found no significant differences for those core measures between the control condition without a virtual table and the condition containing a virtual table.},
  keywords={Task analysis;Data analysis;Virtual environments;Visualization;Haptic interfaces;Three-dimensional displays;Human-centered concepts [Human computer interaction (HCI)]: Interaction paradigms;Virtual reality;Human-centered concepts [Human computer interaction (HCI)]: Visualization;Empirical studies in visualization},
  doi={10.1109/VR.2019.8798068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797900,
  author={Zielasko, Daniel and Krüger, Marcel and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Menus on the Desk? System Control in DeskVR}, 
  year={2019},
  volume={},
  number={},
  pages={1287-1288},
  abstract={In this work, we evaluate the impact of passive haptic feedback on touch-based menus, given the constraints and possibilities of a seated, desk-based scenario in VR. Therefore, we compare a menu that once is placed on the surface of a desk and once mid-air on a surface in front of the user. The study design is completed by two conditions without passive haptic feedback. In the conducted user study (n=33), we found effects of passive haptics (present vs-non-present) and menu alignment (desk vs. mid-air) on the task performance and subjective look & feel. However, the race between the conditions was close. An overall winner was the mid-air menu with passive haptic feedback, which however raises hardware requirements.},
  keywords={Haptic interfaces;Task analysis;Virtual reality;Standards;Cloud computing;Atmospheric measurements;Particle measurements;Human-centered concepts [Human computer interaction (HCI)]: Interaction paradigms;Virtual reality;Human-centered concepts [Human computer interaction (HCI)]: Visualization;Empirical studies in visualization},
  doi={10.1109/VR.2019.8797900},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798041,
  author={Zou, Liyuan and Higuchi, Takatoshi and Noma, Haruo and Roberto, Lopez-Gulliver and Isaka, Tadao},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of a Virtual Reality-based Baseball Batting Training System Using Instantaneous Bat Swing Information}, 
  year={2019},
  volume={},
  number={},
  pages={1289-1290},
  abstract={Batting training practice aims at increasing the batting performance of baseball players. Traditional batting practice methods have been proven effective in increasing the players batting performance in real games. However, the feedback the player gets is limited to: the vibration of the bat, the sound of the impact and the ball trajectory. We propose a Virtual Reality-based (VR) baseball batting system that provides batters with instantaneous bat swing information as feedback. This information includes: exact bat-ball impact location and angle, replay for swing timing and speed. The ability to review this swing information immediately after each swing may help batters to quantitatively adjust their swing to improve batting performance. In order to evaluate its effectiveness, we compared the proposed method against a traditional batting training method, during a short period. Results of our preliminary experiments show that the VR-based group batting performance, after a 10-day practice period, was comparable to the traditional group. Further experiments and analysis are required to assess the efficiency of the proposed method.},
  keywords={Sports;Training;Timing;Trajectory;Games;Virtual reality;Three-dimensional displays;baseball;batting training;impact location;swing angle;virtual reality;[Human computer interaction (HCI)]: Interaction paradigms},
  doi={10.1109/VR.2019.8798041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798347,
  author={Akiyama, Ryo and Yamamoto, Goshiro and Amano, Toshiyuki and Taketomi, Takafumi and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perceptual Appearance Control by Projection-Induced Illusion}, 
  year={2019},
  volume={},
  number={},
  pages={1291-1292},
  abstract={Using projection mapping, we can control the appearance of realworld objects by projecting colored light onto them. Since a projector can only add illumination to the scene, limited color gamut can be presented through projection mapping. However, actual color and perceived color are not always the same, and there is often large difference between them. We intentionally generate this difference by inducing visual illusion for extending the controllable color gamut of a projector. In particular, we induce color constancy. We demonstrate changing object color with and without inducing color constancy. Audiences perceive the controlled colors with and without illusion as different color nevertheless these colors are physically completely same.},
  keywords={Color;Image color analysis;Lighting;Visualization;Observers;Green products;Cameras;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Perception},
  doi={10.1109/VR.2019.8798347},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797999,
  author={Aoyagi, Saizo and Tanaka, Atsuko and Fukumori, Satoshi and Yamamoto, Michiya},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR system to simulate tightrope walking with a standalone VR headset and slack rails}, 
  year={2019},
  volume={},
  number={},
  pages={1293-1294},
  abstract={Slack Rails, which are made of foam rubber, can offer a simulated experience of using slack lines. In this study, we developed a prototype system and its content to enable tightrope walking between skyscrapers with a VR headset. An evaluation experiment was conducted, and results show that it can make VR tightrope walking more realistic, difficult, and suitable for training.},
  keywords={Rails;Legged locomotion;Headphones;Training;Task analysis;Prototypes;Cameras;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;HCI theory, concepts and models},
  doi={10.1109/VR.2019.8797999},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798037,
  author={Chen, Chih-Fan and Rosenberg, Evan Suma},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Generation of Dynamically Relightable Virtual Objects with Consumer-Grade Depth Cameras}, 
  year={2019},
  volume={},
  number={},
  pages={1295-1296},
  abstract={This research demo showcases the results of novel approach for estimating the illumination and reflectance properties of virtual objects captured using consumer-grade RGB-D cameras. This method is implemented within a fully automatic content creation pipeline that generates photo realistic objects in real-time virtual reality scenes with dynamic lighting. The geometry of the target object is first reconstructed from depth images captured using a handheld camera. To get nearly drift-free texture maps of the virtual object, a set of selected images from the original color stream is used for camera pose optimization. Our approach further separates these images into diffuse (view-independent) and specular (view-dependent) components using low-rank decomposition. The lighting conditions during capture and reflectance properties of the virtual object are subsequently estimated from the specular maps. By combining these parameters with the diffuse texture, reconstructed objects are then rendered in a real-time virtual reality demo that plausibly replicates the real world illumination and showcases dynamic lighting with varying direction, intensity, and color.},
  keywords={Lighting;Image reconstruction;Image color analysis;Cameras;Real-time systems;Virtual environments;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Rendering;Reflectance modeling},
  doi={10.1109/VR.2019.8798037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797796,
  author={Date, Munekazu and Isogai, Megumi and Kimata, Hideaki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Full Parallax Table Top 3D Display Using Visually Equivalent Light Field}, 
  year={2019},
  volume={},
  number={},
  pages={1297-1298},
  abstract={A new full parallax light field 3D display that optically interpolates the rays emitted is proposed. Using a barrier with special aperture structure, we achieve interpolation by optical linear blending not only for the horizontal direction, but also for the vertical direction. Linear blending produces visually equivalent intermediate view point images equivalent to the real views if the disparity between the two images being blended are small enough. Therefore, a high feel of existence can be produced as the interpolation yields smooth motion parallax. Since interpolation reduces the number of viewpoints, number of pixels in each direction can be increased yielding high resolution. In this demonstration, we reproduce CG objects and scenes of a ball game on a table top.},
  keywords={Three-dimensional displays;Light fields;Optical imaging;Apertures;Cameras;Liquid crystal displays;Interpolation;Light field 3D display;Parallax barrier;Full parallax;Hardware → Communication hardware;interfaces and storage → Displays and imagers},
  doi={10.1109/VR.2019.8797796},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797884,
  author={Dixken, Manuel and Diers, Daniel and Wingert, Benjamin and Hatzipanayioti, Adamantini and Mohler, Betty J and Riedel, Oliver and Bues, Matthias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Distributed, Collaborative Virtual Reality Application for Product Development with Simple Avatar Calibration Method}, 
  year={2019},
  volume={},
  number={},
  pages={1299-1300},
  abstract={In this work we present a collaborative virtual reality application for distributed engineering tasks, including a simple avatar calibration method. Use cases for the application include CAD review, ergonomics analyses or virtual training. Full-body avatars with approximately natural motion characteristics are used to improve collaboration in terms of co-presence and embodiment. Through a calibration process, scaled avatars based on body measurements can be generated by users from within the VR application. For motion tracking, only the tracking capabilities of the VR devices used (head mounted display and hand controllers) are required, in order to achieve a low initialization effort for usage of the application. We demonstrate a simple, yet effective method for measuring the human body which requires these VR devices only. Tracking only the user's head and hands, we require inverse kinematics (IK) for avatar motion reconstruction. On the other hand, this requires a calibrated avatar, otherwise body postures and gestures will be misrepresented. In the first step of the demo, two users can create their calibrated avatar at the same time and then carry out a collaborative CAD review in the second step.},
  keywords={Avatars;Collaboration;Tracking;Kinematics;Three-dimensional displays;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797884},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797985,
  author={Fukiage, Taiki and Kawabe, Takahiro and Nishida, Shin'ya},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Demonstration of Perceptually Based Adaptive Motion Retargeting to Animate Real Objects by Light Projection}, 
  year={2019},
  volume={},
  number={},
  pages={1301-1302},
  abstract={A recently developed light projection technique can add dynamic impressions to static real objects without changing their original visual attributes such as surface colors and textures. It produces illusory motion impressions in the projection target by projecting gray-scale motion-inducer patterns that selectively drive the motion detectors in the human visual system. However, with this technique, determining the best deformation sizes is often difficult: When users try to add a large deformation, the deviation in the projected patterns from the original surface pattern on the target object becomes apparent. Therefore, to obtain satisfactory results, they have to spend much time and effort to manually adjust the shift sizes. Here, to overcome this limitation, we propose an optimization framework that adaptively retargets the displacement vectors based on a perceptual model. The perceptual model predicts the subjective inconsistency between a projected pattern and an original one by simulating responses in the human visual system. The displacement vectors are adaptively optimized so that the projection effect is maximized within the tolerable range predicted by the model. In the research demonstration, we will present a demo tool that incorporates our optimization technique, where a user can interactively edit dynamic appearances of a real object without cumbersome manual adjustments of deformation sizes.},
  keywords={Strain;Adaptation models;Optimization;Computational modeling;Predictive models;Brain modeling;Dynamics;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;HCI design and evaluation methods;User models},
  doi={10.1109/VR.2019.8797985},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798104,
  author={Furukawa, Masahiro and Matsumoto, Kohei and Kurokawa, Masataka and Miyamoto, Hiroki and Maeda, Taro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Walking Experience Under Equivalent Gravity Condition on Scale Conversion Telexistence}, 
  year={2019},
  volume={},
  number={},
  pages={1303-1304},
  abstract={Scale conversion telexistence is an extended concept of telexistence. It comprises avatars of different scales compared with the human scale. Immersive telexistence experience requires the avatar to realize the operator's behavior instantaneously as soon as possible. It is important to match the avatar's behavior with the operator's behavior because the difference between the operator and the avatar tends to hurt the immersiveness. However, the scale difference between an operator with the avatar makes behavior mismatch and lets the operator experience under different physical constant world. For instance, gravity changes under scale conversion telexistence. Therefore, our previous work reveals its importance termed equivalent gravity condition for walking using a small biped robot. Thus, in this demonstration, we use a small biped robot as the different-scale avatar allow us to experience as if we are within the small robot. Attendees experience several steps walking under different gravity conditions.},
  keywords={Legged locomotion;Gravity;Telexistence;Acceleration;Avatars;Robot sensing systems;Human-centered computing;Interaction design;Interaction design process and methods;User interface design;User centered design},
  doi={10.1109/VR.2019.8798104},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797825,
  author={Hamilton, Rob},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Coretet: A 21st Century Virtual Reality Musical Instrument for Solo and Networked Ensemble Performance}, 
  year={2019},
  volume={},
  number={},
  pages={1305-1306},
  abstract={Coretet is a virtual reality instrument that explores the translation of performance gesture and mechanic from traditional bowed string instruments into an inherently non-physical implementation. Built using the Unreal Engine 4 and Pure Data, Coretet offers musicians a flexible and articulate musical instrument to play as well as a networked performance environment capable of supporting and presenting a traditional four-member string quartet. This paper discusses the technical implementation of Coretet and explores the musical and performative possibilities through the translation of physical instrument design into virtual reality.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;musical instrument;virtual reality;procedural audio;network performance;gesture-based interaction;J.5 [Computer Applications]: Arts and Humanities-fine arts;H.5.5 [Sound and Music Computing]: H5.1 [Information Interfaces and presentation]: Multimedia Information Systems-Artificial, augmented and virtual realities},
  doi={10.1109/VR.2019.8797825},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798245,
  author={Hiratani, Kosuke and Iwai, Daisuke and Punpongsanon, Parinya and Sato, Kosuke},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shadowless Projector: Suppressing Shadows in Projection Mapping with Micro Mirror Array Plate}, 
  year={2019},
  volume={},
  number={},
  pages={1309-1310},
  abstract={Shadowless Projector is projection mapping system in which a shadow (more specifically, umbra) does not suffer the projected result. A typical shadow removal technique used a multiple overlapping projection system. In this paper, we propose a shadow-less projection method with single projector. Inspired by a surgical light system that does not cast shadows on patients' bodies in clinical practice, we apply a special optical system that consists of methodically positioned vertical mirrors. This optical system works as a large aperture lens, it is impossible to block all projected ray by a small object such as a hand. Consequently, only penumbra is caused, which leads to a shadow-less projection.},
  keywords={Mirrors;Apertures;Lenses;Cameras;Entertainment industry;Computer vision;Light sources;Projection Mapping;Micro Mirror Array Plate;Interactive system;Spatial Augmented Reality},
  doi={10.1109/VR.2019.8798245},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797771,
  author={Huang, Jiawei and Lucash, Melissa S. and Scheller, Robert M. and Klippel, Alexander},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visualizing Ecological Data in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1311-1312},
  abstract={Visualizing complex scientific data and models in 2D can be challenging. The result can be hard to interpret and understand for the general audience, and the model accuracy hard to evaluate even for the experts. To address these problems, we created a workflow that translates data of an ecological model, LANDIS-II, into a high-fidelity 3D model in virtual reality (VR). We combined ecological modeling, analytical modeling, procedural modeling, and VR, to allow users to experience a forest in northern Wisconsin (WI), United States, under two climate scenarios. Users can explore and interact with the forest under different climate scenarios, explore the impacts of climate change on different tree species, and retrieve information from a 3D tree database. The VR application can be used as an educational tool for the general public, and as a model checking tool by researchers.},
  keywords={Forestry;Data visualization;Solid modeling;Biological system modeling;Three-dimensional displays;Data models;Climate change;Visualization;Scientific visualization;Geographic visualization;Virtual reality;Human computer interaction;Human centered computing;Interaction paradigms;Software and its engineering;Software organization and properties;Virtual worlds software;Virtual worlds training simulations;Physical sciences and engineering;Earth and atmospheric sciences;Environmental sciences;Visualization application domains},
  doi={10.1109/VR.2019.8797771},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798010,
  author={Hutton, Courtney and Sohre, Nicholas and Davis, Bobby and Guy, Stephen and Rosenberg, Evan Suma},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Augmented Reality Motion Planning Interface for Robotics}, 
  year={2019},
  volume={},
  number={},
  pages={1313-1314},
  abstract={With recent advances in hardware technology, autonomous robots are increasingly present in research activities outside of robotics, performing a multitude of tasks such as image capture and sample collection. However, user interfaces for task-oriented robots have not kept pace with hardware breakthroughs. Current planning and control interfaces for robots are not intuitive and often place a large cognitive burden on those who are not highly trained in their use. Augmented reality (AR) has also seen major advances in recent years. This demonstration illustrates an initial system design for an AR user interface for path planning with robotics.},
  keywords={Robots;Task analysis;User interfaces;Drones;Path planning;Augmented reality},
  doi={10.1109/VR.2019.8798010},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798243,
  author={Ichikawa, Shotaro and Onishi, Yuki and Hayashi, Daigo and Ebi, Akiyuki and Endo, Isamu and Suzuki, Aoi and Niwano, Anri and Kitamura, Yoshifumi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Be Bait!: A Unique Fishing Experience with Hammock-based Underwater Locomotion Method}, 
  year={2019},
  volume={},
  number={},
  pages={1315-1316},
  abstract={We present “Be Bait!”, a unique virtual fishing experience that allows the user to become a bait by lying on the hammocks, instead of holding a fishing rod. We implement the hammock-based locomotion method with haptic feedback mechanisms. In our demonstration, users can enjoy exploring a virtual underwater world and fighting with fishes in direct and intuitive ways.},
  keywords={Marine animals;Acceleration;User interfaces;Sports;Legged locomotion;Aluminum;Mouth;Human-centered computing;Virtual reality},
  doi={10.1109/VR.2019.8798243},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798230,
  author={Inoue, Akifumi and Fukunaga, Takeru and Ishikawa, Ryuta},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Transformable Game Controller and Its Application to Action Game}, 
  year={2019},
  volume={},
  number={},
  pages={1317-1318},
  abstract={Dedicated controllers can provide rich gaming experience. However, a player must prepare many controllers to enjoy modern games that have many game items. Depending on circumstances, the shape of the controller and the shape of the game item are mismatched. This may reduce the player's sense of unity with the game character. In this paper, we propose a transformable game controller. This controller can be multiple dedicated controllers by changing its three-dimensional shape. The game system can also change the shape of the controller immediately when the shape of the corresponding item changes. The continuous shape synchronization between real controller and virtual item can provide the player rich sense of unity with the game character.},
  keywords={Games;Shape;Synchronization;Weapons;Shafts;Haptic interfaces;Presses;Gaming Experience;Haptic Interface;Human-centered computing;Haptic devices;Virtual reality;Software and its engineering~ Interactive games},
  doi={10.1109/VR.2019.8798230},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798064,
  author={Izumihara, Atsushi and Uriu, Daisuke and Hiyama, Atsushi and Inami, Masahiko},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={ExLeap: Minimal and highly available telepresence system creating leaping experience}, 
  year={2019},
  volume={},
  number={},
  pages={1321-1322},
  abstract={We propose “ExLeap”, a minimal telepresence system that creates leaping experience. Multiple “nodes” with an omnidirectional camera, mic and speaker transmit the video to clients, and on the client, videos are rendered in 3D space. When moving to another node, by crossfading two videos, the user can feel as if she/he leaps between two places. Also, on each node, the user can talk with people in that place. Each node consists of very simple hardware, so we can put them on multiple places we want to go to. Moreover, because the system can be used 24/7 by multi-user simultaneously and is very easy to use, it creates various types of chances of communications.},
  keywords={Cameras;Telepresence;Robot sensing systems;Three-dimensional displays;Urban areas;Conferences;Human-centered computing;Computer supported cooperative work;Interaction design;Information systems;Web conferencing},
  doi={10.1109/VR.2019.8798064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797762,
  author={Kataoka, Keishirou and Yamamoto, Takuya and Otsuki, Mai and Shibata, Fumihisa and Kimura, Asako},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A New Interactive Haptic Device for Getting Physical Contact Feeling of Virtual Objects}, 
  year={2019},
  volume={},
  number={},
  pages={1323-1324},
  abstract={The emergence of new and inexpensive virtual reality (VR) technology has made it relatively familiar to most users. We can create virtual 3D objects and paint on them in a VR spaces. Many of the VR controllers used for such operations provide haptic feedbacks by vibration when the user touches the virtual objects. In the real world, we can perceive not only the touch sensation but also the reaction force when touching and stroking the object's surface. However, it is impossible to provide a reaction force only with the vibration feedback. That is, there is a sensory gap between the VR space and the real world. The gap makes it difficult to work in the VR space in a manner similar to that in the real world. In this study, we focused on providing the reaction force from the virtual object to the user, and proposed a device that could provide the force feedback to the user's arm without connecting the device to large equipment.},
  keywords={Force;Aerospace electronics;Force feedback;Vibrations;Three-dimensional displays;Virtual reality;Haptic devices;virtual reality;force feedback;Human-centered computing;Human computer interaction (HCI);Interaction devices;Interaction paradigms},
  doi={10.1109/VR.2019.8797762},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798106,
  author={Kim, Hyundo and Nah, Sukgyu and Oh, Jaeyoung and Ryu, Hokyoung},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-MOOCs: A Learning Management System for VR Education}, 
  year={2019},
  volume={},
  number={},
  pages={1325-1326},
  abstract={This demonstration position paper introduces a first of its kind - VR MOOC LMS. The chemistry experiment VR content is running for the students, and a supervisor can monitor their learning performance and interaction behaviors. Our LMS system (local view, world view and multi-view user interfaces) for the VR MOOC system is expected to shed light on how the interactive VR learning content can be affiliated to the proper instructional design in the near future.},
  keywords={Servers;Monitoring;Virtual reality;Learning management systems;Chemistry;User interfaces;Massive open online course (MOOC);Virtual reality;Learning management systems (LMS);Human-centered computing-Virtual reality;Applied computing;Interactive learning environments;Learning management systems;E-Iearning},
  doi={10.1109/VR.2019.8798106},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798001,
  author={Kimura, Yuto and Manabe, Shinnosuke and Ikeda, Sei and Kimura, Asako and Shibata, Fumihisa},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Can Transparent Virtual Objects Be Represented Realistically on OST-HMDs?}, 
  year={2019},
  volume={},
  number={},
  pages={1327-1328},
  abstract={In mixed reality using video see-through displays, various optical effects can be simulated for representing a transparent object. However, consumer-available OST-HMDs can not reproduce some of their effects correctly because of its mechanism. There has been little discussion concerning to what extent such transparent objects can be represented realistically. In this demonstration, we will show how effective each for combination of existing methods is.},
  keywords={Optical imaging;Optical refraction;Optical attenuators;Optical distortion;Optical reflection;Cameras;Adaptive optics;Human-centered computingHuman-computer interaction (HCI)Interaction paradigmsMixed / augmented reality;Human-centered computingHuman-computer interaction (HCI)Interaction devicesDisplays and imagers},
  doi={10.1109/VR.2019.8798001},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798353,
  author={Lim, Hwasup and Kang, Junseok and Ahn, Sang Chul},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Rapid 3D Avatar Creation System Using a Single Depth Camera}, 
  year={2019},
  volume={},
  number={},
  pages={1329-1330},
  abstract={We present a rapid and fully automatic 3D avatar creation system that can produce personalized 3D avatars within two minutes using a single depth camera and a motorized turntable. The created 3D avatar is able to make all the details of facial expressions and whole body motions including fingers. To our best knowledge, it is the first completely automatic system that can generate realistic 3D avatars in the common 3D file format, which is ready for the direct use in virtual reality applications or various services.},
  keywords={Solid modeling;Avatars;Three-dimensional displays;Deformable models;Optimization;Face;Animation;Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems;Computing methodologies;Computer graphics;Shape modeling;Mesh models},
  doi={10.1109/VR.2019.8798353},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798049,
  author={Manabe, Shinnosuke and Ikeda, Sei and Kimura, Asako and Shibata, Fumihisa},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shadow Inducers: Inconspicuous Highlights for Casting Virtual Shadows on OST-HMDs}, 
  year={2019},
  volume={},
  number={},
  pages={1331-1332},
  abstract={Virtual shadows provide important cues to determine the positional relationship between virtual objects and a real scene. However, it' s difficult to render shadows in the real scene on optical see-through head-mounted displays without occlusion-capability. In the previous work, we cast virtual shadows not with physical light attenuation but with brightness induction caused by virtual objects, referred to as shadow inducers, which surround the shadow area to gradually amplify the intensity of the real scene pattern [4]. However, because the shadow inducer was prepared beforehand, the shape of the shadow is constant, the real scene shadowing is limited to a flat surface, and a large of viewpoint change is impossible. In this demonstration, we propose a method to generate shadow inducers in real time that can change the shape of virtual objects and the viewpoint of users. In this method, depending on the appearance of shadows, the surrounding luminance is gradually amplified with the difference of gaussian (DOG) representing characteristics of human vision. Users can observe shadows of moving and deforming virtual objects on a real tabletop and other non-planar objects.},
  keywords={Shape;Dogs;Brightness;Casting;Optical attenuators;Real-time systems;Three-dimensional displays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction devices;Displays and imagers},
  doi={10.1109/VR.2019.8798049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797987,
  author={Miyamoto, Hiroki and Nishimura, Tomoki and Onishi, Itsuki and Furukawa, Masahiro and Maeda, Taro},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Intuitive Operate the Robot with Unconscious Response in Behavioral Intention: Tsumori Control}, 
  year={2019},
  volume={},
  number={},
  pages={1337-1338},
  abstract={In this work, we proposed the control method that enable intuitive operation, “tsumori control”. Tsumori control is implemented using human discrete intention of motion included in continuous and intuitive motion output. The experience operating a robot using tsumori control will be demonstrated for our control system.},
  keywords={Robot sensing systems;Humanoid robots;Force;Telexistence;Control systems;Force sensors;Human-centered computing;Interaction design;Interaction design process and methods;User centered design},
  doi={10.1109/VR.2019.8797987},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798324,
  author={Nakano, Kizashi and Horita, Daichi and Sakata, Nobuchika and Kiyokawa, Kiyoshi and Yanai, Keiji and Narumi, Takuji},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enchanting Your Noodles: A Gustatory Manipulation Interface by Using GAN-based Real-time Food-to-Food Translation}, 
  year={2019},
  volume={},
  number={},
  pages={1339-1340},
  abstract={In this demonstration, we present a novel gustatory manipulation interface which utilizes the cross-modal effect of vision on taste elicited with real-time food appearance modulation using a generative adversarial network (GAN). Unlike existing systems which only change color or texture pattern of a particular type of food in an inflexible manner, our system changes the appearance of food into multiple types of food in real-time flexibly, dynamically and interactively in accordance with the deformation of the food that the user is actually eating by using GAN-based image-to-image translation. Our system can turn somen noodles into ramen noodles or fried noodles, or steamed rice into curry and rice or fried rice. Users of our demonstration system will taste what is visually presented to some extent rather than what they are actually eating.},
  keywords={Real-time systems;Servers;Streaming media;Resists;Visualization;Augmented reality;Generative adversarial networks;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception;Machine learning;Machine learning approaches;Neural networks},
  doi={10.1109/VR.2019.8798324},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797717,
  author={Nozawa, Takayuki and Wu, Erwin and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Ski Coach: Indoor Ski Training System Visualizing Difference from Leading Skier}, 
  year={2019},
  volume={},
  number={},
  pages={1341-1342},
  abstract={The training of skiing is difficult because of environmental requirements and teaching methods. Therefore, we propose a virtual reality ski training system using an indoor ski simulator. The system is based on a simple indoor ski simulator with two trackers to capture the motion of skis. Users can control the skis in the virtual ski slope we provided and train their skills with a replay of a professional skier. The training system consists of three modules: a coach replay system for reviewing pro-skiers's motion; a time control system that can be used to watch the detailed motion of both the coach and the user; and a visualization of the angle of the skis to compare the difference of motions between the users and the coach.},
  keywords={Training;Tracking;Sports;Visualization;Virtual reality;Gravity;Resists;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797717},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797998,
  author={Rajeswaran, Pavithra and Varghese, Jeremy and Kumar, Praveen and Vozenilek, John and Kesavadas, Thenkurussi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={AirwayVR: Virtual Reality Trainer for Endotracheal Intubation}, 
  year={2019},
  volume={},
  number={},
  pages={1345-1346},
  abstract={Endotracheal Intubation is a lifesaving procedure in which a tube is passed through the mouth into the trachea (windpipe) to maintain an open airway and facilitate artificial respiration. It is a complex psychomotor skill, which requires significant training and experience to prevent complications. The current methods of training, including manikins and cadaver, have limitations in terms of their availability for early medical professionals to learn and practice. These training options also have limitations in terms of presenting high risk/difficult intubation cases for experts to mentally plan their approach in high-risk scenarios prior to the procedure. In this demo, we present AirwayVR: virtual reality-based simulation trainer for intubation training. Our goal is to utilize virtual reality platform for intubation skills training for two different target audience (medical professionals) with two different objectives. The first one is to use AirwayVR as an introductory platform to learn and practice intubation in virtual reality for novice learners (Medical students and residents). The second objective is to utilize this technology as a Just-in-time training platform for experts to mentally prepare for a complex case prior to the procedure.},
  keywords={Training;Virtual reality;Biomedical imaging;Three-dimensional displays;Solid modeling;Medical services;Atmospheric modeling;Endotracheal Intubation;Virtual reality;Medical simulation;Medical skills training;Application of Virtual reality for skill training in medical simulation;Using Virtual reality for Intubation training;Just-in-time training using virtual reality in medicine},
  doi={10.1109/VR.2019.8797998},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798062,
  author={Rebane, Kadri and Hörnmark, David and Shijo, Ryota and Schewe, Tim and Nojima, Takuya},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Dodgeball with Double Layered Balancing}, 
  year={2019},
  volume={},
  number={},
  pages={1347-1348},
  abstract={Playing is most fun when the end result remains unsure until the last minutes of the game. In team games, this means that both teams have comparable skill levels. When playing for fun, this can be hard to accomplish. Instead, game balancing can be used. We introduce a double layer balancing system for dodgeball. Augmented Dodgeball uses a virtual layer to balance differences in the physical world. In the virtual layer, players can choose their character they wish to play in the game. This contributes in more collaboration and teamwork during the play. In addition, we introduce individual parameters to each player based on their self-assessed skill level. These changes will not be introduced to the players. This allows for a more seamless balancing between player's physical skills without publically labeling the players based on their skill levels.},
  keywords={Games;Bars;Teamwork;Labeling;Sports;Microcontrollers;Databases;Augmented Dodgeball;Augmented Sports;Exertion Games;Game Balancing;Sports;Video Gaming;Human-centered computing~Mixed / augmented reality},
  doi={10.1109/VR.2019.8798062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797903,
  author={Rosenkvist, Amalie and Eriksen, David Sebastian and Koehlert, Jeppe and Valimaa, Miicha and Vittrup, Mikkel Brogaard and Andreasen, Anastasia and Palamas, George},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hearing with Eyes in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1349-1350},
  abstract={Sound and light signal propagation have similar physical properties. This provides inspiration for creating an audio-visual echolocation system, where light is mapped to the sound signal, visually representing auralization of the virtual environment (VE). Some mammals navigate using echolocation; however humans are less successful with this. To the authors' knowledge, it remains to be seen if sound propagation and its visualization have been implemented in a perceptually pleasant way and is used for navigation purposes in the VE. Therefore, the core novelty of this research is navigation with visualized echolocation signal using a cognitive mental mapping activity in the VE.},
  keywords={Visualization;Navigation;Reverberation;Virtual environments;Three-dimensional displays;Solid modeling;Human-centered computing;Visualization design and evaluation methods;Sound;Echolocation;Virtual Reality},
  doi={10.1109/VR.2019.8797903},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798005,
  author={Takahashi, Kosuke and Mikami, Dan and Isogawa, Mariko and Kusachi, Yoshinori and Saijo, Naoki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-based Batter Training System with Motion Sensing and Performance Visualization}, 
  year={2019},
  volume={},
  number={},
  pages={1353-1354},
  abstract={This paper aims to establish a novel VR system for evaluating the performance of baseball batters. Existing VR systems for sports have been utilized as a tool for image training. In order to move such VR systems to the next stage, we introduce functions that sense the users' reaction to the VR stimulus. Our VR system has three features; (a) it synthesizes highly realistic VR video from the data captured in actual games, (b) it estimates the reaction of the user to the VR stimulus by capturing the 3D positions of full body parts, and (c) it consists of off-the-shelf devices and is easy to use. Our demonstration provides users with a chance to experience our VR system and give them some quick feedback by visualizing the estimated 3D positions of their body parts.},
  keywords={Three-dimensional displays;Sports;Training;Sensor systems;Games;Laboratories;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality Human;centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798005},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798195,
  author={Tanabe, Naruki and Sato, Yushi and Morita, Kohei and Inagaki, Michiya and Fujino, Yuichi and Punpongsanon, Parinya and Matsukura, Haruka and Iwai, Daisuke and Sato, Kosuke},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic Feedback}, 
  year={2019},
  volume={},
  number={},
  pages={1355-1356},
  abstract={We present fARFEEL, a remote communication system that provides visuo-haptic feedback allows a local user to feel touching distant objects. The system allows the local and remote users to communicate by using the projected virtual hand (VH) for the agency of his/her own hands. The necessary haptic information is provided to the non-manipulating hand of the local user that does not bother the manipulation of the projected VH. We also introduce the possible visual stimulus that could potentially provide the sense of the body ownership over the projected VH.},
  keywords={Haptic interfaces;Cameras;Visualization;Three-dimensional displays;Shape;Conferences;Strain;Virtual hand illusion;Spatial augmented reality;Visuo-haptic;Haptic feedback;Extended body interface},
  doi={10.1109/VR.2019.8798195},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797761,
  author={Tani, Yamato and Fujisawa, Satoshi and Hagiwara, Takayoshi and Ikei, Yasushi and Kitazaki, Michiteru},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Bidirectional Infection Experiences in a Virtual Environment}, 
  year={2019},
  volume={},
  number={},
  pages={1357-1358},
  abstract={Pathogenic infection is usually invisible. Thus, we hardly care of preventing it in the daily life. We aimed to develop a virtual reality system to experience bidirectional infections using vision and tactile sensations; Users are infected by an avatar, and they do infect the avatar. When the avatar coughs toward a user, a cloud of visualized pathogen-droplets jet out and strike the user and objects on a table. At the same time, vibrations are presented to the user's chest. When the user coughs toward the avatar, the cough was detected by a throat microphone, and a cloud of visualized pathogen jets out. The avatar tries to avoid it unpleasantly, but infected. He is getting sick in bad face color. This system enables us to experience pathogenic infection with multimodal sensations in both perspectives (infected and infecting). Thus, it may contribute to increasing user's knowledge of infection and to facilitating prevention behaviors such as wearing a hygiene mask.},
  keywords={Pathogens;Avatars;Microphones;Visualization;Vibrations;Virtual environments;infection;cough detection;vision;touch;embodied learning;public health;I.3.7 [Computer graphics]: Three-Dimensional Graphics and Realism;Virtual Reality;H.l.2 [Models and Principles]: User/Machine Systems;human factors},
  doi={10.1109/VR.2019.8797761},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797877,
  author={Gunkel, Simon N.B.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Multi-user (Social) Virtual Reality Commnunication}, 
  year={2019},
  volume={},
  number={},
  pages={1359-1360},
  abstract={Virtual Reality (VR) applications are currently suffering several gaps to allow a full breakthrough in the consumer market. One aspect is the social isolation. Many VR experiences do not represent yourself or other users, making it lonely rather than a shared experience. VR applications that do offer a user representation mostly use artificial avatars. These applications may not be well suited for all communication use cases (e.g. at work or with your family). In this paper I present our current work, at TNO and in my PhD research, to create Social VR experiences where both the user and the environment are represented in photo-realistic quality. The aim is not only to allow users to experience VR together, but also to allow new ways of natural communication within VR, with an extended presence and immersion. My PhD research focuses on 3 aspects that are building a Social VR system: capture and processing, transmission and client composition. The main goal of my research is to move processing from the capture and orchestration into the cloud (and particularly into the edge network) to allow Social VR with a large set of users (100+) in one session and to support mobile end devices while maximizing the QoS and QoE.},
  keywords={Virtual reality;Three-dimensional displays;Cameras;Quality of service;Quality of experience;Streaming media;Real-time systems;Virtual Reality;VR;Social VR;Requirements;WebRTC;WebVR;Immersive Virtual Environments;Information systems;World Wide Web;Web conferencing;Multimedia information systems;Multimedia streaming},
  doi={10.1109/VR.2019.8797877},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797893,
  author={Hutton, Courtney},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality Interfaces for Semi-Autonomous Drones}, 
  year={2019},
  volume={},
  number={},
  pages={1361-1362},
  abstract={With recent advances in hardware technology, drones are increasingly present in research activities outside of robotics, performing a multitude of tasks such as image capture and sample collection. However, user interfaces for task-oriented drones have not kept pace with hardware breakthroughs. Current planning and control interfaces for drones are not intuitive and often place a large cognitive burden on those who are not highly trained in their use. This paper proposes a course of research that seeks to leverage natural user interfaces in augmented reality (AR) for controlling drones completing task-oriented objectives.},
  keywords={Drones;Task analysis;User interfaces;Robots;Augmented reality;Planning;Human-centered computing;Mixed/augmented reality;Visualization techniques;Treemaps;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797893},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798203,
  author={Schirm, Johannes},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Case-studies of Contemporary Presence Theory: Towards More Objective and Reliable Measures of Presence}, 
  year={2019},
  volume={},
  number={},
  pages={1363-1364},
  abstract={A large body of literature is concerned with models of presence-the sensory illusion of being part of a virtual scene-but there is still no general agreement on how to measure it in an objective and reliable way. When it comes to virtual reality, presence is often considered as one of the main factors contributing to quality of experience, yet existing methods either rely on subjective assessments of users or on specifics of the virtual environment they are applied in, making it difficult for experimental procedures to be generalized. This paper presents ideas for research into promising measures of presence, based on first experiments with novel behavioral measures inside a rich environment which users can feel present in more naturally.},
  keywords={Atmospheric measurements;Particle measurements;Virtual environments;Games;Real-time systems;Reliability;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798203},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798147,
  author={Balint, Beata N.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Designing VR for Teamwork: The Influence of HMD VR Communication Capabilities on Teamwork Competencies}, 
  year={2019},
  volume={},
  number={},
  pages={1365-1366},
  abstract={With recent technological advancements, Virtual Reality (VR) has been advocated as an effective team training delivery method. However, to date, research has focused on demonstrating its effectiveness at the system level rather than isolating those factors within the system design (e.g. display size) that may influence the development of teamwork competencies. Communication is a significant component of teamwork. However, within Head Mounted Display (HMD)-based VR systems, communication is computer-mediated, resulting in the deprivation of important audio and visual cues. This research project therefore investigates the following question: “How do system design attributes that manipulate communication in HMD VR training systems affect the team's ability for effective performance?”.},
  keywords={Teamwork;Training;Resists;Measurement;Task analysis;Solid modeling;Virtual reality;Head-mounted display;teamwork;communication;design;Software and its engineermg-Vlrtual worlds training simulations;Human-centered computing-Virtual reality;Human-centered computing-Collaborative interaction},
  doi={10.1109/VR.2019.8798147},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797907,
  author={Pimentel, Daniel},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Digital Demons: Psychological Effects of Creating, and Engaging with, Virtual Avatars Representing Undesirable Aspects of the Self}, 
  year={2019},
  volume={},
  number={},
  pages={1367-1368},
  abstract={Avatars are extensions of users' identities in virtual environments (VEs), yielding substantial influence over self-perception, affect, and behavior. However, extant research has largely focused on the implications associated with creating, and interacting with, avatars representing desirable aspects of the self (ideal self), overlooking other aspects of the self. Across several studies I examine the psychological and emotional effects of (a) creating avatars representing anxiety within the user (anxious self), and (b) interacting with such “anxiety avatars”. Results from three experiments consistently show that creating, and subsequently destroying, an anxiety avatar in various contexts reduces anxiety, effects explained by self-discrepancy theory's self-regulatory mechanisms. Follow-up studies examine interactions with anxiety avatars in custom-created PC and VR games to identify other potential mechanisms, unique to immersive VEs, contributing to these outcomes. Preliminary results support conclusions from the initial studies. Herein, I review pertinent literature, discuss findings, and outline ongoing experiments.},
  keywords={Avatars;Human computer interaction;Games;Scholarships;Virtual environments;virtual reality;avatars;identity;anxiety;mental health;Human-centered computing;Human-computer interaction},
  doi={10.1109/VR.2019.8797907},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798339,
  author={Chiou, Yan-Ming and Barmaki, Roghayeh},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Learning Tornado Formation via Collaborative Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1369-1370},
  abstract={With the rise of attention to global warming which brings in more extreme weather and climate conditions, the earth science education would be one of the crucial topics for the next generation. Mixed-Reality has been shown to offer more engaging and effective learning solutions on essential science topics, such as math, physics, and chemistry. However, there are few augmented reality and mixed reality applications on earth science subject. Also, collaborative learning has been shown to be beneficial for student learning by aspiring student curiosity, and the ability of cooperation. In this paper, we propose a Mixed Reality Tornado Simulator which offers an earth science education intervention in a collaborative mixed reality setting. Students and their instructor can wear see-through head-mounted displays to cooperate on learning the knowledge of the formation and its damage cause on human-built structures, farming, and vegetation by using our proposed mixed reality application. Also, for evaluating the learning performance in this mixed reality setting, we will study the students cognitive load using standard survey instruments. We will conduct a controlled study with two conditions to compare the proposed intervention in the head-mounted-display setting, versus a desktop setting to test usability and knowledge gain of the students in those settings.},
  keywords={Surveys;Three-dimensional displays;Education;Mixed reality;Collaboration;Vegetation;User interfaces;Tornadoes;Usability;Meteorology;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Applied computing-Education-Collaborative learning},
  doi={10.1109/VR.2019.8798339},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798086,
  author={Hegazy, Muhammad and Yasufuku, Kensuke and Abe, Hirokazu},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Immersive Gamified Environments (IGE) as an Approach to Assess Subjective Qualities of Daylighting in Architectural Spaces}, 
  year={2019},
  volume={},
  number={},
  pages={1371-1372},
  abstract={Daylighting performance is a major factor in shaping the quality of any architectural space. Therefore, visualization and assessment of daylighting are some of the important applications of Virtual Reality (VR) in architecture. However, many of the related current methodologies show limitations in terms of realism, user-engagement, and locomotion. This research introduces and validates the term “Immersive Gamified Environment” (IGE) as a human-oriented, more engaging approach to evaluate and optimize the subjective qualities of daylighting in early stages of the design process. The initial project of this research creates a user-oriented VR game for the assessment of daylighting performance in “Kimbell Art Museum” by Louis Kahn. The methodology is based on fulfilling in-game objectives and a follow-up questionnaire. The outcomes of this study show potentials of the proposed system in terms of realism, light perception, and interaction, despite some limitations regarding scale perception and motion sickness.},
  keywords={Daylighting;Tools;Buildings;Virtual reality;Solid modeling;Engines;Visualization;Daylighting;visual perception;immersion},
  doi={10.1109/VR.2019.8798086},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798008,
  author={Maloney, Divine},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Embodied Virtual Avatars and Potential Negative Effects on Implicit Racial Bias}, 
  year={2019},
  volume={},
  number={},
  pages={1373-1374},
  abstract={Embodied virtual avatars can powerfully affect a user's behavior. Some changes in behavior can be positive, although some can be negative and unknown to the user. If the presence of stereotypical triggers lead to an increase in implicit racial bias a hypothesis could be made that embodying an immersive virtual avatar could negatively effect a user's implicit bias, this would be a serious cause for concern with regard to the recent emergence of consumer virtual reality. Here I explain a pilot study and potential next steps for my research.},
  keywords={Avatars;Games;Solid modeling;Load modeling;Weapons;Skin;embodied virtual avatars;implicit racial bias;social good},
  doi={10.1109/VR.2019.8798008},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798256,
  author={Bönsch, Andrea},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Joint Locomotion with Virtual Agents in Immersive Environments}, 
  year={2019},
  volume={},
  number={},
  pages={1375-1376},
  abstract={Many applications in the realm of social virtual reality require reasonable locomotion patterns for their embedded, intelligent virtual agents (VAs). The two main research areas covered in the literature are pure inter-agent-dynamics for crowd simulations and user-agent-dynamics in, e.g., pedestrian scenarios. However, social locomotion, defined as a joint locomotion of a social group consisting of a human user and one to several VAs in the role of accompanying interaction partners, has not been carefully investigated yet. I intend to close this gap by contributing locomotion models for the social group's VAs. Thereby, I plan to evaluate the effects of the VAs' locomotion patterns on a user's perceived degree of immersion, comfort, and social presence.},
  keywords={Solid modeling;Social groups;Legged locomotion;Virtual reality;Adaptation models;Three-dimensional displays;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality},
  doi={10.1109/VR.2019.8798256},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798171,
  author={Vovk, Alla},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Dimensionality of Augmented Reality Spatial Interfaces}, 
  year={2019},
  volume={},
  number={},
  pages={1377-1378},
  abstract={The next wave of computing is visible on the horizon, heralding the widespread adoption of augmented reality spatial information manipulation on wearable and environment integrated form factors such as head-worn displays. This application for the doctoral consortium at IEEE VR 2019 describes the research conducted and planned in the context of a PhD thesis, which aims at determining a multidimensional model of user performance in spatial user interaction in augmented reality for training.},
  keywords={Training;Augmented reality;Task analysis;Usability;Ergonomics;User interfaces;Augmented reality;spatial user interface;trainings;[Human-centered computing]: Human computer interaction;Interaction paradigms;Mixed/Augmented reality},
  doi={10.1109/VR.2019.8798171},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797780,
  author={Chakravarthula, Praneeth},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Auto-focus Augmented Reality Eyeglasses for both Real World and Virtual Imagery}, 
  year={2019},
  volume={},
  number={},
  pages={1379-1380},
  abstract={Near-eye displays presenting accommodation cues, thereby mitigating the vergence-accommodation conflict, have garnered interest in the recent past. However, considering that at least 40% of US population is presbyopic and similarly a sizable world population suffering other refractive errors in eye, it requires that the users wear their prescription glasses along with the AR goggles, despite focus support for virtual imagery, making the overall experience uncomfortable. In the recent work published at ISMAR-TVCG 2018, which won a Best Paper Award, my collaborators and I presented an AR display which can automatically adjust for focus for both real and virtual imagery, avoiding an extra pair of prescription correcting glasses along with AR glasses. My recent work has been on a near-eye display design which integrates with the bifocals of a presbyopic user, thereby providing depth dependent stimuli to the user who is already well adapted to bifocal lenses. A variant of these ideas are going to be presented at IEEE VR 2018 through our accepted TVCG paper. I propose that the above mentioned works combined with my future work on integrating eye trackers and depth sensors to make the display glasses more robust and completely automatic, followed by evaluating the perceptual qualities of the display are the topics of my dissertation.},
  keywords={Lenses;Glass;Liquids;Augmented reality;Sensors;Liquid crystal displays},
  doi={10.1109/VR.2019.8797780},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798207,
  author={Heyse, Joris},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] Self-Adaptive Technologies for Immersive Trainings}, 
  year={2019},
  volume={},
  number={},
  pages={1381-1382},
  abstract={Online learning is the preferred option for professional training, e.g. Industry 4.0 or e-health, because it is more cost efficient than on-site organisation of realistic training sessions. However, current online learning technologies are limited in terms of personalisation, interactivity and immersiveness that are required by applications such as surgery and pilot training. Virtual Reality (VR) technologies have the potential to overcome these limitations. However, due to its early stage of research, VR requires significant improvements to fully unlock its potential. The focus of this PhD is to tackle research challenges to enable VR for online training in three dimensions: (1) dynamic adaptation of the training content for personalised trainings, by incorporating prior knowledge and context data into self-learning algorithms; (2) mapping of sensor data onto what happens in the VR environment, by focusing on motion prediction techniques that use past movements of the users, and (3) investigating immersive environments with intuitive interactions, by gaining a better understanding of human motion in order to improve interaction. The designed improvements will be characterised though a prototype VR training platform for multiple use cases. This work will not only advance the state of the art on VR training, but also on online e-learning applications in general.},
  keywords={Training;Virtual reality;Tracking;Resists;Haptic interfaces;Surgery;Prototypes;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Applied computing;Life and medical sciences;Health informatics},
  doi={10.1109/VR.2019.8798207},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797872,
  author={Hamzeheinejad, Negin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] VR Simulation as a Motivator in Gait Rehabilitation}, 
  year={2019},
  volume={},
  number={},
  pages={1383-1384},
  abstract={Gait rehabilitation is a necessary process for patients suffering from post-stroke motor impairments. The patients are required to perform repetitive practices using a robot-assisted gait device. Repeated exercises can become extremely frustrating and the patients lose their motivation over time. In my PhD research, I focus on Virtual Reality (VR) as a medium to improve gait rehabilitation in terms of enjoyment, motivation, efficiency, and effectiveness. The objective is to systematically investigate different factors, such as the presence of a trainer, interactivity, gamification, and storytelling in a two-step process. First, by evaluating the applicability of different factors for the clinical use with healthy subjects. Second, by evaluating the effectiveness of the VR simulation with patients with gait deficits, especially stroke patients in collaboration with a [country] clinic.},
  keywords={Medical treatment;Virtual reality;Solid modeling;Robots;Mirrors;Neurons;Stroke (medical condition);Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797872},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798138,
  author={Schroeder, Franziska},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={EMBRACE - a VR piece about disability and inclusion (2018)}, 
  year={2019},
  volume={},
  number={},
  pages={1386-1386},
  abstract={“Embrace” is a work created as part of a UK AHRC (Arts Humanities Research Council) funded project on Immersive and Inclusive Music Technologies. The piece is for VR headset and was developed for one of the grant's proposed outputs. The research conducted investigated how emerging technologies (such as VR) can best be adopted to suit people with different abilities (movement impaired people for example). “Embrace” allows the viewer to experience issues around disability. It tells the story about two disabled musicians (one visually impaired and one wheelchair bound) and how both experience exclusion before a concert situation. We also find out some background with regards to the nature of their disability. The work wants to stimulate the viewer to embrace difference; hence the title “Embrace”. “Embrace” is a short immersive experience about inclusion and embracing difference. It was produced at the Sonic Arts Research Centre, Queen's University Belfast as part of the AHRC/EPSRC Next Generation of Immersive Experiences Programme 2018.},
  keywords={Art;Headphones;Wheelchairs;Next generation networking;Cameras;Conferences;inclusion;disability;VR;360 video/audio},
  doi={10.1109/VR.2019.8798138},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798028,
  author={Idaewor, Hope},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Culturally Responsive Narratives in Virtual Reality To Influence Cognition and Self Efficacy}, 
  year={2019},
  volume={},
  number={},
  pages={1387-1388},
  abstract={NeuroSpeculative AfroFeminism (NSAF) is a transformative Virtual Reality (VR) experience that gives women of color an entry point into the tech narrative at the intersection of neuroscience and speculative design [1]. As the experience begins, the user arrives in a speculative world where they explore a futuristic hair salon. The salon is a Neuro Cosmetology lab owned and occupied by women of color who are the lead scientists in the space. NSAF was created in response to the lack of narratives that include women of color at the center of New Media such as VR. This project is the focus of ongoing research to inform the design of culturally responsive VR experiences (tailored to minorities in Computing) in hopes of influencing self-efficacy within these groups. Research has shown that VR can be used to increase functional activity and influence brain reorganization [2]. However, there is little research that explores how VR could be used to influence self-efficacy through its affordance of embodied cognition [3]. The goal of this study is to show how VR-compared to traditional storytelling methods-could be used to tailor learning experiences. This video briefly describes the experience and introduces the project methodology, goals, and expected outcomes.},
  keywords={Cognition;Lead;History;Medical treatment;Conferences;Virtual reality;Three-dimensional displays;Cognition;Embodied Cognition;Virtual Reality;Self Efficacy;Learning;Computer Science Education;AfroFuturism;Storytelling},
  doi={10.1109/VR.2019.8798028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798365,
  author={Schild, Jonas and Flock, Leonard and Martens, Patrick and Roth, Benjamin and Schünemann, Niklas and Heller, Eduard and Misztal, Sebastian},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={EPICSAVE Lifesaving Decisions – a Collaborative VR Training Game Sketch for Paramedics}, 
  year={2019},
  volume={},
  number={},
  pages={1389-1389},
  abstract={Practical, collaborative training of severe emergencies that occur too rarely within regular curricular training programs (e.g., anaphylactic shock in children patients) is difficult to realize. Multi-user virtual reality and serious game technologies can be used to provide collaborative training in dynamic settings [1], [2]. However, actual training effects seem to depend on a high presence and supportive usability [2]. EPICSAVE Lifesaving Decisions shows a novel approach that aims at further improving on these factors using an emotional scenario and collaborative game mechanics. We present a trailer video of a game sketch which creatively explores serious game design for collaborative virtual reality training systems. The game invites two paramedic trainees and one paramedic trainer into a dramatic scenario at a family theme park: A 5-year old child shows symptoms of anaphylactic shock. While the trainees begin their diagnostics procedures, a bystander, the girl's grandfather, intervenes and challenges the players' authority. Our research explores how VR game mechanics, i.e., optional narrative, authority skills and rewards, mini games, and interactive virtual characters may extend training quality and user experience over pure VR training simulations. The video exemplifies a concept that extends prior developments of a multi-user VR training simulation setup presented in [2], [3].},
  keywords={Games;Training;Virtual reality;Collaboration;Computational modeling;Solid modeling;Software;Serious game;multi-user virtual reality;collaboration;game mechanics;training simulation;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Contextual software domains;Virtual worlds software;Interactive games;Contextual software domains;Virtual worlds software;Virtual worlds training simulations},
  doi={10.1109/VR.2019.8798365},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797795,
  author={Velho, Luiz},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Kino+Theater}, 
  year={2019},
  volume={},
  number={},
  pages={1392-1392},
  abstract={VR Kino+Theatre is a media platform that combines theatrical performance with live cinema using virtual reality technology. The platform integrates traditional forms of entertainment (theater and cinema), with advanced interactive media, (virtual reality and gaming). In this way, it solves scalability of audience and presentation familiarity, while providing greater flexibility for innovative formats. The foundations of our solution lies onto three pillars: On the technology front: i) 3D content captured from real data with the help of advanced sensors and machine learning; ii) procedural and real-time physical simulations powered by high-end graphics hardware; iii) distributed systems interconnected by low-latency wireless networks. On the production side: i) unified process, in terms of ubiquitous data access and augmented content generation; ii) collaborative real-time integrated authoring shared by all members of creative teams. On the delivery scenarium: i) diversified media and application options; ii) multiplicity of presentation formats; iii) stratified and complementary fruition allowing to fully explore the content in many forms. The operation of an ecosystem based on these principles entails new roles for producers, performers and participants. As a demonstration of the platform we produced a play “The Tempest”, by William Shakespeare. The project was developed by a multidisciplinary group at IMPA [1].},
  keywords={Media;Virtual reality;Motion pictures;Three-dimensional displays;Real-time systems;Entertainment industry;Scalability},
  doi={10.1109/VR.2019.8797795},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797769,
  author={McKnight, Michael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Echoes of Murlough}, 
  year={2019},
  volume={},
  number={},
  pages={1393-1393},
  abstract={Echoes of Murlough is an electroacoustic composition presented in VR. The listener is enveloped in a virtual space that explores the intersection of musical and environmental sonic materials gathered from Murlough Beach, Co. Down in Northern Ireland. The user is a passive observer in the piece which unfolds around them in a spatially head tracked experience. The auditory virtual environment (AVE) has an authentic approach that moves into the creational, as described by Novo [1]. Taking the listener on a journey from the real to the abstract but where the sounds retain a connection to place. All sounds where gathered at the beach using a combination ambisonic, MS stereo, mono and contact microphones including the electric guitar except for two parts recorded in the studio. The music was composed of improvised guitar parts using an “Ebow” and recorded on the beach using an ambisonic microphone in conjunction with a lavalier wireless system to allow freedom of movement that would be captured and become integral to the piece. The contrast and interplay between environmental and instrumental sources are explored in relation to space. The intention is that the listener and composition itself will be rooted in a sense of place that will provide a foundation for immersion.},
  keywords={Space exploration;Virtual environments;Microphones;Art;Music;Observers;Virtual Reality;360 Video;Immersive;Electroacoustic;Composition;Ambisonic},
  doi={10.1109/VR.2019.8797769},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797963,
  author={Vallance, Michael and Kurashige, Yuto and Magaki, Takurou},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Creative learning in VR: An antidisciplinary approach}, 
  year={2019},
  volume={},
  number={},
  pages={1394-1394},
  abstract={Joichi Ito, MIT Media Lab Director, suggests that the way ahead in education is to support endeavors where learning processes and peer collaborations are valued above end products such as exam scores. He terms this antidisciplinary [1]. To engage students in an antidisciplinary construction of their learning environments, a 3D virtual Fukushima Dai-ichi nuclear power plant scenario is designed for familiarity with operator challenges, nuclear plant risks, and basic nuclear power content [2]. Donning the Oculus Rift HMD, students are immersed in the Fukushima nuclear power plant and tasked with retrieving 5 radioactive bins randomly positioned throughout the plant. Due to the radiation levels, students must maneuver a robot throughout the plant. To locate the bins, the students can maneuver a drone over the plant. While retrieving the bins the student also collects virtual `Information cards' for later questioning. In addition, the student must locate the entrance (a teleport) to Reactor 2 and turn on the cooling water pump. The activity is timed so that on 8 minutes a tsumani alarm is sounded and water rises after 10 minutes. To learn about the accident, the student enters a Control Room and undertakes a `cause-and-effect' quiz utilizing the collected Information cards.},
  keywords={Media;Power generation;Indium tin oxide;Education;Three-dimensional displays;Virtual reality;Information science;education;nuclear;virtual reality},
  doi={10.1109/VR.2019.8797963},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797680,
  author={Hamilton, Rob},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Coretet: A Dynamic Virtual Musical Instrument for the Twenty-First Century}, 
  year={2019},
  volume={},
  number={},
  pages={1395-1395},
  abstract={Coretet is a virtual reality musical instrument that explores the translation of performance gesture and mechanic from traditional bowed string instruments into an inherently non-physical implementation. Built using the Unreal Engine 4 and Pure Data, Coretet offers musicians both a flexible and articulate musical instrument to playas well as a networked performance environment capable of supporting and presenting a traditional four-member string quartet. Building on traditional stringed instrument performance practices, Coretet was designed as a futuristic `21st Century' implementation of the core gestural and interaction modalities that generate musical sound in the violin, viola and cello. Coretet exists as a client-server software system designed to be controlled using an Oculus Rift head-mounted display (HMD) and the Oculus Touch hand-tracking controllers. The instrument and performance environment are built using the Unreal Engine 4. Gesture and audio output is generated using interaction data from the engine streamed to a Pure Data (PD) [2] server via Open Sound Control (OSC) [3]. Within PD, gestural control data from Coretet is processed and used to control a variety of audio generation and manipulation processes including the [bowed~] string physical model from the Synthesis Toolkit (STK) [1].},
  keywords={Music;Instruments;Engines;Virtual reality;Conferences;Manipulator dynamics;Solid modeling;Virtual Instruments for Music Expression (VIME)},
  doi={10.1109/VR.2019.8797680},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798022,
  author={Park, Seonock and Kim, Jusub},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Color Space: 360 VR Hanbok Art Performance}, 
  year={2019},
  volume={},
  number={},
  pages={1396-1396},
  abstract={This project explores the possibility of VR as an alternative theatre form for performing arts. For the past hundreds of years, the proscenium stage has been the most used form of stage in the performing arts. At the proscenium stage, the audience sees the dramatic facts through the frame, so it has the advantage of preventing the attention of the audience from being dispersed. However, it has the disadvantage of making the audience have a sense of distance from the stage since the world in the stage is completely separated from the world of the audience. In this 360 VR performance work, we remove the barrier between the audience and the stage allowing the audience to immerse themselves more in the performance, and experiment a new performance type where the performing is done around the audience rather than the audience surrounding performers. For this work, we used a 360 video camera (a rig of 6 GoPro cameras) to capture the stage, where a group of dancers wearing the Hanbok - the Korea traditional costume - performed the traditional dance specially choreographed for this show. This video was created to promote the beauty of the Hanbok as a more immersive approach.},
  keywords={Art;Cameras;Color;Conferences;Virtual reality;Three-dimensional displays;User interfaces;VR Performance;360 VR;Hanbok;Computing methodologies—Image and video acquisition;Computing methodologies—Virtual reality},
  doi={10.1109/VR.2019.8798022},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798269,
  author={Itamiya, Tomoki and Tohara, Hideaki and Nasuda, Yohei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality Floods and Smoke Smartphone App Disaster Scope utilizing Real-time Occlusion}, 
  year={2019},
  volume={},
  number={},
  pages={1397-1397},
  abstract={Natural disasters occur frequently in Japan. In the Great East Japan Earthquake in 2011 and the heavy rain disaster at western Japan in 2018, many people didn't have the crisis consciousness enough to evacuate safely. We developed the augmented reality smartphone-application Disaster Scope that enables immersive experience in order to improve the crisis awareness of disasters in peacetime. The application can superimpose the occurrence situation of disasters such as CG floods and debris and fire smoke in the actual scenery, using only a smartphone and paper headset. By using a smartphone equipped with a 3D depth sensor, it is possible to sense the height from the ground and recognize surrounding objects. The real-time occlusion processing enabled using only by a smartphone. The collision detection of the real world's objects and CG debris is possible. The floods height and flow speed can be changed by each user's setting. As a result, it has become possible to understand more realistically the dangerous of floods and a fire smoke charge. We utilized this system in evacuation drills organized by elementary schools and municipalities. As a result of the survey and verification, it was very useful for improving crisis awareness of students and citizens.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Augmented Reality;Evacuation drill;Smartphone;Occlusion;Training;Floods;Tsunami;Fire;Smoke;[Computational and artificial intelligence]: Computer science - Programming - Augmented reality},
  doi={10.1109/VR.2019.8798269},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798169,
  author={Takala, Tuukka M. and Hsin, Chen Chun and Kawai, Takashi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Stand-alone, Wearable System for Full Body VR Avatars: Towards Physics-based 3D Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={1398-1398},
  abstract={We introduce a stand-alone, wearable system with full body and finger tracking for first-person virtual reality (VR) avatars. The system does not rely on any external trackers or components. It comprises of a head-mounted display, inertial motion capture suit, VR gloves, and VR backpack PC. Making use of the wearable system and RUIS toolkit [1], we present an example implementation of our vision for physics-based full body avatar interaction. This envisioned interaction involves three elements from the reality-based interaction framework of Jacob et al. [2]: naïve physics, body awareness, and environment awareness. These elements lend common sense affordances within the virtual world and allow users to employ their everyday knowledge of the real world. We argue that when it comes to full body avatar interfaces, it is not only users, but also developers who benefit from utilizing physics simulation as the basis upon which different interaction techniques are built on. This physics-based approach provides intuitive manipulation and locomotion interactions without requiring individually crafted scripts. Our example implementation presents several such interactions. Furthermore, the many interaction techniques emerging from physical simulation are congruous with each other, which promotes user interface consistency. We also introduce the idea of using physics components (colliders, joints, materials, etc.) as 3D user interface building blocks, as opposed to scripting or visual programming.},
  keywords={Avatars;Three-dimensional displays;Physics;Wearable computers;Tracking},
  doi={10.1109/VR.2019.8798169},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797995,
  author={Doğan, Yalım and Demirci, Serkan and Güdükbay, Uğur},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmentation of Virtual Agents in Real Crowd Videos}, 
  year={2019},
  volume={},
  number={},
  pages={1399-1399},
  abstract={Augmentation of virtual agents in real crowd videos is an important task for different applications from design simulations of social environments to modeling abnormalities in crowd behavior. We propose a framework for this task, namely for augmenting virtual agents in real crowd videos. Our framework utilizes homography-based video stabilization, Dalal-Triggs detector [1] for pedestrian detection and state-based tracking algorithms to automatically locate the pedestrians in video frames and project them into our 3D simulated environment, where the navigable area of the simulated environment is available as a manually designed and located navigation mesh. We represent the real pedestrians in the video as simple three-dimensional (3D) models in our simulation environment. 3D models representing real, projected agents and the augmented virtual agents are simulated using local path planning coupled with a collision detection and avoidance algorithm, called Reciprocal Velocity Obstacles (RVO) [2]. The virtual agents augmented into the video move plausibly without colliding with static and dynamic obstacles, including other virtual agents and real pedestrians. We provide an extensive graphical user interface for controlling the virtual agents in the scene, including collision avoidance parameters, adjusting the camera in the scene and some standard video player options.},
  keywords={Videos;Solid modeling;Three-dimensional displays;Conferences;Task analysis;Computational modeling;Dogs},
  doi={10.1109/VR.2019.8797995},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797753,
  author={Muntean, Reese and Park, Mei-Ling and Rubleva, Yulia and Hennessy, Kate},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sustainable Production and Consumption in 360}, 
  year={2019},
  volume={},
  number={},
  pages={1400-1400},
  abstract={SCP in 360: Sustainable Production and Consumption in 360 Degrees [1] is a series of six 360° videos aiming to make sustainable production and consumption engaging, memorable, and relatable to a wider audience. Produced by the United Nations Environment Programme and researchers at Simon Fraser University, SCP in 360 takes viewers around the world to see the work of the One Planet Network on the ground, including projects on sustainable building in Nepal, community tourism in South Africa, consumer information in Chile, sustainable and healthy gastronomy in Costa Rica, circular procurement in the Netherlands, and low-carbon sustainable lifestyle initiatives in rural Armenia. For example, viewers visit organic vineyards and mushroom farms in Chile to learn about a project to inform Chilean citizens about the environmental and social impacts of everyday consumer products. Researchers at Simon Fraser University are using these videos to examine the use of new media technology and to investigate if viewers better understand concepts and values around sustainability or if they care more about sustainability after viewing such videos in a 360° environment. This research will explore if and how new immersive visual technologies might better communicate and transmit values and the importance of sustainability efforts.},
  keywords={Government;Production;Videos;Sustainable development;Planets;Virtual reality;Buildings;360° video;virtual reality;values;sustainability;sustainable production and consumption},
  doi={10.1109/VR.2019.8797753},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797865,
  author={Calandra, Davide and Pratticò, F. Gabriele and Cannavò, Alberto and Micelli, Luca and Lamberti, Fabrizio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Building Reconfigurable Passive Haptic Interfaces On Demand Using Off-the-shelf Construction Bricks}, 
  year={2019},
  volume={},
  number={},
  pages={1403-1404},
  abstract={Although passive haptic interfaces have been shown to be capable to enhance the user's sense of presence in Mixed Reality experiences, their use is still constrained by the need to rely on exact replicas of virtual objects or on custom-made devices mimicking the original ones. Unfortunately, the former are not flexible enough in terms of reconfigurability, whereas the latter may be difficult to reproduce. To tackle these issues, this paper explores the possibility to build passive haptic interfaces using off-the-shelf toy construction bricks. Bricks can be assembled to provide the intended feedback in more than one task. Moreover, they may be reassembled in another application to mimic completely new objects and support totally different tasks.},
  keywords={Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Hardware;Communication hardware;interfaces and storage;Tactile and hand-based interfaces;Haptic devices},
  doi={10.1109/VR.2019.8797865},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798051,
  author={Davari, Shakiba and Li, Yuan and Lisle, Lee and Lu, Feiyu and Zhang, Lei and Blustein, Leslie and Feng, Xueting and Gabaldon, Brianna and Kwiatkowski, Marc and Bowman, Doug A.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Save the Space Elevator: An Escape Room Scenario Involving Passive Haptics in Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1405-1406},
  abstract={This paper presents our solution to the 2019 3DUI Contest challenge, which focuses on passive haptics. We aimed to provide a compelling user experience in virtual reality while overcoming the limitations of physical space and current tracking devices. To meet these goals, we designed a time travel scenario that incorporates several novel features. A time machine increases efficiency in the use of physical space. A passive haptic camera prop provides a help system that is integrated into the storyline. Finally, the concept of a “temporal stabilizer” provides a plausible way to reuse a single tracking device to track multiple passive haptic props.},
  keywords={3D interaction;passive haptics;H.5.2 [Information interfaces and presentation]: User Interfaces;Interaction techniques},
  doi={10.1109/VR.2019.8798051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798285,
  author={Figueroa, Pablo and Guo, Rongkai and Takashima, Kazuki and Weyers, Benjamin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Escape Room in Mixed Reality: 10th Annual 3DUI Contest}, 
  year={2019},
  volume={},
  number={},
  pages={1407-1408},
  abstract={The 10th annual IEEE 3DUI Contest focuses on the development of 3D User Interfaces (3DUIs) using passive haptic feedback in Mixed Reality Environments. Material properties of real objects are enhanced with virtual behaviors while used for interaction in a mixed reality scenario. The contest was open to anyone interested in 3DUIs, from researchers to students, enthusiasts, and professionals. The purpose of the contest is to stimulate innovative and creative solutions to challenging 3DUI problems.},
  keywords={Computer Graphics;Interaction Techniques;Methodology and Techniques;Spatial Interaction},
  doi={10.1109/VR.2019.8798285},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797848,
  author={Brument, Hugo and Fribourg, Rebecca and Gallagher, Gerard and Howard, Thomas and Lécuyer, Flavien and Luong, Tiffany and Mercado, Victor and Peillard, Etienne and de Tinguy, Xavier and Marchal, Maud},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Pyramid Escape: Design of Novel Passive Haptics Interactions for an Immersive and Modular Scenario}, 
  year={2019},
  volume={},
  number={},
  pages={1409-1410},
  abstract={In this paper, we present the design of ten different 3D user interactions using passive haptics and embedded in an escape game scenario in which users have to escape from a pyramid in a limited time. Our solution is innovative by its modularity, allowing interactions with virtual objects using tangible props manipulated either directly using the hands and feet or indirectly through a single prop held in the hand, in order to perform several interactions with the virtual environment (VE). We also propose a navigation technique based on the “impossible spaces” design, allowing users to naturally walk through several overlapping rooms of the VE. All together, our different interaction techniques allow the users to solve several enigmas built into a challenging scenario inside a pyramid.},
  keywords={},
  doi={10.1109/VR.2019.8797848},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798370,
  author={Kashiwagi, Toshiro and Sumi, Kaoru and Fels, Sidney and Zhou, Qian and Wu, Fan},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Crystal Palace: Merging Virtual Objects and Physical Hand-held Tools}, 
  year={2019},
  volume={},
  number={},
  pages={1411-1412},
  abstract={We developed a mixed-reality approach, called Crystal Palace, which provides prop-based passive haptic feedback for the 3DUI contest. In this system, we propose an interface of controlling virtual object with physical tools such as scissors, a spray and a screwdriver as game controllers. Our prop-based approach provides natural visual affordance and passive haptics by associating physical tools with various 3D tasks. Using 360 degrees spherical display, players will be able to use common real-world tools in an intuitive way to interact with 3D objects in a virtual escape room.},
  keywords={Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Interaction techniques;Gestural input},
  doi={10.1109/VR.2019.8798370},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798241,
  author={Hanus, Austin and Hoover, Mindy and Lim, Alex and Miller, Jack},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Collaborative Virtual Reality Escape Room with Passive Haptics}, 
  year={2019},
  volume={},
  number={},
  pages={1413-1414},
  abstract={Escape rooms have recently become a popular way to socialize and problem solve in an immersive environment. However, it can be difficult and expensive to create elaborate escape rooms with realistic props. Virtual reality (VR) technology allows developers to create and customize escape rooms more easily. However, to truly make the VR escape room immersive, physical and cooperative interactions are necessary. In this paper, the authors propose and demonstrate a two-player VR escape room developed for the HTC Vive. Physical interactions were made possible by using passive haptics in the form of simple props tracked using HTC Vive trackers and controllers. Additionally, the HTC Vives were networked, and players hands were tracked using the Leap Motion to provide head and hand position cues to teammates.},
  keywords={Virtual reality. Passive haptics. Mixed reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques},
  doi={10.1109/VR.2019.8798241},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798280,
  author={Suhail, Mohamed and Gainer, Scott and Haskins, Jason and Boyd, Blake and Laird, Charles and Huse, Will and Eadara, Suraj and Jerald, Jason},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulating a Futuristic Fire Pump Panel in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1415-1416},
  abstract={Based on input and feedback from firefighters, we built a futuristic pump panel simulator using consumer VR hardware, basic electronics components, and simple physical props. The simulator mixes the physical and the virtual together in order to demonstrate how passive haptics can be used to enhance the user experience. This prototype will be used to explore how VR might be used with future first-responder training.},
  keywords={Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction devices;Haptic devices;Hardware;Emerging technologies;Emerging interfaces},
  doi={10.1109/VR.2019.8798280},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797880,
  author={Rodrigues, André Montes and Nagamura, Mário and Da Costa, Luis Gustavo Freire and Faria, Rodrigo Rodrigues Gesuatto and Ricchetti, Pier Luigi Nakai and Tatsuta, Eric Nozomi and De Deus Lopes, Roseli and Zuffo, Marcelo Knorich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Batmen X The Puzzler - Escaping AR's Drawbacks with Augmented Virtuality and Low Cost Sensors}, 
  year={2019},
  volume={},
  number={},
  pages={1417-1418},
  abstract={We present an augmented virtuality system using low-cost or outdated devices as sensors, attached to tangible objects. Five interactions were implemented, featuring rotation, pressing, pulling, pushing and insertion movements, including passive mechanical resistance. Interactions were tied together by a puzzling narrative. Concerning storytelling, in contrast to AR, pure virtual reality offers unlimited visual possibilities and higher flexibility. Augmenting VR with physical objects allows ignoring most details of the physical environment and exploring ‘magical’ interactions. Precise registration was accomplished using wireless tracking system. The main challenge was the absent physical counterparts of the scenario, which was tackled by limiting user's reach to available interactions. Despite Leap Motion's erratic hand tracking, mouse and tablet sensors were precise enough and the augmented environment allowed a high sense of presence.},
  keywords={I.3.6 [Computer Graphics]: Methodology and Techniques;Interaction Techniques},
  doi={10.1109/VR.2019.8797880},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798133,
  author={Bitzas, Dimitrios and Zouras, Sokratis and Chrysanthakopoulou, Agapi and Laskos, Dimitrios and Kalatzis, Konstantinos and Pavlou, Michail and Balasi, Ioanna and Moustakas, Konstantinos},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VitaZ: Gamified Mixed Reality Multisensorial lnteractions}, 
  year={2019},
  volume={},
  number={},
  pages={1419-1420},
  abstract={This paper presents multiple Mixed Reality 3D interaction, manipulation and simulation techniques in the context of the 2019 3DUI contest of the IEEE VR conference. The proposed schemes provide smart, seamless transition from the real to the virtual world and demonstrate passive haptics, mid-air haptics, object manipulation and abstract entities (time) manipulation. All techniques are integrated in the context of a mixed reality escape-room or treasure-hunt game, where information from both the real and the virtual world is necessary to solve the puzzle. The paper concludes with a discussion on the extensibility and translational application of the approaches in practical problem solving.},
  keywords={Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction devices;Haptic devices},
  doi={10.1109/VR.2019.8798133},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797763,
  author={Hamzeheinejad, Negin and Roth, Daniel and Götz, Daniel and Weilbach, Franz and Latoschik, Marc Erich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Physiological Effectivity and User Experience of Immersive Gait Rehabilitation}, 
  year={2019},
  volume={},
  number={},
  pages={1421-1429},
  abstract={Gait impairments from neurological injuries require repeated and exhaustive physical exercises for rehabilitation. Prolonged physical training in clinical environments can easily become frustrating and de-motivating for various reasons which in turn risks to decrease efficiency during the healing process. This paper introduces an immersive VR system for gait rehabilitation which targets user experience and increase of motivation while evoking comparable physiological responses needed for successful training effects. The system provides a virtual environment consisting of open fields, forest, mountains, waterfalls, animals, and a beach for inspiring strolls and is able to include a virtual trainer as a companion during the walks. We evaluated the ecological validity of the system with healthy subjects before performing the clinical trial. We assessed the system's target qualities with a longitudinal study with 45 healthy participants in three consecutive days in comparison to a baseline non-VR condition. The system was able to evoke similar physiological responses. The workload was increased for the VR condition but the system also elicited a higher enjoyment and motivation which was the main goal. The latter benefits slightly decreased over time (as did workload) while they were still higher than in the non-VR condition. The virtual trainer did not show to be beneficial, the corresponding implications are discussed. Overall, the approach shows promising results which renders the system a viable alternative for the given use case while it motivates interesting direction for future work.},
  keywords={Training;Legged locomotion;Virtual environments;Stroke (medical condition);Task analysis;Forestry;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797763},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797729,
  author={Cidota, Marina A. and Bank, Paulina J.M. and Lukosch, Stephan G.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design Recommendations for Augmented Reality Games for Objective Assessment of Upper Extremity Motor Dysfunction}, 
  year={2019},
  volume={},
  number={},
  pages={1430-1438},
  abstract={In clinical practice, objective and quantitative assessment of motor dysfunction is required for monitoring disease progression over time and evaluating response to therapeutic interventions. Thereby, clinicians typically want their patients to make movements to their full physical potential. Augmented reality (AR) games using 3D hand and body tracking that engage patients, could motivate them to perform repetitive tasks to the limit of their physical capabilities in a safe environment. This paper reports on different AR games developed for objective upper extremity motor dysfunction assessment of Parkinson's Disease (PD) patients and stroke patients. Quantitative and qualitative evaluations of various user studies involving 23 PD patients, 22 stroke patients and 39 healthy persons are discussed to make design recommendations for designing engaging AR games for objective assessment of upper extremity motor dysfunction.},
  keywords={Games;Task analysis;Extremities;Visualization;Three-dimensional displays;Usability;Augmented reality;Augmented reality;engagement;games design;motor function assessment;PD;stroke;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented and virtual realities;K.8.0 [Personal Computing]: General—Games;J.3 [Life and Medical Sciences]: Health},
  doi={10.1109/VR.2019.8797729},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798032,
  author={Adjorlu, Ali and Serafin, Stefania},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Teachers' Views on how to use Virtual Reality to Instruct Children and Adolescents Diagnosed with Autism Spectrum Disorder}, 
  year={2019},
  volume={},
  number={},
  pages={1439-1442},
  abstract={Virtual Reality (VR) technologies are becoming more and more adopted for treatment of mental health, especially due to the availability of low cost and usable hardware and software solutions. In this paper we present the views of three teachers on how to use VR to instruct children and adolescents diagnosed with Autism Spectrum Disorder (ASD). All three teachers work with students diagnosed with ASD on a daily basis. Three categories of skills that would benefit from being taught using VR emerged from the discussion with the teachers: everyday living skills, social skills, and academic skills. Skills within these categories as well as some of the main advantages of VR according to the teachers are discussed in this paper.},
  keywords={Resists;Variable speed drives;Education;Avatars;Virtual environments;Autism;K.3.2 [Learning]: Knowledge acquisition;[k.3.1]: Computer Uses in Education;Computer-assisted instruction(CAI) H.5.2 [User-centered design]: [: I-.3.7-I.3.7Three-Dimensional Graphic and RealismVirtual Reality},
  doi={10.1109/VR.2019.8798032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797805,
  author={Bialkova, Svetlana and Dickhoff, Bob},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Encouraging Rehabilitation Trials: The Potential of 360° Immersive Instruction Videos}, 
  year={2019},
  volume={},
  number={},
  pages={1443-1447},
  abstract={Despite the rapid growth of the VR/AR/ XR applications in the health-care sector, enhancing health and well-being with innovative technologies often is a challenge. Part of the challenge is the limited knowledge transfer between the healthcare, technology, the patients demands, and how these demands could be appropriately met. The current study addressed this challenge when exploring the potential of 360° immersive instruction videos in encouraging rehabilitation trials. A professional VR/ video maker studio created the video for the purpose of the current research. A rehabilitation therapist was recorded while performing rehabilitation exercise as it is done in the real life practice. Patients currently in various rehabilitation trials (motor vs. cardiovascular) were exposed to the 360° immersive instruction video. Their experience was compared with the control group, i.e. healthy people. The VR experience and the rehabilitation exercise experience were evaluated as measures of the Virtual Reality Rehabilitation (VRR) impact. Results showed that 360° immersive videos engaged patients well, irrespective of the rehabilitation trial they are currently in. Regression modelling further demonstrated that the more people liked the VR experience, the more they enjoyed the rehabilitation activity. The more the rehabilitation activity was enjoyed, the more people were satisfied with the VRR. Current outcomes are discussed in the framework of a model of VRR impact, which is a solid base for long-term exercise adherence. The model could be implemented to successfully develop immersive instructional videos as efficient tools in the course of physical rehabilitation trials, and thus, to enhance health and well-being.},
  keywords={Videos;Virtual reality;Solid modeling;Injuries;Atmospheric measurements;Particle measurements;Virtual reality rehabilitation;360° videos;patients engagement;Augmented and virtual realities;information interfaces and presentation;User/Machine Systems-Human Factors},
  doi={10.1109/VR.2019.8797805},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798014,
  author={Elor, Aviv and Lessard, Steven and Teodorescu, Mircea and Kurniawan, Sri},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Project Butterfly: Synergizing Immersive Virtual Reality with Actuated Soft Exosuit for Upper-Extremity Rehabilitation}, 
  year={2019},
  volume={},
  number={},
  pages={1448-1456},
  abstract={Immersive Virtual Reality paired with soft robotics may be synergized to create personalized assistive therapy experiences. Virtual worlds hold power to stimulate the user with newly instigated low-cost, high-performance commercial Virtual Reality (VR) devices to enable engaging and accurate physical therapy. Soft robotic wearables are a versatile tool in such stimulation. This preliminary study investigates a novel rehabilitative VR experience, Project Butterfly (PBF), that synergizes VR Mirror Visual Feedback Therapy with soft robotic exoskeletal support. Nine users of ranging ability explore an immersive gamified physio-therapy experience by following and protecting a virtual butterfly, completed with an actuated robotic wearable that motivates and assists the user to perform rehabilitative physical movement. Specifically, the goals of this study are to evaluate the feasibility, ease-of-use, and comfort of the proposed system. The study concludes with a set of design considerations for future immersive physio-rehab robotic-assisted games.},
  keywords={Medical treatment;Virtual reality;Soft robotics;Tools;Visualization;Aging;Virtual Reality;Soft Robotics;Wearable Robotics;Exosuit;Physio-Immersive Rehabilitation;Physical Therapy;Immersive Experiences;Serious Games;Human Computer Interaction},
  doi={10.1109/VR.2019.8798014},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797986,
  author={Kao, Peng-Yuan and Han, Ping-Hsuan and Jan, Yao-Fu and Yang, Zhi-Wei and Li, Chun-Hsien and Hung, Yi-Ping},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={On Learning Weight Distribution of Tai Chi Chuan Using Pressure Sensing Insoles and MR-HMD}, 
  year={2019},
  volume={},
  number={},
  pages={1457-1464},
  abstract={Tai Chi Chuan (TCC) is a famous Chinese martial art. In addition to reading instruction books and watching demonstration videos, the traditional way of learning TCC for most people is to observe and mimic the movements of the coach. However, it is not easy for a novice to determine the weight distribution of the coach, and it is also difficult for a novice to strike a pose with the correct weight distribution. To help people to learn correct weight distribution when practicing TCC, we proposed a TCC augmented reality (AR)/mixed reality (MR) learning system that consists of a mixed reality head-mounted display (MR-HMD) and a pair of pressure sensing insoles. We have designed three kinds of visual hints to help people strike poses with correct weight distributions. Two user studies have shown that our TCC learning system is helpful for people to learn the weight distribution correctly and helpful for people to improve their proprioceptive sensitivity on weight distribution.},
  keywords={Tai Chi Chuan;weight distribution;pressure sensing insoles;mixed reality head-mounted display;Human-centered computing;User studies;Usability testing;Empirical studies in HCI},
  doi={10.1109/VR.2019.8797986},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797813,
  author={Mostajeran, Fariba and Katzakis, Nikolaos and Ariza, Oscar and Freiwald, Jann Philipp and Steinicke, Frank},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Welcoming a Holographic Virtual Coach for Balance Training at Home: Two Focus Groups with Older Adults}, 
  year={2019},
  volume={},
  number={},
  pages={1465-1470},
  abstract={We report on findings from two focus groups for designing an application for balance training at home with an augmented reality virtual coach. Following a User-Centered Design approach, we performed two focus groups with older adults at the early stages of development. Focus group participants were shown a prototype using a Meta 2 head mounted display. Their movements were tracked using a Kinect 2. The virtual coach gave balance training instructions and demonstrated their correct performance. Results suggest that, given the trade-offs of traditional health care, older adults are positive towards using an AR coach for their balance training.},
  keywords={Training;Prototypes;Medical services;Guidelines;Three-dimensional displays;Monitoring;Avatars;Augmented reality;balance disorder;physical therapy;virtual coach;Focus Groups;Human-centered computing;Human computer interaction (HCI);Applied Computing;Life and medical sciences;consumer health},
  doi={10.1109/VR.2019.8797813},
  ISSN={2642-5254},
  month={March},}
@INPROCEEDINGS{8797817,
  author={Mostajeran, Fariba and Kirsten, Aila and Steinicke, Frank and Gallinat, Jürgen and Kühn, Simone},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Gamified Alcohol Use Disorder Therapy in Virtual Reality: A Preliminary Usability Study}, 
  year={2019},
  volume={},
  number={},
  pages={1471-1476},
  abstract={The combination of virtual reality (VR) and gamification opens up new vistas for innovative forms of therapy for alcohol use disorder (AUD) and have enormous potential to improve traditional therapy methods. In this paper, three gamified and one non-gamified AUD therapy applications for VR are introduced and evaluated. The games are based on two behavioral therapy methods, which are Cue Exposure Therapy (CET) and Approach Avoidance Training (AAT). The games are realized in the context of a virtual supermarket, which is considered as a relapse-risky environment. The aim is to help AUD patients practice avoiding alcohol first in a VR-based simulation and later in a real supermarket. In preparation for a long-term clinical study, a usability study was conducted with 13 healthy participants. The results show that the VR game was enjoyed, increased the motivation, and fewer errors were made than in the comparable non-gamified application.},
  keywords={Medical treatment;Games;Alcoholic beverages;Training;Virtual reality;Usability;Containers;Virtual reality therapy;alcohol use disorder;approach avoidance training;cue exposure therapy;Human-centered computing—Interaction paradigms—Virtual Reality;Applied Computing—Life and medical sciences—consumer health},
  doi={10.1109/VR.2019.8797817},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797682,
  author={Wisotzky, Eric L. and Rosenthal, Jean-Claude and Eisert, Peter and Hilsmann, Anna and Schmid, Falko and Bauer, Michael and Schneider, Armin and Uecker, Florian C.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interactive and Multimodal-based Augmented Reality for Remote Assistance using a Digital Surgical Microscope}, 
  year={2019},
  volume={},
  number={},
  pages={1477-1484},
  abstract={We present an interactive and multimodal-based augmented reality system for computer-assisted surgery in the context of ear, nose and throat (ENT) treatment. The proposed processing pipeline uses fully digital stereoscopic imaging devices, which support multispectral and white light imaging to generate high resolution image data, and consists of five modules. Input/output data handling, a hybrid multimodal image analysis and a bi-directional interactive augmented reality (AR) and mixed reality (MR) interface for local and remote surgical assistance are of high relevance for the complete framework. The hybrid multimodal 3D scene analysis module uses different wavelengths to classify tissue structures and combines this spectral data with metric 3D information. Additionally, we propose a zoom-independent intraoperative tool for virtual ossicular prosthesis insertion (e.g. stapedectomy) guaranteeing very high metric accuracy in sub-millimeter range (1/10 mm). A bi-directional interactive AR/MR communication module guarantees low latency, while consisting surgical information and avoiding informational overload. Display agnostic AR/MR visualization can show our analyzed data synchronized inside the digital binocular, the 3D display or any connected head-mounted-display (HMD). In addition, the analyzed data can be enriched with annotations by involving external clinical experts using AR/MR and furthermore an accurate registration of preoperative data. The benefits of such a collaborative surgical system are manifold and will lead to a highly improved patient outcome through an easier tissue classification and reduced surgery risk.},
  keywords={Surgery;Three-dimensional displays;Calibration;Microscopy;Optical imaging;Biomedical optical imaging;Optical microscopy;H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.2.1 [Artificial Intelligence] Applications and Expert Systems—Medicine and science;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—3D/stereo scene analysis;I.4.8 [Image Processing And Computer Vision]: Scene Analysis—Stereo;J.3 [Computer Applications]: Life And Medical Sciences—Medical information systems},
  doi={10.1109/VR.2019.8797682},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797996,
  author={Huang, Jiawei and Lucash, Melissa S. and Simpson, Mark B. and Helgeson, Casey and Klippel, Alexander},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visualizing Natural Environments from Data in Virtual Reality: Combining Realism and Uncertainty}, 
  year={2019},
  volume={},
  number={},
  pages={1485-1488},
  abstract={Understanding complex scientific data visualizations in 2D can be challenging. Virtual Reality (VR) provides an alternative, combining realistic 3D representations with intuitive, natural interactions with data through embodied experiences. However, realistic 3D representations and associated immersive experiences are prone to misrepresentations as they are selectively representative and often leave little room for abstraction. This is particularly challenging for topics such as modeling natural environments where users value realism. We discuss the causes and categories of potential misrepresentations in VR with a particular focus on scientific visualization. We contextualize our discussion by presenting an application prototype that translates ecological model output data into a high-fidelity VR experience that allows users to walk through forests of the future. We also designed and implemented two methods to display uncertainties in high-fidelity VR environments: A multi-scenarios approach to provide users access to alternative scenarios, and a slide-and-show approach to view the environment within the confidence interval.},
  keywords={Data visualization;Uncertainty;Three-dimensional displays;Forestry;Two dimensional displays;Solid modeling;Biological system modeling;Visualization;scientific visualization;virtual reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Human-centered computing;Visualization application domains;Visualization theory, concepts and paradigms;Modeling and simulation;Simulation types and techniques},
  doi={10.1109/VR.2019.8797996},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797926,
  author={Maloney, Divine and Rajasabeson, Sandhya and Moore, Alex and Caldwell, Jacob and Archer, Jacob and Robb, Andrew},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Ethical Concerns of the Use of Virtual Avatars in Consumer Entertainment}, 
  year={2019},
  volume={},
  number={},
  pages={1489-1492},
  abstract={Many questions still remain of the uses both positive and negative of virtual avatars. With virtual avatars primed for consumer entertainment, the effects they can elicit must be further investigated before mainstream adoption. In this paper we present an overview of the potential risks posed by virtual avatars. Followed by a study designed to investigate these risks, and finally ethical research considerations for researchers interested in conducting research with virtual avatars.},
  keywords={Avatars;Atmospheric measurements;Particle measurements;Entertainment industry;Task analysis;Games;Jacobian matrices;virtual avatars;vr ethics;implicit racial bias;social good},
  doi={10.1109/VR.2019.8797926},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798160,
  author={Muntean, Reese and Hennessy, Kate and Denes, Alexandra and Phuttitarn, Linina},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={On Research Ethics and Representation in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1493-1496},
  abstract={This position paper approaches issues in the ethics of virtual reality particularly around notions of art as academic research, collaborative ethnographic media production, consent, and representation. These issues are explored through examples of research with the Yong community in Thailand, investigating the use of 360° video for cultural documentation and preservation. Possible ways of addressing some of the ethical issues in virtual reality are suggested, drawing upon collaborative ethnography, collaborative media production, and Value-Sensitive Design.},
  keywords={Collaboration;Ethics;Media;Production;Cultural differences;Virtual reality;Art;360° video;virtual reality;ethics;ethnography;Value Sensitive Design;Human-centered computing~ Virtual reality;Computing methodologies~ Virtual reality;Social and professional topics~ Codes of ethics},
  doi={10.1109/VR.2019.8798160},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797768,
  author={Buckley, Zach and Carlson, Kristin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards a Framework for Composition Design for Music-Led Virtual Reality Experiences}, 
  year={2019},
  volume={},
  number={},
  pages={1497-1499},
  abstract={Virtual reality is an exciting new medium for artists and content creators, including composers. The ease of content delivery coupled with the capacity for immersive sound diffusion makes VR of particular interest to composers. However, a framework would be useful to support the composition and designing of music-led virtual reality experiences. This paper begins that process by exploring perspectives such as acoustic ecology, the phenomenology of embodiment, affordances and discoverability in design, instrument design, the techno-somatic dimension, and 20th century aleotoric compositional poetics that can help to establish a theoretical framework for composing engaging sonic experiences in virtual environments.},
  keywords={Music;Virtual reality;Instruments;Ecology;Tools;Navigation;Music Composition;Audience Engagement;Virtual Reality},
  doi={10.1109/VR.2019.8797768},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798210,
  author={Çamcı, Anıl},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Some Considerations on Creativity Support for VR Audio}, 
  year={2019},
  volume={},
  number={},
  pages={1500-1502},
  abstract={As the consumer interest in VR grows, the community of content creators working in this domain expands accordingly. Given the intrinsic role of audio in VR experiences, this growth necessitates authoring tools for immersive audio that can cater to a wide range of designers regardless of their expertise in spatial audio. In this article, we discuss some of the modern considerations on designing interactive tools for creativity support in VR audio. We provide examples from existing spatial audio design software, and discuss areas in which new tools can facilitate, for expert and novice users alike, the use of immersive audio in compelling new VR experiences.},
  keywords={Tools;Three-dimensional displays;Creativity;Software;Two dimensional displays;Games;Human-centered computing—Interaction Paradigms—Virtual Reality;Applied computing—Arts and Humanities—Sound and music computing},
  doi={10.1109/VR.2019.8798210},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797683,
  author={Chon, Song Hui and Kim, Sungyoung},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Matter of Attention and Motivation – Understanding Unexpected Results from Auditory Localization Training Using Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1503-1506},
  abstract={We present the results from a seven-week auditory localization training using Microsoft HoloLens. Eight participants were divided into two groups. Both groups showed a generally declining pattern in localization performance over the eight tests, unlike the results from our previous study. The decreasing slope was smaller for the train group than for the control group, which might reflect some mild effect of training. There was a one-time performance improvement after two trainings, which was not observed from subsequent tests. The training program might have been too simple to maintain participants' attention for weeks. Possible extraneous factors such as the academic calendar are discussed that might have had an impact on this decreasing pattern against the hypotheses.},
  keywords={Training;Visualization;Augmented reality;Task analysis;Ear;Atmospheric measurements;Particle measurements;Localization;Training effect;Augmented reality;HRTFs;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Applied Computing;Education;Interactive learning environments},
  doi={10.1109/VR.2019.8797683},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798166,
  author={Hamilton, Rob},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative and Competitive Futures for Virtual Reality Music and Sound}, 
  year={2019},
  volume={},
  number={},
  pages={1510-1512},
  abstract={The histories of virtual reality systems draw heavily from foundational work in telepresence and robotics, cinema and gaming. Across each of these disciplines the roles of “player” and “audience” member vary significantly, as do the rules, affordances and experiential goals put forth by the systems themselves. As such, designers and developers of interactive virtual (here defined as including mixed and augmented) reality systems are faced with a fundamental choice: to create experiences that are inherently collaborative or competitive. While on the surface such a choice might seem a simple articulation of the core design principles for any one given project, the directions in which virtual reality systems are guided and the impacts these choices will have on societal acceptance of VR as a principal component of our technological futures should not be ignored. This paper discusses models of collaboration and competition as put forth within a series of interactive virtual experiences and proposes an ideal future for virtual reality in which interactivity and telepresence leverage collaboration as a core mechanic.},
  keywords={Music;Virtual reality;Games;Collaboration;Solid modeling;Instruments;Art;J.5 [Computer Applications]: Arts and Humanities—fine arts;H.5.5 [Sound and Music Computing];H.5.1 [Information Interfaces and presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798166},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797904,
  author={Reed, Luke and Phelps, Philip},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Audio Reproduction in Virtual Reality Cinemas – Position Paper}, 
  year={2019},
  volume={},
  number={},
  pages={1513-1516},
  abstract={Virtual Reality (VR) and 360 film have caught the attention of audiences and content creators and emerged as a new media, however, the market penetration of VR and head mounted hardware has remained slow despite the availability of more affordable mobile options. This has resulted in some audiences turning to VR cinemas, festivals and out-of-home exhibitions. Creating affordable, scalable VR cinemas presents a number of challenges and many of the decisions taken in both developing and facilitating these curated exhibitions directly impact audience's reception of spatial audio soundtracks. This workshop position paper looks to discuss the potential issues and future solutions in the use of current synchronous exhibition applications, the competing formats, standardised Head Related Transfer Functions, headphone build/colourisation, and the on-boarding process.},
  keywords={Motion pictures;Media;Hardware;Headphones;Software;Virtual reality;Tools;Cinematic VR;360 film;Spatial audio;3D Audio;Binaural;Ambisonics;Object Based Audio;VR cinemas;Exhibition;Playback software;HRTF;Headphones;On-boarding},
  doi={10.1109/VR.2019.8797904},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797894,
  author={Aso, Kohei and Hwang, Dong-Hyun and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Generating Synthetic Humans for Learning 3D Pose Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={1519-1520},
  abstract={We generate synthetic annotated data for learning 3D human pose estimation using an egocentric fisheye camera. Synthetic humans are rendered from a virtual fisheye camera, with a random background, random clothing, random lighting parameters. In addition to RGB images, we generate ground truth of 2D/3D poses and location heat-maps. Capturing huge and various images and labeling manually for learning are not required. This approach will be used for the challenging situation such as capturing training data in sports.},
  keywords={Cameras;Three-dimensional displays;Solid modeling;Lighting;Pose estimation;Clothing;Shape;Computing methodologies;Motion capture;Activity recognition and understanding;Gestural input},
  doi={10.1109/VR.2019.8797894},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797802,
  author={Hamanishi, Natsuki and Rekimoto, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SuppleView: decreasing physically limitations on the movement imitation with viewing motions in the video}, 
  year={2019},
  volume={},
  number={},
  pages={1521-1523},
  abstract={Development of 3D joints inference technology from 2D RGB video enables us to create 3D annotations for each frame of a video including movements. We here proposed SuppleView which is supple about the user's physically posing while viewing a video and enables the coordinate translation free viewing. By inferring the 3D joints of frames in a video, we create the 3D model as an actor of movements which are same with the actor's one in an original 2D video. The system transitions these two actor depends on the physical rotation of the user's head. The angle of view for observing the actor also changes. Hence rendering contents of this viewer can be ease for user to move one's body to be same with a target one. From the results of the prototype system, we considered our proof of concept had worked properly in the actual scene of movement imitation. We assumed that SuppleView has a potential to ease movement imitations by decreasing physically limitations on viewing motion with traditional displays.},
  keywords={Two dimensional displays;Three-dimensional displays;Task analysis;Head;Computational modeling;Prototypes;Biological system modeling;Computing methodologies;Scene understanding;Video inspection;Human-centered computing;Human computer interaction (HCI)},
  doi={10.1109/VR.2019.8797802},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798060,
  author={Hwang, Dong-Hyun and Aso, Kohei and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward human motion capturing with an ultra-wide fisheye camera on the chest}, 
  year={2019},
  volume={},
  number={},
  pages={1524-1526},
  abstract={We are interested in utilizing egocentric view from a wearable camera and are working on MonoEye system, a novel system to estimate the wearer's motion using a chest-mounted camera equipped with an ultra-wide fisheye lens. Because our system has a wide field of view, it provides a balanced capacity of recognizable pose types and broad egocentric view. The prototype deep neural network estimates camera wearer's 3D pose and acquires motion without complex configuration like conventional motion capture systems.},
  keywords={Cameras;Three-dimensional displays;Prototypes;Videos;Lenses;Neural networks;Heating systems;Computing methodologies;Motion capture;Activity recognition and understanding;Gestural input},
  doi={10.1109/VR.2019.8798060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798196,
  author={Ikeda, Atsuki and Hwang, Dong-Hyun and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Real-Time Projection System for Golf Training using Virtual Shadow}, 
  year={2019},
  volume={},
  number={},
  pages={1527-1528},
  abstract={In this work, we propose a real-time golf learning system using visual feedback. The system projects the visual feedback which is imitating shadow on the ground in front of the learner. It makes it possible for the learner to receive the feedback without the posture collapse. The learner can receive the feedback with the fixed face orientation as the actual golf swing, which could not be solved in existing systems projecting feedbacks on the wall. Furthermore, a posture difference is provided by projecting the expert's outline on the learner's virtual shadow. In addition, because shadow was also used in golf practice conventionally, the cost of adaptation to this learning system which uses virtual shadow as visual feedback may be low.},
  keywords={Visualization;Real-time systems;Learning systems;Solid modeling;Virtual reality;Three-dimensional displays;Mirrors;Human-centered computing;Visualization;Visualization application domains;Information visualization;Applied computing;Education;Interactive learning environments},
  doi={10.1109/VR.2019.8798196},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798102,
  author={Kono, Michinari and Rekimoto, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={wavEMS: Improving Signal Variation Freedom of Electrical Muscle Stimulation}, 
  year={2019},
  volume={},
  number={},
  pages={1529-1532},
  abstract={There has been a long history in electrical muscle stimulation (EMS), which has been used for medical and interaction purposes. Human-computer interaction (HCI) researchers are now working on various applications, including virtual reality (VR), notification, and learning. For the electric signals applied to the human body, various types of waveforms have been considered and tested. In typical applications, pulses with short duration are applied, however, many perspectives are required to be considered. In addition to the duration and polarity of the pulse/waves, the wave shapes can also be an essential factor to consider. A problem of conventional EMS toolkits and systems are that they have a limitation to the variety of signals that it can produce. For example, some may be limited to monophonic pulses. Furthermore, they are usually limited to rectangular pulses and a limited range of frequencies, and other waveforms cannot be produced. These kinds of limitations make us challenging to consider variations of EMS signals in HCI research and applications. The purpose of “wavEMS” is to encourage testing of a variety of waveforms for EMS, which can be manipulated through audio output. We believe that this can help improve HCI applications, and to open up new application areas.},
  keywords={Energy management;Human computer interaction;Medical services;Muscles;Safety;History;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;General and reference;Document types;General literature},
  doi={10.1109/VR.2019.8798102},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797767,
  author={Nishida, Jun and Suzuki, Kenji},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HYPERSPECTIVE: Shaping Experiences beyond Perspectives}, 
  year={2019},
  volume={},
  number={},
  pages={1546-1549},
  abstract={Embodied knowledge of one's experiences are broadly categorized as tacit knowledge, that can be acquired through active physical experiences only and not by conversations or visual media. We hypothesize that shaping a person's bodily sensation, such as physical representation or a physiological characteristic, to that of another person, including a child or a patient, while allowing active, embodied, and social interactions with the surrounding objects and people in a real-world environment would provide an authentic as well as empathic knowledge to school teachers, product designers, and medical staff when designing living environments or communicating with them. We propose and demonstrate a new style to experience one's perspectives, called HYPERSPECTIVE (hyper + perspective), that can be achieved with wearable devices, and discuss its challenges, benefits, and feasibility in real scenarios.},
  keywords={Muscles;Visualization;Haptic interfaces;Tools;Pediatrics;Media;Virtual reality;Human-centered computing;Interaction Design},
  doi={10.1109/VR.2019.8797767},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798367,
  author={Nith, Romain and Rekimoto, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Falconer: A Tethered Aerial Companion for Enhancing Personal Space}, 
  year={2019},
  volume={},
  number={},
  pages={1550-1553},
  abstract={With the growing popularity of drones, we start to see more wearable drone concepts. For the user to carry around a drone, it must be lightweight as well as have a small portable form factor. However, these constraints affect the battery capacity and therefore decrease the flight time of the vehicle. With Aerial Tethered Companion, a bigger battery is installed in the user's backpack allowing to extend significantly the flight time. Moreover, without an on-board battery, the quadcopter can carry more payload. Such system can be used in various scenarios for example in sports augmentation where the user would see itself through the drone's camera. Furthermore, Aerial Tethered Companion can be applied in telepresence where an external user would be able to see and navigate around the local user.},
  keywords={Drones;Batteries;Tracking;Cameras;Telepresence;Power cables;Wires;Quadcopter;Tethered Vehicle;Augmented Perception;Augmented Sports;Telepresence;Human-centered computing;HCI theory;Concepts and Models},
  doi={10.1109/VR.2019.8798367},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797765,
  author={Oku, Takanori and Furuya, Shinichi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Novel Vibrotactile Biofeedback Device for Optimizing Neuromuscular Control in Piano Playing}, 
  year={2019},
  volume={},
  number={},
  pages={1554-1555},
  abstract={Along with advances in virtual reality technologies, augmented biofeedback technique has been getting attention as a feasible tool for guiding motor skill acquisition. To supervise the optimal way of muscular activation in skillful piano playing, we developed a vibrotactile biofeedback device for noticing excessive muscular activities to a learner. A pilot experiment indicates that training with the vibrotactile biofeedback device can potentially inhibit involuntarily-exerted excessive muscular activation without deteriorating fine motor control during playing the piano.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Sports;Instruments;Vibrotactile;Biofeedback;Hardware ‐‐‐ Sensors and actuatros;Human centered computing ‐‐‐ Interaction devices ‐‐‐ Haptic device},
  doi={10.1109/VR.2019.8797765},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798012,
  author={Oota, Satoshi and Murai, Akihiko and Mochimaru, Masaaki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Lucid Virtual/Augmented Reality (LVAR) Integrated with an Endoskeletal Robot Suit: StillSuit: A new framework for cognitive and physical interventions to support the ageing society}, 
  year={2019},
  volume={},
  number={},
  pages={1556-1559},
  abstract={Japanese society is ageing ever faster. One of the most critical issues here is the shortage of working population, which is both cause and effect of the `super-ageing' problem. We propose a new framework to `desterilize' and utilize the elderly population as a new social resource. To sustain and hopefully enhance cognitive and physical functions of the elderly, we integrate cognitive and physical interventions by using high-fidelity (Hi-Fi) virtual/augmented reality (Lucid Virtual/Augmented Reality, LVAR) and an endoskeletal robot suit (StillSuit), respectively. LVAR has a physics-fidelity (Phy-Fi) digital-self for each LVAR space, and provides real time dynamic feedbacks to the user through StillSuit. Physical interventions are governed with a biologically relevant musculoskeletal model tailored for each user. To realize social cognition, furthermore, LVAR provides a social networking service with high quality 3D immersive experiences, which is able to be shared by remote users. With the fine-tuned interventions based on biological data of human and non-human animals, we prolong the healthy life expectancy of the elderly population to be the social resource, by which we will overcome the negative spiral of the super-ageing society.},
  keywords={Senior citizens;Statistics;Reactive power;Robots;Real-time systems;Biological system modeling;Virtual/augmented reality;endoskeletal robot suit;dynamic simulation;musculoskeletal model;Cinematographic representation;Human-centered computing~Mixed / augmented reality;Human-centered computing~Virtual reality;Human-centered computing~Collaborative interaction;Computer systems organization~External interfaces for robotics;Computer systems organization~Sensors and actuators;Software and its engineering~Virtual worlds training simulations},
  doi={10.1109/VR.2019.8798012},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798081,
  author={Saito, Kenta and Miyaki, Takashi and Rekimoto, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Method of Reducing Phantom Limb Pain Using Optical See-Through Head Mounted Display}, 
  year={2019},
  volume={},
  number={},
  pages={1560-1562},
  abstract={One of the possible causes of the phantom limb pain (PLP), which have not been clarified medically, is said that a brain is unable to adapt to the loss of limbs. Even though a brain sends a signal to move limbs, the feedback on the signal is not given because the limbs to which a brain sent the signal are no longer in existence. Recently, some kinds of treatment are introduced, which is making patients feel that their phantom limbs are in existence and they are able to move the phantom limbs actually by their own will, but each of them has some difficulty. In this research, we propose the method of PLP treatment using an optical see-through Head Mounted Display (HMD) that can adapt to the telescoping of phantom limbs and make it easy for patients to communicate with people around them.},
  keywords={Phantoms;Pain;Resists;Medical treatment;Optical feedback;Optical imaging;Mirrors;Computing methodologies;Computer graphics;Animation;Motion capture;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality Human-centered computing},
  doi={10.1109/VR.2019.8798081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798366,
  author={Shiro, Keisuke and Egawa, Kazme and Miyaki, Takashi and Rekimoto, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={InterPoser: Visualizing Interpolated Movements for Bouldering Training}, 
  year={2019},
  volume={},
  number={},
  pages={1563-1565},
  abstract={Bouldering is an urban form of rock climbing that requires precise and complex movement. Similarly to other sports, the simplest way to learn bouldering skill is to mimic professional's motion. However, ordinary beginner boulders cannot learn to coaches, so that they learn by themselves or tutorial videos. Even if they managed, bouldering has a communication difficulty between a trainee and a trainer, that is, climbers cannot mimic the trainer's movement in parallel. Accordingly, we considered a video feedback system would be useful for beginners and suggested InterPoser: a novel visualization system for intermediate motion between a beginner climber and a more experienced. InterPoser receives two videos of different subjects climbing the sample problem and generates an intermediate movement. In addition, this motion is transferred into realistic images of the climber. The proposed system is expected to support beginner to acquire more detailed observation and understanding of the motion.},
  keywords={Sports;Training;Visualization;Interpolation;Pose estimation;Human computer interaction;Two dimensional displays;Computing methodologies;Motion Processing;Human-centered computing;Visualization;Visualization techniques;Human computer interaction (HCI)},
  doi={10.1109/VR.2019.8798366},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797919,
  author={Takahashi, Nobuhiro and Takahashi, Hayato and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Novel Soft Exoskeleton Glove for Motor Skill Acquisition Similar to Anatomical Structure of Forearm Muscles}, 
  year={2019},
  volume={},
  number={},
  pages={1568-1569},
  abstract={We introduce a novel soft exoskeleton glove, which is capable of generating human-like finger joint movements with little constraints on volitional motions. Four pneumatic artificial muscles (approx. 2.5 mm in diameter weight and less than 2 g) were attached to each finger, which consists of two antagonistic pairs of the muscles (i.e. flexor and extensor) and thereby enables to control different joints of each finger independently. Implementation of this structure for all five digits resulted in the hand exoskeleton with 20 DOFs for one hand. This architecture was designed similar to the human anatomy of the forearm muscle, which eventually ensured supporting the natural, unconstrained hand motion. Furthermore, as functions of our system, we showed that it is possible to generate a pressing force of approx. 10 N and to manipulate a finger to perform high-speed tapping at approx. 10 Hz. These results suggest that the system can be used not only as a rehabilitation device but also as an instruction device for playing an instrument or sports, which requires dexterous motion control and quick performance.},
  keywords={Muscles;Exoskeletons;Force;Human anatomy;Instruments;Sports;Fingers;Exoskeleton;Robotic Glove;Soft Robotics;Haptics;Force Feedback;Motor Skill Acquisition;Hardware → Haptic devices;Computing methodologies → Bio-inspired approaches},
  doi={10.1109/VR.2019.8797919},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798221,
  author={Toyoda, Kohei and Kono, Michinari and Rekimoto, Jun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Post-Data Augmentation to Improve Deep Pose Estimation of Extreme and Wild Motions}, 
  year={2019},
  volume={},
  number={},
  pages={1570-1574},
  abstract={Contributions of recent deep-neural-network (DNN) based techniques have been playing a significant role in human-computer interaction (HCI) and user interface (UI) domains. One of the commonly used DNNs is human pose estimation. This kind of technique is widely used for motion capturing of humans, and to generate or modify virtual avatars. However, in order to gain accuracy and to use such systems, large and precise datasets are required for the machine learning (ML) procedure. This can be especially difficult for extreme/wild motions such as acrobatic movements or motions in specific sports, which are difficult to estimate in typically provided training models. In addition, training may take a long duration, and will require a high-grade GPU for sufficient speed. To address these issues, we propose a method to improve the pose estimation accuracy for extreme/wild motions by using pre-trained models, i.e., without performing the training procedure by yourselves. We assume our method to encourage usage of these DNN techniques for users in application areas that are out of the ML field, and to help users without high-end computers to apply them for personal and end use cases.},
  keywords={Pose estimation;Training;Sports;Data models;Human computer interaction;Three-dimensional displays;Video sequences;Computing methodologies;Motion capture;Neural networks;Human-centered computing;Human computer interaction (HCI)},
  doi={10.1109/VR.2019.8798221},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798178,
  author={Wu, Erwin and Koike, Hideki},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-time Human Motion Forecasting using a RGB Camera}, 
  year={2019},
  volume={},
  number={},
  pages={1575-1577},
  abstract={This paper propose a real-time human motion forecasting system which visualize the future pose in virtual reality using a RGB camera. Our system consists of three parts: 2D pose estimation from RGB frames using a residual neural network, 2D pose forecasting using a recurrent neural network, and 3D recovery from the predicted 2D pose using a residual linear network. To improve the prediction learning quantity of temporal feature, we propose a special method using lattice optical flow for the joints movement estimation. After fitting the skeleton, a predicted 3d model of target human will be built 0.5s in advance in a 30-fps video.},
  keywords={Three-dimensional displays;Two dimensional displays;Real-time systems;Forecasting;Optical imaging;Pose estimation;Lattices;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Computing methodologies;Artificial intelligence;Computer vision;Computer vision tasks},
  doi={10.1109/VR.2019.8798178},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798054,
  author={Yeo, Hui-Shyong and Koike, Hideki and Quigley, Aaron},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Learning for Sports Using Wearable Head-worn and Wrist-worn Devices}, 
  year={2019},
  volume={},
  number={},
  pages={1578-1580},
  abstract={Novices can learn sports in a variety of ways ranging from guidance from an instructor to watching video tutorials. In each case, subsequent and repeated self-directed practice sessions are an essential step. However, during such self-directed practice, constant guidance and feedback is absent. As a result, the novices do not know if they are making mistake or if there are any areas for improvement. In this position paper, we propose using wearable devices to augment such self-directed practice sessions by providing augmented guidance and feedback. In particular, a head-worn display can provide real-time guidance whilst wrist-worn devices can provide real-time tracking and monitoring of various states. We envision this approach being applied to various sports, and in particular this is suitable for sports that utilize precise hand motion such as snooker, billiards, golf, archery, cricket, tennis and table tennis.},
  keywords={Sports;Sensors;Real-time systems;Tracking;Training;Radar tracking;Trajectory;H.5.2 [User Interfaces]: User Interfaces—Graphical user interfaces (GUI);H.5.m [Information Interfaces and Presentation]: Miscellaneous},
  doi={10.1109/VR.2019.8798054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798296,
  author={Zhang, Bin and Suzuki, Atsufumi and Lim, Hunok},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Development of Sensitive Glove Type Wearable Robot System}, 
  year={2019},
  volume={},
  number={},
  pages={1581-1582},
  abstract={In this paper, a sensitive glove type wearable robot is designed, and its remote-control system is developed for operating a robot hand remotely. By sensing the motions of human fingers through pressure sensors in the glove and sending the information to the robot hand, the robot hand can accurately repeat the motions of users.},
  keywords={Robot sensing systems;Wearable robots;Immune system;Pressure sensors;Monitoring;Wearable sensors;human augmentation;glove type wearable robot;remote-control system},
  doi={10.1109/VR.2019.8798296},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798101,
  author={Bäck, Regina and Plecher, David A. and Wenrich, Rainer and Dorner, Birgit and Klinker, Gudrun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality in Art Education}, 
  year={2019},
  volume={},
  number={},
  pages={1583-1587},
  abstract={In this paper both receptive and productive Augmented Reality (AR)/Virtual Realty (VR)-Concepts to teach art are presented and discussed. Receptive means learning about art as an (immersed) spectator while productive means offering opportunities for own mixed reality artistic creations. The prototypes presented are pioneer research in terms of VR and AR, so called Mixed Reality (MR), informed art education. One of the core aims is to exemplify art educational applications by exploring usage of MR beyond traditional. The three MR based art educational concepts presented here invite users to experience art in heterogenous spatial contexts. MR is used here to highlight semantics of historically and art historically relevant spaces offering immersive as well as creative and participatory ways of learning. Three prototypes of MR informed art education are presented. First, a AR-based art history city guide is described with integration of teacher-based evaluation. Art is taught here in an interdisciplinary approach as situated learning experience visiting Munich's art historical sites and architecture. Teachers' attitudes towards AR/VR-based art history learning were conducted after trial of one of the applications. According to the educator's feedback, motivation, attention but also spatial imagination and perception are estimated to be fostered. Next, an AR-based installation of a missing painting by Franz Marc is presented conveying complex meanings of both architecture in art historical context as well as the art work itself. Finally, an immersive VR-art educational concept integrating two paintings of Caspar David Friedrich is described, offering sequential ways to inform about the painting's iconography. When it comes to implementing MR-concepts in day-to-day practice, media competence as well es technology acceptance or media cultural negotiations are vital aspects beyond the questions of it-technological equipment and infrastructure.},
  keywords={Art;Virtual reality;Education;Painting;Three-dimensional displays;Media;Urban areas;Virtual Reality;Augmented Reality;Mixed Reality;Art Education},
  doi={10.1109/VR.2019.8798101},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798013,
  author={Banic, Amy and Gamboa, Ruben},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Design Problem-based Learning in a Virtual Environment Improves Computational Thinking and Programming Knowledge}, 
  year={2019},
  volume={},
  number={},
  pages={1588-1593},
  abstract={In this paper, we present our design of a high school summer course which uses our Visual Design Problem-based Learning Pedagogy using Virtual Environments as a strategy to teach computer science. Students solved visual design problems by creating 3D sculptures in an online virtual environment. These creations were further explored and refined in immersive display systems fostering embodied learning and remote peer presence and support. To achieve the desired design, students use programming and computing concepts, such as loops, to solve those visual design centered problems, i.e. solving for composition, positive/negative space, balance, as opposed to computational problems first, i.e. create a loop, a fractal, randomized lines, etc. We present results from a study conducted on three high school summer courses. We compared the use of our Visual Design Problem-based teaching strategy (students wrote code to solve challenges based on art and design principles) to a traditional strategy (students wrote code to demonstrate comprehension of computer science concepts). Our results showed that test scores were higher for students in our Visual Design Problem-based courses. This work may have a positive impact on computer science education by increasing engagement, knowledge acquisition, and self-directed learning.},
  keywords={Art;Visualization;Virtual environments;Programming profession;Three-dimensional displays;Visual Design Problem-based Pedagogy;problem-based learning;Online Virtual Environments;Embodied Learning;Virtual Sculpting;Increase Programming Knowledge;Spatial Art;CCS [Human-centered Computing]: Human-computer interaction (HCI);Interaction paradigms-Virtual Reality;[Human-centered Computing]: CCS Human-computer interaction (HCI);Empirical studies in HCI},
  doi={10.1109/VR.2019.8798013},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798058,
  author={Kouzi, Malek El and Mao, Abdihakim and Zambrano, Diego},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Educational Augmented Reality Application for Elementary School Students Focusing on the Human Skeletal System}, 
  year={2019},
  volume={},
  number={},
  pages={1594-1599},
  abstract={Augmented Reality (AR) as a new field regarding Human Computing Interaction (HCI) has been gaining momentum in the last few years. Being able to project interactive graphics into real-life environments can be applied in various fields, research and commercial goals. In the field of education, textbooks are still considered to be the primary tool used by students to learn about new topics. Since AR requires interaction and exploration, it brings a ludic component that is hard to replicate using regular textbooks. The application we developed allows elementary school students to interact with a fully three-dimensional human skeleton model, using specialized virtual buttons. Students can understand this complex structure and learn the names of important bones just by using a tablet, a picture and their hands. Results show that the majority of students consider that our AR application helped them visualize and learn more about the human skeletal system. Additionally, the data we gathered shows that there was a 16% increase in correct responses regarding bone names after using our AR application. Our AR application successfully helped the students learn about the human skeletal system by introducing them to AR technologies.},
  keywords={Three-dimensional displays;Bones;Solid modeling;Games;Augmented reality;Education;Visualization;Augmented Reality;Human Computer Interaction (HCI);Education;Elementary School;Skeleton System;3D object},
  doi={10.1109/VR.2019.8798058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798356,
  author={Fominykh, Mikhail and Prasolova-Førland, Ekaterina},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Job Taste: a Concept of Demonstrating Workplaces with Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1600-1605},
  abstract={This paper presents a new concept of `Immersive Job Taste' - interactive virtual reality demonstration of a workplace that aims to give a feeling of going through an average workday of a professional with elements of basic training. The main target audiences of Job Taste simulations are young job seekers who can be aided in selecting a career path at school or a welfare center, choosing the first or a new occupation, often after a period of being unemployed. The design methodology behind the Immersive Job Taste concept includes presentation of a workplace, typical tasks, feedback on performance, and advice on applying for jobs in the specific industry. We developed several scenarios and applied different virtual and augmented reality concepts to build prototypes for different types of devices. The prototypes were evaluated by several groups of primary users and experts. The results indicate a generally very positive attitude towards the concept. In this paper, we discuss the potential impact of applying the concept and directions for future work.},
  keywords={Task analysis;Employment;Interviews;Fish;Industries;Training;Engineering profession;Virtual Reality;Career guidance;unemployment},
  doi={10.1109/VR.2019.8798356},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798358,
  author={Huynh, Brandon and Orlosky, Jason and Höllerer, Tobias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={In-Situ Labeling for Augmented Reality Language Learning}, 
  year={2019},
  volume={},
  number={},
  pages={1606-1611},
  abstract={Augmented Reality is a promising interaction paradigm for learning applications. It has the potential to improve learning outcomes by merging educational content with spatial cues and semantically relevant objects within a learner's everyday environment. The impact of such an interface could be comparable to the method of loci, a well known memory enhancement technique used by memory champions and polyglots. However, using Augmented Reality in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is a significant challenge, and interaction with arbitrary (unmodeled) physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a framework for in-situ object labeling and selection in Augmented Reality, with a particular focus on language learning applications. Our framework uses a generalized object recognition model to identify objects in the world in real time, integrates eye tracking to facilitate selection and interaction within the interface, and incorporates a personalized learning model that dynamically adapts to student's growth. We show our current progress in the development of this system, including preliminary tests and benchmarks. We explore challenges with using such a system in practice, and discuss our vision for the future of AR language learning applications.},
  keywords={Three-dimensional displays;Real-time systems;Two dimensional displays;Labeling;Augmented reality;Object recognition;Cameras;Human-centered computing;Mixed and augmented reality;Theory and algorithms for application domains;Semi-supervised learning},
  doi={10.1109/VR.2019.8798358},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798153,
  author={Klippel, Alexander and Zhao, Jiayan and Oprean, Danielle and Wallgrün, Jan Oliver and Chang, Jack Shen-Kuen},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Research Framework for Immersive Virtual Field Trips}, 
  year={2019},
  volume={},
  number={},
  pages={1612-1617},
  abstract={Virtual field trips have been thought of and implemented for several decades. For the most part, these field trips were delivered through desktop computers and often as interactive but strictly two-dimensional experiences. The advent of immersive technologies for both creating content and experiencing places in three dimensions provides ample opportunities to move beyond the restrictions of two dimensional media. We propose here a framework we developed to assess immersive learning experiences, specifically immersive virtual field trips (iVFTs). We detail the foundations and provide insights into associated empirical evaluations.},
  keywords={Sensors;Solid modeling;Three-dimensional displays;Tracking;Taxonomy;Education;Cameras;immersive learning;virtual field trips;research framework;K.6.1 [Management of Computing and Information Systems]: Project and People Management—Life Cycle;K.7.m [The Computing Profession]: Miscellaneous—Ethics},
  doi={10.1109/VR.2019.8798153},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797846,
  author={Plecher, David A. and Wandinger, Maximilian and Klinker, Gudrun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality for Cultural Heritage}, 
  year={2019},
  volume={},
  number={},
  pages={1618-1622},
  abstract={In this paper we present two different approaches with Mixed (Virtual and Augmented) Reality to give pupils an understanding of ancient Greek culture and history in a motivating way. We identify the possibilities of AR in a museum for ancient statues to represent virtual information and to guide the visitor. Moreover, we discuss typical issues of mobile and stationary Virtual Reality (VR) systems in the context of public usage within schools and museums. Additionally, a new VR-streaming approach called SaMaXVR is presented that combines the benefits of mobile and stationary consumer ready VR systems in terms of usability, maintenance, safety and graphics-quality by mitigating many of their individual disadvantages when used in daily business. As this streaming solution consists of cost-effective hardware, this approach can be seen as a valuable and affordable alternative for schools, museums and students. To demonstrate its potential, an application was developed that visualizes complex 3D scans of cultural heritage in their former original environment to give users the possibility to travel back in time (and place) to see how the artifacts might have looked like in the past.},
  keywords={Hardware;Cultural differences;Virtual reality;Three-dimensional displays;Visualization;Resists;Maintenance engineering;Mixed Reality;Virtual Reality;Augmented Reality;Cultural Heritage},
  doi={10.1109/VR.2019.8797846},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798115,
  author={Raikwar, Aditya and D'Souza, Newton and Rogers, Ciana and Kress, Mathew and Williams, Adam and Rishe, Naphtali D. and Ortega, Francisco R.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={CubeVR: Digital Affordances for Architecture Undergraduate Education using Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1623-1626},
  abstract={CubeVR incorporates the traditional studio-based learning in architecture with virtual reality (VR) and in doing so it enables features that are not available or difficult to achieve in the real-world. CubeVR allows students and instructors to move from the existing studio to a virtual environment, allowing for a more impactful learning experience. This paper presents our current state of the CubeVR prototype, informal impressions of users, and the use of different affordances in the virtual world.},
  keywords={Education;Solid modeling;Computer architecture;Task analysis;Augmented reality;Lighting;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8798115},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798150,
  author={Salyers, Joshua and Cliburn, Daniel and Canniff, Keely and Barajas, Stephany},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Information Widgets for a Virtual Reality Serious Game}, 
  year={2019},
  volume={},
  number={},
  pages={1627-1632},
  abstract={As K-12 humanities teachers increasingly use virtual reality programs as pedagogical tools, creators of virtual reality serious games should better understand the way that the design of 3D user interfaces influence learning and user engagement. This study explores user preferences towards selection technique, level of transparency, and positioning of 3D information widgets in an open-world virtual reality environment. Using a recently constructed virtual heritage game, Little Manila Recreated, this article adds to a growing body of user studies that attempt to establish best practices for the use of 3D information widgets in VR history projects designed for K-12 classrooms.},
  keywords={Three-dimensional displays;Games;History;Testing;Virtual environments;Tools;Human-centered computing—Virtual Reality;Human-centered computing—user studies},
  doi={10.1109/VR.2019.8798150},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797841,
  author={Southgate, Erica},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality for Deeper Learning: An Exemplar from High School Science}, 
  year={2019},
  volume={},
  number={},
  pages={1633-1639},
  abstract={This paper uses qualitative data, from a study which embedded virtual reality (VR) in high school science classes, to produce an exemplar designed to illustrate how the technology can promote engagement and Deeper Learning for students. Application of the Deeper Learning conceptual framework to the exemplar provides unique insights into how VR can be deployed in curriculum-aligned ways to develop and combine content mastery, self-directed, collaboration and, perhaps most importantly, creative endeavour in high school science, especially in low income school communities.},
  keywords={Collaboration;Virtual environments;Human computer interaction;Training;Task analysis;Virtual Reality;Children;School;Pedagogy;Curriculum;Learning;STEM;Science Education;Human-centred computing;Human computer interaction (HCI);Interaction paradigms;Social and professional topics;User characteristics;Age;Human centred computing;HCI design and evaluation methods;Field studies},
  doi={10.1109/VR.2019.8797841},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798129,
  author={Sun, Bo and Chikwem, Uzoma and Nyingifa, Donald},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VRLearner: A Virtual Reality Based Assessment Tool in Higher Education}, 
  year={2019},
  volume={},
  number={},
  pages={1640-1645},
  abstract={We developed a Virtual Reality based assessment tool, VRLearner, to assess and help reinforce student learning outcomes in higher education. The tool was evaluated and compared with a regular assessment tool with the same contents. The evaluation results show that students, particularly underrepresented minority students from our focus group, prefer to use the VR based tool and find it is much more engaging and increased participants' sense of involvement. Our results suggest that virtual reality technology provides an opportunity to leverage learning relevant interaction, which can enhance the design of learning technology and serious games.},
  keywords={Virtual reality;Tools;Tutorials;Navigation;Games;Legged locomotion;Learning;Virtual Reality;Serious Game},
  doi={10.1109/VR.2019.8798129},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797734,
  author={Zhang, Guangwei},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Simulation for History Education}, 
  year={2019},
  volume={},
  number={},
  pages={1646-1651},
  abstract={Virtual simulation is an ideal approach for providing situated learning environments in history education, in which virtual reality (VR) and video games are the most widely used approaches in teaching history because of their convenience and popularity. Already-made historical VR materials are usually used for experiencing history, and historical VR materials made by the students could help learn history by “doing history”. We have been teaching the history and culture of Tang dynasty by the virtually reconstructed Dunhuang Mogao caves as well as the ancient Chang'an city with Minecraft, 3D modeling systems and 360-degree panoramic videos and images. Based on our practices, we propose a workable method for applying virtual simulation in history teaching. According to our experiences using virtual simulation for teaching history, the engagement and the performance of the students increase after virtual simulation based learning is used.},
  keywords={History;Education;Solid modeling;Virtual reality;Three-dimensional displays;Urban areas;Computational modeling;Virutal simulation;Minecraft;360-degree;WebVR;Computer Graphics;I.3.7;Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VR.2019.8797734},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798059,
  author={Dorado, José and Figueroa, Pablo and Chardonnet, Jean-Rémy and Merienne, Frédéric and Hernández, Tiberio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Homing by triangle completion in consumer-oriented virtual reality environments}, 
  year={2019},
  volume={},
  number={},
  pages={1652-1657},
  abstract={Homing is a fundamental task which plays a vital role in spatial navigation. Its performance depends on the computation of a homing vector, where human beings can use simultaneously two different cognitive strategies: an online strategy based on the self-motion cues known as path integration (PI), and an offline strategy called piloting based on the spatial image of the path. Studies using virtual reality environments (VE) have shown that human being can perform homing tasks with acceptable performance. However, in these studies, subjects were able to walk naturally across large tracking areas, or researchers provided them with high-end large-immersive displays. Unfortunately, these configurations are far from current consumer-oriented devices, and very little is known about how their limitations can influence these cognitive processes. Using a triangle completion paradigm, we assessed homing tasks in two consumer-oriented displays (an HTC Vive and a GearVR) and two consumer-oriented interaction devices (a Virtuix Omni Treadmill and a Touchpad Control). Our results show that when locomotion is available (treadmill condition), there exist significant effects regarding display and path complexity. In contrast, when locomotion is restricted (touchpad condition), some effects on path complexity were found. Thus, some future research directions are therefore proposed.},
  keywords={Task analysis;Optical sensors;Optical imaging;Visualization;Performance evaluation;Virtual reality;Complexity theory},
  doi={10.1109/VR.2019.8798059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797757,
  author={Frangos, Andreas S. and Lee, Tae-Jun and To, Dylan and Giannopulu, Irini},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dorsal and Ventral Pathways Implications in an Augmented Reality Environment}, 
  year={2019},
  volume={},
  number={},
  pages={1658-1662},
  abstract={The Ventral Pathway (VP) and Dorsal Pathway (DP) are responsible for identifying (what) and localising (where) objects in 3D space. These pathways project integrated information into the frontal (FC) and the prefrontal (PF) cortices making possible for humans not only to interact with objects but also express motivations and emotions directly or indirectly associated with the actions they perform with objects. The neural implications of these pathways were examined in a sample of healthy participants ( n = 30) aged 22 years old while invited to create an installation using a set of common (CO) and uncommon objects (UO) in augmented reality (AR). All participants were equipped with a 32 channels EEG, a frequency analysis of brain waves has been performed. The results have shown an alpha power increase in the ventral pathway when UO were involved in the creative task. This finding is attributed to a greater need to attend the UO as no representation of them exists in the VP. Increased theta and delta powers were observed in the FC when creating an installation with UO, indicating more higher-order functioning involving emotion and motivation. Engaging multimodal complex brain areas, it seems that creativity in augmented reality requires similar brain patterns as in real world.},
  keywords={Electroencephalography;Task analysis;Augmented reality;Object recognition;Creativity;Electrodes;Three-dimensional displays;Augmented Reality (AR);Neuro-imaging;Ventral Pathway (VP);Dorsal Pathway (DP);Electro-Encephalography (EEG);Frontal Cortex (FC);Prefrontal Cortex (PFC)},
  doi={10.1109/VR.2019.8797757},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797756,
  author={Hopper, Jonathan E. and Finney, Hunter and Jones, J. Adam},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Field of View and Forward Motion Discrimination in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1663-1666},
  abstract={There has long been interest in illusions of self-motion and their perception in virtual environments. Prior work has indicated that an observer's field of view size is an important factor in the perception of self-motion both real and illusory. Restricted fields of view in some virtual reality displays has limited the extent to which this can be studied. In this paper, we discuss a pilot study examining how well observers can discriminate forward motion velocities as viewed through two common, but differing, field of view configurations. We find that observers are quite sensitive to changes in forward motion. The perceived magnitude of this motion is also found to be affected by field of view size with a smaller field of view resulting in slower perceived velocity.},
  keywords={Legged locomotion;Visualization;Resists;Observers;Virtual environments;Adaptive optics;Human-centered computing;Interaction paradigms;Virtual reality;Computer graphics;Graphics systems and interfaces;Perception},
  doi={10.1109/VR.2019.8797756},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798258,
  author={Jones, J. Adam},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optical and Neural Properties of Vision as Applied to Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1667-1670},
  abstract={The physiological optics and neurology of vision are topics with much breadth and perhaps even more depth. Though many people have a cursory understanding of the visual system, there are some less-commonly known aspects that are vitally important to virtual environments research and practice. In this paper, we present an elementary discussion of the optical and neural elements involved in the early stages of vision and their relationship to virtual environments.},
  keywords={Lenses;Virtual environments;Cornea;Retina;Iris;Photoreceptors;Human-centered computing;Interaction paradigms;Virtual reality;Human computer interaction (HCI);HCI theory, concepts and models},
  doi={10.1109/VR.2019.8798258},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798289,
  author={Matthes, Carl and Weissker, Tim and Angelidis, Emmanouil and Kulik, Alexander and Beck, Stephan and Kunert, Andre and Frolov, Anton and Weber, Sandro and Kreskowski, Adrian and Froehlich, Bernd},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Collaborative Virtual Reality Neurorobotics Lab}, 
  year={2019},
  volume={},
  number={},
  pages={1671-1674},
  abstract={We present the collaborative Virtual Reality Neurorobotics Lab, which allows multiple collocated and remote users to experience, discuss and participate in neurorobotic experiments in immersive virtual reality. We describe the coupling of the Neurorobotics Platform of the Human Brain Project with our collaborative virtual reality and 3D telepresence infrastructure and highlight future opportunities arising from our work for research on direct human interaction with simulated robots and brains.},
  keywords={Robots;Three-dimensional displays;Collaboration;Solid modeling;Telepresence;Virtual environments},
  doi={10.1109/VR.2019.8798289},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797798,
  author={Velonaki, Mari},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Revisiting Jean Louis Baudry's Concept of the Ideological Apparatus: Linking Virtual Reality to The Dream Analogy}, 
  year={2019},
  volume={},
  number={},
  pages={1675-1676},
  abstract={This paper aims to link Jean Louis Baudry's theory of cinematographic apparatus, and in particular his “dream analogy”, to the way we interpret experiential states within a Virtual Reality environment. Analogous elements that constitute and assist in the reading of the “dream analogy” will be presented as threads for consideration of the relevance and validity of such theory to Virtual Reality. The paper extends research previously undertaken by the author which introduced and linked the theory of the apparatus to electronic media art and in particular to interactive installations and responsive environments.},
  keywords={Virtual reality;Robots;Media;Art;Conferences;Three-dimensional displays;User interfaces;Film theory;media art;participatory experience;Jean Louis Baudry},
  doi={10.1109/VR.2019.8797798},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797842,
  author={Yamazoe, Hirotake},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A pilot study of gaze-gait relations analysis in a VR environment using HMD and LRF}, 
  year={2019},
  volume={},
  number={},
  pages={1677-1679},
  abstract={In this paper, we propose a VR-based experimental environment for analyzing the relationship among gaze and body movements using a head-mounted camera (HMD) and laser range finders (LRF). So far, we have analyzed the relations among gaze and whole body movements using a treadmill-based environment. Since the participants walked on treadmills during experiments, walking on a treadmill may be different from natural walking. Thus, in this research, we create a VR-based environment combined with LRF for tracking participants's positions. We first estimate human positions by using LRF, then generate a virtual environment corresponding to estimated human positions. The generated environment with the gaze targets is presented via HMD. We conducted a pilot experiment in this environment and show the experimental results.},
  keywords={Legged locomotion;Resists;Lasers;Visualization;Cameras;Target tracking;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality;Computing methodologies—Artificial intelligence—Computer vision—Activity recognition and understanding},
  doi={10.1109/VR.2019.8797842},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798302,
  author={Yonezawa, Tomoko and Yamazoe, Hirotake},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Preliminary Experiment on Shareable and Portable Voice Sticky using Sound Orientation}, 
  year={2019},
  volume={},
  number={},
  pages={1680-1683},
  abstract={In this research, we considered a scheme for recording, browsing, and sharing personal voice memos using three-dimensional (3D) auditory space, and we proposed a voice memo system that can achieve auditory localization of multiple voice memos. In the system, the users can record the voice memos and place them on the surrounding relative directions. The direction of each memo is determined by head direction during the recording. After the recording, the users can browse numerous voice memos using head directions and can define the sharing attribute of each voice memo in the edit mode so that personal and public voice memos are appropriately shared with the permitted users. In this paper, we evaluate the effectiveness of the proposed scheme when the participants conduct a series flow, from recording the voice memos to browsing the recorded voice memos, especially focusing on the effects of the user's relative. directions and head motions.},
  keywords={Head;Magnetic heads;Three-dimensional displays;Visualization;Headphones;Prototypes;Analysis of variance;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Usability testing;Ubiquitous and mobile computing;Ubiquitous and mobile computing systems and tools},
  doi={10.1109/VR.2019.8798302},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797964,
  author={Yonezawa, Tomoko and Yoshida, Naoto and Ishikawa, Nanase},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Japanese Tea Ceremony Experience with Multimodal AR Expressing Mental Concentration}, 
  year={2019},
  volume={},
  number={},
  pages={1684-1686},
  abstract={In this paper, we introduce our VR system which uses projection, speakers, and a fan to express the distinctive experience of concentration in a Japanese tea ceremony. This system aims to broaden the participant's field of perception (view, hearing, and wind) from a narrow field of view in the tea bowl to multiple sensations of the outside of the tea ceremony room. The continuous concentration, which is presumed by the user's stable motion while mixing tea using a bamboo tea whisk, broadens the field of view and other senses. The accelerometer sensor values of a bamboo whisk were used in an auto-correlation function for detecting the user's stable mixing motion in the tea bowl. In the demonstration experiment in IVRC 2013 (International Virtual Reality Contest 2013), the participants commented positively on the system.},
  keywords={Bamboo;Virtual reality;Visualization;Accelerometers;Correlation;Solid modeling;Psychology;Human-centered computing;Interaction design;Interaction design process and methods;Scenario-based design Human-centered computing;Virtual reality;Human-centered computing;Human computer interaction (HCI)Interaction paradigmsMixed / augmented reality},
  doi={10.1109/VR.2019.8797964},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798038,
  author={Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of 3D Rotational Jitter and Selection Methods on 3D Pointing Tasks}, 
  year={2019},
  volume={},
  number={},
  pages={1687-1692},
  abstract={3D pointing is an integral part of Virtual Reality interaction. Typical pointing devices rely on 3D trackers and are thus subject to fluctuations in the reported pose, i.e., jitter. In this work, we explored how different levels of rotational jitter affect pointing performance and if different selection methods can mitigate the effects of jitter. Towards this, we designed a Fitts' Law experiment with three selection methods. In the first method, subjects used a single controller to position and select the object. In the second method, subjects used the controller in their dominant hand to point at objects and the trigger button of a second controller, held in their non-dominant hand, to select objects. Finally, subjects used the controller in their dominant hand to point the objects and pressed the space bar on a keyboard to select the object in the third condition. During the pointing task we added five different levels of jitter: no jitter, ±0.5°, ±1°, and ±2° uniform noise, as well as White Gaussian noise with 1° standard deviation. Results showed that the Gaussian noise and ±2° of jitters significantly reduced the throughput of the participants. Moreover, subjects made fewer errors when they performed the experiment with two controllers. Our results inform the design of 3D user interfaces, input devices and interaction techniques.},
  keywords={Jitter;Three-dimensional displays;Task analysis;Performance evaluation;Keyboards;Gaussian noise;Standards;Human-centered computing;Virtual Reality;Keyboards;Pointing devices},
  doi={10.1109/VR.2019.8798038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797869,
  author={Englmeier, David and Schönewald, Isabel and Butz, Andreas and Höllerer, Tobias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Feel the Globe: Enhancing the Perception of Immersive Spherical Visualizations with Tangible Proxies}, 
  year={2019},
  volume={},
  number={},
  pages={1693-1698},
  abstract={Recent developments in the commercialization of virtual reality open up many opportunities for enhancing human interaction with three-dimensional objects and visualizations. Spherical visualizations allow for convenient exploration of certain types of data. Our tangible sphere, exactly aligned with the sphere visualizations shown in VR, implements a very natural way of interaction and utilizes senses and skills trained in the real world. In a lab study, we investigate the effects of the perception of actually holding a virtual spherical visualization in hands. As use cases, we focus on surface visualizations that benefit from or require a rounded shape. We compared the usage of two differently sized acrylic glass spheres to a related interaction technique that utilizes VR controllers as proxies. On the one hand, our work is motivated by the ability to create in VR a tangible, lightweight, handheld spherical display that can hardly be realized in reality. On the other hand, gaining insights about the impact of a fully tangible embodiment of a virtual object on task performance, comprehension of patterns, and user behavior is important in its own right. After a description of the implementation we discuss the advantages and disadvantages of our approach, taking into account different handheld spherical displays utilizing outside and inside projection.},
  keywords={Visualization;Three-dimensional displays;Data visualization;Hardware;Shape;Layout;Glass;Human-centered computing—Interaction paradigms—Virtual reality},
  doi={10.1109/VR.2019.8797869},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797684,
  author={Khadka, Rajiv and Banic, Amy},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Body-Prop Interaction: Evaluation of Augmented Open Discs and Egocentric Body-Based Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={1705-1710},
  abstract={This paper presents a novel interaction technique to combine multiple inputs and output modalities with a 3D printed open disc which is tracked and augmented with a virtual information. Props have been used in virtual and augmented environments to provide tangible associations to virtual elements. When used in a larger immersive environment or where a user is walking around, users may not have the capability to set these objects/props down or are limited to the number of hands, so users are reduced to the capability of using only two props or controllers. In our technique, users can instead use their body to store and organize the virtual objects associated with tangible props by physically hanging them on or attaching them to the body. This type of interaction is well-suited for both immersive visualizations and immersive virtual environments. In this paper, we present the results of an experimental evaluation of our Body Prop Interaction technique for each type of immersive environment and corresponding task type, demonstrating more effective usage in each scenario than a joystick or gesture-based interaction.},
  keywords={Data visualization;Task analysis;Three-dimensional displays;Augmented reality;Headphones;Tracking;Virtual environments;Human-centered computing;Mixed / augmented reality;User studies Human-centered computing;Usability Testing},
  doi={10.1109/VR.2019.8797684},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798374,
  author={Luckett, Ethan and Key, Tykeyah and Newsome, Nathan and Jones, J. Adam},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Metrics for the Evaluation of Tracking Systems for Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={1711-1716},
  abstract={In this paper, we present three generalizable metrics by which tracking systems for virtual environments can be evaluated. These metrics include positional accuracy, rotational accuracy, and tracking resolution. Additionally, we present methods for acquiring these measurements using components commonly available at hardware and hobby shops. The methods are tested using a consumer-grade virtual reality system but are widely generalizable to most tracking systems, both professional-and consumer-grade.},
  keywords={Resists;Position measurement;Measurement by laser beam;Three-dimensional displays;Testing;Surface emitting lasers;Human-centered computing;Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798374},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797982,
  author={O-larnnithipong, Nonnarit and Ratchatanantakit, Neeranut and Tangnimitchok, Sudarat and Ortega, Francisco and Barreto, Armando},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Hand Tracking Interface for Virtual Reality Interaction Based on MARG sensors}, 
  year={2019},
  volume={},
  number={},
  pages={1717-1722},
  abstract={Recent years have seen an accelerated development of Virtual Reality (VR) and Augmented Reality (AR) systems. Through these systems, computers immerse users in a completely artificial environment or in one that is partially based on the user's physical surroundings, but partially created artificially. One primary objective of these systems is to cause the perception of actually “being there” (in the environment created by the computer). However, this perception of full immersion may be weakened by the need to use “unnatural” devices (e.g., game controller) to execute actions within the VR or AR environment. Accordingly, increased interest has emerged in developing mechanisms by which the user could interact with VR and AR environments using natural actions, such as the movements of his/her hands.},
  keywords={Magnetometers;Accelerometers;Quaternions;Gyroscopes;Gravity;Magnetic sensors;Hand tracking;MARG sensors;Accelerometer;Gyroscope;Magnetometer;Orientation estimation;H.5.2 [User Interfaces]: Input devices and strategies},
  doi={10.1109/VR.2019.8797982},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798105,
  author={Ortega, Francisco R. and Tarre, Katherine and Kress, Mathew and Williams, Adam S. and Barreto, Armando B. and Rishe, Naphtali D.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Selection and Manipulation Whole-Body Gesture Elicitation Study In Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1723-1728},
  abstract={We present a whole-body gesture elicitation study using Head Mounted Displays, including a legacy bias reduction. The motivation for this study was to understand the type of gesture agreement rates for selection and manipulation interactions and to improve the user experience for whole-body interactions. We looked at 23 participants and 20 distinct referents (with multiple gestures per referent). We found that regardless of the production technique used to remove legacy bias, legacy bias was still found in some of the produced gestures. In some instances, gestures were derived from previous interactions but were still appropriate for the environment presented. This study provides a rich set of information and useful recommendations for future designers and/or developers.},
  keywords={Production;Virtual reality;User interfaces;Three-dimensional displays;Resists;Headphones;User experience;Gesture Elicitation—Gestures—Virtual Reality—Whole-Body},
  doi={10.1109/VR.2019.8798105},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797740,
  author={Otte, Alexander and Menzner, Tim and Gesslein, Travis and Gagel, Philipp and Schneider, Daniel and Grubert, Jens},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Utilizing Touch-sensitive Physical Keyboards for Text Entry in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1729-1732},
  abstract={Text entry is a challenge for Virtual Reality (VR) applications. In the context of immersive VR Head-Mounted Displays, text entry has been investigated for standard physical keyboards as well as for various hand representations. Specifically, prior work has indicated that minimalistic fingertip visualizations are an efficient hand representation. However, they typically require external tracking systems. Touch-sensitive physical keyboards allow for on-surface interaction, with sensing integrated into the keyboard itself. However, they have not been thoroughly investigated within VR. Our work brings together the domains of VR text entry and touch-sensitive physical keyboards. Specifically, we propose to utilize touch-sensitive physical keyboards for text entry as an alternative sensing mechanism for tracking user's fingertips and study its performance in a preliminary user study.},
  keywords={Keyboards;Sensors;Visualization;Pins;Resists;Microcontrollers;Standards;H.5.2: [User Interfaces - Input devices and strategies.]},
  doi={10.1109/VR.2019.8797740},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798189,
  author={Rothe, Sylvia and Pothmann, Pascal and Drewe, Heiko and Hussmann, Heinrich},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interaction Techniques for Cinematic Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1733-1737},
  abstract={For watching omnidirectional movies via HMD, turning the head is the most relevant and natural input technique to choose the visible part of the movie. However, there is a need for additional interactivity in cinematic virtual reality (CVR), e.g. for navigating the movie, for nonlinear story lines or for communication with other viewers watching a movie together. The input device should not disturb the viewing experience and the viewer should not be primarily aware of it. We present a design space based on numerous methods in literature and our own experiences. As a result of the design space we describe interaction techniques which meet the challenges of cinematic virtual reality. For doing this, various dimensions of the design space will be combined. The most promising method, eye-based head gestures, is described in more detail and was implemented for CVR. The conducted user study will be analyzed in future work.},
  keywords={Motion pictures;Head;Virtual reality;Aerospace electronics;Resists;Navigation;Input devices;cinematic VR;interaction;input device;eye tracking gestures},
  doi={10.1109/VR.2019.8798189},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798053,
  author={Sun, Yongbin and Armengol-Urpi, Alexandre and Reddy Kantareddy, Sai Nithin and Siegel, Joshua and Sarma, Sanjay},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={MagicHand: Interact with IoT Devices in Augmented Reality Environment}, 
  year={2019},
  volume={},
  number={},
  pages={1738-1743},
  abstract={We present an Augmented Reality (AR) visualization and interaction tool for users to control Internet of Things (IoT) devices with hand gestures. Today, smart IoT devices are becoming increasingly ubiquitous with diverse forms and functions, yet most user controls over them are still limited to mobile devices and web interfaces. Recently, AR has been developed rapidly, and provided immersive solutions to enhance user experience of applications in many fields. Its capability to create immersive interactions allows AR to improve the way smart devices are controlled via more direct visual feedback. In this paper, we create a functional prototype of one such system, enabling seamless interactions with sound and lighting systems through the use of augmented hand-controlled interaction panels. To interpret users' intentions, we implement a standard 2D convolution neural network (CNN) for real-time hand gesture recognition and deploy it within our system. Our prototype is also equipped with a simple but effective object detector which can identify target devices within a proper range by analyzing geometric features. We evaluate the performance of our system qualitatively and quantitatively and demonstrate it on two smart devices.},
  keywords={Three-dimensional displays;Shape;Gesture recognition;Visualization;Object detection;Internet of Things;User experience;Augmented Reality;Visual and interactive control;IoT devices;Head mounted display;Deep learning;Image and 3D data processing;Object detection;Hand gesture recognition},
  doi={10.1109/VR.2019.8798053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797718,
  author={Valkov, Dimitar and Mantler, Andreas and Linsen, Lars},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Haptic Prop: A Tangible Prop for Semi-passive Haptic Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={1744-1748},
  abstract={In this paper, we present Haptic Prop, a semi-passive, pico-powered, tangible prop, which is able to provide programmable friction for interaction with a tabletop setup, such as interactive workbenches or fish-tank VR. We explore the interaction space, its basic components, and constraints. Haptic Prop can be used to provide haptic feedback to the user at different levels and in different directions. We have conducted a preliminary user study evaluating the users' acceptance for the device and their ability to detect the programmed level of friction for rotation and linear movements. While currently still preliminary, the results demonstrate the utility of our device and outline some promising directions for future work.},
  keywords={Haptic interfaces;Actuators;DC motors;Friction;User interfaces;Maintenance engineering;Power demand;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Input Devices and Strategies, Interaction Styles;H.5.m [Information Interfaces and Presentation]: Miscellaneous},
  doi={10.1109/VR.2019.8797718},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798155,
  author={Chessa, Manuela and Maiello, Guido and Klein, Lina K and Paulun, Vivian C and Solari, Fabio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Grasping objects in immersive Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1749-1754},
  abstract={Grasping is one of the fundamental actions we perform to interact with objects in real environments, and in the real world we rarely experience difficulty picking up objects. Grasping plays a fundamental role for interactive virtual reality (VR) systems that are increasingly employed not only for recreational purposes, but also for training in industrial contexts, in medical tasks, and for rehabilitation protocols. To ensure the effectiveness of such VR applications, we must understand whether the same grasping behaviors and strategies employed in the real world are adopted when interacting with objects in VR. To this aim, we replicated in VR an experimental paradigm employed to investigate grasping behavior in the real world. We tracked participants' forefinger and thumb as they picked up, in a VR environment, unfamiliar objects presented at different orientations, and exhibiting the same physics behavior of their real counterparts. We compared grasping behavior within and across participants, in VR and in the corresponding real world situation. Our findings highlight the similarities and differences in grasping behavior in real and virtual environments.},
  keywords={Grasping;Three-dimensional displays;Tracking;Task analysis;Thumb;Virtual environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computing methodologies;Computer graphics;Graphics systems and interfaces},
  doi={10.1109/VR.2019.8798155},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798140,
  author={Kalia, Megha and Navab, Nassir and Fels, Sidney and Salcudean, Tim},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Method to Introduce & Evaluate Motion Parallax with Stereo for Medical AR/MR}, 
  year={2019},
  volume={},
  number={},
  pages={1755-1759},
  abstract={Incorrect depth perception and lack of good evaluation systems are major barriers in clinical translation of augmented and mixed reality AR/MR. Thus, a systematic study of depth cues is necessary. Therefore, in the current paper we present a method to introduce the quantitative depth cue Motion Parallax (MP) in surgical scenes and study its effect on depth perception when combined with binocular disparity. In addition to this we present an innovative virtual tool method to evaluate depth. To introduce MP, we reconstructed the tissue surface using structure from motion technique. Then to get accurate absolute scale of the reconstructed surface stereo-triangulation was used. The simulated tumor was rendered beneath the reconstructed point-cloud by rendering a hole for `X-ray' like vision. The MP was introduced by rotating the entire scene from side-to-side with a tumor-surface-point as pivot for maximum impact. Finally for evaluation, we used a virtual surgical tool rendered using real-time da Vinci surgical API's forward kinematics data. In total, 12 subjects participated in a within-subjects-experiment design to study four cases, i.e., Stereo + MP ( S+MP), Mono + MP ( M+MP), Stereo + No MP (S+N-MP) and Mono + No MP (M+N-MP). The subjects significantly overestimated Judged Percentage of True Distance in M+MP when compared to M+N-MP (probability (p)=0.000, Number of Samples (N))=120) and S+N-MP cases ( p=0.001, N = 120). Furthermore, the observed VariableError was less in S+MP and S+N-MP cases when compared to M+MP and M+N-MP cases. The use of Motion Parallax in console interfaces for surgical robotics showed overestimation of judged distance. But to our knowledge it is the first work studying the effect of motion parallax and stereo in the surgical context. Therefore, its further study is warranted.},
  keywords={Tools;Image reconstruction;Surface reconstruction;Surgery;Observers;Microsoft Windows;Tumors;Depth Perception—Augmented Reality—Mixed Reality—Medical Augmented Reality;Motion Parallax—Evaluation method},
  doi={10.1109/VR.2019.8798140},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797991,
  author={Khadka, Rajiv and Banic, Amy},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Position Paper: Factors of Perceived Tactile Cue Dominance when Interacting with Moving Virtual Objects}, 
  year={2019},
  volume={},
  number={},
  pages={1760-1764},
  abstract={Prior work has shown that visual cues are more dominant than tactile cues for static objects. When texture and material properties are added, the reverse is true. However, it is not well understood which cues play a role in perceiving these textures when objects are moving. In this paper, we present an evaluation investigating speed and angular direction mismatch between visual and tactile information while objects are moving. Our results show that when objects are stationary or moving slowly, the sense of touch dominates visual cues; but as the speed increases, visual cues dominate tactile cues. When tactile and visual cues were not matching for moving virtual objects, visual cues were more dominant when the virtual objects moved at angles less than 30 degrees different than the tactile feedback in either direction. Therefore, potentially a 1D directional haptic device with physical texture substitutions may be able to convey sufficient realism when touching virtual objects that are moving faster than 4-8cm/s and with a difference in angular direction of 30 degrees or less. In this paper, we also present the limitations of this work and outline a research agenda for future investigation of visual and tactile cue dominance for perceiving textures of virtual objects moving in a virtual environment.},
  keywords={Visualization;Haptic interfaces;Friction;Skin;Rubber;DC motors;Virtual environments;Human-centered computing—Virtual reality—Haptic devices;Human-centered computing—User studies—Mixed / augmented reality},
  doi={10.1109/VR.2019.8797991},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797891,
  author={McNamara, Ann and Boyd, Katherine and George, Joanne and Jones, Weston and Oh, Somyung and Suther, Annie},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Information Placement in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1765-1769},
  abstract={In this paper, we develop a technique for placing informational labels in complex Virtual Environments (VEs). The ability to effectively and efficiently present labels in VEs is valuable in Virtual Reality (VR) for many reasons, but the motivation is to bring us closer to a system that delivers information in VR in an optimal way without causing information overload. The novelty of this technique lies in the use of eye tracking as an accurate indicator of attention to identifying objects of interest. Labels associated with such objects of interest are revealed when the user attends to them. We conduct a series of experiments to evaluate label placement based on user attention. This investigation is a first step toward integrating attention into information placement strategies in VR.},
  keywords={Task analysis;Gaze tracking;Visualization;Clutter;Information retrieval;Virtual environments;Human-centered computing—Virtual Reality—Information Placement;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797891},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798335,
  author={Phillips, Nate and Massey, Kristen and Arefin, Mohammed Safayet and Swan, J. Edward},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design, Assembly, Calibration, and Measurement of an Augmented Reality Haploscope}, 
  year={2019},
  volume={},
  number={},
  pages={1770-1774},
  abstract={A haploscope is an optical system which produces a carefully controlled virtual image. Since the development of Wheatstone's original stereoscope in 1838, haploscopes have been used to measure perceptual properties of human stereoscopic vision. This paper presents an augmented reality (AR) haploscope, which allows the viewing of virtual objects superimposed against the real world. Our lab has used generations of this device to make a careful series of perceptual measurements of AR phenomena, which have been described in publications over the previous 8 years. This paper systematically describes the design, assembly, calibration, and measurement of our AR haploscope. These methods have been developed and improved in our lab over the past 10 years. Despite the fact that 180 years have elapsed since the original report of Wheatstone's stereoscope, we have not previously found a paper that describes these kinds of details.},
  keywords={Optical imaging;Optical distortion;Lenses;Calibration;Monitoring;Adaptive optics},
  doi={10.1109/VR.2019.8798335},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797965,
  author={Wu, Hansen and Adams, Haley and Pointon, Grant and Stefanucci, Jeanine and Creem-Regehr, Sarah and Bodenheimer, Bobby},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Danger from the Deep: A Gap Affordance Study in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1775-1779},
  abstract={It is an open question as to whether people perceive and act in augmented reality environments in the same way that they do in real environments. The current work investigated participants' judgments of whether or not they could act on an obstacle portrayed with augmented reality. Specifically, we presented gaps of varying widths and depths to participants in augmented reality using the Microsoft HoloLens. We asked users to indicate whether or not they believed that they could step across the virtual gaps given their widths and depths. Averaging across changes in width and depth, users generally underestimated their abilities to cross gaps. However, this underestimation was significantly greater when the gaps were deep. Thus, our findings suggest that users in augmented reality respond with more conservative judgments when presented with risky stimuli-a response that mimics real world behavior. Their altered reactions to deeper gaps may provide early evidence for augmented reality's capacity to evoke a sense of realism or immersion and its use in evaluating perception and action.},
  keywords={Augmented reality;Visualization;Atmospheric measurements;Particle measurements;Length measurement;Augmented reality;Affordances;Perception;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;J.4 [Computer Applications]: Social and Behavioral Sciences—Psychology},
  doi={10.1109/VR.2019.8797965},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797902,
  author={Grogorick, Steve and Ueberheide, Matthias and Tauscher, Jan-Philipp and Bittner, Paul Maximilian and Magnor, Marcus},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Gaze and Motion-aware Real-Time Dome Projection System}, 
  year={2019},
  volume={},
  number={},
  pages={1780-1783},
  abstract={We present the ICG Dome, a research facility to explore human visual perception in a high-resolution virtual environment. Current state-of-the-art VR devices still suffer from some technical limitations, like limited field of view, screen-door effect or so-called god-rays. These issues are not present or at least strongly reduced in our system, by design. Latest technology for real-time motion capture and eye tracking open up a wide range of applications.},
  keywords={Cameras;Head;Real-time systems;Gaze tracking;Synchronization;Rendering (computer graphics);Tracking;Human-centered computing—Human computer interaction (HCI)—Interactive systems and tools;Human-centered computing—Visualization—Visualization systems and tools;Computer systems organization—Real-time systems},
  doi={10.1109/VR.2019.8797902},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797850,
  author={Oshiro, Wakana and Kagami, Shingo and Hashimoto, Koichi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perception of Motion-Adaptive Color Images Displayed by a High-Speed DMD Projector}, 
  year={2019},
  volume={},
  number={},
  pages={1790-1793},
  abstract={Recent progress of high-speed projectors using DMD (Digital Micromirror Device) has enabled low-latency motion adaptability of displayed images, which is a key challenge in achieving projection-based dynamic interaction systems. This paper presents evaluation of different approaches in achieving fast motion adaptability with DMD projectors through a subjective image evaluation experiment and a discrimination experiment. The results suggest that the approach proposed by the authors, which updates the image position for every binary frame instead of for every video frame, applied to 60-fps video input offers perceptual image quality comparable with the quality offered by 500-fps projection.},
  keywords={Light sources;Pulse width modulation;Color;Image color analysis;Image quality;Brightness;Delays;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR.2019.8797850},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797858,
  author={Tauscher, Jan-Philipp and Schottky, Fabian Wolf and Grogorick, Steve and Bittner, Paul Maximilian and Mustafa, Maryam and Magnor, Marcus},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive EEG: Evaluating Electroencephalography in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1794-1800},
  abstract={We investigate the feasibility of combining off-the-shelf virtual reality headsets and electroencephalography. EEG is a highly sensitive tool and subject to strong distortions when exerting physical force like mounting a VR headset on top of it that twists sensors and cables. Our study compares the signal quality of EEG in VR against immersive dome environments and traditional displays using an oddball paradigm experimental design. Furthermore, we compare the signal quality of EEG when combined with a commodity VR headset without modification against a modified version that reduces physical strain on the EEG headset. Our results indicate, that it is possible to combine EEG and VR even without modification under certain conditions. VR headset customisation improves signal quality results. Additionally, display latency of the different modalities is visible on a neurological level.},
  keywords={Electroencephalography;Headphones;Sensors;Electrodes;Virtual reality;Atmospheric measurements;Particle measurements;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797858},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798056,
  author={Eichhorn, Christian and Plecher, David A. and Inami, Masahiko and Klinker, Gudrun},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Physical Objects in AR Games – Offering a Tangible Experience}, 
  year={2019},
  volume={},
  number={},
  pages={1801-1806},
  abstract={Most AR games focus primarily on static augmentation, leaving out the potential for rich and tangible interactions. Recently, with the creation of the Superhuman Sports genre, new AR games emerged with the goal to extend the sports experience by utilizing technology. With the understanding of various related projects towards physical object interaction in AR and with the rich selection of sensors in mobile devices, we want to present two innovative visions in the direction of AR Sports Games: 1.Floating Ball: A ball shaped object is augmented and while flying in a circular pattern, two players are competing against each other by interacting with it. 2.Augmented Drone: Resulting out of the experiences with the Floating Ball, a competitive game concept with a drone as a versatile augmented game ball will be presented. We want to utilize the results as a step towards a competitive multi-player AR Sports Game based on mobile devices. In both concepts, one device is functioning as a Tangible AR Interface with the task to let the player manipulate the flying path of the physical ball. For this purpose, the ball is incorporated into the augmented world to create a realistic, perceivable playing experience.},
  keywords={Games;Sports;Drones;Sensors;Cameras;Electric motors;Human computer interaction;AR Superhuman Sports;Augmented Drone;Human Computer Interaction;Tangible Interfaces},
  doi={10.1109/VR.2019.8798056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798026,
  author={Godse, Anushka and Khadka, Rajiv and Banic, Amy},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Visual Perception Manipulation in Virtual Reality Training Environments to Improve Golf Performance}, 
  year={2019},
  volume={},
  number={},
  pages={1807-1812},
  abstract={In the real world, prior research in the field of perception and action has shown that individuals visually perceive objects differently based on their actual performance of an action on those objects, especially for sporting activities. For example, a golfer who performs well on putting a ball into the hole, will perceive that ball as larger (easier to hit) and the hole as larger (easier to put the ball in). We asked the following research question, can manipulation of visual perception of objects influence actual performance in the real world? Virtual objects are easily manipulated in Virtual Reality environments, therefore we investigated the use of Virtual Reality training where the properties of objects, such as size, were manipulated to influence perception on a golf putting task. In this paper, we present the results of our experimental user study. Putting performance increased after virtual reality training exposure when virtual objects were larger (perceived as easier to hit) and decreased when virtual objects were smaller (more difficult to hit). Our research has the potential to broaden the study of how virtual reality training can be used to improve sports training in a unique way.},
  keywords={Training;Task analysis;Sports;Visualization;Virtual environments;Human computer interaction;Virtual Reality training;perception and action;visual manipulation;virtual environments;sports training;change in size of virtual objects;sports performance;golf task;CCS [Human-centered Computing]: Human-computer interaction (HCI)- Interaction paradigms-Virtual Reality;[Human-centered Computing]: CCS Human-computer interaction (HCI)- Empirical studies in HCI;CCS [Software and its engineering]: Software Organization and properties-Virtual worlds software-Virtual worlds training simulation},
  doi={10.1109/VR.2019.8798026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797962,
  author={Kishishita, Yusuke and Das, Swagata and Ramirez, Antonio Vega and Thakur, Chetan and Tadayon, Ramin and Kurita, Yuichi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Muscleblazer: Force-Feedback Suit for Immersive Experience}, 
  year={2019},
  volume={},
  number={},
  pages={1813-1818},
  abstract={The increasing use of virtual reality (VR) and augmented reality (AR) systems has opened the possibility of providing immersive experiences to the general population around the world. However, most of the existing systems do not provide highly effective force-feedback experiences to the user. To provide such augmented systems in combination with force-feedback, novel ideas must be introduced that can be easily integrated with the existing VR and AR technologies. This work proposes a first-person VR game integrated with a soft exoskeleton that enhances the quality of interaction between the subject and the virtual environment (VE) through an additional force-feedback element. The effect of introducing the force-feedback element on the user is analyzed by using biosensors and questionnaire feedback. The biosensors are used to measure the level of anxiety induced in the subject during interaction with the virtual environment. Conditions in which this interaction occurs with and without force-feedback are compared. The questionnaire analyzes the perceived change in emotions of users as a result of the introduction of the force-feedback element. This game can be played by most individuals, regardless of age and physical fitness.},
  keywords={Games;Virtual environments;Muscles;Valves;Solenoids;Force;Biosensors;Virtual reality;Force-feedback;Pneumatic actuators;Wearable technology;Biosensors;Galvanic skin response (GSR)},
  doi={10.1109/VR.2019.8797962},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798275,
  author={Miedema, Nico Arjen and Vermeer, Jop and Lukosch, Stephan and Bidarra, Rafael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Superhuman sports in mixed reality: The multi-player game League of Lasers}, 
  year={2019},
  volume={},
  number={},
  pages={1819-1825},
  abstract={In recent years, many promising developments have taken place around augmented, virtual and mixed-reality technology. One could wonder whether these technologies can be combined to yield a mix of video games and sports, involving physical activities previously deemed impossible: creating a superhuman sport. This work investigates how mixed-reality can be used to create a fun, intuitive and engaging superhuman sport. To this end, the game League of Lasers was developed, in which two teams compete in a mix between football and Pong, using the physical movement of the players as main means to interact within mixed-reality. An evaluation of League of Lasers with a user study with 32 participants showed that League of Lasers is perceived as a fun and immersive skill-based game.},
  keywords={Sports;Games;Servers;Laser theory;Virtual reality;Mirrors;Human-centered computing;Mixed / augmented reality Human-centered computing;User studies Human-centered computing;User interface design Software and its engineering;Interactive games},
  doi={10.1109/VR.2019.8798275},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797806,
  author={Numan, Nels and Kolster, Ayla and Hoogerwerf, Niels and Kreynen, Bernd and Romeijnders, Jeanique and Huala, Tomas Heinsohn and Salamon, Nestor Z. and Balint, J. Timothy and Lukosch, Stephan and Bidarra, Rafael},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Star Tag: A superhuman sport to promote physical activity}, 
  year={2019},
  volume={},
  number={},
  pages={1826-1830},
  abstract={Superhuman sports (SHS) is a field where technological augmentations of human abilities and environment are combined to play a new and exciting sport. SHS make use of artificial senses or virtual reality to create a new experience that involves physical fitness and skills. Star Tag aims to combine these aspects with an engaging audience experience. This augmented reality game uses the Microsoft HoloLens, making it possible to move through a mixed reality space effectively. Star Tag is a competitive multiplayer game where players need to conquer all planets from their opponent to win the game. Players need to move around in a physical space from virtual planet to virtual planet in order to navigate the game-space and reach the planets. The audience is involved with the game via their phones, through which they can support the players. Through playtesting and conducting a survey, the results show that Star Tag is a superhuman sport that motivates people to be physically active.},
  keywords={Planets;Games;Sports;Augmented reality;Energy states;Navigation;Human-centered computing;Mixed / augmented reality Human-centered computing;User studies Human-centered computing;User interface design Software and its engineering;Interactive games},
  doi={10.1109/VR.2019.8797806},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798125,
  author={Ragozin, Kirill and Chernyshov, George and Kunze, Kai},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HeatSense – Thermal Sensory Supplementation for Superhuman Sports}, 
  year={2019},
  volume={},
  number={},
  pages={1831-1833},
  abstract={This paper presents a sensory supplementation experience in virtual reality based on the thermal and vibrotactile feedback. We have implemented a physical system that provides hot and cold sensations around the hand and forearm of the person wearing it. After being equipped with the system the user is exposed to the virtual reality environment with the goal to deflect projectiles coming their way while relying on the thermal sensations to detect them. Presented experience makes a case for using thermal sensory supplementation as an interaction modality in virtual reality environments.},
  keywords={Sports;Games;Virtual reality;Projectiles;Haptic interfaces;Thermal sensors;Visualization;Superhuman Sports;Sensory Supplementation;Haptic Feedback;Thermal Feedback;Virtual Reality},
  doi={10.1109/VR.2019.8798125},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797758,
  author={Bozkir, Efe and Geisler, David and Kasneci, Enkelejda},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Person Independent, Privacy Preserving, and Real Time Assessment of Cognitive Load using Eye Tracking in a Virtual Reality Setup}, 
  year={2019},
  volume={},
  number={},
  pages={1834-1837},
  abstract={Eye tracking is handled as key enabling technology to VR and AR for multiple reasons, since it not only can help to massively reduce computational costs through gaze-based optimization of graphics and rendering, but also offers a unique opportunity to design gaze-based personalized interfaces and applications. Additionally, the analysis of eye tracking data allows to assess the cognitive load, intentions and actions of the user. In this work, we propose a person-independent, privacy-preserving and gaze-based cognitive load recognition scheme for drivers under critical situations based on previously collected driving data from a driving experiment in VR including a safety critical situation. Based on carefully annotated ground-truth information, we used pupillary information and performance measures (inputs on accelerator, brake, and steering wheel) to train multiple classifiers with the aim of assessing the cognitive load of the driver. Our results show that incorporating eye tracking data into the VR setup allows to predict the cognitive load of the user at a high accuracy above 80%. Beyond the specific setup, the proposed framework can be used in any adaptive and intelligent VR/AR application.},
  keywords={Vehicles;Gaze tracking;Safety;Real-time systems;Roads;Graphics;Task analysis;Eye tracking;cognitive load recognition;virtual reality;driving simulation;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality, Perception;Computing methodologies—Machine learning—Machine learning approaches—Classification and regression trees, Kernel methods;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI},
  doi={10.1109/VR.2019.8797758},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798273,
  author={Dunn, David},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Required Accuracy of Gaze Tracking for Varifocal Displays}, 
  year={2019},
  volume={},
  number={},
  pages={1838-1842},
  abstract={Varifocal displays are a practical method to solve vergence-accommodation conflict in near-eye displays for both virtual and augmented reality, but they are reliant on knowing the user's focal state. One approach for detecting the focal state is to use the link between vergence and accommodation and employ binocular gaze tracking to determine the depth of the fixation point; consequently, the focal depth is also known. In order to ensure the virtual image is in focus, the display must be set to a depth which causes no negative perceptual or physiological effects to the viewer, which indicates error bounds for the calculation of fixation point. I analyze the required gaze tracker accuracy to ensure the display focus is set within the viewer's depth of field, zone of comfort, and zone of clear single binocular vision. My findings indicate that for the median adult using an augmented reality varifocal display, gaze tracking accuracy must be better than 0.541°. In addition, I discuss eye tracking approaches presented in the literature to determine their ability to meet the specified requirements.},
  keywords={Gaze tracking;Three-dimensional displays;Hardware;Optical imaging;Optical variables measurement;Augmented reality;Optical sensors;Human-centered computing;Human computer interaction (HCI);Interaction devices Hardware;Hardware validation;Functional verification Hardware;Functional verification;Coverage metrics Hardware;Robustness Human-centered computing;Interaction devices;Displays and imagers},
  doi={10.1109/VR.2019.8798273},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798030,
  author={Hotta, Katsuyoshi and Prima, Oky Dicky Ardiansyah and Imabuchi, Takashi and Ito, Hisayoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-HMD Eye Tracker in Active Visual Field Testing}, 
  year={2019},
  volume={},
  number={},
  pages={1843-1847},
  abstract={Visual field defects (VFDs) is difficult to recognize by most patients because of the filling-in mechanism in the human brain. The current visual field test displays light sources within the range of the effective visual field and takes the responses from the patient after recognizing this light stimulus. Since, these responses are determined subjectively by the patient, the resulted measure may be less reliable. This method may take more than 30 minutes, requiring the patient to fix his gaze and head where it may give a physical burden in the patient. In this study, we propose an active visual field testing (AVFT) based on a high-speed virtual reality head-mounted display (VR-HMD) eye tracker which enables to increase the testing reliability and to reduce the physical burden during the test. Our tracker runs up to 240Hz allowing the measurement of rapid eye movement to precisely detect visual fixation and saccades which provide essential elements to evaluate defects in the visual field. The characteristics of visual fixation and saccades are utilized to confirm when each stimulus is recognized by the patient during the test. Our experiment shows that each test can be conducted in 5 minutes.},
  keywords={Visualization;Testing;Calibration;Reliability;Tracking;Gaze tracking;Cameras;Active visual field testing;eye tracker;corrective saccade;VR-HMD;J.3 [Computer Applications]: Life and Medical Sciences;I.4.m [Image Processing and Computer Vision]: Miscellaneous},
  doi={10.1109/VR.2019.8798030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798107,
  author={Martschinke, Jonathan and Martschinke, Jana and Stamminger, Marc and Bauer, Frank},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Gaze-Dependent Distortion Correction for Thick Lenses in HMDs}, 
  year={2019},
  volume={},
  number={},
  pages={1848-1851},
  abstract={Common VR headsets require lenses that increase the field of view and allow the user to focus the display. In most systems, thick lenses are used that generate strong pincushion distortions. To account for this, the content is warped by a corresponding barrel distortion before displaying, resulting in undistorted images for the viewer. This approach assumes that the eye is exactly positioned, typically on the optical axis of the lens. However, in real systems the eye's location deviates from this optimal position even at rest; moreover, the pupil - and thus the optical center of the eye - moves by several millimeters when the user looks around. Thus, eye movement results in additional distortion, which is ignored in current VR headsets. Also in literature on head-mounted displays, the effect is most often not considered, or at least badly documented. The contribution of this paper are experiments that emphasize the importance of this mostly ignored effect. To this end, we have built a simple setup with a camera at variable eye positions in a standard VR headset, that allows us to directly measure the variation of distortion during eye movement. Our experiments show that distortion varies by several dozens of pixels within the full range of eye movements, which emphasizes that the effect is definitely significant. We also demonstrate how in a headset with built-in eye tracker, the knowledge of the eye position can be used to achieve a view-dependent lens correction with only minimal additional effort at run-time.},
  keywords={Distortion;Optical distortion;Lenses;Adaptive optics;Distortion measurement;Cameras;Optical imaging},
  doi={10.1109/VR.2019.8798107},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798297,
  author={Dennison, Mark S. and Krum, David M.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Unifying Research to Address Motion Sickness}, 
  year={2019},
  volume={},
  number={},
  pages={1858-1859},
  abstract={Be it discussed as cybersickness, immersive sickness, simulator sickness, or virtual reality sickness, the ill effects of visuo-vestibular mismatch in immersive environments are of great concern for the wider adoption of virtual reality and related technologies. In this position paper, we discuss a unified research approach that may address motion sickness and identify critical research topics.},
  keywords={Virtual reality;Visualization;Solid modeling;Biology;Real-time systems;Conferences;Machine learning;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798297},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797781,
  author={Ng, Adrian K. T. and Chan, Leith K. Y. and Lau, Henry Y. K.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effect of Sensory Conflict and Postural Instability on Cybersickness}, 
  year={2019},
  volume={},
  number={},
  pages={1860-1861},
  abstract={Sensory conflict theory and postural instability theory were often tested individually to explain cybersickness in VR systems, but they were seldom systematically compared. An earlier study evaluated them on a large screen using 2D videos. This study evaluated sensory conflict and postural instability on the discomfort in VR. Virtual visual locomotion were shown on an head-mounted display. A motion platform vibrated in low-frequency while the participant stood on top. Each factor was manipulated alone or in combination. Results showed that the visual motion only condition led to the highest miserable score, higher than the physical vibration only condition. This suggested that consistent with previous literature, sensory conflict may be a major contributing factor of cybersickness.},
  keywords={Visualization;Vibrations;Virtual reality;Navigation;Resists;Two dimensional displays;Videos;Motion sickness;VIMS;motion platform;HMD;visual motion;vestibular vibration;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Computing methodologies—Computer graphics—Graphics systems and interfaces—Perception},
  doi={10.1109/VR.2019.8797781},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797722,
  author={Onuki, Yoshikazu and Kumazawa, Itsuo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reorient the Gazed Scene Towards the Center: Novel Virtual Turning Using Head and Gaze Motions and Blink}, 
  year={2019},
  volume={},
  number={},
  pages={1864-1871},
  abstract={Stationary subjects exposed to compelling scene movements in virtual reality environments often experience symptoms of visually induced motion sickness (cybersickness). This study specifically focuses on virtual turning and evaluates user experience in the task of searching and collecting objects appeared around the player in the first-person view. Typical conventional control schemes for turning includes the smooth and snap turns. The smooth turn is continuous turning performed by moving a scene in the horizontal direction. In this case, reflexive eye movement occurs even in the absence of physical motion, which often causes eye strain and nausea. The snap turn achieves a prompt direction change by omitting intervening turning, which often damages the sense of reality. To address these negative effects, we propose the novel turning method inspired by the particular human behavior, which achieves to reorient the gazed view towards the center. Prompt reorientation during rapid head motion and blinking performed unnoticeable scene switching that achieved the seamless user experience, especially for the wide-angle turning. Whereas, continuous narrow-angle turning by horizontally rotating the virtual world corresponding to the face orientation achieved enhanced the sense of reality. The proposal comprises a hybrid of these two turning schemes. Experiments using simulator sickness and presence questionnaires revealed that our methods achieved comparable or lower sickness scores and higher presence scores than either of conventional turning schemes.},
  keywords={Turning;Visualization;Games;Strain;User experience;Face;Human-centered computing—Interaction paradigms—Virtual reality;Human-centered computing—Interaction design—Interaction design process and methods—Interface design prototyping},
  doi={10.1109/VR.2019.8797722},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797800,
  author={Stevens, Andrew H. and Butkiewicz, Thomas},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reducing Seasickness in Onboard Marine VR Use through Visual Compensation of Vessel Motion}, 
  year={2019},
  volume={},
  number={},
  pages={1872-1873},
  abstract={We developed a virtual reality interface for cleaning sonar point cloud data. Experimentally, users performed better when using this VR interface compared to a mouse-and-keyboard with a desktop monitor. However, hydrographers often clean data aboard moving vessels, which can create motion sickness. Users of VR experience motion sickness as well, in the form of simulator sickness. Combining the two is a worst-case scenario for motion sickness. Advice for avoiding seasickness includes focusing on the horizon or objects in the distance, to keep your frame of reference external. We explored moving the surroundings in a virtual environment to match vessel motion, to assess whether it provides similar visual cues that could prevent seasickness. An informal evaluation in a seasickness-inducing simulator was conducted, and subjective preliminary results hint at such compensation's potential for reducing motion sickness, enabling the use of immersive VR technologies aboard underway ships.},
  keywords={Visualization;Cleaning;Virtual environments;Marine vehicles;Tracking;Sonar;Human-centered computing;Virtual reality},
  doi={10.1109/VR.2019.8797800},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798213,
  author={Wang, Yuyang and Chardonnet, Jean-Rémy and Merienne, Frédéric},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR Sickness Prediction for Navigation in Immersive Virtual Environments using a Deep Long Short Term Memory Model}, 
  year={2019},
  volume={},
  number={},
  pages={1874-1881},
  abstract={This paper proposes a new objective metric of visually induced motion sickness (VIMS) in the context of navigation in virtual environments (VEs). Similar to motion sickness in physical environments, VIMS can induce many physiological symptoms such as general discomfort, nausea, disorientation, vomiting, dizziness and fatigue. To improve user satisfaction with VR applications, it is of great significance to develop objective metrics for VIMS that can analyze and estimate the level of VR sickness when a user is exposed to VEs. One of the well-known objective metrics is the postural instability. In this paper, we trained a LSTM model for each participant using a normal-state postural signal captured before the exposure, and if the postural sway signal from post-exposure was sufficiently different from the pre-exposure signal, the model would fail at encoding and decoding the signal properly; the jump in the reconstruction error was called loss and was proposed as the proposed objective measure of simulator sickness. The effectiveness of the proposed metric was analyzed and compared with subjective assessment methods based on the simulator sickness questionnaire (SSQ) in a VR environment, achieving a Pearson correlation coefficient of. 89. Finally, we showed that the proposed method had the potential to be deployed within a closed-loop system and get real-time performance to predict VR sickness, opening new insights to develop user-centered and customized VR applications based on physiological feedback.},
  keywords={Navigation;Virtual environments;Physiology;Real-time systems;Three-dimensional displays;Logic gates;Deep learning;Human-centered computing;Virtual reality;Walkthrough evaluations;User interface design;Interaction devices;Computing methodologies;Machine learning;Machine learning approaches;Neural networks},
  doi={10.1109/VR.2019.8798213},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798015,
  author={Wu, Fei and Rosenberg, Evan Suma},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Combining Dynamic Field of View Modification with Physical Obstacle Avoidance}, 
  year={2019},
  volume={},
  number={},
  pages={1882-1883},
  abstract={Motion sickness is a major cause of discomfort for users of virtual reality (VR) systems. Over the past several years, several techniques have been proposed to mitigate motion sickness, such as high-quality “room-scale” tracking systems, dynamic field of view modification, and displaying static or dynamic rest frames. At the same time, an absence of real world spatial cues may cause trouble during movement in virtual reality, and users may collide with physical obstacles. To address both of these problems, we propose a novel technique that combines dynamic field of view modification with rest frames generated from 3D scans of the physical environment. As the users moves, either physically and/or virtually, the displayed field of view can be artificially reduced to reveal a wireframe visualization of the real world geometry in the periphery, rendered in the same reference frame as the user. Although empirical studies have not yet been conducted, informal testing suggests that this approach is a promising method for reducing motion sickness and improving user safety at the same time.},
  keywords={Three-dimensional displays;Virtual environments;Dynamics;Collision avoidance;Visualization;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8798015},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797837,
  author={Zielasko, Daniel and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Non-Stationary Office Desk Substitution for Desk-Based and HMD-Projected Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1884-1889},
  abstract={The ongoing migration of HMDs to the consumer market also allows the integration of immersive environments into analysis workflows that are often bound to an (office) desk. However, a critical factor when considering VR solutions for professional applications is the prevention of cybersickness. In the given scenario the user is usually seated and the surrounding real world environment is very dominant, where the most dominant part is maybe the desk itself. Including this desk in the virtual environment could serve as a resting frame and thus reduce cybersickness next to a lot of further possibilities. In this work, we evaluate the feasibility of a substitution like this in the context of a visual data analysis task involving travel, and measure the impact on cybersickness as well as the general task performance and presence. In the conducted user study ( n=52), surprisingly, and partially in contradiction to existing work, we found no significant differences for those core measures between the control condition without a virtual table and the condition containing a virtual table. However, the results also support the inclusion of a virtual table in desk-based use cases.},
  keywords={Task analysis;Data analysis;Virtual environments;Data visualization;Visualization;Three-dimensional displays;Human-centered concepts [Human computer interaction (HCI)]: Interaction paradigms—Virtual reality;Human-centered concepts [Human computer interaction (HCI)]: Visualization—Empirical studies in visualization},
  doi={10.1109/VR.2019.8797837},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8797784,
  author={Raimbaud, Pierre and Lou, Ruding and Merienne, Frédéric and Danglade, Florence and Figueroa, Pablo and Hernández, José Tiberio},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={BIM-based Mixed Reality Application for Supervision of Construction}, 
  year={2019},
  volume={},
  number={},
  pages={1903-1907},
  abstract={Building Information Modelling (BIM) is an up-and-coming methodology and technology used in the Architecture, Engineering and Construction (AEC) industry, that allows data centralization and stakeholders' collaboration. But to check the accuracy of the work done on the worksite, it is necessary first to go on site and then to modify the BIM model. This paper presents a mixed reality (MR) application based on BIM data and drone videos, allowing off-site construction supervision. It permits to make annotations about differences between what has been planned in BIM and what has been built, using superimposition of the two sources. Then these ones can be transferred to the BIM model for corrections. Finally, we evaluate our work with building construction experts, providing to them a questionnaire to grade the application and to get feedback. Our major result is that as for them the application does really help to do construction supervisions; however, they suggest that the application should provide more interactions with the 3D model and with the videos.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Erbium;BIM;mixed reality;construction;superimposition;drone;videos;I.2.1 [Human-centered computing]: Interaction design—Interaction design process and methods;J.6.4[Computing methodologies]: Computer graphics—Graphics systems and interfaces},
  doi={10.1109/VR.2019.8797784},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{8798371,
  author={Saunders, Jonathan and Davey, Steffi and Bayerl, Petra Saskia and Lohrmann, Philipp},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Validating Virtual Reality as an Effective Training Medium in the Security Domain}, 
  year={2019},
  volume={},
  number={},
  pages={1908-1911},
  abstract={Virtual Reality (VR) training simulations are an idea which is being explored in numerous industries and professions. However, evidence purporting to the effectiveness of VR technology in relation to standard real-world exercises is still relatively thin. In this paper, we discuss our approach for validating the effectiveness of a VR training for law enforcement professionals in the context of the AUGGMED project, and present results of the validation study. Our study indicates that realistic VR-based trainings, either by themselves or in combination with the traditional hands-on training, can be as effective as highly resource-intensive practical training sessions.},
  keywords={Training;Virtual reality;Law enforcement;Solid modeling;Games;Industries;Force;Virtual Reality;Training;Evaluation;Police},
  doi={10.1109/VR.2019.8798371},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9044164,
  author={Schulz, Christoph and Rodrigues, Nils and Amann, Marco and Baumgartner, Daniel and Mielke, Arman and Baumann, Christian and Sedlmair, Michael and Weiskopf, Daniel},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Framework for Pervasive Visual Deficiency Simulation}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={We present a framework for rapid prototyping of pervasive visual deficiency simulation in the context of graphical interfaces, virtual reality, and augmented reality. Our framework facilitates the emulation of various visual deficiencies for a wide range of applications, which allows users with normal vision to experience combinations of conditions such as myopia, hyperopia, presbyopia, cataract, nyctalopia, protanopia, deuteranopia, tritanopia, and achromatopsia. Our framework provides an infrastructure to encourage researchers to evaluate visualization and other display techniques regarding visual deficiencies, and opens up the field of visual disease simulation to a broader audience. The benefits of our framework are easy integration, configuration, fast prototyping, and portability to new emerging hardware. To demonstrate the applicability of our framework, we showcase a desktop application and an Android application that transform commodity hardware into glasses for visual deficiency simulation. We expect that this work promotes a greater understanding of visual impairments, leads to better product design for the visually impaired, and forms a basis for research to compensate for these impairments as everyday help.},
  keywords={Visualization;Lenses;Retina;Cataracts;Pipelines;Visual systems;Image color analysis;Human-centered computing-Visualization-Visualization systems and tools;Human-centered computing-Accessibility-Accessibility systems and tools},
  doi={10.1109/VR44988.2019.9044164},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9044162,
  author={Koumaditis, Konstantinos and Venckute, Sarune and Jensen, Frederik S. and Chinello, Francesco},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Training: Outcomes from Small Scale AR/VR Pilot-Studies}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={From paper-based and face-to-face workshops to electronic and digital simulated educational programs, vast amounts are spent to train workers to perform assembly-tasks, especially in manufacturing. This need for robust, cost-effective, adaptable learning environments supported the rise of Immersive Training. The phenomenon of utilizing the immersive capabilities of Augmented and Virtual Reality (AR/VR) to construct learning environments, i.e. Immersive Training, has flourished, mainly due to the rise and availability of commercial headsets and applications. In this paper, we describe and analyze two such scenarios, a VR lab experiment created to test the VR capabilities in training and a field study exploring the use of AR in a large manufacturing company. Our results provide a) guidelines on how to set up such experiments, b) indicate the temporal demand, and accuracy as significant improvements in VR and c) indicate the effort, physical demand and the time performance in AR.},
  keywords={Training;Task analysis;Glass;Manufacturing;Companies;Games;Indexes;Virtual Reality;Augmented Reality;Immersive Training;Cognitive Workload;NASA-TLX},
  doi={10.1109/VR44988.2019.9044162},
  ISSN={2642-5254},
  month={March},}
