@INPROCEEDINGS{10937341,
  author={Li, Ziming and Zhang, Huadong and Peng, Chao and Peiris, Roshan},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Large Language Model-Driven Agents for Environment-Aware Spatial Interactions and Conversations in Virtual Reality Role-Play Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  abstract={Recent research has begun adopting Large Language Model (LLM) agents to enhance Virtual Reality (VR) interactions, creating immersive chatbot experiences. However, while current studies focus on generating dialogue from user speech inputs, their abilities to generate richer experiences based on the perception of LLM agents’ VR environments and interaction cues remain unexplored. Hence, in this work, we propose an approach that enables LLM agents to perceive virtual environments and generate environment-aware interactions and conversations for an embodied human-AI interaction experience in VR environments. Here, we define a schema for describing VR environments and their interactions through text prompts. We evaluate the performance of our method through five role-play scenarios created using our approach in a study with 14 participants. The findings discuss the opportunities and challenges of our proposed approach for developing environment-aware LLM agents that facilitate spatial interactions and conversations within VR role-play scenarios.},
  keywords={Solid modeling;Ethics;Translation;Three-dimensional displays;Generative AI;Large language models;Virtual environments;Oral communication;User interfaces;Reliability;Virtual reality;role-play simulations;generative AI;human-AI interaction;large language models;context-awareness},
  doi={10.1109/VR59515.2025.00025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937416,
  author={Xu, Xuanhui and Puggioni, Antonella and Kilroy, David and Campbell, Abraham G.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality and Real-Time X-Ray Simulation in Vet Radiography Training: A User-Centered Comparative Study}, 
  year={2025},
  volume={},
  number={},
  pages={12-22},
  abstract={Proficient training in Diagnostic Imaging (DI) is crucial for veterinary students. However, the training process faces challenges, including limited access to animals and strict radiation safety regulations. Can Mixed Reality (MR) address these issues? Two co-located collaborative MR teaching systems were implemented to simulate the horse DI procedure with real-time X-ray simulation. One system integrated actual X-ray equipment, while the other was entirely virtual. A user-centered comparative study was conducted, assessing students’ performance and opinions to explore the differences and the necessity of physical versus virtual equipment. The performance of 22 veterinary students and four DI experts was evaluated, focusing on the quality of the readiographs and the time taken to obtain the X-ray images. Students initially underperformed compared to experts using physical equipment but matched expert performance by the final image, while they performed similarly with virtual equipment. Experts took X-rays faster than students in both cases. Both systems offered comparable user experiences, though most preferred virtual equipment for its usability and presentation. Physical equipment remained essential for realistic simulation. The results show that the MR system, combined with real-time X-ray image synthesis, has great potential to overcome obstacles in veterinary DI training and serve as a support tool for both training and assessment.},
  keywords={Training;Radiography;Three-dimensional displays;Mixed reality;Virtual reality;User interfaces;Real-time systems;Regulation;Usability;X-ray imaging;Mixed Reality;Hybrid Reality environment;Co-location;Tangible user interface;Radiography},
  doi={10.1109/VR59515.2025.00026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937462,
  author={Giovannelli, Alexander and Pavanatto, Leonardo and Davari, Shakiba and Miao, Haichao and Chheang, Vuthea and Giera, Brian and Bremer, Timo and Bowman, Doug A.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Influence of Playback Interactivity during Guided Tours for Asynchronous Collaboration in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={23-33},
  abstract={Collaborative virtual environments allow workers to contribute to team projects across space and time. While much research has closely examined the problem of working in different spaces at the same time, few have investigated the best practices for collaborating in those spaces at different times aside from textual and auditory annotations. We designed a system that allows experts to record a tour inside a virtual inspection space, preserving knowledge and providing later observers with insights through a 3D playback of the expert’s inspection. We also created several interactions to ensure that observers are tracking the tour and remaining engaged. We conducted a user study to evaluate the influence of these interactions on an observing user’s information recall and user experience. Findings indicate that independent viewpoint control during a tour enhances the user experience compared to fully passive playback and that additional interactivity can improve auditory and spatial recall of key information conveyed during the tour.},
  keywords={Three-dimensional displays;Federated learning;Annotations;Collaboration;Virtual environments;Inspection;Observers;User interfaces;User experience;Best practices;Virtual reality;computer-supported cooperative work;collaborative virtual environments;asynchronous collaboration;guided tours},
  doi={10.1109/VR59515.2025.00027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937393,
  author={Hong, Siyuan and Wang, Ruiqi and Cao, Guohong},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Adaptive 360-Degree Video Streaming with Super-Resolution and Interpolation}, 
  year={2025},
  volume={},
  number={},
  pages={34-43},
  abstract={360° video streaming requires considerable bandwidth, and many techniques have been proposed to address this problem. One such technique is super-resolution, where the video is compressed at the server, and the client runs a deep learning model to enhance the video quality. However, most of today’s off-the-shelf mobile devices cannot support super-resolution for all tiles in real time. As a result, some tiles cannot be reconstructed to high resolution, significantly reducing users’ Quality of Experience (QoE). To address this problem, we utilize linear interpolation, which requires much less computational overhead. Through experiments, we observe that interpolation can achieve comparable quality, and even outperform super-resolution for some tiles with low spatial complexity. Building on this, we develop a 360° video streaming system that adaptively selects the most suitable downloading strategy, whether interpolation, super-resolution, or ABR at the appropriate bitrate, for each tile to maximize user QoE while considering network bandwidth limitations and the computational constraints of mobile devices. We formalize the 360° video streaming problem as an optimization problem and propose an efficient algorithm to solve it. Extensive evaluations using real user viewing data and 5G network traces demonstrate that our solution significantly outperforms existing techniques in terms of QoE under various scenarios.},
  keywords={Interpolation;Tiles;Superresolution;Bit rate;Bandwidth;Streaming media;Mobile handsets;Quality assessment;Quality of experience;Video recording},
  doi={10.1109/VR59515.2025.00028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937418,
  author={Franck, Mathurin and Lefebvre, Maëlis and Roy, Raphaëlle N. and Peysakhovich, Vsevolod},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual teleoperation performance under load factor in real flight}, 
  year={2025},
  volume={},
  number={},
  pages={44-51},
  abstract={Manned-Unmanned Teaming (MUM-T) represents the future of aerial operations by enabling an operator aboard a manned aircraft to control one or more Unmanned Aerial Vehicles (UAVs). As MUM-T interfaces are still developing, understanding how dynamic environments affect operators’ perception and control performance is essential. This study investigates the effect of load factor and visual perspective on teleoperation performance, workload, and motion sickness. Fourteen participants completed a simulated teleoperation task onboard a light Robin DR400 aircraft, subjected to turn angles of 30°, 45°, and 60° (load factors of 1.2, 1.4, and 2g respectively). Preliminary results reveal a significant increase in mental and physical demands under higher load factors without affecting teleoperation performance. No difference was observed between the two visual perspectives tested (first person and third person). This study shows that teleoperators can maintain their performance in real flight conditions at the cost of a higher subjective workload.},
  keywords={Visualization;Teleoperators;Three-dimensional displays;Protocols;Dynamics;Virtual reality;Motion sickness;User interfaces;Aircraft;Vehicle dynamics;Teleoperation;Load factor;In-flight experimentation;Interface;Perspective},
  doi={10.1109/VR59515.2025.00029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937413,
  author={Hu, Xuning and Xu, Wenxuan and Wei, Yushi and Zhang, Hao and Huang, Jin and Liang, Hai-Ning},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimizing Moving Target Selection in VR by Integrating Proximity-Based Feedback Types and Modalities}, 
  year={2025},
  volume={},
  number={},
  pages={52-62},
  abstract={Proximity-based feedback provides users with real-time guidance as they approach an interaction goal. This type of feedback is particularly useful for tasks that require guidance during the interaction process, such as selecting moving targets. This work explores proximity-based feedback types and modalities to improve the selection of moving targets in VR by leveraging three feedback types that combine visual, auditory, and haptic modalities. We evaluated the performance of these mechanisms through two user studies, analyzing both objective data (e.g., selection time, error rate) and subjective data (e.g., user experience, preferences) to explore the characteristics of feedback types across different modalities and to examine the roles of various modalities within multimodal combinations. Our findings suggest optimal selection mechanisms for developers and should be tailored to different goals: achieving user precision, enabling quick movement to a target, considering task duration, and enhancing entertainment value. We also discuss applications that correspond to these different perspectives.},
  keywords={Visualization;Three-dimensional displays;Error analysis;Entertainment industry;Virtual reality;User interfaces;User experience;Real-time systems;Haptic interfaces;Virtual Reality;Moving target selection;Multi-modal interaction and perception;Feedback Mechanism},
  doi={10.1109/VR59515.2025.00030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937445,
  author={Eckhoff, Daniel and Schnupp, Jan and Wong, Pui Chung and Cassinelli, Alvaro},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={When Flames Feel Real in Augmented Reality: Effects of Plausibility and Placement of Virtual Flames on the Burning Hand Illusion and Physiological Responses}, 
  year={2025},
  volume={},
  number={},
  pages={63-71},
  abstract={In the Burning Hand Illusion (BHI), participants perceive illusory heat as they observe virtual flames on their hands in Augmented Reality. In this study, we investigate the role of stimulus location within peri-hand space and its resemblance to a realistic fire in the emergence of the illusion. Additionally, we examine the associated autonomic nervous responses, quantified through skin conductance and pupil dilation responses. We used the following four conditions: (A) Realistic flames on the right hand paired with the sound of a burning torch; (B) Green, flame-like visuals on the hand paired with a sound that does not evoke fire but is spectrally matched to the sound of the burning torch; (C) Realistic flames between the hands; and (D) Green, flame-like visuals between the hands. Sixteen out of twenty-five participants reported feeling heat in condition (A), while seven reported a sensation of warmth in condition (C). Condition (B) introduced ambiguity, as participants reported warmth as well as cold sensations, but overall, fewer participants reported thermal illusions. No participants reported thermal illusions in (D). These findings suggest that both the location and resemblance of the flames significantly influence the emergence of the BHI, with the response diminishing as the flames become less plausible or not placed on the actual hand. Skin conductance responses were highest in condition (A) and lower in condition (C), with the lowest responses observed in conditions (B) and (D). Pupil dilation responses were largest in conditions where the flames were directly on the hand. The observed autonomic responses reinforce the notion that the BHI elicits physiological effects that parallel those of real thermal stimuli. This provides evidence that the virtual fire in the BHI is sufficiently plausible to be perceived as affecting the hand, and is consequently processed by the brain’s homeostatic emotional systems.},
  keywords={Hands;Heating systems;Visualization;Three-dimensional displays;Fires;User interfaces;Skin;Physiology;Pupils;Augmented reality;Augmented Reality;Cross-Modal Illusion;Multi-sensory Integration;Autonomic Responses},
  doi={10.1109/VR59515.2025.00031},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937457,
  author={Baldoni, Sara and Benhamadi, Salim and Chiariotti, Federico and Zorzi, Michele and Battisti, Federica},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Movement- and Traffic-based User Identification in Commercial Virtual Reality Applications: Threats and Opportunities}, 
  year={2025},
  volume={},
  number={},
  pages={72-81},
  abstract={With the unprecedented diffusion of virtual reality, the number of application scenarios is continuously growing. As commercial and gaming applications become pervasive, the need for the secure and convenient identification of users, often overlooked by the research in immersive media, is becoming more and more pressing. Networked scenarios such as Cloud gaming or cooperative virtual training and teleoperation require both a user-friendly and streamlined experience and user privacy and security. In this work, we investigate the possibility of identifying users from their movement patterns and data traffic traces while playing four commercial games, using a publicly available dataset. If, on the one hand, this paves the way for easy identification and automatic customization of the virtual reality content, it also represents a serious threat to users’ privacy due to network analysis-based fingerprinting. Based on this, we analyze the threats and opportunities for virtual reality users’ security and privacy.},
  keywords={Training;Privacy;Accuracy;Three-dimensional displays;Virtual reality;Pressing;Fingerprint recognition;User interfaces;Security;Reliability;Virtual Reality;User Identification;Traffic Analysis;Movement Patterns},
  doi={10.1109/VR59515.2025.00032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937475,
  author={Schlesener, Elizabeth A. and Shivakumar, Vyomakesh and Breeze, Dave and Rennison, Brooke and Soehmelioglu, Burak and Babu, Sabarish V.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={‘Age Isn’t Just a Number’: Effects of Virtual Human Age and Gender on Persuasion, Social Presence and Influence in Interpersonal Social Encounters in VR}, 
  year={2025},
  volume={},
  number={},
  pages={82-92},
  abstract={As virtual reality (VR) technology continues to gain traction as a powerful tool for persuasion and marketing, its application in immersive sales environments has become increasingly relevant. VR’s ability to simulate realistic interactions allows for novel approaches to influencing consumer behavior, particularly through the use of interactive Virtual Humans (VHs). In this study, we investigate the effects of user and virtual human age and gender on persuasion and social influence, via various social psychological factors such as brand attitude, interpersonal attraction, source credibility, and presence. In the process, we conducted a 3 (virtual agent age) × 2 (virtual agent gender) × 3 (module topic) multi-factorial design (N=45). The study included two agent gender conditions, namely gender-matched and gender-mismatched with the user, assigned in a between-subjects manner, and three agent age conditions, namely (1) young adult (20-39), (2) middle-aged adult (40-59), and (3) older adult (60-79). Our findings indicate that the age and gender of virtual humans significantly influence their persuasiveness and social impact, which varies based on the user’s own age and gender. Specifically, virtual humans of a different gender from the user may exhibit higher levels of persuasion and social influence. Furthermore, female and older users appear to be more susceptible to influence compared to male and younger users. Our findings provide novel, critical insights for designing more effective virtual sales strategies and tailoring persuasive technologies to diverse user profiles, thus filling a vital gap in understanding the interaction between user and agent characteristics in virtual environments.1},
  keywords={Three-dimensional displays;Consumer behavior;Psychology;Virtual reality;User interfaces;Filling;Older adults;Virtual Humans;Persuasion;Social Presence;Social Influence;Virtual Reality},
  doi={10.1109/VR59515.2025.00033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937427,
  author={Xu, Wenxuan and Wei, Yushi and Hu, Xuning and Stuerzlinger, Wolfgang and Wang, Yuntao and Liang, Hai-Ning},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Predicting Ray Pointer Landing Poses in VR Using Multimodal LSTM-Based Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={93-103},
  abstract={Target selection is one of the most fundamental tasks in VR interaction systems. Prediction heuristics can provide users with a smoother interaction experience in this process. Our work aims to predict the ray landing pose for hand-based raycasting selection in Virtual Reality (VR) using a Long Short-Term Memory (LSTM)-based neural network with time-series data input of speed and distance over time from three different pose channels: hand, Head-Mounted Display (HMD), and eye. We first conducted a study to collect motion data from these three input channels and analyzed these movement behaviors. Additionally, we evaluated which combination of input modalities yields the optimal result. A second study validates raycasting across a continuous range of distances, angles, and target sizes. On average, our technique’s predictions were within 4.6° of the true landing Pose when 50% of the way through the movement. We compared our LSTM neural network model to a kinematic information model and further validated its generalizability in two ways: by training the model on one user’s data and testing on other users (cross-user) and by training on a group of users and testing on entirely new users (unseen users). Compared to the baseline and a previous kinematic method, our model increased prediction accuracy by a factor of 3.5 and 1.9, re spectively, when 40% of the way through the movement.},
  keywords={Training;Solid modeling;Neural networks;Kinematics;Virtual reality;Resists;Predictive models;User interfaces;Data models;Testing;Index Terms: Virtual Reality;Pointing Selection;Modeling;Neural Networks},
  doi={10.1109/VR59515.2025.00034},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937399,
  author={Ke, Shaoteng and Zheng, Lijie and Lee, Boon Giin},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Influence of Interpersonal Relationships on Gamification Preferences in Collaborative IVR Environments}, 
  year={2025},
  volume={},
  number={},
  pages={104-114},
  abstract={Recent advancements in immersive virtual reality (IVR) have highlighted its expanding potential in educational settings. However, the influence of learners’ interpersonal relationships on their preferences for gamification elements in collaborative environments remains underexplored. This study introduces an avatar-based multiplayer IVR game designed to promote collaborative learning through improved social interaction. The game features a maze with three challenging puzzles, which require teamwork and problem-solving skills, encouraging communication and information sharing among participants. In addition, the study investigates the impact of different gamification elements on groups consisting of either peers or strangers, involving a total of 44 participants. The findings revealed significant improvements in communication and collaboration skills, along with increased motivation and engagement, across both group types. Moreover, participants exhibited distinct preferences for specific gamification elements, largely influenced by the nature of their social relationships and individual tendencies. This study offers valuable insights into the design of IVR-based collaborative educational environments with an emphasis on social interaction and game mechanics. It also identifies key factors shaping individual preferences for gamification elements, particularly in relation to social dynamics and personal preferences.},
  keywords={Human computer interaction;Three-dimensional displays;Federated learning;Buildings;Information sharing;Games;Virtual reality;Teamwork;Problem-solving;Creativity;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing Human computer interaction (HCI);Collaborative interaction},
  doi={10.1109/VR59515.2025.00035},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937439,
  author={Gloumeau, Paul Christopher and Pettifer, Stephen Robert},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Intuitive Visual Feedback in Virtual Reality for the Teleoperation of Robots}, 
  year={2025},
  volume={},
  number={},
  pages={115-124},
  abstract={Although the benefits of teleoperation are well known, the current implementations require high levels of skill which exclude the average user. One of the difficulties is the delay between sending a command and the movement of the robot which can impact an operator’s ability to safely, and effectively perform tasks. Furthermore, the rejection of commands due to unattainable goals (e.g. outside the robot’s joint limits) can leave the operator confused and frustrated. Advancements in research, such as the use of virtual reality (VR) and inverse kinematic techniques are steadily making robot control more feasible for everyday users, but there is still a lack of understanding of what the robot can or cannot do based on its limits and environment. A benefit of VR over other display methods is the ability to customise the immersive environment that the robot is in. In this paper we take advantage of that benefit to create three visual feedback techniques which give the user information on their commands. A user study was conducted comparing our techniques. The results show that higher levels of visual feedback result in quicker task completion times, lower mental workload and a better user experience.},
  keywords={Visualization;Three-dimensional displays;Robot control;Virtual reality;User interfaces;User experience;Planning;Trajectory;Collision avoidance;Robots;Virtual Reality;Teleoperation;3D User Interfaces},
  doi={10.1109/VR59515.2025.00036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937390,
  author={Sakha, Muhammad Moiz and Daiber, Florian and Enders, Matthias and Tieben, Christoph and Kisliuk, Benjamin and Krüger, Antonio},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enhancing Plant Variety Discovery Process with Visual Trait Assessment in VR}, 
  year={2025},
  volume={},
  number={},
  pages={125-134},
  abstract={Plant breeders use field trials across locations and years to identify superior plant varieties with traits such as disease resistance and higher yield. However, comparing breeding candidates across locations and years is challenging and resource demanding. To address this, we developed an integrated system that combines data acquisition through a field robot with an immersive virtual reality (VR) interface for remote assessments. The robot autonomously collects images, spectral data and 3D scans of canola breeding trials. Our VR application, developed through a user-centered approach, offers photo-realistic 3D visualizations, enabling breeders to compare candidates across locations and growth stages—capabilities unavailable in field assessments. In a user study, five breeders conducted visual trait scoring in VR to evaluate how well the system supported typical field trial tasks. The results demonstrated consistent scoring patterns among raters. Feedback from breeders indicated that the ability to compare candidates across locations and growth stages enhanced their decision making in trait assessment. This work highlights the potential of combining robotics and VR to transform data-intensive processes in agriculture.},
  keywords={Visualization;Three-dimensional displays;Plants (biology);Decision making;Data acquisition;Prototypes;Data visualization;Virtual reality;Agriculture;Robots;VR in Agriculture;Immersive Visualization;Agricultural Robotics;Plant Breeding},
  doi={10.1109/VR59515.2025.00037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937408,
  author={Laouénan, Gaspard and Didier, Jean-Yves and Dossou, Paul-Eric},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Performance and ergonomics of automated versus manual validation for AR-supervised industrial operations}, 
  year={2025},
  volume={},
  number={},
  pages={135-145},
  abstract={Augmented Reality has proven to be a viable solution compared to paper instructions for the supervision of maintenance procedures in industrial contexts to reduce error rates and increase overall performance. With the use of computer vision and cyber physical systems, the validation of specific tasks in AR-supervised environments can be automated to increase process quality even more. However, in the context of human-centered approaches such as industry 5.0, the impact of such methods on operator’s acceptability has yet to be estimated. Therefore, the paper presents a quantitative study on 24 participants to compare 4 modalities of validation of operational procedures. SCRAM, a web application based on WebXR, is developed for the supervision of maintenance operations to compare 2 types of devices: tablets and AR headsets. All scenarios are compared on an assembly use case based on performance criteria and subjective questionnaires such as NASATLX, System Usability Scale (SUS) and Technology Acceptance Model to measure acceptability. In the end, results mostly show significant differences in completion times, usability, and perceived ease of use of the SCRAM application regarding the choice of the device. On the other hand, the study does not highlight any significant difference between scenarios with automated validation of operations and scenarios with manual validation.},
  keywords={Performance evaluation;Headphones;Three-dimensional displays;Technology acceptance model;User interfaces;Maintenance;Usability;Fifth Industrial Revolution;Augmented reality;Assembly;Augmented reality;WebXR;HoloLens 2;Usability;Acceptance;Maintenance;Assembly;Industry 5.0},
  doi={10.1109/VR59515.2025.00038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937446,
  author={Zhang, Jingyi and Steed, Anthony},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Single Actor Controlling Multiple Avatars for Social Virtual Realities}, 
  year={2025},
  volume={},
  number={},
  pages={146-155},
  abstract={Social virtual reality applications aim to provide immersive, interactive experiences in populated environments with virtual characters. However, developing characters capable of natural verbal and non-verbal interactions remains a significant challenge, particularly when it comes to managing complex and unexpected interactions with users. To address these issues, we present a system that supports full-body avatars with six-point tracking and a streamlined switch control procedure, allowing one actor to assume control of multiple virtual humans and interact seamlessly with the users. The system supports both verbal and non-verbal interactions. In an experiment, we showed that our system enhances the sense of co-presence, creating the feeling that multiple distinct, human-controlled characters are present in the scene.},
  keywords={Human computer interaction;Three-dimensional displays;Avatars;Switches;Control systems;Systems support;Human computer interaction;Virtual reality;Social virtual reality},
  doi={10.1109/VR59515.2025.00039},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937412,
  author={DeGuzman, Jasmine Joyce and Hirano, Kaori and Peck, Tabitha and Guth, Alice and Rosenberg, Evan Suma and Nie, Tongyu},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reduction of Motion Complexity as an Objective Indicator of Cybersickness in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={156-164},
  abstract={Subjective measures, such as the Simulator Sickness Questionnaire (SSQ), Fast Motion Sickness Questionnaire (FMS), and discomfort scores, are widely used to assess cybersickness, but they often interrupt the user experience and are prone to bias. To overcome these limitations, researchers have also investigated objective indicators, though some approaches, such as using physiological data, can be cumbersome and impractical. Based on the loss of complexity hypothesis, which suggests that certain conditions, such as disease or aging, can produce a reduction of complexity in physiological system dynamics, we conducted an initial investigation of the relationship between movement complexity and cybersickness. We analyzed motion tracking collected from two previous cybersickness studies using the d95 score, a complexity metric derived using principal component analysis. The results revealed a systematic relationship between movement complexity and cybersickness across both experiments. Higher discomfort scores were associated with a reduction in complexity, thereby supporting the loss of complexity hypothesis. Furthermore, the 9-DOF complexity measure, which includes both physical head movement and virtual camera motion, was a more sensitive indicator than the 6-DOF measure computed from physical movements alone. These initial findings suggest that movement complexity may be a useful objective indicator for future cybersickness research.},
  keywords={Cybersickness;Tracking;Virtual environments;User interfaces;Cameras;Physiology;6-DOF;User experience;Complexity theory;Velocity measurement;Cybersickness;virtual reality;motion complexity},
  doi={10.1109/VR59515.2025.00040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937414,
  author={Kim, ByungMin and Han, DongHeun and Kang, HyeongYeop},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shaping the Future of VR Hand Interactions: Lessons Learned from Modern Methods}, 
  year={2025},
  volume={},
  number={},
  pages={165-174},
  abstract={In virtual reality (VR), it is widely assumed that increased realism in hand-object interactions enhances user immersion and overall experience. However, recent studies challenge this assumption, suggesting that faithfully replicating real-world physics and visuals is not always necessary for improved usability or immersion. This has led to ambiguity for developers when choosing optimal hand interaction methods for different applications. Currently, there is a lack of comprehensive research to resolve this issue. This study aims to fill this gap by evaluating three contemporary VR hand interaction methods—Attachment, Penetration, and Torque—across two distinct task scenarios: simple manipulation tasks and more complex, precision-driven tasks. By examining key technical features, we identify the strengths and limitations of each method and propose development guidelines for future advancements. Our findings reveal that while Attachment, with its simplified control mechanisms, is well-suited for commercial applications, Penetration and Torque show promise for next-generation interactions. The insights gained from our study provide practical guidance for developers and researchers seeking to balance realism, usability, and user satisfaction in VR environments.},
  keywords={Hands;Visualization;Torque;Force;Virtual reality;Fatigue;User experience;Usability;Next generation networking;Guidelines;Human-Computer Interaction(HCI);hand-object manipulation;hand manipulation;object manipulation},
  doi={10.1109/VR59515.2025.00041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937394,
  author={Asish, Sarker M. and Karki, Bhoj B. and KC, Bharat and Kolahchi, Niloofar and Sutradhar, Shaon},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Synthesizing Six Years of AR/VR Research: A Systematic Review of Machine and Deep Learning Applications}, 
  year={2025},
  volume={},
  number={},
  pages={175-185},
  abstract={Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), when combined with machine learning (ML) and deep learning (DL), represent both challenging and promising research areas. However, there is currently a lack of comprehensive surveys reviewing their contributions. In this review paper, we present a thorough analysis of the most recent research on AR/VR/MR applications with ML and DL models in the IEEE VR and ISMAR conferences from 2018 to 2023. Our literature review process, which involved multiple filtering steps, resulted in 154 relevant publications focusing on ML/DL. The paper covers a broad spectrum of topics, including object recognition, tracking, segmentation, depth estimation, 3D reconstruction, and interactive systems. We highlight the significant contributions of ML/DL and their potential impact on the AR/VR/MR fields and provide a curated list of publicly available datasets 1 from AR/VR/MR environments to support further research. This review offers a valuable resource for researchers and practitioners interested in the latest advancements and future directions in ML/DL applications within AR/VR/MR technologies. Additionally, we discuss emerging research trends and challenges, providing insights into the opportunities for future work in these fields.},
  keywords={Deep learning;Surveys;Solid modeling;Analytical models;Three-dimensional displays;Mixed reality;User interfaces;Market research;Augmented reality;Systematic literature review;Virtual Reality;Augmented Reality;Mixed Reality;Survey;Machine Learning;Deep Learning;Review;General Literature},
  doi={10.1109/VR59515.2025.00042},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937471,
  author={Pavanatto, Leonardo and Grubert, Jens and Bowman, Doug A.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatial Bar: Exploring Window Switching Techniques for Large Virtual Displays}, 
  year={2025},
  volume={},
  number={},
  pages={186-194},
  abstract={Virtual displays provided through head-worn displays (HWDs) offer users large screen space for productivity, but managing this space effectively presents challenges. Existing research shows that while increased screen space improves performance, it can also introduce significant window management overhead. This paper explores how to enhance window-switching strategies for virtual displays by leveraging eye tracking provided by HWDs and underutilized spaces around the main display area. We investigate the efficiency and usability of different cursor behaviors and selection modes in a Spatial Bar interface for window-switching tasks in augmented reality environments. Our study involved two primary selection modes: Gaze and Cursor, each tested with two cursor behaviors: Teleport and Stay. We measured objective performance metrics, including task completion time and error rates, and subjective evaluations using the NASA TLX and custom questionnaires. Results show that Cursor Stay, while familiar and comfortable for participants, was laborious and led to longer task completion times, particularly over large distances. Gaze Teleport led to the quickest window switching times, particularly in tasks where the original cursor position or the target window were far from the Spatial Bar.},
  keywords={Productivity;Three-dimensional displays;Head-mounted displays;NASA;Measurement uncertainty;Switches;Particle measurements;Time measurement;Usability;Bars;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Empirical studies in interaction design},
  doi={10.1109/VR59515.2025.00043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937397,
  author={Rendle, Gareth and Kreskowski, Adrian and Froehlich, Bernd},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={On Virtual Pointing Rays and Motion Artefacts}, 
  year={2025},
  volume={},
  number={},
  pages={195-205},
  abstract={Virtual pointing rays are a commonly employed interaction metaphor allowing users of mixed reality applications to select out-of-reach virtual objects. However, fast hand movements can cause the visual representation of the ray to move quickly across the visual field, leading to motion artefacts that cause multiple copies of the ray to appear in discrete locations. In this work, we conduct a user study to investigate whether the conditions for motion artefact occurrence arise during a selection task that participants undertake with a pointing ray. We analyse the recorded gaze and ray movement data from the study to show that pointing actions create the conditions necessary for motion artefacts to appear. In addition, we propose a motion blur effect that we apply to the pointing ray with the aim of mitigating motion artefacts. The blur effect is evaluated in the aforementioned user study and is found to improve the perceived smoothness of ray selection, without affecting selection performance.},
  keywords={Hands;Visualization;Three-dimensional displays;Mixed reality;Virtual reality;User interfaces;Motion artifacts;Pointing Ray;Virtual Reality;Motion Artifacts},
  doi={10.1109/VR59515.2025.00044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937441,
  author={Chen, Junlong and Grubert, Jens and Kristensson, Per Ola},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analyzing Multimodal Interaction Strategies for LLM-Assisted Manipulation of 3D Scenes}, 
  year={2025},
  volume={},
  number={},
  pages={206-216},
  abstract={As more applications of large language models (LLMs) for 3D content in immersive environments emerge, it is crucial to study user behavior to identify interaction patterns and potential barriers to guide the future design of immersive content creation and editing systems which involve LLMs. In an empirical user study with 12 participants, we combine quantitative usage data with post-experience questionnaire feedback to reveal common interaction patterns and key barriers in LLM-assisted 3D scene editing systems. We identify opportunities for improving natural language interfaces in 3D design tools and propose design recommendations. Through an empirical study, we demonstrate that LLM-assisted interactive systems can be used productively in immersive environments.},
  keywords={Three-dimensional displays;Large language models;Interactive systems;Natural languages;Virtual reality;User interfaces;Design tools;Virtual reality;large language models;3D scene editing},
  doi={10.1109/VR59515.2025.00045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937444,
  author={Aziz, Samantha and Komogortsev, Oleg},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Uncoordinated Privacy Protections of Eye Tracking and VR Motion Data for Unauthorized User Identification}, 
  year={2025},
  volume={},
  number={},
  pages={217-227},
  abstract={Virtual reality (VR) sensors capture large amounts of user data, including body motion and eye tracking, that contain personally identifying information. While privacy-enhancing techniques can obfuscate this data, incomplete privacy protections risk privacy leakage, which may allow adversaries to leverage unprotected data to identify users without consent. This work examines the extent to which unprotected body motion data can undermine privacy protections for eye tracking data, and vice versa, to enable user identification in VR. These findings highlight a privacy consideration at the intersection of eye tracking and VR, and emphasize the need for privacy protections that address these technologies comprehensively.},
  keywords={Data privacy;Target tracking;Three-dimensional displays;Gaze tracking;Virtual reality;User interfaces;Sensors;Synchronization;Protection;Streams;privacy;eye tracking;virtual reality;biometrics;user profiling;user identification},
  doi={10.1109/VR59515.2025.00046},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937442,
  author={Pavanatto, Leonardo and Giovannelli, Alexander and Giera, Brian and Bremer, Timo and Miao, Haichao and Bowman, Doug A.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Multiscale Navigation of Homogeneous and Dense Objects with Progressive Refinement in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={228-237},
  abstract={Locating small features in a large, dense object in virtual reality (VR) poses a significant interaction challenge. While existing multiscale techniques support transitions between various levels of scale, they are not focused on handling dense, homogeneous objects with hidden features. We propose a novel approach that applies the concept of progressive refinement to VR navigation, enabling focused inspections. We conducted a user study where we varied two independent variables in our design, navigation style (STRUCTURED vs. UNSTRUCTURED) and display mode (SELECTION vs. EVERYTHING), to better understand their effects on efficiency and awareness during multiscale navigation. Our results showed that unstructured navigation can be faster than structured and that displaying only the selection can be faster than displaying the entire object. However, using an everything display mode can support better location awareness and object understanding.},
  keywords={Human computer interaction;Location awareness;Three-dimensional displays;Navigation;Virtual reality;Inspection;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality; Human-centered computing;Empirical studies in interaction design},
  doi={10.1109/VR59515.2025.00047},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937409,
  author={Guo, Zixuan and Shi, Yuekai and Ye, Tiantian and Wan, Tingjie and Liang, Hai-Ning},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={No More Head-Turning: Exploring Passthrough Techniques for Addressing Rear Interruptions from the Front in VR}, 
  year={2025},
  volume={},
  number={},
  pages={238-248},
  abstract={Virtual reality (VR) users often encounter interruptions, posing challenges to maintaining real-world awareness during immersive experiences. The Passthrough feature in VR headsets allows users to view their physical surroundings without removing the headset. However, when interruptions come from the rear, users need to turn their heads to see the real world, which can lead to negative experiences in VR. Study 1, conducted through semi-structured interviews involving 13 participants, found that users are less likely to use Passthrough for rear interruptions due to large head-turning movements, which cause inconvenience, increase the risk of motion sickness, and reduce the experience. Building on these findings, we introduced three Passthrough techniques in Study 2 for displaying the rear view in front of the user: Full Rear Passthrough + Pause (FRPP), Rear Passthrough Window (RPW), and Rear Passthrough AR (RPAR). Compared to the Baseline method that requires head-turning, all three systems reduced physical and temporal demands, alleviated disorientation caused by motion sickness, and provided a better user experience for managing rear interruptions. Among these, FRPP and RPAR were the most preferred. These findings provide valuable insights for future VR design, emphasizing the need for solutions that effectively manage rear interruptions while maintaining user comfort and experience.},
  keywords={Headphones;Three-dimensional displays;Buildings;Virtual reality;Motion sickness;User interfaces;Turning;User experience;Windows;Interviews;Virtual Reality;Interruptions;Rear Awareness;Passthrough},
  doi={10.1109/VR59515.2025.00048},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937449,
  author={Young, Jacob and Wood, Matthew and Pantidi, Nadia and Carroll, Dene and Crampton, James and Atkins, Cliff},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={From Picks to Pixels: An Exploration of Virtual Reality in Geoscience Education}, 
  year={2025},
  volume={},
  number={},
  pages={249-257},
  abstract={In this work we present our system for teaching practical geology field skills through a combination of 360° video, photogrammetry, and virtual content. The system was evaluated with first- and second-year undergraduate geoscience students to determine if it was effective in teaching practical skills that could be transferred to the real world. Second-year students who had performed the task before saw a significant improvement in their abilities, however this improvement was absent in the first-year students, suggesting the tool may be more effective for revision rather than first-time learning. We discuss these findings and their implications for future virtual training tools, as well as the challenges in developing and deploying such systems in a university environment.},
  keywords={Training;Three-dimensional displays;Geology;Virtual reality;User interfaces;Photogrammetry;Immersive Education;Virtual Reality},
  doi={10.1109/VR59515.2025.00049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937392,
  author={Ma, Sizhuo and Bayer, Karl and Krishnan, Gurunandan and Gupta, Mohit and Nayar, Shree},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Privacy-Enabled Parallax Display}, 
  year={2025},
  volume={},
  number={},
  pages={258-267},
  abstract={Privacy filters for displays are designed to obfuscate or hide visual content from unintended observers, while making the displayed information visible only to selected viewers. Existing privacy filters suffer from either wide viewing field (low selectivity) or limited user positioning. To solve this dilemma, we propose a display technology that allows a narrow but adaptive viewing field that can be directed to arbitrary user location. While conventional parallax barriers provide such capability of modulating the light field according to the user location, it suffers from repeated views. Our key observation is that this view repetition originates from the periodicity of barrier patterns, and we propose a privacy-enabled parallax display based on randomized barrier design. In addition to randomizing the locations of 1D slits, we also propose breaking down the slits into pinholes and randomizing their 2D locations, which results in privacy-preservation along the vertical direction as well. We build a hardware prototype using two off-the-shelf liquid-crystal displays. Experiments show that the proposed randomized parallax barrier can direct to the user a narrow viewing field of about ±6°, providing a significantly improved privacy protection as compared to traditional privacy screens.},
  keywords={Privacy;Visualization;Three-dimensional displays;Filters;Prototypes;Virtual reality;User interfaces;Observers;Liquid crystal displays;Protection;Privacy display;parallax display;parallax barrier},
  doi={10.1109/VR59515.2025.00050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937434,
  author={Zhang, Yan and Hong, Shulin and Qian, Weike and You, Keyao and Zhou, Hangyu and Kiyokawa, Kiyoshi and Yang, Xubo},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Color Correction for Occlusion-Capable Optical See-Through Head-Mounted Displays by Using Phase-Modulation}, 
  year={2025},
  volume={},
  number={},
  pages={268-276},
  abstract={Occlusion-capable optical see-through head-mounted displays (OC-OSTHMDs) overcome the deficiency of semi-transparent virtual images by selectively cutting off light emitted from the physical background, considerably improving the graphics performance of augmented reality (AR). Existing OC-OSTHMDs achieve compact form factors by compressing the optical system based on the modulation of light polarization. However, the wavelength sensitivity of polarizing optical elements (POEs) causes color aberration in the see-through view. In this paper, we propose the spectrum- tuning method that mitigates color aberration of the see-through view caused by the wavelength sensitivity of OC-OSTHMDs. The methods operate on a spectrum-based color perception model that formulates the variation of the visible spectrum through OC-OSTHMDs. The optimization is performed globally, requiring minimal computation at runtime. A bench-top prototype of the OC-OSTHMD was built to validate these methods. Experimental results demonstrate that the spectrum-tuning method reduces the color difference by 18.1%. Additionally, the advantages of OC-OSTHMDs in presenting fluid animations in AR scenarios are demonstrated based on the prototype and a multi-buffer mask synthesis method.},
  keywords={Optical polarization;Fluids;Head-mounted displays;Image color analysis;Stimulated emission;Prototypes;Optical imaging;Adaptive optics;Optical sensors;Optical modulation;Augmented reality;mutual occlusion;optical see-through head-mounted displays;color aberration;polarizing optical elements;fluid animation},
  doi={10.1109/VR59515.2025.00051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937443,
  author={Cho, Hyunsung and Edgar, Drew and Lindlbauer, David and O’Hagan, Joseph},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Dynamic Delivery of Audio+Visual Message Notifications in XR}, 
  year={2025},
  volume={},
  number={},
  pages={277-287},
  abstract={The spatial flexibility of Extended Reality (XR) allows for personalized, context-aware organization of applications aligned with a user’s tasks and priorities. Notifications play a crucial role here, e.g., informing users of received messages they might otherwise miss. However, questions remain around how attention-grabbing they should be, how much information they should present, and how the presentation should adapt to the message’s context and content. While prior studies examined facets of message notification design, the impact of multimodal notifications and how they could be used holistically to support message awareness has not yet been explored. We address this by evaluating nine audio-visual notifications, investigating usability, interruptibility, preferences, and their use to inform of received messages. Our results show differing effects of multimodal notification designs and that individuals want notification modality and design to vary based on delivered message content. These results offer new insights into developing context-aware multimodal interaction strategies for spatial notifica¬tions and XR messaging.},
  keywords={Visualization;Privacy;Three-dimensional displays;Adaptive systems;Extended reality;Focusing;Organizations;Time factors;Usability;Augmented reality;Human-centered computing — Human computer interaction (HCI) — Empirical studies in HCI;Human-centered computing — Human computer interaction (HCI) — Interaction paradigms — Mixed / augmented reality},
  doi={10.1109/VR59515.2025.00052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937470,
  author={Bartlett, Kristin A. and Krogmeier, Claudia},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Observations on Virtual Reality Avatar Alignment with Research Participants’ Skin Tone and Gender}, 
  year={2025},
  volume={},
  number={},
  pages={288-295},
  abstract={Previous research has drawn attention to the fact that Virtual Reality (VR) technology is primarily designed around the interests and needs of male, western, educated, and wealthy individuals. In this paper, we look specifically at the avatars used in VR research. We analyzed 40 recent studies in which research participants embodied full avatars or were given virtual hands. While nearly every study matched the gender of the participant to the gender of the avatar, similar efforts were not made regarding skin color. We draw attention to two serious problems in VR research: the explicit and implicit exclusion of participants on the basis of skin color through avatar design. Explicit exclusion, in which only participants whose skin tones matched the light-skinned avatar were recruited occurred in 5% of the studies analyzed. Implicit exclusion occurred when participants embodied an avatar whose skin tone did not match their own. Only 10 (17.5%) of the studies analyzed provided avatar personalization which included different skin tone options that participants could select. Some of the only studies (five out of 40) which utilized Black or dark-skinned avatars were doing so in the context of studying racial bias with all white participants. We argue that using light-skinned default avatars is not acceptable from a research ethics standpoint and that doing so serves to further maintain and normalize systemic racism. We provide recommendations for researchers with the goal of both mitigating and bringing awareness to current issues in VR research.},
  keywords={Hands;Surveys;Three-dimensional displays;Avatars;Color;User interfaces;Skin;Trajectory;Standards;Gender issues;Virtual reality;Embodiment;Avatars;Race;Skin color;Design justice;Diversity and Gender issues},
  doi={10.1109/VR59515.2025.00053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937435,
  author={Cha, Pauline and Li, Fangyi and Peck, Tabitha C.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Avatar Clothing on the Proteus Effect and Stereotype Threat}, 
  year={2025},
  volume={},
  number={},
  pages={296-306},
  abstract={Body-swap illusions investigating the Proteus effect have often investigated major identity changes such as gender, race, or age. However, the impact of an avatar’s clothing on the Proteus effect is often overlooked. In this work, we investigate if simple avatar clothing-swaps combined with a minor environment change to match the clothing are able to induce a Proteus effect. Specifically, we investigate the impact of a woman avatar wearing business or science attire during a stereotype threatening situation. Our results support that clothing-swaps can induce a Proteus effect. Participants in a successful woman business avatar experienced gender stereotype threat while participants in a successful woman science avatar were instead buffered from gender stereotype threat. Further, subjective embodiment was a significant predictor of response measures such that participants with higher subjective embodiment scores had better cognitive performance. This work demonstrates the potential of the Proteus effect to promote and support traditionally marginalized groups.},
  keywords={Three-dimensional displays;Atmospheric measurements;Avatars;Clothing;User interfaces;Particle measurements;Time factors;Business;Testing;Body-Ownership;Embodiment;Proteus Effect;Stereotype Threat},
  doi={10.1109/VR59515.2025.00054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937410,
  author={Chiu, Chao-Kuo and Chuang, Jung-Hong and Pagano, Christopher C. and Babu, Sabarish V.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Absolute Size Perception in Optical See-Through Augmented Reality and Real World Viewing Using Verbal and Physical Judgments}, 
  year={2025},
  volume={},
  number={},
  pages={307-317},
  abstract={This empirical evaluation aimed to investigate how size perception differs between OST AR and the real world, focusing on two judgment methods: verbal reports and physical judgments. Using a within-subjects experimental design, participants viewed target objects in different sizes in both AR and real-world conditions and estimated their sizes using verbal and physical judgment methods across multiple trials. The study addressed two key hypotheses: (H1) that size perception in AR would differ from the Real World, potentially due to rendering limitations in OST-HMDs, and (H2) that verbal reports and physical judgments would yield different levels of accuracy due to distinct cognitive and perceptual processes involved in each method. Our findings supported these hypotheses, revealing key differences in size perception between the two judgment methods and viewing conditions. Participants consistently underestimated object sizes when using verbal reports in both AR and real-world conditions, with more pronounced errors in AR. In contrast, physical judgments yielded more accurate size estimates under both viewing conditions. Notably, the accuracy of verbal reports decreased as target sizes increased, a trend that was particularly evident in AR. These results underscore the perceptual challenges associated with verbal size judgments in AR and their potential limitations in applications requiring precise size estimations. By highlighting the differences in accuracy and consistency between verbal and physical judgment methods, this study contributes to a deeper understanding of size perception in OST AR and real-world contexts.},
  keywords={Accuracy;Three-dimensional displays;Focusing;Estimation;User interfaces;Rendering (computer graphics);Market research;Adaptive optics;Augmented reality;Augmented Reality;Absolute Size Perception;Perception-action;User Studies},
  doi={10.1109/VR59515.2025.00055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937476,
  author={Schieber, Hannah and Young, Jacob and Langlotz, Tobias and Zollmann, Stefanie and Roth, Daniel},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Semantics-Controlled Gaussian Splatting for Outdoor Scene Reconstruction and Rendering in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={318-328},
  abstract={Advancements in 3D rendering like Gaussian Splatting (GS) allow novel view synthesis and real-time rendering in virtual reality (VR). However, GS-created 3D environments are often difficult to edit. For scene enhancement or to incorporate 3D assets, segmenting Gaussians by class is essential. Existing segmentation approaches are typically limited to certain types of scenes, e.g., "circular" scenes, to determine clear object boundaries. However, this method is ineffective when removing large objects in non-"circling" scenes such as large outdoor scenes.We propose Semantics-Controlled GS (SCGS), a segmentation-driven GS approach, enabling the separation of large scene parts in uncontrolled, natural environments. SCGS allows scene editing and the extraction of scene parts for VR. Additionally, we introduce a challenging outdoor dataset, overcoming the "circling" setup. We outperform the state-of-the-art in visual quality on our dataset and in segmentation quality on the 3D-OVS dataset. We conducted an exploratory user study, comparing a 360-video, plain GS, and SCGS in VR with a fixed viewpoint. In our subsequent main study, users were allowed to move freely, evaluating plain GS and SCGS. Our main study results show that participants clearly prefer SCGS over plain GS. We overall present an innovative approach that surpasses the state-of-the-art both technically and in user experience.},
  keywords={Visualization;Three-dimensional displays;Semantics;Virtual reality;User interfaces;Rendering (computer graphics);User experience;Real-time systems;Gaussian Splatting;Semantic Gaussian Splatting;Novel View Synthesis;Virtual Reality},
  doi={10.1109/VR59515.2025.00056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937436,
  author={Zhao, Yue and Lindeman, Robert W. and Piumsomboon, Thammathip},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Daddy Long Legs: A Scale and Speed Up Virtual Reality Locomotion Technique for Medium-scale Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={329-339},
  abstract={Locomotion techniques are essential to enhance the sense of presence or "being there" in virtual reality. This research proposes a novel technique, "Daddy Long Legs" (DLL), for navigating medium-scale virtual environments. DLL builds on principles from Seven-league Boots (7LB) and Ground-level Scaling (GLS), aiming to enhance positional accuracy by offering an elevated perspective while preserving immersion through maintaining the user's interpupillary distance. A user study (n = 24) compared three methods across comfort, embodiment, workload, and walking behavior. Participants completed walking tasks in a virtual garden, followed by questionnaires and interviews to capture their experiences. Results show that DLL outperformed 7LB in all measures. However, GLS was noted for its near-natural walking behavior, received the most favorable feedback, and was the preferred locomotion method.},
  keywords={Legged locomotion;Three-dimensional displays;Accuracy;Navigation;Atmospheric measurements;Virtual environments;User interfaces;Particle measurements;Interviews;Virtual Reality;Scaled Walking;Locomotion Technique;Seven-league Boots;Ground-level Scaling},
  doi={10.1109/VR59515.2025.00057},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937438,
  author={Ahn, Jaehyeok and Choi, Seungmoon},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Generation of Haptic Motion Effects Expressing Human Dance}, 
  year={2025},
  volume={},
  number={},
  pages={340-350},
  abstract={Haptic motion effects refer to physical stimuli that stimulate the human vestibular system for various purposes, such as flight simulation and entertainment. Motion effects are intensively utilized in 4D films, which provide viewers with multisensory special effects with audiovisual stimuli. This paper focuses on the automatic creation of motion effects that express the physical movements in human dance. Such effects can significantly enhance the user’s multisensory experiences of watching and feeling a human dance. Taking the human motion capture data as input, our algorithms compute full six-degree-of-freedom motion effects featuring: (1) Separate processing of the external and internal movements of a dancer’s body; (2) Decomposing the internal movements into a few segments to obtain optimal motion effects between clarity and expressiveness; and (3) Merging of all external and internal movements with appropriate scaling and weights. Our novel method can elicit better user experiences than two previous motion effect generation algorithms, which respectively represent the movements in general articulated bodies and all dynamic components in the scene.},
  keywords={Humanities;Three-dimensional displays;Heuristic algorithms;Motion segmentation;Virtual reality;User interfaces;User experience;Motion capture;Haptic interfaces;Standards;Motion Effects;Haptics;Automatic Generation;Dance;User Experience},
  doi={10.1109/VR59515.2025.00058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937466,
  author={Itoh, Yuta and Nakamura, Tomoya and Hiroi, Yuichi and Aksit, Kaan},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Slim Diffractive Waveguide Glasses for Beaming Displays with Enhanced Head Orientation Tolerance}, 
  year={2025},
  volume={},
  number={},
  pages={351-358},
  abstract={Augmented Reality (AR) glasses must be slim, lightweight, and energy-efficient to achieve widespread adoption. Beaming Displays present a promising solution by offloading active components, such as the power-supplied light engine, into the surrounding environment while leaving only passive elements, like the eyepiece, in the wearable device. However, existing approaches still struggle to achieve both a slim design and a wide tolerance for projection angles relative to the user’s head orientation. In this work, we introduce a design for light-receiving glasses using a diffractive waveguide with in-coupling and out-coupling gratings. Our approach expands the allowable range of incident angles while maintaining a compact, lightweight form factor. We developed a proof-of-concept prototype and demonstrated an incident angle tolerance of approximately 20-30 degrees range, overcoming the previous design of 5 degrees.},
  keywords={Three-dimensional displays;Head;Optical design;Prototypes;Glass;Optical imaging;Diffraction gratings;Wearable devices;Optical waveguides;Augmented reality;Beaming display;Augmented reality;Near-eye display;Waveguide;DOEs},
  doi={10.1109/VR59515.2025.00059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937426,
  author={Popescu, Voicu and Sacks, Elisha and Zhang, Zirui and Vazquez, Jorge},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Complex VEs on All-In-One VR Headsets Through Continuous From-Segment Visibility Computation}, 
  year={2025},
  volume={},
  number={},
  pages={359-369},
  abstract={All-in-one VR headsets have limited rendering power which limits the complexity of the virtual environments (VEs) that can be used in VR applications. This paper describes a novel visibility algorithm for making complex VEs tractable on all-in-one VR headsets. Given a view segment, the algorithm finds the set of triangles visible as a camera translates on the view segment. When run on the perimeter of a user view region, the algorithm provides a quality approximation of the visible set from inside the view region. The visibility algorithm supports static and dynamic VEs, and it solves visibility with either triangle, particle, or object granularity. The visible sets yield output frames that are virtually indistinguishable from ground truth frames rendered from the original VEs.},
  keywords={Headphones;Three-dimensional displays;Heuristic algorithms;Virtual environments;User interfaces;Approximation algorithms;Rendering (computer graphics);Cameras;Complexity theory;VE complexity reduction;all-in-one virtual reality headsets;visibility computation},
  doi={10.1109/VR59515.2025.00060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937437,
  author={Batra, Kabir and Zhang, Zirui and Yang, Shuwen and Agrawal, Anima and Gu, Yiyin and Benes, Bedrich and Magana, Alejandra and Popescu, Voicu},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={XRXL: A System for Immersive Visualization in Large Lectures}, 
  year={2025},
  volume={},
  number={},
  pages={370-380},
  abstract={This paper describes XRXL, an extended-reality system for increasing student engagement in large lectures. Students wear XR headsets to see 3D visualizations controlled by the instructor. The instructor can virtually retract the roof and walls of the classroom to allow for large-scale visualizations that extend beyond the physical boundaries of the classroom, or to turn the classroom into a 360° theater. The instructor can also partition the classroom into small groups of students and to assist individual groups as needed. XRXL was tested in an IRB-approved user study with 82 students in the context of a mock-lecture on neural networks. To the best of our knowledge, the study is the largest deployment of a co-located collaborative XR application to date. The study shows that students had a favorable opinion of XRXL, that XRXL had a low task load, an acceptable usability level, and that it did not cause cybersickness.},
  keywords={Headphones;Visualization;Three-dimensional displays;Cybersickness;Neural networks;Collaboration;User interfaces;Usability;Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Applied computing;Education;Interactive learning environments},
  doi={10.1109/VR59515.2025.00061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937455,
  author={Li, Nianlong and Wu, Tong and He, Zhenxuan and Shen, Luyao and Luo, Tianren and Han, Teng and Gao, BoYu and Zhang, Yu and Zhang, Liuxin and Tian, Feng and Wang, Qianying},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Dual-Stick Controller for Enhancing Raycasting Interactions with Virtual Objects}, 
  year={2025},
  volume={},
  number={},
  pages={381-391},
  abstract={This work presents Dual-Stick, a novel controller with two sticks connected at the end that innovates a Dual-Ray interaction paradigm to enrich raycasting input in Virtual Reality (VR). Dual-Stick leverages the inherent human dexterity in using everyday tools such as clamps and tweezers to adjust the relative angle between two sticks. This design supports Dual-Ray interactions that provide with a heuristics-based enhanced mechanism. It also offers more flexible manipulation by taking advantages of additional degrees of freedom provided by clamping angle. We conducted two studies to evaluate the effectiveness of Dual-Ray in target selection and manipulation tasks. The results indicated that Dual-Ray significantly improved efficiency in target selection compared to single-ray input but did not outperform the enhanced single-ray technique. In terms of manipulation, Dual-Ray effectively reduced completion time and mode switching compared to single-ray input.},
  keywords={Three-dimensional displays;Error analysis;Virtual reality;Switches;User interfaces;Clamps;Handheld controller;virtual reality;raycasting;3D target selection and manipulation},
  doi={10.1109/VR59515.2025.00062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937440,
  author={Bellucci, Andrea and Jacucci, Giulio and Trung, Kien Duong and Das, Pritom Kumar and Smirnov, Sergei Viktorovich and Ahmed, Imtiaj and Lugrin, Jean-Luc},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Tailoring of Embodied Agents Using Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={392-400},
  abstract={LLM-based embodied agents are recently emerging in VR, supporting various scenarios such as pedagogical assistants, virtual companions, and NPCs for games. These agents hold potential to enhance user interactions but require thoughtful design to cater diverse user needs and contexts. We present an architecture that leverages different LLM modules to enable conversational interactions with an embodied agent in multi-user VR. Our system’s primary goal is to facilitate immersive tailoring through conversational input, allowing users to dynamically adjust an agent’s behavior and properties (e.g., role, personality, and appearance), directly within the virtual space, rather than during development or via separate interfaces. We evaluate the system’s performance, measuring latency during tailoring tasks, and share insights from a six-week study involving five users exploring various scenarios. While the approach shows promise, challenges remain, including reducing latency in the speech-to-text-to-speech pipeline and addressing the black-box limitations of LLMs, highlighting areas for future research.},
  keywords={Ethics;Three-dimensional displays;Navigation;Large language models;Pipelines;Virtual reality;Games;User interfaces;Reliability;Iterative methods;tailoring;Embodied Conversational Agent (ECA);Virtual Reality (VR);Large Language Model (LLM)},
  doi={10.1109/VR59515.2025.00063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937472,
  author={Cao, Wei and Li, Zhengyang and He, Yuchen and Liu, Zehua and Zhang, Meng and Lv, Jianping and Si, Weixin},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cost-effective Tangible Rehearsal Interface for Microsurgical Clipping of Intracranial Aneurysm}, 
  year={2025},
  volume={},
  number={},
  pages={401-411},
  abstract={Microsurgical clipping (MC) is widely used for the treatment of intracranial aneurysms (IA). However, it is a high risk for neurosurgeons to perform this operation due to the complex intracranial anatomy, limited microscopic view, and confined operational space. To tackle the above issues, meticulous preoperative rehearsal is in urgent need to improve the neurosurgeons’ perception of patient-specific anatomy while designing the optimal surgical plan, which can greatly enhance the patients’ safety. Most existing MC simulators only support limited interaction methods, such as haptic devices, leading to substantial differences between the training experience and actual surgical situations. To this end, we present a mixed reality (MR) interface for IA MC rehearsal, which can provide neurosurgeons with a more immersive experience. Firstly, considering the labor-intensive labeling cost of reconstructing a 3D full-brain vascular model from CTA images, the simulator employs a cost-effective specific-to-general vessel stitching technique to generate 3D lesion-specific IA geometric models, which replaces normal vascular segments in a standard brain with a personalized-specific operating region by a scaling-constrained iterative closest point (ICP) algorithm. Besides, we design a marker-based tracking method allowing accessible natural human-computer interaction using real surgical instruments which can fuse virtual anatomy and real operation environments, enhancing the users’ spatial perception and tangible stimuli. Additionally, to ensure simulation stability while providing visually plausible clipping operations, we develop a collision distance constrained position-based dynamics (PBD) method with low-resolution sampling particles to simulate the deformation of aneurysm vessels. Quantitative experiments demonstrate the accuracy of our surgical instruments tracking and vessel deformation simulation, which can also fulfill the real-time performance of MC rehearsal. User study indicates that virtual rehearsals significantly improve spatial awareness and dexterity in handling aneurysms, and have the great potential to be applied in practical applications.},
  keywords={Training;Three-dimensional displays;Accuracy;Deformation;Aneurysm;Microsurgery;Stability analysis;User experience;Neurosurgery;Anatomy;Virtual rehearsal;intracranial aneurysm;microsurgical clipping;tangible interaction},
  doi={10.1109/VR59515.2025.00064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937400,
  author={Chang, Woei-Chyi and Yu, Lap-Fai and Hasanzadeh, Sogand},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Worker-Drone Interaction in Mixed Reality: Balancing Distraction and Situational Awareness}, 
  year={2025},
  volume={},
  number={},
  pages={412-419},
  abstract={Mixed-reality (MR) technology has been widely used to simulate high-risk workplaces in order to minimize safety concerns. However, its use in understanding worker attentional allocation during interactions with drones in future construction environments remains underexplored. This study developed a futuristic bricklaying MR environment, where human-drone interaction was mandatory, to capture participants’ naturalistic behaviors (i.e., attention, productivity, and distraction) across different interaction levels (i.e., no interaction, coexistence, and collaboration). The core research question explored whether workers maintained situational awareness of the drones or were distracted by them. The results confirmed that participants experienced a high sense of presence in the MR environment, driven by the use of environmental modalities, passive haptics, and drones’ sounds and spinning blades. Moreover, the findings demonstrated that participants were distracted by the drones during coexistence, as evidenced by lower productivity and reflections indicating they felt they were over-allocating attention to the drones. Conversely, participants exhibited situational awareness of the drones during collaboration, deliberately allocating attention to ensure safety, despite a reduction in productivity. These findings highlight the value of immersive technology in investigating workers’ naturalistic behaviors in future construction scenarios where workers and robots must function as teammates.},
  keywords={Productivity;Three-dimensional displays;Collaboration;Mixed reality;Virtual reality;User interfaces;Robustness;Safety;Spinning;Drones;Mixed Reality (MR);worker-drone interaction;situational awareness;distraction;future construction},
  doi={10.1109/VR59515.2025.00065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937453,
  author={Chen, Xiaoming and Han, Dehao and Qu, Qiang and Shen, Yiran},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={VF-Lens: Enhancing Visual Perception of Visually Impaired Users in VR via Adversarial Learning with Visual Field Attention}, 
  year={2025},
  volume={},
  number={},
  pages={420-430},
  abstract={This research aims to enhance the image perception of visually impaired users in VR environments. We propose VF-Lens, a model that adaptively compensates for light sensitivity based on the user’s visual field impairment, acting as a virtual lens between the visually impaired users and the VR world. VF-Lens is designed as a tailored generative adversarial learning model with a generator and discriminator, offering applicability to various types of visual impairments while bypassing engineering complexities. The generator creates a "hyperimage" tailored to the user’s visual field impairment, which then undergoes a particular regression process to predict and replicate the real perception of the visually impaired user. The discriminator then evaluates the similarity between the replicated perception and the original image. Through adversarial training, the generator can produce hyperimages that adapt to the user’s visual field parameters, enabling them to perceive the image more similarly to normal-vision users. We further improve VF-Lens by proposing new "visual field attention" mechanisms that prioritize and refine visual information in the user’s visual field. Extensive evaluation, encompassing both visually impaired participants and simulations, has been conducted to demonstrate the effectiveness of VF-Lens in improving visual perception for visually impaired users. Moreover, we establish a standardized evaluation process involving tailored metrics as well as objective and subjective evaluations to promote reusability and comparability for future research in this field.},
  keywords={Human computer interaction;Training;Visualization;Adaptation models;Three-dimensional displays;Visual impairment;Virtual reality;Generators;Adversarial machine learning;Visual perception;Human-centered computing—Human computer interaction (HCI);Human-centered computing—Virtual Reality},
  doi={10.1109/VR59515.2025.00066},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937395,
  author={Wang, Bingyuan and Meng, Hengyu and Cao, Rui and Cai, Zeyu and Li, Lanjiong and Ma, Yue and Chen, Qifeng and Wang, Zeyu},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={MagicScroll: Enhancing Immersive Storytelling with Controllable Scroll Image Generation}, 
  year={2025},
  volume={},
  number={},
  pages={431-441},
  abstract={Scroll images are a unique medium commonly used in virtual reality (VR) providing an immersive visual storytelling experience. Despite rapid advances in diffusion-based image generation, it remains an open research question to generate scroll images suitable for immersive, coherent, and controllable storytelling in VR. This paper proposes a multi-layered, diffusion-based scroll image generation framework with a novel semantic-aware denoising process. We incorporate layout prediction and style control modules to generate coherent scroll images of any aspect ratio. Based on the scroll image generation framework, we use different multi-window strategies to render diverse visual forms such as chains, rings, and forks for VR storytelling. Quantitative and qualitative evaluations demonstrate that our techniques can significantly enhance text-image consistency and visual coherence in scroll image generation, as well as the level of immersion and engagement of VR storytelling. We will release our source code to facilitate better collaborations on immersive storytelling between AI researchers and creative practitioners. https://magicscroll.github.io/},
  keywords={Visualization;Three-dimensional displays;Image synthesis;Source coding;Noise reduction;Layout;Process control;Coherence;User interfaces;User experience;scroll image generation;diffusion models;visual storytelling;immersive experience},
  doi={10.1109/VR59515.2025.00067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937417,
  author={Bondesan, Pierre and Canal, Audrenne and Fleury, Sylvain and Boisadan, Andréa and Richir, Simon},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Implicit Learning of Professional Skills through Immersive Virtual Reality: a Media Comparison Study}, 
  year={2025},
  volume={},
  number={},
  pages={442-449},
  abstract={This study investigates the effectiveness of Immersive Virtual Reality (IVR) compared to traditional slideshow lessons in teaching implicit knowledge. For this purpose, the research focuses on professional decision-making skills in viticulture. Most existing research on immersive learning concentrates on explicit learning strategies. In contrast, this study explores the potential of IVR to foster the transfer of implicit knowledge to real-world situations.Forty third-year engineering students were randomly assigned to an IVR or a traditional slideshow group. They learned to assess vine vigour through an implicit learning phase, followed by a real-world evaluation in an actual vineyard. Learning outcomes were measured by decision-making accuracy, response time, and intrinsic motivation.The findings show that the IVR group did not significantly outperform the slideshow group in decision-making accuracy. However, the IVR group took more time to make decisions. This observation suggests an impact of immersion during the transfer to real-world situations. Additionally, the IVR group showed a higher level of intrinsic motivation than the slideshow group.These results suggest that although the immersion effect does not directly enhance learning outcomes for this cognitive objective, it does affect how knowledge is transferred to the real world. They also confirm that the positive impact of immersion is difficult to generalize and may depend on the nature of the knowledge. Still, the immersion effect significantly improves learner motivation. This consistent finding could be a key factor in long-term educational success. Further research exploring the nuanced effects of immersion on different learning strategies and educational objectives could offer new practical perspectives for the future of educational technologies.},
  keywords={Human computer interaction;Solid modeling;Accuracy;Three-dimensional displays;Decision making;Educational technology;Media;Time measurement;Time factors;Immersive learning;Immersive Virtual Reality;Immersive Learning;Implicit Learning;Professional Skills Acquisition;Educational Technology},
  doi={10.1109/VR59515.2025.00068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937429,
  author={Peng, Xi and Chen, Kenneth and Roman, Iran and Bello, Juan Pablo and Sun, Qi and Chakravarthula, Praneeth},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Perceptually-Guided Acoustic "Foveation"}, 
  year={2025},
  volume={},
  number={},
  pages={450-460},
  abstract={Realistic spatial audio rendering improves immersion in virtual environments. However, the computational complexity of acoustic propagation increases linearly with the number of sources. Consequently, real-time accurate acoustic rendering becomes challenging in highly dynamic scenarios such as virtual and augmented reality (VR/AR). Exploiting the fact that human spatial sensitivity of acoustic sources is not equal at azimuth eccentricities in the horizontal plane, we introduce a perceptually-aware acoustic "foveation" guidance model to the audio rendering pipeline, which can integrate audio sources that are not spatially resolvable by human listeners. To this end, we first conduct a series of psychophysical studies to measure the minimum resolvable audible angular distance under various spatial and background conditions. We leverage this data to derive an azimuth-characterized real-time acoustic foveation algorithm. Numerical analysis and subjective user studies in VR environments demonstrate our method’s effectiveness in significantly reducing acoustic rendering workload, without compromising users’ spatial perception of audio sources. We believe that the presented research will motivate future investigation into the new frontier of modeling and leveraging human multimodal perceptual limitations — beyond the extensively studied visual acuity — for designing efficient VR/AR systems.},
  keywords={Solid modeling;Visualization;Sensitivity;Computational modeling;Virtual environments;Rendering (computer graphics);Acoustics;Real-time systems;Psychophysics;Spatial resolution;Perception;Virtual reality;Mixed/Augmented reality},
  doi={10.1109/VR59515.2025.00069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937391,
  author={Zhang, Dayou and Liang, Zhicheng and Cao, Zijian and Wang, Dan and Wang, Fangxin},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={SRBF-Gaussian: Streaming-Optimized 3D Gaussian Splatting}, 
  year={2025},
  volume={},
  number={},
  pages={461-471},
  abstract={3D Gaussian Splatting (3DGS) has emerged as a groundbreaking 3D scene representation technique, offering unprecedented visual quality and rendering efficiency. However, the substantial data volume of 3DGS scenes poses significant challenges for streaming applications. Existing research on 3DGS has primarily focused on compression and rendering efficiency, neglecting the specific requirements of streaming transmission. Moreover, the Spherical Harmonics color representation in 3DGS complicates viewport-based transmission partitioning. Achieving hierarchical Gaussian streaming without noticeable quality degradation also remains a significant challenge.To address these challenges, we propose SRBF-Gaussian, a new paradigm that revolutionizes the traditional 3DGS format. Our approach introduces viewport-dependent color encoding based on Spherical Radial Basis Functions (SRBFs) and HSL color space, enabling selective transmission of viewport-relevant color data. This reduces data transmission while maintaining visual quality. We implement adaptive Gaussian pruning and transmission, optimized for current viewports and network conditions. Additionally, we develop coherent multi-level Gaussian representations for smooth transitions between quality levels. Our system incorporates user-behavior-aware streaming strategies to anticipate and pre-fetch relevant data. In cloud VR scenarios, our approach demonstrates substantial improvements, achieving a 5.63% - 14.17% increase in PSNR, a 7.61% - 59.16% reduction in latency, and a 10.45% - 30.12% improvement in overall Quality of Experience (QoE).},
  keywords={Visualization;Cloud computing;Three-dimensional displays;Image color analysis;Virtual reality;User interfaces;Rendering (computer graphics);Encoding;User experience;Quality of experience;3D Gaussian Splatting;3D Scene Transmission;Quality of Experience;Bitrate Adaptation},
  doi={10.1109/VR59515.2025.00070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937415,
  author={Ahmed Tamboli, Danish Nisar and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Raveendranath, Balagopal and Woodward, Julia and Wang, Isaac and Smith, Jesse and Ruiz, Jaime},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={May The Force be With You: Cloning Distant Objects to Improve Medium-Field Interactions in Augmented Reality}, 
  year={2025},
  volume={},
  number={},
  pages={472-482},
  abstract={Augmented Reality (AR) interactions feature users interacting with virtual objects registered in the physical world. With contemporary AR experiences increasingly featuring interactions at distances, we conceptualized The Force, a technique that allows users to clone distant objects and manipulate their replicas. An empirical evaluation was conducted, comparing it against two well-established techniques including controller-based ray-casting and a gaze-based pinching technique in a pick-and-place task. We employed a within-subjects design, collecting data on both objective performance and subjective user experience. Results suggest that The Force allows for higher levels of accuracy and efficiency in medium-field tasks that require precision and fine motor control. Furthermore, we discovered avenues towards iteratively refining this technique. We go on to discuss the implications of our findings in an effort to facilitate better interactions in augmented reality.},
  keywords={Motor drives;Three-dimensional displays;Accuracy;Force;Refining;Cloning;User interfaces;User experience;Augmented reality;Augmented Reality;Interaction Techniques;3D User Interaction;Distant Object Manipulation;Cloning},
  doi={10.1109/VR59515.2025.00071},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937398,
  author={Boorboor, Saeed and Gutiérrez-Rosales, Doris and Shoaib, Ahamed and Kalsi, Chahat and Wang, Yue and Cao, Yuyang and Gu, Xianfeng and Kaufman, Arie E.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Silo: Half-Gigapixel Cylindrical Stereoscopic Immersive Display}, 
  year={2025},
  volume={},
  number={},
  pages={483-493},
  abstract={We present the design and construction of the Silo, a fully immersive stereoscopic cylindrical tiled-display visualization facility. Comprising 168 high-density LCD displays, the facility provides an ultra-high-resolution image of 619 million pixels, and close to 360 horizontal field-of-regards (FoR), aiming to maximize visual acuity and completely engage the human visual sensorium and its periphery. In this article, we outline the motivations, design principles, hardware selection and software systems, and interaction modalities used in constructing the Silo. To address missing visual information due to the absence of a ceiling and floor, we have designed a method that utilizes conformal mapping and optimal mass transport to reproject the entire 360 volumetric FoR of the virtual scene to the available display real estate. We showcase several applications demonstrating the utility of the Silo and report the findings of our user studies that highlight the effectiveness of the Silo layout compared to curved mono and flat powerwall display facilities. Our user evaluations and studies have shown that the Silo supports natural exploration and enhanced visualization due to its capability to render surround ultra-high-resolution stereoscopic views.},
  keywords={Visualization;Three-dimensional displays;Stereo image processing;Data visualization;Transportation;Virtual reality;Software systems;Liquid crystal displays;Floors;Next generation networking;Immersive Visualization;Ultra-high-resolution Display;Data Visualization;CAVE;Conformal Visualization},
  doi={10.1109/VR59515.2025.00072},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937468,
  author={Ogawa, Kumpei and Fujita, Kazuyuki and Takashima, Kazuki and Kitamura, Yoshifumi},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirected Drawing: Expanding the Perceived Canvas Size in VR}, 
  year={2025},
  volume={},
  number={},
  pages={494-504},
  abstract={Drawing, writing, and painting on 2D surfaces in VR offers arbitrary interaction with virtual canvases of unlimited size. However, this advantage has not been fully exploited because physical surfaces are needed to support accurate and comfortable user interactions, and the size of such surfaces is limited. We propose Redirected Drawing, a novel methodology that applies visual manipulation to the user’s drawing movement in VR to expand the perceived surface size required for the drawing experience. To this end, this study specifically explores translation gain manipulation, where the displacement of a drawing stroke in reality is simply multiplied by a gain to represent it in VR. We conducted a user study (N = 18) to measure the noticeability, acceptability, and task performance of a simple VR line-tracing task under different gain and input methods (i.e., finger in mid-air, finger on surface, pen on surface). Our results show that pen-based drawing on a physical surface allows for a gain of up to 1.16 without being noticed by the user, which is more suited to expanding a virtual canvas size than finger-based drawing on a physical surface or in mid-air. Furthermore, we show that user acceptability of the gain manipulation could be estimated by a model using the subjective discrepancy caused by the gain. Results for this model suggest that users could accept gains of up to 1.30. Based on our results, we derive guidelines for the effective use of gain manipulation.},
  keywords={Human computer interaction;Hands;Solid modeling;Visualization;Translation;Three-dimensional displays;Virtual reality;Writing;Painting;Guidelines;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality},
  doi={10.1109/VR59515.2025.00073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937428,
  author={Fujita, Hikaru and Hosoi, Juro and Ban, Yuki and Warisawa, Shin’Ichi},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Presenting Tingling Aftereffects Using Vibro-Thermal Feedback to Enhance Impact Sensation in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={505-515},
  abstract={Impact sensation is vital for achieving high realism and immersion in virtual reality (VR) applications such as sports games. There has been substantial research focusing on the moment of impact, using methods like electrical muscle stimulation (EMS) and vibrotactile feedback. However, the size and complexity of these devices, along with concerns for user safety, limit their effectiveness in producing strong impact sensations. Additionally, less attention has been given to sensations experienced after a strong impact, like the tingling that follows a collision. This research focuses on an overlooked area: the aftereffects that emerge within the body following a physical impact. It specifically examines the tingling sensation that occurs after a strong hit, using a VR baseball bat-like device designed to deliver impulsive force, vibrotactile, and thermal feedback to the user’s hand. We conducted a user study (18 male, 6 female participants) to assess how these types of feedback affect the realism and intensity of impact sensations, as well as the tingling sensation perceived by users. The results revealed that both vibrotactile and thermal feedback as aftereffects significantly enhanced the impact sensation, confirming their ability to induce tingling sensations. Additionally, high-frequency vibrotactile stimuli particularly enhanced the tingling sensation and realism of impact. Although the sample had a gender imbalance, these findings highlight the potential for incorporating aftereffects feedback in VR systems to enhance impact sensations for entertainment and safety education purposes.},
  keywords={Vibrations;Three-dimensional displays;Force;Virtual reality;User interfaces;Muscles;Thermal force;Safety;Haptic interfaces;Sports;Haptics;aftereffect;impact;vibration;thermal feedback;virtual reality},
  doi={10.1109/VR59515.2025.00074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937461,
  author={Au, Domenic and Allison, Robert S. and Gunasekera, Iroshini and Wilcox, Laurie M.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Misperception of the distance of virtual augmentations}, 
  year={2025},
  volume={},
  number={},
  pages={516-525},
  abstract={Binocular disparity provides metric depth information, while monocular cues like occlusion offer depth order. In augmented reality (AR), conflicts between these cues can occur when virtual objects fail to be occluded by real-world surfaces, creating a depth cue conflict and subsequently impacting depth perception. The integration of occlusion and binocular disparity was investigated under this cue conflict in AR using distance-matching paradigms. These paradigms were applied within reach space (0.35–0.5 m) and beyond reach space (0.9–1.5 m). Observers matched the distance of a virtual letter ’A,’ superimposed on a physical surface, using a virtual probe. In addition to the probe, manual reach responses were also made with the index finger for the within reach space condition. Results revealed consistent underestimation of the letter’s distance when it was rendered beyond the surface, with errors proportional to distance. These biases persisted even when proprioceptive information was available, highlighting the robust influence of occlusion cue conflicts on perceived depth. Thus, designers must carefully plan and position virtual augmentations to avoid such errors and their impact on user interaction.},
  keywords={Measurement;Location awareness;Accuracy;Three-dimensional displays;Propioception;User interfaces;Observers;Rendering (computer graphics);Probes;Augmented reality;Occlusion;stereopsis;augmented reality;binocular disparity},
  doi={10.1109/VR59515.2025.00075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937424,
  author={Hsu, Pei-Chin and Babu, Sabarish V. and Chuang, Jung-Hong},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparative Evaluation of Differing Levels of Information Presentation in 3D Mini-Maps on Spatial Knowledge Acquisition in VR}, 
  year={2025},
  volume={},
  number={},
  pages={526-536},
  abstract={Maps have long been a favored tool for navigation in both physical and virtual environments. As a navigation aid in virtual reality, map content and appearance can differ significantly. In this paper, three mini-maps are addressed: the WiM-3DMap, which provides a standard World-in-Miniature of the city model; the novel UC-3DMap, featuring important landmarks alongside ordinary buildings within the user’s vicinity; and the LM-3DMap, presenting only important landmarks. These mini-maps offer varying levels of building detail, potentially affecting spatial knowledge acquisition performance in diverse ways. A comparative study was conducted to evaluate the effectiveness of WiM-3DMap, UC-3DMap, LM-3DMap, and a baseline condition without a mini-map in spatial tasks such as spatial updating, landmark recall, landmark placement, and route recall. The findings demonstrated that LM-3DMap and UC-3DMap outperform WiM-3DMap in the tasks of spatial updating, landmark placement and route recall. However, the absence of detailed local context around the user may impede the effectiveness of LM-3DMap, as evidenced by UC-3DMap’s superior performance in the landmark placement task. These findings underscore the differences in effectiveness among various mini-maps that present distinct levels of building detail. A key conclusion is that including ordinary building information in the user’s immediate surroundings can significantly enhance the performance of a mini-map relying solely on landmarks.},
  keywords={Solid modeling;Three-dimensional displays;Navigation;Knowledge acquisition;Buildings;User centered design;Urban areas;Virtual environments;User interfaces;Standards;User-centered design;World-in-Miniature;3D mini-maps;Navigation aids;Spatial knowledge acquisition (SKA)},
  doi={10.1109/VR59515.2025.00076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937450,
  author={Westermeier, Franziska and Murmu, Chandni and Kohm, Kristopher and Pagano, Christopher and Wienrich, Carolin and Babu, Sabarish V. and Erich Latoschik, Marc},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interpupillary to Inter-Camera Distance of Video See-Through AR and its Impact on Depth Perception}, 
  year={2025},
  volume={},
  number={},
  pages={537-547},
  abstract={Interpupillary distance (IPD) is the most important parameter for creating a user-specific stereo parallax, which in turn is crucial for correct depth perception. This is why contemporary Head-Mounted Displays (HMDs) offer adjustable lenses to adapt to users’ individual IPDs. However, today’s Video See-Through Augmented Reality (VST AR) HMDs use fixed camera placements to reconstruct the stereoscopic view of a user’s environment. This leads to a potential mismatch between individual IPD settings and the fixed Inter-Camera Distances (ICD), which can lead to perceptual incongruencies, limiting the usability and, potentially, the applicability of VST AR in depth-sensitive use cases. To investigate this incongruency between IPD and ICD, we conducted a 2 × 3 mixed-factor design user study using a near-field, open-loop reaching task comparing distance judgments of Virtual Reality (VR) and VST AR. We also investigated changes in reaching performance via perceptual calibration by incorporating a feedback phase between pre- and post-phase conditions, with a particular focus on the influence of IPD-ICD differences. Our Linear Mixed Model (LMM) analysis showed a significant difference between VR and VST AR, an effect of IPD-ICD mismatch, and a combined effect of both factors. However, subjective measures showed no effect underlining the subconscious nature of the perception of VST AR. This novel insight and its consequences are discussed specifically for depth perception tasks in AR, eXtended Reality (XR), and potential use cases.},
  keywords={Solid modeling;Three-dimensional displays;Limiting;Extended reality;Stereo image processing;Transportation;User interfaces;Calibration;Usability;Augmented reality;Index Terms: eXtended Reality;Depth Perception;Virtual Reality;Augmented Reality;Video See-Through;Interpupillary Distance;Perception-Action;Perceptuomotor Calibration},
  doi={10.1109/VR59515.2025.00077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937422,
  author={Zoeppig, Tony Jan and Benjamin Lammert, Anton and Schott, Ephraim and Muehlhaus, Sebastian and Froehlich, Bernd},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Follow Me: Confirmation-based Group Navigation in Collocated Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={548-558},
  abstract={In collocated social virtual reality, the relative physical and virtual positions of users are often synchronised to reduce audio-visual inconsistencies and the risk of collisions between users. For virtual navigation previous work has proposed group navigation techniques that maintain spatial synchronisation. This, however, limits user autonomy and can be particularly frustrating when some users would prefer to remain at their current virtual position, as can be the case when users have differing goals, interests, or tasks within the virtual environment. In this paper, we introduce a two-phase confirmation-based group navigation concept for collocated scenarios which allows users to stay behind and catch up to the group based on individual confirmation. The confirmation to catch up can be triggered by the group’s guide, the individual user or both. Ghost avatars, which visualise the physical locations of other users, are used to avoid physical collisions in situations where part of the group has already virtually navigated to a new location. We evaluate the three confirmation-based techniques in a user study (N = 24) in the context of a guided tour in a virtual museum and compare it to a baseline group navigation technique. Our findings show that users prefer having the autonomy to decide when to follow their group. Despite increased complexity, the proposed techniques achieved comparable levels of co-presence, spatial orientation, and understanding of others’ positions in both the virtual and physical environments as the baseline, effectively balancing user autonomy, navigation understanding and social experience.},
  keywords={Visualization;Three-dimensional displays;Navigation;Avatars;Virtual environments;User interfaces;Virtual museums;Complexity theory;Synchronization;Indexes;Index Terms: Group Navigation;Social Virtual Reality;Spatial Desynchronisation;Guided Tours;Virtual Museums},
  doi={10.1109/VR59515.2025.00078},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937425,
  author={Powley, Benjamin and Anslow, Craig and De Róiste, Mairéad and Marshall, Stuart},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Immersive Analytics for Understanding Ecosystem Services Data}, 
  year={2025},
  volume={},
  number={},
  pages={559-568},
  abstract={Planning land use changes requires input from experts across several domains with diverse expertise. Currently available tools were not designed for analysing the required ecosystem services before and after land use changes, or for analysing tradeoffs. Our study is motivated by the need for better tools designed for expert analysts and location knowledgable stakeholders to analyse data relating to land use planning, such as ecosystem services data. The effectiveness of an immersive VR visualization system (Immersive ESS Visualizer) for analysing and visualizing ecosystem services data, is evaluated and compared to existing methods of analysis, paper maps and a 2D screen. Although VR presented some difficulties, the benefits of spatially arranging maps, inspecting fine details, navigation by zooming, and inspection of hillshades provided sufficient advantages to make VR an effective tool for ecosystem services analysis. Participants using Immersive ESS Visualizer arranged handheld maps into 3D layouts, which would not be possible with paper or on the 2D screen. Our research demonstrates that geospatial analysis can be supported by comparative visualization in VR.},
  keywords={Solid modeling;Three-dimensional displays;Navigation;Stereo image processing;Ecosystems;Layout;Data visualization;Virtual reality;Inspection;Stakeholders;Index Terms: Ecosystems Services;Visualization;Virtual Reality},
  doi={10.1109/VR59515.2025.00079},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937401,
  author={Rendle, Gareth and Immohr, Felix and Kehling, Christian and Lammert, Anton and Kreskowski, Adrian and Brandenburg, Karlheinz and Raake, Alexander and Froehlich, Bernd},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Influence of Audiovisual Realism on Communication Behaviour in Group-to-Group Telepresence}, 
  year={2025},
  volume={},
  number={},
  pages={569-579},
  abstract={Group-to-group telepresence systems immerse geographically separated groups in a shared interaction space where remote users are represented as avatars. Notably, such systems allow users to interact with collocated and remote interlocutors simultaneously. In this context, where virtual user representations can be directly compared with real users, we investigate how visual realism (avatar type) and aural realism (presence of spatial audio) affect communication. Furthermore, we examine how communication differs between collocated and remote pairs of interlocutors. In our user study, groups of four participants perform a collaborative conversation task under the aforementioned visual and aural realism conditions. Our results indicate that avatar realism has positive effects on subjective ratings of perceived message understanding and group cohesion, and yields behavioural differences that indicate more interactivity and engagement. Few significant effects of aural realism were observed. Comparisons between collocated and remote communication found that collocated communication was perceived as more effective, but that more visual attention was paid to both remote participants than the collocated user.},
  keywords={Visualization;Telepresence;Three-dimensional displays;Avatars;Spatial audio;Mixed reality;Oral communication;User interfaces;Indexes;Computer mediated communication;Index Terms: Telepresence;Mixed reality;Mediated communication;Avatars;Spatial audio},
  doi={10.1109/VR59515.2025.00080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937421,
  author={Yin, Jianing and Zheng, Weicheng and Wang, Yuntao and Tong, Xin and Yan, Yukang},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparison Study Understanding the Impact of Mixed Reality Collaboration on Sense of Co-Presence}, 
  year={2025},
  volume={},
  number={},
  pages={580-590},
  abstract={Sense of co-presence refers to the perceived closeness and interaction between participants in a collaborative context, which critically impacts the collaboration experience and task performance. With the emergence of Mixed Reality (MR) technologies, we would like to investigate the effect of MR immersive collaboration environment on promoting co-presence in a remote setting by comparing it with non-MR methods, such as video conferencing. We conduct a comparison study, where we invited 14 dyads of participants to collaborate on block assembly tasks with video conferencing, MR system, and in a physically co-located scenario. Each participant of a dyad was assigned either a local worker to assemble the blocks or a remote helper to give the instructions. Results show that MR system can create comparable sense of co-presence with co-located situation, and allow users to interact more naturally with both the environment and each other. The adoption of mixed reality enhances collaboration and task performance by reducing reliance on verbal communication and favoring action-based interactions through gestures and direct manipulation of virtual objects.},
  keywords={Three-dimensional displays;Collaboration;Mixed reality;Virtual reality;User interfaces;Indexes;Assembly;Videoconferences;Index Terms: Remote Collaboration;Co-presence;Mixed Reality},
  doi={10.1109/VR59515.2025.00081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937467,
  author={Finney, Hunter and Gagnon, Holly and Creem-Regehr, Sarah H. and Bodenheimer, Bobby and Stefanucci, Jeanine K.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Children’s Calibration of Reaching Estimates in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={591-601},
  abstract={How children perceive and interact in immersive virtual environments (IVEs) is an important and emerging topic as virtual reality devices proliferate among this age group. How children understand their opportunities for action in an IVE — the perceived affordances of the space — in comparison to adults has, in particular, been rarely studied. This paper addresses this gap by examining children and adults’ reaching affordances in IVEs. We compared how children and adults judge their ability to reach up and out to targets at different distances in an IVE and how feedback from their actions may influence their accuracy. We found that both children and adults tend to overestimate their reaching capabilities, particularly when reaching out. However, by allowing them to actually reach up or out to the targets (action-outcome feedback), both groups significantly re-calibrated their judgments to align their estimates more closely with their actual capabilities. Notably, children exhibited greater initial overestimation than adults but recalibrated at a similar rate. These findings provide novel insights into how children interact with IVEs and suggest that well-designed feedback mechanisms can enhance their accuracy in action-based tasks. This work has implications for the design of IVEs for children, particularly in educational and training contexts, where ensuring accurate perception of capabilities is critical for learning and safety. By investigating both horizontal and vertical reach, this paper contributes to the understanding of affordance perception across age groups in IVEs, marking an essential step in adapting virtual reality technology to younger users.},
  keywords={Training;Human computer interaction;Ethics;Accuracy;Three-dimensional displays;Affordances;Virtual environments;Psychology;Safety;Information systems;Applied computing—Psychology;Human-centered computing;Human-centered computing—Virtual reality;Human-centered computing—Empirical studies in HCI},
  doi={10.1109/VR59515.2025.00082},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937463,
  author={Hajahmadi, Shirin and Cascarano, Pasquale and Mostajeran, Fariba and Heuer, Kevin and Lux, Anton and Mends-Cole, Gil Otis and Steinicke, Frank and Marfia, Gustavo},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Impact of Voice-only and Embodied Conversational Virtual Agents on Mixed Reality Puzzle Solving}, 
  year={2025},
  volume={},
  number={},
  pages={602-612},
  abstract={Conversational Virtual Agents (CVAs) offer a promising approach for enhancing user task performance in Mixed Reality (MR) environments. This paper explores the integration of a CVA into an MR application designed to assist in solving a 2D physical puzzle, offering enhanced spatial cognitive capabilities. Using the CVA classification architecture and the MiRAS (Mixed Reality Agents) Cube Taxonomy, we developed an MR system with a state-aware assistant to guide users in a puzzle-solving task only when requested. The primary research question is whether or not the CVA needs to be embodied. We conducted a study with 34 participants to investigate the influence of Voice-only and Embodied CVAs on puzzle-solving performance, user interactions with the assistant, the assistant’s social presence, overall system cognitive workload, and users’ perceptions of future system use. Both modalities showed equivalent outcomes regarding the number of position- and orientation-related queries, perceived usability, message and affective understanding, performance, frustration, and usefulness. However, results showed that Voice-only CVA significantly enhanced task efficiency: participants completed puzzles more quickly and accurately, reporting lower effort than in the Embodied condition. These findings suggest that Voice-only CVAs may be more effective for tasks like puzzle solving, where auditory guidance alone appears sufficient to support better performance.},
  keywords={Visualization;Three-dimensional displays;Taxonomy;Mixed reality;Virtual reality;User interfaces;User experience;Problem-solving;Indexes;Usability;Index Terms: Mixed Reality;Puzzle Solving;Conversational Virtual Agent;Human-Agent Interaction;Social Presence;Task Performance;Cognitive Workload},
  doi={10.1109/VR59515.2025.00083},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937347,
  author={Hosseini-Toudeshky, Hamideh and Seidnitzer, Sarah and Bickelhaupt, Sebastian and Harmouche, Rola and Kersten-Oertel, Marta},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Impact of Immersiveness in Virtual Reality Simulations on Anxiety Reduction for MRI Procedures: A Preliminary Study}, 
  year={2025},
  volume={},
  number={},
  pages={613-622},
  abstract={Magnetic Resonance Imaging (MRI) examinations are frequently associated with significant anxiety and phobias in patients, negatively impacting imaging quality and patient compliance. In this study, we explore the use of Virtual Reality Exposure Therapy (VRET) to reduce MRI-related anxiety by examining physiological and subjective responses across three virtual scenarios: a 2D video, a 360° video, and a fully immersive VR environment. The study aimed to determine how different levels of immersion and the order in which scenarios are experienced impact anxiety. Thirteen participants engaged in all three scenarios, with heart rate (HR), skin temperature (SKT), and electrodermal activity (EDA) monitored, and self-reported anxiety assessments collected before, during, and after the study. Results showed no significant differences in average or maximum heart rates between the three scenarios. However, the fully immersive VR environment generally elicited higher HR peaks, higher EDA, and lower SKT, suggesting stronger physiological responses in some participants. Self-reported anxiety decreased after the VR experience, particularly for participants with moderate to high anxiety levels prior to the sessions, independent of the scenario order. These findings suggest that individual responses to VRET vary, emphasizing the need for personalized approaches rather than a one-size-fits-all solution. While larger studies are necessary to validate these outcomes, the results suggest that incorporating real-time biofeedback monitoring in VRET could allow for dynamic adjustments to exposure levels based on participants’ physiological responses, creating a more adaptive and therapeutic environment.},
  keywords={Heart rate;Solid modeling;Magnetic resonance imaging;Biological system modeling;Anxiety disorders;Medical treatment;Virtual reality;Real-time systems;Biomedical monitoring;Monitoring;Virtual Reality;Anxiety;Magnetic Resonance Imaging;Simulation;Physiological Signals},
  doi={10.1109/VR59515.2025.00084},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937447,
  author={Sajid, Maha and Shah Bukhari, Syed Ibrahim Mustafa and Ji, Bo and David-John, Brendan},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={"Just stop doing everything for now!": Understanding security attacks in remote collaborative mixed reality}, 
  year={2025},
  volume={},
  number={},
  pages={623-633},
  abstract={Mixed Reality (MR) devices are being increasingly adopted across a wide range of real-world applications, ranging from education and healthcare to remote work and entertainment. However, the unique immersive features of MR devices, such as 3D spatial interactions and the encapsulation of virtual objects by invisible elements, introduce new vulnerabilities leading to interaction obstruction and misdirection. We implemented latency, click redirection, object occlusion, and spatial occlusion attacks within a remote collaborative MR platform using the Microsoft HoloLens 2 and evaluated user behavior and mitigations through a user study. We compared responses to MR-specific attacks, which exploit the unique characteristics of remote collaborative immersive environments, and traditional security attacks implemented in MR. Our findings indicate that users generally exhibit lower recognition rates for immersive attacks (e.g., spatial occlusion) compared to attacks inspired by traditional ones (e.g., click redirection). Our results demonstrate a clear gap in user awareness and responses when collaborating remotely in MR environments. Our findings emphasize the importance of training users to recognize potential threats and enhanced security measures to maintain trust in remote collaborative MR systems.},
  keywords={Human computer interaction;Training;Privacy;Three-dimensional displays;Prevention and mitigation;Collaboration;Mixed reality;Virtual reality;Remote working;Security;Human computer interaction (HCI);interaction paradigms;mixed/augmented reality;human and societal aspects of security and privacy},
  doi={10.1109/VR59515.2025.00085},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937477,
  author={Homami, Helia and Quigley, Adria and Barrera Machuca, Mayra Donaji},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Omnidirectional VR Treadmills Walking Techniques: Comparing Walking-in-Place and Sliding vs Natural Walking}, 
  year={2025},
  volume={},
  number={},
  pages={634-644},
  abstract={In Virtual Reality (VR), exploring a large virtual environment is still an open research area, as software-based solutions can be challenging to use or contribute to motion sickness. Past work has used omnidirectional treadmills to emulate natural walking in a small space. Yet, the impact on user performance and experience of the different walking techniques these omnidirectional treadmills utilize is still unclear. This study assessed the effects of WALK-IN-PLACE, used with devices like KAT VR mini, and SLIDING, as used by devices like Cyberith Virtualizer ELITE 2, and compared them to NATURAL WALKING. Using a within-study design methodology, eighteen participants navigated a VR maze. We found that participants completed tasks fastest with NATURAL WALKING while SLIDING provided a balanced compromise between immersion and moderate physical effort, and WALK-IN-PLACE required the highest exertion with lower usability. These findings provide practical insights into performance and user-experience differences among the three VR walking techniques, informing VR-ODT selection for applications such as rehabilitation and training.},
  keywords={Legged locomotion;Training;Three-dimensional displays;Navigation;Virtual environments;Immersive experience;User interfaces;User experience;Safety;Usability;Index Terms: Virtual Reality;Navigation;Walking;Treadmills},
  doi={10.1109/VR59515.2025.00086},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937419,
  author={Komatsu, Ryoya and Ogura, Ayumu and Yoshida, Shigeo and Tanaka, Kazutoshi},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Transtiff: A Stylus-shaped Interface for Rendering Perceived Stiffness of Virtual Objects via Stylus Stiffness Control}, 
  year={2025},
  volume={},
  number={},
  pages={645-655},
  abstract={The replication of object stiffness is essential for enhancing haptic feedback in virtual environments. However, existing research has overlooked how stylus stiffness influences the perception of virtual object stiffness during tool-mediated interactions. To address this, we conducted a psychophysical experiment demonstrating that changing stylus stiffness combined with visual stimuli altered users’ perception of virtual object stiffness. Based on these insights, we developed Transtiff, a stylus-shaped interface capable of on-demand stiffness control using a McKibben artificial muscle mechanism. Unlike previous approaches, our method manipulates the perceived stiffness of virtual objects via the stylus by controlling the stiffness of the stylus without altering the properties of the real object being touched, creating the illusion of a hard object feeing soft. Our user study confirmed that Transtiff effectively simulates a range of material properties, such as sponge, plastic, and tennis balls, providing haptic rendering that is closely aligned with the perceived material characteristics. By addressing the challenge of delivering realistic haptic feedback through tool-based interactions, Transtiff represents a significant advancement in the haptic interface design for VR applications.},
  keywords={Artificial muscles;Visualization;Three-dimensional displays;Sports equipment;Virtual environments;User interfaces;Rendering (computer graphics);Motors;Haptic interfaces;Material properties;Index Terms: Haptic Interface;Stick Object;Stiffness;Visuohaptic Illusion},
  doi={10.1109/VR59515.2025.00087},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937388,
  author={Wen, Shaoyue and Middleton, Michael and Ping, Songming and Chawla, Nayan N. and Wu, Guande and Feest, Bradley S. and Nadri, Chihab and Liu, Yunmei and Kaber, David and Zahabi, Maryam and McMahan, Ryan P. and Castelo, Sonia and McKendrick, Ryan and Qian, Jing and Silva, Cláudio T.},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={AdaptiveCoPilot: Design and Testing of a NeuroAdaptive LLM Cockpit Guidance System in both Novice and Expert Pilots}, 
  year={2025},
  volume={},
  number={},
  pages={656-666},
  abstract={Pilots operating modern cockpits often face high cognitive demands due to complex interfaces and multitasking requirements, which can lead to overload and decreased performance. This study introduces AdaptiveCoPilot, a neuroadaptive guidance system that adapts visual, auditory, and textual cues in real time based on the pilot’s cognitive workload, measured via functional Near-Infrared Spectroscopy (fNIRS). A formative study with expert pilots (N=3) identified adaptive rules for modality switching and information load adjustments during preflight tasks. These insights informed the design of AdaptiveCoPilot, which integrates cognitive state assessments, behavioral data, and adaptive strategies within a context-aware Large Language Model (LLM). The system was evaluated in a virtual reality (VR) simulated cockpit with licensed pilots (N=8), comparing its performance against baseline and random feedback conditions. The results indicate that the pilots using AdaptiveCoPilot exhibited higher rates of optimal cognitive load states on the facets of working memory and perception, along with reduced task completion times. Based on the formative study, experimental findings, qualitative interviews, we propose a set of strategies for future development of neuroadaptive pilot guidance systems and highlight the potential of neuroadaptive systems to enhance pilot performance and safety in aviation environments.},
  keywords={Visualization;Adaptive systems;Three-dimensional displays;Virtual reality;User interfaces;Cognitive load;Real-time systems;Time measurement;Functional near-infrared spectroscopy;Testing;Aviation in virtual reality;Adaptive user interface},
  doi={10.1109/VR59515.2025.00088},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937480,
  author={Brübach, Larissa and Celikhan, Deniz and Rüffert, Lennard and Westermeier, Franziska and Latoschik, Marc Erich and Wienrich, Carolin},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={When Fear Overshadows Perceived Plausibility: The Influence of Incongruencies on Acrophobia in VR}, 
  year={2025},
  volume={},
  number={},
  pages={667-677},
  abstract={Virtual Reality Exposure Therapy (VRET) has become an effective, customizable, and affordable treatment for various psychological and physiological disorders. Specifically, it is used to treat specific anxiety disorders, such as acrophobia or arachnophobia, for decades. However, to ensure a positive outcome for patients, we must understand and control the effects potentially caused by the technology and medium of Virtual Reality (VR) itself. This article specifically investigates the impact of the Plausibility illusion (Psi), as one of the two theorized presence components, on the fear of heights. In two experiments, 30 participants each experienced two different heights with congruent and incongruent object behaviors in a 2 x 2 within-subject design. Results show that the strength of the congruence manipulation plays a significant role. Only when incongruencies are strong enough will they be recognized by users, specifically in high fear conditions, as triggered by exposure to increased heights. If incongruencies are too subtle, they seem to be overshadowed by the stronger fear reactions. Our evidence contributes to recent theories of VR effects and emphasizes the importance of understanding and controlling factors potentially assumed to be incidental, specifically during VRET designs. Incongruencies should be controlled so that they do not have an unwanted influence on the patient’s fear response.},
  keywords={Three-dimensional displays;Anxiety disorders;Semantics;Medical treatment;Psychology;Cause effect analysis;Virtual reality;User interfaces;Physiology;Safety;XR;VR;Plausibility;Congruence;Acrophobia;Virtual reality exposure therapy},
  doi={10.1109/VR59515.2025.00089},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937343,
  author={Kim, Kirak and Kim, Hyojin and Choi, Youjin and Nam, Juhan and Lee, Jeongmi},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Designing VR Music Game for Stress Reduction}, 
  year={2025},
  volume={},
  number={},
  pages={678-685},
  abstract={Many individuals experience everyday stress. Effective stress management in daily life is crucial before this stress accumulates. Music has been extensively studied as a method for reducing stress. In particular, music therapy is widely used to reduce stress and enhance the well-being of various clinical groups. However, traditional music therapy has physical constraints that require patients to visit a therapeutic location. VR music therapy has been studied to address these issues, but most research focuses on receptive music therapy, failing to utilize VR’s interactive potential fully. Additionally, the potential of applying gamification to VR active music therapy to enhance user engagement and encourage long-term use of the therapy application has not been explored. This paper proposes VR active music therapy based on conventional rhythm-following music therapy methods and VR gamified active music therapy by adding game elements based on Self-Determination Theory (SDT). Our between-subject comparative study (n=33) revealed the stress reduction effects of VR receptive music therapy, VR active music therapy, and VR gamified active music therapy. Importantly, participant interviews provided valuable insights into the user experiences with each VR content, confirming the potential for long-term use of VR gamified active music therapy. Moreover, this research delves into the effect of gamification on stress reduction. Through pilot test and experiments, we identify game elements that could potentially increase stress and provide guidelines for applying gamification to mitigate these factors, thereby enhancing the effectiveness of VR gamified active music therapy.},
  keywords={Human computer interaction;Three-dimensional displays;Medical treatment;Psychology;Human factors;Games;Virtual reality;Interviews;Stress;Guidelines;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality;Applied computing—Law, social, and behavioral sciences—Psychology},
  doi={10.1109/VR59515.2025.00090},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937452,
  author={Li, Jiasheng and Zhang, Zining and Yan, Zeyu and Zhao, Yuhang and Peng, Huaishu},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing Vibrotactile and Skin-Stretch Haptic Feedback for Conveying Spatial Information of Virtual Objects to Blind VR Users}, 
  year={2025},
  volume={},
  number={},
  pages={686-696},
  abstract={Perceiving spatial information of a virtual object (e.g., direction, distance) is critical yet challenging for blind users seeking an immersive virtual reality (VR) experience. To facilitate VR accessibility for blind users, in this paper, we investigate the effectiveness of two types of haptic cues—vibrotactile and skin-stretch cues—in conveying the spatial information of a virtual object when applied to the dorsal side of a blind user’s hand. We conducted a user study with 10 blind users to investigate how they perceive static and moving objects in VR with a custom-made haptic apparatus. Our results reveal that blind users can more accurately understand an object’s location and movement when receiving skin-stretch cues, as opposed to vibrotactile cues. We discuss the pros and cons of both types of haptic cues and conclude with design recommendations for future haptic solutions for VR accessibility.},
  keywords={Vibrations;Hands;Three-dimensional displays;Accuracy;Virtual reality;User interfaces;Haptic interfaces;Accessibility;Haptic Feedback;Vibration;SkinStretch;Virtual Reality;Human-Subjects Quantitative Studies},
  doi={10.1109/VR59515.2025.00091},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937474,
  author={Wang, Xinkai and Yang, Yue and Zhou, Kehong and Xie, Xue and Zhu, Lifeng and Song, Aiguo and Daniel, Bruce},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic Computed Tomography}, 
  year={2025},
  volume={},
  number={},
  pages={697-707},
  abstract={Chinese acupuncture practitioners primarily depend on muscle memory and tactile feedback to insert needles and accurately target acupuncture points, as the current workflow lacks imaging modalities and visual aids. Consequently, new practitioners often learn through trial and error, requiring years of experience to become proficient and earn the trust of patients. Medical students face similar challenges in mastering this skill. To address these challenges, we developed an innovative system, MRUCT, that integrates ultrasonic computed tomography (UCT) with mixed reality (MR) technology to visualize acupuncture points in real-time. This system offers offline image registration and real-time guidance during needle insertion, enabling them to accurately position needles based on anatomical structures such as bones, muscles, and auto-generated reference points, with the potential for clinical implementation. In this paper, we outline the non-rigid registration methods used to reconstruct anatomical structures from UCT data, as well as the key design considerations of the MR system. We evaluated two different 3D user interface (3DUI) designs and compared the performance of our system to traditional workflows for both new practitioners and medical students. The results highlight the potential of MR to enhance therapeutic medical practices and demonstrate the effectiveness of the system we developed.},
  keywords={Three-dimensional displays;Computed tomography;Mixed reality;Virtual reality;Anatomical structure;User interfaces;Needles;Acoustics;Acupuncture;Biomedical imaging;Acupuncture;Mixed Reality;Ultrasonic Computed Tomography;Medical Assistance},
  doi={10.1109/VR59515.2025.00092},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937406,
  author={Bovo, Riccardo and Abreu, Steven and Ahuja, Karan and Gonzalez, Eric J and Cheng, Li-Te and Gonzalez-Franco, Mar},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={EmBARDiment: an Embodied AI Agent for Productivity in XR}, 
  year={2025},
  volume={},
  number={},
  pages={708-717},
  abstract={XR devices running chat-bots powered by Large Language Models (LLMs) have the to become always-on agents that enable much better productivity scenarios. Current screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query. We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment. Our work minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.},
  keywords={Productivity;Visualization;Three-dimensional displays;Extended reality;Large language models;Memory management;Gaze tracking;User interfaces;Chatbots;Artificial intelligence;AI Agents;Chatbots;XR productivity;Multi-window;AI input},
  doi={10.1109/VR59515.2025.00093},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937459,
  author={Sonlu, Sinan and Bendiksen, Bennie and Durupinar, Funda and Güdükbay, Uğur},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Embodiment and Personality in LLM-Based Conversational Agents}, 
  year={2025},
  volume={},
  number={},
  pages={718-728},
  abstract={This work investigates the effects of personality expression and embodiment in conversational agents. We extend a personality-driven conversational agent framework by integrating LLM-based conversation support to provide information about contemporary scientific topics. We describe a user study built on this system to evaluate two opposing personality styles using three models: a dialogue-only model that conveys personality verbally, an animated human model that expresses personality only through dialogue, and an animated human model expressing personality through dialogue and expressive animations. The users perceive all models positively regarding personality and learning outcomes; however, models with high personality traits are perceived as more engaging than those with low personality traits. We provide an analysis of personality perception, learning, and user experience.},
  keywords={Solid modeling;Three-dimensional displays;Generative Pre-trainer transformer;Oral communication;Virtual reality;User interfaces;Animation;Transformers;User experience;Stability analysis;Five-factor personality;Generative Pre-trained Transformer (GPT);Large Language Model (LLM);Conversational agent;Dialogue;Character animation},
  doi={10.1109/VR59515.2025.00094},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937432,
  author={Peck, Tabitha C. and Cha, Pauline},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Reliability and Validity of the Avatar Embodiment Questionnaire}, 
  year={2025},
  volume={},
  number={},
  pages={729-737},
  abstract={The study of self-avatars in virtual and augmented reality research continues to be an important research area. These virtual self-avatars have been shown to improve user performance and to modify people’s behaviors through the Proteus effect. However, performance and behavior changes are impacted by how embodied one is in their self-avatar such that higher avatar embodiment improves performance and induces greater behavior changes. It has become increasingly important to be able to measure how embodied one is within their self-avatar to better investigate and understand the impact of avatar embodiment on users. Multiple questionnaires have been proposed and have demonstrated their validity. However no questionnaire has demonstrated reliability—the degree of agreement between repeated uses of the same measurement instrument over time. The reliability of a questionnaire is essential for within-participant studies where a single user’s embodiment level is assessed multiple times. In this paper we assess the reliability of the Avatar Embodiment Questionnaire (AEQ) proposed by Peck and Gonzalez-Franco with an undergraduate population (n=58) through 2-5 repeated exposures, each approximately two-weeks apart. Participants wore same or cross-gendered avatars for the duration of the experiment. The questionnaire results were assessed using three common reliability analyses. For participants in same-gendered avatars, the AEQ demonstrated acceptable or better reliability across all trials. For participants in cross-gendered avatars, the AEQ demonstrated questionable reliability for the initial trial and acceptable to excellent reliability for all other trials. Results support that the AEQ is a reliable measurement tool for assessing virtual embodiment. However, to produce reliable results, minor changes to experimental design should be considered when including avatars that are diversely different from the user.},
  keywords={Training;Three-dimensional displays;Atmospheric measurements;Avatars;Psychology;User interfaces;Particle measurements;Reliability engineering;Time measurement;Reliability;Virtual Reality;Virtual Embodiment;BodyOwnership Illusions;Avatars;Questionnaires},
  doi={10.1109/VR59515.2025.00095},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937423,
  author={Zhao, Sheng and Zhu, Junrui and Zhang, Shuning and Wang, Xueyang and Li, Hongyi and Yi, Fang and Yi, Xin and Li, Hewu},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={CoordAuth: Hands-Free Two-Factor Authentication in Virtual Reality Leveraging Head-Eye Coordination}, 
  year={2025},
  volume={},
  number={},
  pages={738-748},
  abstract={We present CoordAuth, a gaze-based two-factor authentication technique in VR utilizing implicit head-eye motion features to offer a more secure and natural alternative to traditional pattern-based authentication. Users authenticate by performing gestures across 3×3 grid points using their eyes. We first optimized CoordAuth’s UI by evaluating participants’ input performance and experiences across different grid sizes. Then we extracted the head-eye coordination features during pattern entering to construct CoordAuth’s authentication algorithm, which ensembles Random Forest classifiers across saccade and fixations segments. CoordAuth demonstrated strong security with a 1.6% FAR and 1.5% FRR across 24 registered users in the password collision scenarios. A subsequent study demonstrated that CoordAuth achieved an 0.6% Attack Success Rate (ASR) against shoulder-surfing attack from a 1-meter distance. Usability evaluations in standing, sitting, and moving postures showed that CoordAuth achieved significantly faster authentication speed and lower rejection rate than laser and touch-based baselines. Meanwhile, it was the most preferred by the participants in terms of social acceptance and physical effort.},
  keywords={Three-dimensional displays;Authentication;Virtual reality;Passwords;User interfaces;Feature extraction;Robustness;Security;Usability;Random forests;Authentication;Behavioral biometrics;Gaze interaction;Fixation and saccade;Virtual Reality;Applied Machine Learning},
  doi={10.1109/VR59515.2025.00096},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937420,
  author={Wang, Yuhui and Takashima, Kazuki and Ito, Masamitsu and Kobori, Takeshi and Asakura, Tomo and Fujita, Kazuyuki and Hong, Guang and Kitamura, Yoshifumi},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={VirtuEleDent: A Compact XR Tooth-Cutting Training System Using a Physical EMR-based Dental Handpiece and Teeth Model}, 
  year={2025},
  volume={},
  number={},
  pages={749-758},
  abstract={Dental cutting is a crucial skill for dental students. However, current VR dental cutting training systems rely on bulky and costly haptic devices, which reduce opportunities for individual practice. Moreover, the limitations imposed by the maximum reaction force of an active haptic device would impact the range of tooth hardness that can be reproduced. We propose a compact XR tooth-cutting training system, VirtuEleDent, that employs a passive haptic approach using a 3D-printed physical teeth model and a three-dimensionally tracked handpiece. Their spatial relationship is accurately rendered in the virtual environment of a mobile head-mounted display (HMD), providing users with realistic haptic sensations during virtual tooth-cutting exercises. Our tracking platform is operated using electromagnetic resonance (EMR) stylus technology and consists of a digitizer (i.e., tracking board) and a handpiece device. A customized EMR stylus unit (i.e., resonance coil) and an inertial measurement unit (IMU) sensor are installed inside the handpiece, allowing for precise measurement of its tip’s 3D position and orientation. This setup enables the learner to physically manipulate dexterous handpieces on the teeth model while experiencing virtual tooth-cutting in the HMD. First, we detail the design and implementation of our initial prototype system, including the tracking platform. We then describe an expert user study conducted with our prototype, where licensed dentists compare VirtuEleDent to a baseline with a conventional haptic device. Finally, the educator’s feedback demonstrates its potential and informs our further improvements and the directions of future development.},
  keywords={Solid modeling;Three-dimensional displays;Prototypes;Virtual environments;Teeth;Resists;Resonance;Haptic interfaces;Dentistry;Electromagnetics;Extended reality application;electromagnetic resonance motion tracker},
  doi={10.1109/VR59515.2025.00097},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937342,
  author={Wei, Jishang and Siegel, Erika and Sundaramoorthy, Prahalathan and Gomes, Antônio and Zhang, Shibo and Vankipuram, Mithra and Smathers, Kevin and Ghosh, Sarthak and Horii, Hiroshi and Bailenson, Jeremy and Ballagas, Rafael ‘Tico’},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Cognitive Load Inference Using Physiological Markers in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={759-769},
  abstract={Virtual reality (VR) has become an increasingly popular way for learning and training. The assessment of the amount of mental effort, or cognitive load required to perform a task, is essential to create adaptive VR experiences. In this work, we conducted a large-scale study (N=738) to collect behavioral and physiological measures under different cognitive load conditions in a VR environment, and developed a novel machine learning solution to predict cognitive load in real time. Our model predicts cognitive load as a continuous value in the range from 0 to 1, where 0 and 1 correspond to the lowest and highest reported cognitive loads across all participants. On top of the point estimation of cognitive load, our model quantifies prediction uncertainty using a prediction interval. We propose a novel dual-branch attention model to accurately predict the cognitive load. We achieve a MAE (Mean Absolute Error) of 0.11. The result indicates that, with a combination of behavioral and physiological indicators, we can reliably predict cognitive load in real-time, without calibration. To support further research, we are releasing a test dataset comprising data from 100 participants for use by researchers and developers interested in machine learning, virtual reality, learning & memory, cognition, or psychophysiology. This dataset includes recordings from multiple sensors (including pupillometry, eye-tracking, and pulse plethysmography), self-reported cognitive effort, behavioral task performance, and demographic information on the sample.},
  keywords={Training;Solid modeling;Virtual reality;Machine learning;Predictive models;Cognitive load;Physiology;Real-time systems;Reliability;Load modeling;Virtual Reality;Cognitive Load;Physiological Signals;Machine Learning},
  doi={10.1109/VR59515.2025.00098},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{10937396,
  author={Zhu, Fengyuan and Qian, Xun and Kalmar, Daniel and Tayarani, Mahdi and Gonzalez, Eric J. and Gonzalez-Franco, Mar and Kim, David and Du, Ruofei},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Beyond the Phone: Exploring Phone-XR Integration through Multi-View Transitions for Real-World Applications}, 
  year={2025},
  volume={},
  number={},
  pages={770-780},
  abstract={Despite the growing prevalence of Extended Reality (XR) headsets, their integration with mobile phones remains limited. Existing approaches primarily replicate the phone’s interface in XR or use the phone solely as a 6DOF controller. This paper introduces a novel framework for seamless transitions among mirrored, magnified, and augmented views, dynamically adapts the interface with the content and state of mobile applications. To achieve this, we establish a design space through literature reviews and expert workshops, outline user journeys with common real-world applications, and develop a prototype system that automatically analyzes UI layouts to provide enhanced controls and spatial augmentation. We validate our prototype system with a user study to assess its adaptability to a broad spectrum of applications at runtime, reported its strengths and weaknesses, and suggest directions to advance the future adaption in Phone-XR integration.},
  keywords={Technological innovation;Solid modeling;Three-dimensional displays;Runtime;User centered design;Prototypes;User interfaces;Real-time systems;Smart phones;Systematic literature review;Cross-Device Interaction;Phone-XR Intergration},
  doi={10.1109/VR59515.2025.00099},
  ISSN={2642-5254},
  month={March},}
