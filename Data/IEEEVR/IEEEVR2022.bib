@INPROCEEDINGS{9756782,
  author={Li, Yi-Jun and Shi, Jinchuan and Zhang, Fang-Lue and Wang, Miao},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Bullet Comments for 360°Video}, 
  year={2022},
  volume={},
  number={},
  pages={1-10},
  abstract={Time-anchored on-screen comments, as known as bullet comments, are a popular feature for online video streaming. Bullet comments reflect audiences’ feelings and opinions at specific video timings, which have been shown to be beneficial to video content understanding and social connection level. In this paper, we for the first time investigate the problem of bullet comment display and insertion for 360° video via head-mounted display and controller. We design four bullet comment display methods and evaluate their effects on 360° video experiences. We further propose two controller-based methods for bullet comment insertion. Combining the display and insertion methods, the user can experience 360° videos with bullet comments, and interactively post new ones by selecting among existing comments. User study results revealed how the factors of display and insertion methods affect 360° video experience. With the experiment findings, we also discuss useful design insights for 360° video bullet comments.},
  keywords={Headphones;Visualization;Three-dimensional displays;Head-mounted displays;Conferences;Virtual reality;Streaming media;Bullet Comments;360° Video;Danmaku;Dan Mu},
  doi={10.1109/VR51125.2022.00017},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756815,
  author={Thomas, Sean and Ferstl, Ylva and McDonnell, Rachel and Ennis, Cathy},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating how speech and animation realism influence the perceived personality of virtual characters and agents}, 
  year={2022},
  volume={},
  number={},
  pages={11-20},
  abstract={The portrayed personality of virtual characters and agents is understood to influence how we perceive and engage with digital applications. Understanding how the features of speech and animation drive portrayed personality allows us to intentionally design characters to be more personalized and engaging. In this study, we use performance capture data of unscripted conversations from a variety of actors to explore the perceptual outcomes associated with the modalities of speech and motion. Specifically, we contrast full performance-driven characters to those portrayed by generated gestures and synthesized speech, analysing how the features of each influence portrayed personality according to the Big Five personality traits. We find that processing speech and motion can have mixed effects on such traits, with our results highlighting motion as the dominant modality for portraying extraversion and speech as dominant for communicating agreeableness and emotional stability. Our results can support the Extended Reality (XR) community in development of virtual characters, social agents and 3D User Interface (3DUI) agents portraying a range of targeted personalities.},
  keywords={Three-dimensional displays;Extended reality;Conferences;User interfaces;Animation;Stability analysis;Cognition;Embodied agents;virtual humans and (self-)avatars;Perception and cognition},
  doi={10.1109/VR51125.2022.00018},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756743,
  author={Nicolau, Francisco and Gielis, Johan and Simeone, Adalberto L. and Simões Lopes, Daniel},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring and Selecting Supershapes in Virtual Reality with Line, Quad, and Cube Shaped Widgets}, 
  year={2022},
  volume={},
  number={},
  pages={21-28},
  abstract={Supershapes are used in Parametric Design to model, literally, thou-sands of natural and man-made shapes with a single 6 parameter formula. However, users are left to probe such a rich yet dense collection of supershapes using a set of independent 1-D sliders. Some of the formula’s parameters are non-linear in nature, making them particularly difficult to grasp with conventional 1-D sliders alone. VR appears as a promising setting for Parametric Design with supershapes since it empowers users with more natural visual inspection and shape browsing techniques, with multiple solutions being displayed at once and the possibility to design more interesting forms of slider interaction. In this work, we propose VR shape widgets that allow users to probe and select supershapes from a multitude of solutions. Our designs take leverage on thumbnails, mini-maps, haptic feedback and spatial interaction, while supporting 1-D, 2-D and 3-D supershape parameter spaces. We conducted a user study (N = 18) and found that VR shape widgets are effective, more efficient, and natural than conventional VR 1-D sliders while also usable for users without prior knowledge on supershapes. We also found that the proposed VR widgets provide a quick overview of the main supershapes, and users can easily reach the desired solution without having to perform fine-grain handle manipulations.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Shape;Conferences;Virtual reality;User interfaces;Human-centered computing;User interface design;Virtual reality Human-centered computing;Graphical user interfaces},
  doi={10.1109/VR51125.2022.00019},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756783,
  author={Kavakli, Manolya and Cremona, Cinzia},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Virtual Production Studio Concept – An Emerging Game Changer in Filmmaking}, 
  year={2022},
  volume={},
  number={},
  pages={29-37},
  abstract={This paper analyses methodological and conceptual shifts in filmmaking brought about by the integrated use of VR technology, game engines, and LED walls in a Virtual Production Studio (VPS). The paper focuses not only on the components of an integrated VPS system, but also on what makes it qualitatively different from traditional filmmaking technologies. The VPS concept transforms the relationships among display technology, live action, image production, and postproduction (VFX) by causing a qualitative and performative shift in filmmaking. The paper analyses the current state of VP research from both conceptual and technological perspectives, global VP facilities, and significant trends in Australia by presenting two business case studies. We have observed three major trends: Globalisation, Collaboration and Industrialisation. The paper concludes by stating that there is a need for an integrated VPS network through shared infrastructure and resources to foster high quality research and to solve industry-identified problems through industry-led collaborative research partnerships.},
  keywords={Tracking;Collaboration;Globalization;Entertainment industry;Production;Games;Cameras;Cinematography;Virtual reality;Virtual production studio;LED walls;filmmaking;in camera VFX;Film;K.6.1 [Management of Computing and Information Systems];Project and People Management—Life Cycle; K.7.m [The Computing Profession];Miscellaneous—Ethics},
  doi={10.1109/VR51125.2022.00020},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756803,
  author={Zhang, Aijia and Zhao, Yan and Wang, Shigang and Wei, Jian},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An improved augmented-reality method of inserting virtual objects into the scene with transparent objects}, 
  year={2022},
  volume={},
  number={},
  pages={38-46},
  abstract={In augmented reality, the insertion of virtual objects into the real scene needs to meet the requirements of visual consistency. The virtual objects rendered by the augmented reality system should be consistent with the illumination of the real scene. However, for complex scenes, it is not enough to just complete the illumination estimation. When there are transparent objects in the real scene, the difference in refractive index and roughness of transparent objects will influence the effect of the virtual and real fusion. To tackle this problem, this paper proposes a new approach to jointly estimate the illumination and transparent material for inserting virtual objects into the real scene. We solve for the material parameters of objects and illumination simultaneously by nesting microfacet model and hemispherical area illumination model into inverse path tracing. Although there is no geometry model of light sources in the recovered geometry model, the proposed hemispherical area illumination model can be used to recover scene appearance. Multiple experiments on both virtual and real-world datasets verify that the proposed approach subjectively and objectively performs better than the state-of-the-art method.},
  keywords={Geometry;Solid modeling;Visualization;Three-dimensional displays;Lighting;Refractive index;Estimation;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Artificial intelligence—Computer vision—Computer vision representations—Appearance and texture representations},
  doi={10.1109/VR51125.2022.00021},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756742,
  author={Zhang, Chunlan and Lin, Chunyu and Liao, Kang and Nie, Lang and Zhao, Yao},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SivsFormer: Parallax-Aware Transformers for Single-image-based View Synthesis}, 
  year={2022},
  volume={},
  number={},
  pages={47-56},
  abstract={Single-image-based view synthesis is significant for generating a 3D scene and gains increasing attention in recent years. However, this task is challenging as it requires inferring contents beyond what is immediately visible. Previous methods directly predict the unknown views using the convolutional neural networks, but the generated views suffer from visually unpleasant holes, deformations, and artifacts. In this paper, we propose a Single-image-based view synthesis transformer (named SivsFormer) for high-quality and realistic view synthesis. In particular, a warping and occlusion handing module is designed to reduce the influence of parallax on the network. Subsequently, a disparity alignment module captures the long-range information over the scene and ensures that pixels move in a geometrically correct manner with soft probabilistic disparity maps. Moreover, we present a parallax-aware loss function to improve the quality of the synthetic images, which explicitly quantifies the magnitude of parallaxes. We conduct extensive experiments on popular KITTI and Cityscapes datasets. Benefitting from the proposed parallax-aware transformer, our approach achieves superior performance in both quantitative and qualitative evaluations.},
  keywords={Visualization;Three-dimensional displays;Conferences;User interfaces;Transformers;Probabilistic logic;Motion pictures;Single-image-based view synthesis;SivsFormer;Vision Transformers;Parallax-aware Alignment},
  doi={10.1109/VR51125.2022.00022},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756729,
  author={Ma, Fang and Pan, Xueni},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Fidelity Effects on Expressive Self-avatar in Virtual Reality: First Impressions Matter}, 
  year={2022},
  volume={},
  number={},
  pages={57-65},
  abstract={Owning a virtual body inside Virtual Reality (VR) offers a unique experience where, typically, users are able to control their self-avatar’s body via tracked VR controllers. However, controlling a self-avatar’s facial movements is harder due to the HMD being in the way for tracking. In this work we present (1) the technical pipeline of creating and rigging self-alike avatars, whose facial expressions can be then controlled by users wearing the VIVE Pro Eye and VIVE Facial Tracker, and (2) based on this setting, two within-group studies on the psychological impact of the appearance realism of self-avatars, both the level of photorealism and self-likeness. Participants were told to practise their presentation, in front of a mirror, in the body of a realistic looking avatar and a cartoon like one, both animated with body and facial mocap data. In study 1 we made two bespoke self-alike avatars for each participant and we found that although participants found the cartoon-like character more attractive, they reported higher Body Ownership with whichever the avatar they had in the first trial. In study 2 we used generic avatars with higher fidelity facial animation, and found a similar "first trial effect" where they reported the avatar from their first trial being less creepy. Our results also suggested participants found the facial expressions easier to control with the cartoon-like character. Further, our eye-tracking data suggested that although participants were mainly facing their avatar during their presentation, their eye-gaze were focused elsewhere half of the time.},
  keywords={Visualization;Three-dimensional displays;Tracking;Atmospheric measurements;Avatars;Pipelines;Psychology;Virtual Reality;Facial Expressions;Embodiment},
  doi={10.1109/VR51125.2022.00023},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756768,
  author={Bernal, Guillermo and Hidalgo, Nelson and Russomanno, Conor and Maes, Pattie},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Galea: A physiological sensing system for behavioral research in Virtual Environments}, 
  year={2022},
  volume={},
  number={},
  pages={66-76},
  abstract={The pairing of Virtual Reality technology with Physiological Sensing has gained much interest in clinical settings and beyond: from developing novel methods for diagnosis of perception and cognition impairments, biofeedback for anxiety treatment, to enhancing everyday practices such as self-guided meditation. However, conducting this type of research does not come without challenges. For example, accessing the equipment for recording data from the user and synchronizing physiological response data with the stimuli or interactive environment are not trivial tasks, and generating virtual content in response to the user’s real-time data is costly and complex. This paper presents Galea, a device for multi-modal signal acquisition able to measure the physiological response of a user when experiencing virtual content, enabling behavioral, affective computing , and human-computer interaction research and applications to access data from the Parasympathetic nervous system and Sympathetic nervous system simultaneously. We present a primer on detectable human physiology as an input source for Physiological Computing from the perspective of the signals available through our device. We describe the primary design considerations and circuit characterization results of in-vivo recordings from the wearer’s brain, eyes, heart, skin, and muscles. We also present an example to help contextualize how these signals can be used in a virtual reality setting. Galea makes working with physiological sensors in virtual reality more accessible and can offer a standard for inter and intra experiment data comparisons. Lastly, we discuss the importance and contributions of this work as well as future challenges that need to be considered.},
  keywords={Heart;Visualization;Virtual environments;Sensor phenomena and characterization;Physiology;Skin;Real-time systems;Human-centered computing;Human Computer Interaction (HCI);Input devices;Tracking and sensing;Toolkits;User/Machine Systems;Human factors;Brain-Computer Interfaces;Steady State Visual Evoked Potentials (SSVEP);Virtual Reality (VR)},
  doi={10.1109/VR51125.2022.00024},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756795,
  author={Rettinger, Maximilian and Schmaderer, Christoph and Rigoll, Gerhard},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Do You Notice Me? How Bystanders Affect the Cognitive Load in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={77-82},
  abstract={In contrast to the real world, users are not able to perceive bystanders in virtual reality (VR). Bystanders may distract users and influence their cognitive load. This involves users to feel discomfort at the thought of unintentionally touching or even bumping into a physical bystander while interacting with the virtual environment. Not knowing the intentions of a bystander or whether one is present can unsettle the user. We investigate how a bystander affects a user’s cognitive load since it has a decisive impact on applications such as VR training. In a between-subjects lab study (N = 42), three conditions were compared: 1) no bystander, 2) an invisible bystander, and 3) a visible bystander (as an avatar). Over a series of iterations, the participants were asked to memorize four pairs of letters, perform a mental rotation task and then recall the pairs of letters. The results of our study demonstrate that a bystander acting as an avatar in the virtual environment increases the user’s cognitive load more than an invisible bystander. Moreover, the cognitive load of a VR user is significantly increased by a bystander. Therefore, our work suggests that either the examiner must be separated from the participant or the examiner’s influence (as a bystander) must be included in the analysis.},
  keywords={Training;Social computing;Three-dimensional displays;Avatars;Virtual environments;User interfaces;Particle measurements;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Collaborative and social computing;Empirical studies in collaborative and social computing},
  doi={10.1109/VR51125.2022.00025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756750,
  author={Zhou, Yuqi and Popescu, Voicu},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tapping with a Handheld Stick in VR: Redirection Detection Thresholds for Passive Haptic Feedback}, 
  year={2022},
  volume={},
  number={},
  pages={83-92},
  abstract={This paper investigates providing grounded passive haptic feedback to a user of a VR application through a handheld stick with which the user taps virtual objects. Such an investigation benefits VR applications beyond those where the stick interaction is actually an integral part of the narrative. Providing passive haptic feedback through a handheld stick as opposed to directly through the user’s hand has the potential for more believable and more frequent feedback opportunities. The stick is likely to dull the user’s haptics perception and proprioception, potentially avoiding a haptics perception uncanny valley and increasing the redirection detection thresholds. Two haptics redirection methods are proposed: the DriftingHand method, which alters the position of the user’s virtual hand, and the Vari-Stick method, which alters the length of the virtual stick. Detection thresholds were measured in a user study (N = 60) by testing the two methods for a range of offsets between the virtual and the real object, for multiple stick lengths, and multiple distances from the user to the real object. Overall, the study reveals that VariStick and DriftingHand provide an undetectable range of offsets of [-20cm, +13cm] and [-11cm, +11cm], respectively.},
  keywords={Visualization;Three-dimensional displays;Design methodology;Conferences;Virtual reality;User interfaces;Length measurement;Grounded passive haptics redirection;haptic retargeting;detection thresholds;handheld prop;virtual reality;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing— Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR51125.2022.00026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756774,
  author={Lottridge, Danielle and Weber, Rebecca and McLean, Eva-Rae and Williams, Hazel and Cook, Joanna and Bai, Huidong},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Design Space for Immersive Embodiment in Dance}, 
  year={2022},
  volume={},
  number={},
  pages={93-102},
  abstract={There are decades of work investigating how to support users in inhabiting avatars’ bodies in immersive experiences, however we are still learning about how Virtual Reality (VR) can support people in more deeply inhabiting their own bodies. Over the course of five action research workshops, we explored the design space for how VR can support feeling embodied and generating movement within a dance context. We explored design factors and participants’ experiences within: games intended to stimulate physical movement, single player and multiplayer 3D painting, 360 live video, and custom real-time audio and visual feedback based on motion capture. We found that participants spontaneously explored collaboration including synchronous, asynchronous, remote and collocated. Where there were mismatches between visual and kinesthetic perceptions, participants explored how to recalibrate, alternate between, and integrate perceptions. Participants were compelled to explore their control of real-time effects, which led to rich movement generation as we iterated through the effects’ parameters. We present a design space that encapsulates the insights of our action research, with axes for control, collaboration, auditory and visual feedback. We discuss implications for the support of immersive embodiment in dance.},
  keywords={Visualization;Three-dimensional displays;Conferences;Collaboration;Virtual reality;Games;Aerospace electronics;virtual reality;embodiment;creativity support;dance},
  doi={10.1109/VR51125.2022.00027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756746,
  author={Wu, Fei and Rosenberg, Evan Suma},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Asymmetric Lateral Field-of-View Restriction to Mitigate Cybersickness During Virtual Turns}, 
  year={2022},
  volume={},
  number={},
  pages={103-111},
  abstract={Field-of-view (FOV) restriction is a common technique to reduce cybersickness in commercial virtual reality (VR) applications. However, the majority of existing FOV restriction techniques are implemented as symmetric imagery, which occludes users’ views during virtual rotation. In this paper, we proposed and evaluated a novel variant of FOV restriction, referred to as a side restrictor. Side restriction uses an asymmetric mask to obscure only one side region of the periphery during virtual rotation and laterally shifts the center of restriction towards the direction of the turn. We conducted a study using a between-subjects design that compared the side restrictor, a traditional symmetric restrictor, and a control condition without FOV restriction. Participants were required to navigate through a complex maze-like environment using a controller using one of three restrictors. Compared to the control condition, the side restrictor was effective in mitigating cybersickness, reducing discomfort, improving subjective visibility, and enabling users to remain immersed for a longer period of time. Additionally, we found no empirical evidence of negative drawbacks when compared to the symmetric restrictor, which suggests that side restriction is an effective cybersickness mitigation technique for virtual environments with frequent turns.},
  keywords={Three-dimensional displays;Cybersickness;Navigation;Design methodology;Conferences;Virtual environments;Games;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;HCI design and evaluation methods;User studies},
  doi={10.1109/VR51125.2022.00028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756818,
  author={Achberger, Alexander and Arulrajah, Pirathipan and Sedlmair, Michael and Vidackovic, Kresimir},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={STROE: An Ungrounded String-Based Weight Simulation Device}, 
  year={2022},
  volume={},
  number={},
  pages={112-120},
  abstract={We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.},
  keywords={Solid modeling;Three-dimensional displays;Conferences;Computational modeling;Force;Virtual reality;Footwear;Human-centered computing;Haptic devices},
  doi={10.1109/VR51125.2022.00029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756830,
  author={Kahl, Denise and Ruble, Marc and Krüger, Antonio},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Influence of Environmental Lighting on Size Variations in Optical See-through Tangible Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={121-129},
  abstract={Optical see-through head-mounted displays (OST HMDs) are becoming increasingly popular as they get better and smaller. One application area is interaction with virtual content, which is more intuitive when using physical objects as tangibles. Since it is not possible to use a matching replica for each virtual object, it is necessary to identify physical objects that can represent several different virtual objects. As a first step, we investigated to what extent a physical object can differ in size from its virtual counterpart.Since the perception of content in optical see-through Augmented Reality (OST AR) is strongly influenced by the ambient lighting, the illumination intensity was considered in our study. We investigated three indoor lighting conditions and their effects on the perception of seven different size variations between the physical object and its virtual overlay.The results of the study show that there is a decrease in usability and presence with increasing illuminance. However, this cannot be avoided when applications are run under realistic interior lighting conditions. Furthermore, the results demonstrate that the size ranges in which a physical object can deviate from its virtual counterpart without having a strong negative impact on usability, presence and performance increase with increasing environmental illumination. Therefore, it is possible to interact with even smaller and even larger physical props to manipulate the associated virtual content under brighter lighting conditions.},
  keywords={Three-dimensional displays;Head-mounted displays;Shape;Conferences;Lighting;User interfaces;Object recognition;Tangible augmented reality;optical see-through augmented reality;tangible interaction;illumination},
  doi={10.1109/VR51125.2022.00030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756810,
  author={Beever, Lee and John, Nigel W.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={LevelEd SR: A Substitutional Reality Level Design Workflow}, 
  year={2022},
  volume={},
  number={},
  pages={130-138},
  abstract={Virtual reality (VR) and augmented reality (AR) have continued to increase in popularity over the past decade. However, there are still issues with how much space is required for room-scale VR and experiences are still lacking from haptic feedback. We present LevelEd SR, a substitutional reality level design workflow that combines AR and VR systems and is built for consumer devices. The system enables passive haptics through the inclusion of physical objects from within a space into a virtual world. A validation study (17 participants) has produced quantitative data that suggests players benefit from passive haptics in entertainment VR games with an improved game experience and increased levels of presence. Including objects, such as real-world furniture that is paired with a digital proxy in the virtual world, also opens up more spaces to be used for room-scale VR. We evaluated the workflow and found that participants were accepting of the system, rating it positively using the System Usability Scale questionnaire and would want to use it again to experience substitutional reality.},
  keywords={Three-dimensional displays;Entertainment industry;Virtual environments;Games;Organizations;User interfaces;Software;Substitutional reality;Virtual reality;Augmented reality;Level design;Level editor;User generated content},
  doi={10.1109/VR51125.2022.00031},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756794,
  author={Grinyer, Kristen and Teather, Robert J.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Field of View on Dynamic Out-of-View Target Search in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={139-148},
  abstract={We present a study of the effects of field of view (FOV), target movement, and number of targets on visual search performance in virtual reality. We compared visual search tasks in two FOVs (~65°, ~32.5°) under two target movement speeds (static, dynamic) while varying the visible target count, with targets potentially out of the user’s view. We examined the expected linear relationship between search time and number of items, to explore how moving and/or out-of-view targets affected this relationship. Overall, search performance increased with a wide FOV, but decreased when targets were moving and with more visible targets. FOV more strongly influenced search performance than target movement. Neither FOV nor target movement meaningfully altered the linear relationship between visual search time and number of items. Participants also rated perceived workload for each condition; FOV and target movement both negatively affected the perceived workload, with target movement being a more significant factor.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Conferences;Virtual environments;User interfaces;Search problems;Mobile VR;field of view;moving targets;search},
  doi={10.1109/VR51125.2022.00032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756760,
  author={Park, Noel and Regenbrecht, Holger and Duncan, Stuart and Mills, Steven and Lindeman, Robert W. and Pantidi, Nadia and Whaanga, Hēmi},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality Co-Design for Indigenous Culture Preservation & Continuation}, 
  year={2022},
  volume={},
  number={},
  pages={149-157},
  abstract={As a result of urban concentration, colonisation, increased physical distance from tribal homelands, and globalisation, many indigenous people, including Māori, are seeking digital solutions to connect to their culture and identity. Through co-design, close collaboration with our indigenous partners, and careful consideration of cultural context, we show that mixed reality experiences can be an effective mechanism for the growing diaspora of Māori to access and experience their language, genealogy, families, histories and knowledge. Inductive analysis of semi-structured interviews highlights the importance of cultural values and context in this experience, and confirms that this approach can support connections to community and culture. Our work is deeply embedded in a particular indigenous group’s context and culture, but we believe that it holds important lessons that can generalise to other groups. In particular, collaborative co-design and recognition of cultural values throughout the project are essential for producing experiences that meet the needs of specific communities, and that reflect and respect their culture and worldview.},
  keywords={Three-dimensional displays;Mixed reality;Collaboration;Globalization;Virtual reality;User interfaces;Cultural differences;Mixed Reality;Virtual Reality;Application;Field Studies;Presence;Co-presence;Qualitative;Indigenous},
  doi={10.1109/VR51125.2022.00033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756731,
  author={Valente, Andreia and Lopes, Daniel Simões and Nunes, Nuno and Esteves, Augusto},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Empathic AuRea: Exploring the Effects of an Augmented Reality Cue for Emotional Sharing Across Three Face-to-Face Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={158-166},
  abstract={The Empathy-Effective Communication hypothesis states the better a speaker can understand their listener’s emotions, the better can they transmit information; and the better a listener can understand the speaker’s emotions, the better can they apprehend the information. Previous emotional sharing systems have managed to create a space of emotional understanding between collaborators on remote locations using bio-sensing, but how a context of face-to-face communication can benefit from biofeedback is still to be studied. This study introduces a new Augmented Reality communication cue from an emotion recognition neural network model, trained using electrocardiogram physiological data (AuRea). The proposed de-sign is meant to facilitate emotional state understanding, increasing cognitive empathy without compromising the existing verbal, non-verbal, and paraverbal communication cues. We conducted a study where pairs of participants (N=12) engaged in three tasks where AuRea was found to positively affect performance and emotional understanding, but negatively affect memorization.},
  keywords={Emotion recognition;Solid modeling;Three-dimensional displays;Neural networks;Collaboration;Physiology;Data models;Human-centered computing;Collaborative and social computing;Empirical studies in collaborative and social computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/VR51125.2022.00034},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756751,
  author={Han, Jihae and Moere, Andrew Vande and Simeone, Adalberto L.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Foldable Spaces: An Overt Redirection Approach for Natural Walking in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={167-175},
  abstract={Overt redirection is a class of virtual reality locomotion that uses perceptible transformations to enable the user to naturally walk through a virtual environment larger than the physical tracking space. In this research, we propose Foldable Spaces, a novel redirection approach based on the idea of dynamically ‘folding’ the geometry of the virtual environment to reveal new locations depending on the trajectory of the virtual reality user. Based on this approach, we developed three distinct techniques for overt redirection: (1) Horizontal, which folds and reveals layers of virtual space like the pages in a book; (2) Vertical, which rotates virtual space towards the user along a vertical axis; and (3) Accordion, which corrugates and flattens virtual space to bring faraway places closer to the user. In a within-subjects user study, we compared our proposed foldable techniques against each other along with a similarly situated redirection technique, Stop & Reset. Our findings show that Accordion was the most well-received by participants in terms of providing a smooth, continuous, and ‘natural’ experience of walking that does not involve shifts in orientation and provides an overarching view through the virtual environment.},
  keywords={Legged locomotion;Geometry;Three-dimensional displays;Conferences;Virtual environments;Medical treatment;User interfaces;Human-centered computing;Interaction Paradigms;Virtual Reality},
  doi={10.1109/VR51125.2022.00035},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756740,
  author={Medeiros, Marina L. and Schlager, Bettina and Krösl, Katharina and Fuhrmann, Anton},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Potential of VR-based Tactical Resource Planning on Spatial Data}, 
  year={2022},
  volume={},
  number={},
  pages={176-185},
  abstract={Planning tactical operations on topographic maps, for rescue or military missions, is a complex process conducted by interdisciplinary experts and involves the time-consuming derivation of 3D information from 2D maps, mostly solely executed by experienced professionals. Previous research repeatedly showed that virtual reality (VR) can convey spatial relationships and complex 3D structures intuitively. In this work, we leverage the benefits of immersive head-mounted displays (HMDs) and present the design, implementation, and evaluation of a collaborative VR application for tactical resource planning on spatial data. We derived system and design requirements from consultations with domain experts and observations of a military on-site staff exercise, a simulation-based training aiming to strengthen rapid decision-making and teamwork during a time of crisis. To evaluate our prototype, we conducted semi-structured interviews with domain experts who organized and observed field tests at different military staff exercises. The interviews support the proposed design of the prototype and show general design implications for planning tools in VR. Our results show that the potential of VR-based tactical resource planning is dependent on the technical features as well as on non-technical environmental aspects, such as user attitude, prior experience, and interoperability.},
  keywords={Training;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;Spatial databases;Planning;Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VR51125.2022.00036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756816,
  author={Richard, Grégoire and Pietrzak, Thomas and Argelaguet, Ferran and Lécuyer, Anatole and Casiez, Géry},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Within or Between? Comparing Experimental Designs for Virtual Embodiment Studies}, 
  year={2022},
  volume={},
  number={},
  pages={186-195},
  abstract={When designing virtual embodiment studies, one of the key choices is the nature of the experimental factors, either between-subjects or within-subjects. However, it is well known that each design has ad-vantages and disadvantages in terms of statistical power, sample size requirements and confounding factors. This paper reports a within-subjects experiment with 92 participants comparing self-reported embodiment scores under a visuomotor task with two conditions: synchronous motions and asynchronous motions with a latency of 300 ms. With the gathered data, using a Monte-Carlo method, we created numerous simulations of within- and between-subjects experiments by selecting subsets of the data. In particular, we explored the impact of the number of participants on the replicability of the results from the 92 within-subjects experiment. For the between-subjects simulations, only the first condition for each user was considered to create the simulations. The results showed that while the replicability of the results increased as the number of participants increased for the within-subjects simulations, no matter the number of participants, between-subjects simulations were not able to replicate the initial results. We discuss the potential reasons that could have led to this surprising result and potential methodological practices to mitigate them.},
  keywords={Three-dimensional displays;Monte Carlo methods;Conferences;Virtual reality;User interfaces;Particle measurements;Data models;Virtual Embodiment;Methodology;Latency},
  doi={10.1109/VR51125.2022.00037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756822,
  author={Weiß, Sebastian and Busse, Steffen and Heuten, Wilko},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Inducing Emotional Stress From The Intensive Care Context Using Storytelling In VR}, 
  year={2022},
  volume={},
  number={},
  pages={196-204},
  abstract={Nurses in intensive care units are exposed to permanent stress. Emotional stress contributes to psycho-physiological exhaustion symptoms such as chronic sleep disturbances, restlessness or burnout, which negatively affects the quality of care and interaction with patients. A promising method for learning coping strategies to deal with stress is Stress Inoculation Training, which involves practicing stressors in a controlled environment at increasing intensity. In this work, using virtual reality, the stressor of having to comply with the patient’s or family’s wishes, even if one does not agree with them, was selected. The stressor was defined based on a literature review and interviews with experts which was then implemented in three intensity levels. In a Wizard-of-Oz trial, participants interacted with virtual characters using natural speech. The stress response of the induced stress was measured using objective (heart rate, time between R peaks, skin conductance, and respiratory rate) and subjective measures (Perceived Stress Questionnaire and a 7-point Likert item). While objective results do not show significant differences between intensity levels, a significant increase is detected in the subjective measures. We show that emotional stress can be induced by increasing the intensity of a stressor in VR using virtual characters.},
  keywords={Training;Solid modeling;Three-dimensional displays;Atmospheric measurements;Anxiety disorders;Virtual reality;User interfaces;Human-centered computing;User Studies Human-centered computing;Virtual Reality Human-centered computing;HCI theory;concepts and models},
  doi={10.1109/VR51125.2022.00038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756828,
  author={Patras, Cristian and Cibulskis, Mantas and Nilsson, Niels Christian},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Body Warping Versus Change Blindness Remapping: A Comparison of Two Approaches to Repurposing Haptic Proxies for Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={205-212},
  abstract={When using tangible props as proxies for virtual objects, it is important that these haptic proxies are similar to and co-located with their virtual counterparts. This makes it challenging to scale virtual scenarios because more proxies are needed as scenarios grow more complex. Haptic retargeting, or virtual remapping, makes it possible to repurpose the same physical prop as a proxy for multiple virtual objects. This paper details a user study comparing two techniques for repurposing haptic proxies; namely haptic retargeting based on body warping and change blindness remapping. Participants performed a simple button-pressing task, and 24 virtual buttons were mapped onto four haptic proxies with varying degrees of misalignment. Body warping and change blindness remapping were used to realign the real and virtual buttons, and the results indicate that users failed to reliably detect realignment of up to 7.9 cm for body warping and up to 9.7 cm for change blindness remapping. Moreover, change blindness remapping yielded significantly higher self-reported agency, and marginally higher ownership. Taken together these results suggest that this less explored technique has potential when it comes to repurposing haptic proxies for virtual reality.},
  keywords={Visualization;Three-dimensional displays;Conferences;Blindness;Virtual reality;User interfaces;Haptic interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR51125.2022.00039},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756788,
  author={Barbotin, Nicolas and Baumeister, James and Cunningham, Andrew and Duval, Thierry and Grisvard, Olivier and Thomas, Bruce H.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Visual Cues for Future Airborne Surveillance Using Simulated Augmented Reality Displays}, 
  year={2022},
  volume={},
  number={},
  pages={213-221},
  abstract={This work explores the interaction between Augmented Reality (AR) and eye accommodation for airborne surveillance by simulating AR environments in Virtual Reality (VR). We simulate the AR display as displays with the capabilities needed for airborne surveillance are limited and because it would be hazardous to experiment directly on surveillance aircraft. While there is precedent for simulating AR in a VR environment, our study account for two of the physical and physiological aspects of AR: we factor in the focal plane of the AR technology and simulate the eye accommodation reflex of the user to provide focus. We ran a study with 24 participants examining AR cues to support visual search. We also compare the effects of having secondary tasks (that surveillance operators are normally responsible for) directly on the observation window using AR. Our results show that the effectiveness of the AR cues is dependent on the modality of the secondary task. We also found that, under certain situations, operators’ performances for the search task are improved if the focal plane of the AR display is at the same distance as subsequent search targets.},
  keywords={Visualization;Three-dimensional displays;Surveillance;User interfaces;Physiology;Time factors;Sea level;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR51125.2022.00040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756776,
  author={Amaro, Guilherme and Mendes, Daniel and Rodrigues, Rui},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Evaluation of Travel and Orientation Techniques for Desk VR}, 
  year={2022},
  volume={},
  number={},
  pages={222-231},
  abstract={Typical VR interactions can be tiring, including standing up, walking, and mid-air gestures. Such interactions result in decreased comfort and session duration compared with traditional non-VR interfaces, which may, in turn, reduce productivity. Nevertheless, current approaches often neglect this aspect, making the VR experience not as promising as it can be. As we see it, desk VR experiences provide the convenience and comfort of a desktop experience and the benefits of VR immersion, being a good compromise between the overall experience and ergonomics. In this work, we explore navigation techniques targeted at desk VR users, using both controllers and a large multi-touch surface. We address travel and orientation techniques independently, considering only continuous approaches for travel as these are better suited for exploration and both continuous and discrete approaches for orientation. Results revealed advantages for a continuous controller-based travel method and a trend for a dragging-based orientation technique. Also, we identified possible trends towards task focus affecting overall cybersickness symptomatology.},
  keywords={Productivity;Legged locomotion;Three-dimensional displays;Navigation;Cybersickness;Ergonomics;Conferences;Human-centered computing—Human computer interaction (HCI)—Interaction techniques;Computing methodologies—Computer graphics—Graphics systems and interfaces—Virtual reality},
  doi={10.1109/VR51125.2022.00041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756786,
  author={Lin, Wan-Yi and Wang, Ying-Chu and Wu, Dai-Rong and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Ebrahimi, Elham and Pagano, Christopher and Babu, Sabarish V. and Lin, Wen-Chieh},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Empirical Evaluation of Calibration and Long-term Carryover Effects of Reverberation on Egocentric Auditory Depth Perception in VR}, 
  year={2022},
  volume={},
  number={},
  pages={232-240},
  abstract={Distance compression, which refers to the underestimation of ego-centric distance to objects, is a common problem in immersive virtual environments. Besides visually compensating the compressed distance, several studies have shown that auditory information can be an alternative solution for this problem. In particular, reverberation time (RT) has been proven to be an effective method to compensate distance compression. To further explore the feasibility of applying audio information to improve distance perception, we investigate whether users’ egocentric distance perception can be calibrated, and whether the calibrated effect can be carried over and even sustain for a longer duration. We conducted a study to understand the perceptual learning and carryover effects by using RT as stimuli for users to perceive distance in IVEs. The results show that the carryover effect exists after calibration, which indicates people can learn to perceive distances by attuning reverberation time, and the accuracy even remains a constant level after 6 months. Our findings could potentially be utilized to improve the distance perception in VR systems as the calibration of auditory distance perception in VR could sustain for several months. This could eventually avoid the burden of frequent training regimens.},
  keywords={Human computer interaction;Training;Solid modeling;Visualization;Three-dimensional displays;Virtual environments;Calibration;Depth Perception;Auditory Reverberation;Calibration;Perceptual Learning;Computing methodologies [Computer Graphics]: Graphics systems and interfaces—Perception Human-centered computing [Human computer interaction (HCI)]: Interaction paradigms— Virtual reality},
  doi={10.1109/VR51125.2022.00042},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756819,
  author={Bahremand, Alireza and Manetta, Mason and Lai, Jessica and Lahey, Byron and Spackman, Christy and Smith, Brian H. and Gerkin, Richard C. and LiKamWa, Robert},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Smell Engine: A system for artificial odor synthesis in virtual environments}, 
  year={2022},
  volume={},
  number={},
  pages={241-249},
  abstract={Mimicking physical odor sensations virtually can present users with a real - time odor synthesis that approximates what users would smell in a virtual environment, e.g., as they walk around in virtual reality. To this end, we devise a Smell Engine that includes: (i) a Smell Composer framework that allows developers to configure odor sources in virtual space, (ii) a Smell Mixer that dynamically estimates the odor mix that the user would smell, based on diffusion models and relative odor source distances, and (iii) a Smell Controller that coordinates an olfactometer to physically present an approximation of the odor mix to the user’s mask from a set of odorants channeled through controllable flow valves. Through a three - part user study, we found that the Smell Engine can help measure a subject’s olfactory detection threshold and improve their ability to precisely localize odors in the virtual environment, as compared to existing trigger - based solutions.},
  keywords={Solid modeling;Three-dimensional displays;Conferences;Olfactory;Virtual environments;User interfaces;Aerospace electronics;Human-centered computing;Interactive systems and tools;Computer systems organization;Sensors and actuators},
  doi={10.1109/VR51125.2022.00043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756787,
  author={Jing, Allison and Lee, Gun and Billinghurst, Mark},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Speech to Visualise Shared Gaze Cues in MR Remote Collaboration}, 
  year={2022},
  volume={},
  number={},
  pages={250-259},
  abstract={In this paper, we present a 360° panoramic Mixed Reality (MR) sys-tem that visualises shared gaze cues using contextual speech input to improve task coordination. We conducted two studies to evaluate the design of the MR gaze-speech interface exploring the combinations of visualisation style and context control level. Findings from the first study suggest that an explicit visual form that directly connects the collaborators’ shared gaze to the contextual conversation is preferred. The second study indicates that the gaze-speech modality shortens the coordination time to attend to the shared interest, making the communication more natural and the collaboration more effective. Qualitative feedback also suggest that having a constant joint gaze indicator provides a consistent bi-directional view while establishing a sense of co-presence during task collaboration. We discuss the implications for the design of collaborative MR systems and directions for future research.},
  keywords={Manufacturing industries;Visualization;Three-dimensional displays;Collaboration;Mixed reality;Virtual reality;Bidirectional control;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/VR51125.2022.00044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756753,
  author={Heinrich, Florian and Schwenderling, Lovis and Joeres, Fabian and Hansen, Christian},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={2D versus 3D: A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices}, 
  year={2022},
  volume={},
  number={},
  pages={260-269},
  abstract={Surgical procedures requiring needle navigation assistance suffer from complicated hand-eye coordination and are mentally demanding. Augmented reality (AR) can help overcome these issues. How-ever, only an insufficient amount of fundamental research has focused on the design and hardware selection of such AR needle navigation systems. This work contributes to this research area by presenting a user study (n=24) comparing three state-of-the-art navigation concepts displayed by an optical see-through head-mounted display and a stereoscopic projection system. A two-dimensional glyph visualization resulted in higher targeting accuracy but required more needle insertion time. In contrast, punctures guided by a three-dimensional see-through vision concept were less accurate but faster and were favored in a qualitative interview. The third concept, a static representation of the correctly positioned needle, showed too high target errors for clinical accuracy needs. This concept per-formed worse when displayed by the projection system. Besides that, no meaningful differences between the evaluated AR display devices were detected. User preferences and use case restrictions, e.g., sterility requirements, seem to be more crucial selection criteria. Future work should focus on improving the accuracy of the see-through vision concept. Until then, the glyph visualization is recommended.},
  keywords={Visualization;Three-dimensional displays;Target tracking;Navigation;Stereo image processing;Surgery;User interfaces;Human-centered computing—Visualization—Empirical studies in visualization;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Hardware—Communication hardware, interfaces and storage—Displays and imagers;Applied computing—Life and medical sciences—Health informatics},
  doi={10.1109/VR51125.2022.00045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756756,
  author={Martinez, Esteban Segarra and Wu, Annie S. and McMahan, Ryan P.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Research Trends in Virtual Reality Locomotion Techniques}, 
  year={2022},
  volume={},
  number={},
  pages={270-280},
  abstract={Virtual reality (VR) researchers have had a long-standing interest in studying locomotion for developing new techniques, improving upon prior ones, and analyzing their effects on users. To help organize prior work, several researchers have presented taxonomies for categorizing locomotion techniques in general. More recently, researchers have begun to conduct systematic reviews to better understand what locomotion techniques have been investigated. In this paper, we present our own systematic review of locomotion techniques based on a well-established taxonomy, and we use k-means clustering to identify to what extent locomotion techniques have been explored. Our results indicate that selection-based, walking-based, and steering-based locomotion techniques have been moderately to highly explored while manipulation-based and automated locomotion techniques have been less explored. We also present results on what types of metrics have been used to evaluate locomotion techniques. While usability, discomfort, and travel performance metrics have been moderately to highly explored, other metrics, such as biometrics, user experience, and emotions, have been less explored.},
  keywords={Measurement;Systematics;Three-dimensional displays;Biometrics (access control);Taxonomy;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR51125.2022.00046},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756829,
  author={Raimbaud, Pierre and Jovane, Alberto and Zibrek, Katja and Pacchierotti, Claudio and Christie, Marc and Hoyet, Ludovic and Pettré, Julien and Olivier, Anne-Hélène},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Stare-in-the-Crowd Effect in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={281-290},
  abstract={Nonverbal cues are paramount in real-world interactions. Among these cues, gaze has received much attention in the literature. In particular, previous work has shown a search asymmetry between directed and averted gaze towards the observer using photographic stimuli, with faster detection and longer fixation towards directed gaze by the observer. This is known as the stare-in-the-crowd effect. In this study, we investigate whether stare-in-the crowd effect is preserved in Virtual Reality (VR). To this end, we designed a within-subject experiment where 30 human users were immersed in a virtual environment in front of an audience of 11 virtual agents following 4 different gaze behaviours. We analysed the user’s gaze behaviour when observing the audience, computing fixations and dwell time. We also collected the users’ social anxiety score using a post-experiment questionnaire to control for some potential influencing factors. Results show that the stare-in-the-crowd effect is preserved in VR, as demonstrated by the significant differences between gaze behaviours, similarly to what was found in previous studies using photographic stimuli. Additionally, we found a negative correlation between dwell time towards directed gazes and users’ social anxiety scores. Such results are encouraging for the development of expressive and reactive virtual humans, which can be animated to express natural interactive behaviour.},
  keywords={Three-dimensional displays;Correlation;Design methodology;Conferences;Anxiety disorders;Virtual environments;Observers;Human-centered computing;Visualization;Visualization design and evaluation methods;Human-centered computing;User studies;Applied computing;Law;social and behavioral sciences},
  doi={10.1109/VR51125.2022.00047},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756826,
  author={Mathis, Florian and O’Hagan, Joseph and Khamis, Mohamed and Vaniea, Kami},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Observations: Using Virtual Reality to Augment Lab-Based Shoulder Surfing Research}, 
  year={2022},
  volume={},
  number={},
  pages={291-300},
  abstract={Given the difficulties of studying the shoulder surfing resistance of authentication systems in a live setting, researchers often ask study participants to shoulder surf authentications by watching two-dimensional (2D) video recordings of a user authenticating. How-ever, these video recordings do not provide participants with a realistic shoulder surfing experience, creating uncertainty in the value and validity of lab-based shoulder surfing experiments. In this work, we exploit the unique characteristics of virtual reality (VR) and study the use of non-immersive/immersive VR recordings for shoulder surfing research. We conducted a user study (N=18) to explore the strengths and weaknesses of such a VR-based shoulder surfing research approach. Our results suggest that immersive VR observations result in a more realistic shoulder surfing experience, in a significantly higher sense of being part of the authentication environment, in a greater feeling of spatial presence, and in a higher level of involvement than 2D video observations without impacting participants’ observation performance. This suggests that studying shoulder surfing in VR is advantageous in many ways compared to currently used approaches, e.g., participants can freely choose their observation angle rather than being limited to a fixed observation angle as done in current methods. We discuss the strengths and weaknesses of using VR for shoulder surfing research and conclude with four recommendations to help researchers decide when (and when not) to employ VR for shoulder surfing research in the authentication research domain.},
  keywords={Resistance;Privacy;Uncertainty;Three-dimensional displays;Two dimensional displays;Authentication;Virtual reality;Virtual Reality;Shoulder Surfing;Authentication},
  doi={10.1109/VR51125.2022.00048},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756785,
  author={Mathis, Florian and Vaniea, Kami and Khamis, Mohamed},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Can I Borrow Your ATM? Using Virtual Reality for (Simulated) In Situ Authentication Research}, 
  year={2022},
  volume={},
  number={},
  pages={301-310},
  abstract={In situ evaluations of novel authentication systems, where the system is evaluated in its intended usage context, are often infeasible due to ethical and legal constraints. Consequently, researchers evaluate their authentication systems in the lab, which questions the eco-logical validity. In this work, we explore how VR can overcome the shortcomings of authentication studies conducted in the lab and contribute towards more realistic authentication research. We built a highly realistic automated teller machine (ATM) and a VR replica to investigate through a user study (N=20) the impact of in situ evaluations on an authentication system‘s usability results. We evaluated and compared: Lab studies in the real world, lab studies in VR, in situ studies in the real world, and in situ studies in VR. Our findings highlight 1) VR‘s great potential to circumvent potential restrictions researchers experience when evaluating authentication schemes and 2) the impact of the context on an authentication system‘s usability evaluation results. In situ ATM authentications took longer (+24.71% in the real world, +14.17% in VR) than authentications in a traditional (VR) lab environment and elicited a higher sense of being part of an ATM authentication scenario compared to a real-world and VR-based evaluation in the lab. Our quantitative findings, along with participants‘ qualitative feedback, provide first evidence of increased authentication realism when using VR for in situ authentication research. We provide researchers with a novel research approach to conduct (simulated) in situ authentication re-search, discuss our findings in the light of prior works, and conclude with three key lessons to support researchers in deciding when to use VR for in situ authentication research.},
  keywords={Human computer interaction;Ethics;Three-dimensional displays;Online banking;Law;Conferences;Authentication;Virtual Reality;Authentication;In Situ Research},
  doi={10.1109/VR51125.2022.00049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756752,
  author={Norouzi, Nahal and Gottsacker, Matthew and Bruder, Gerd and Wisniewski, Pamela J. and Bailenson, Jeremy and Welch, Greg},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Humans with Pets and Robots: Exploring the Influence of Social Priming on One’s Perception of a Virtual Human}, 
  year={2022},
  volume={},
  number={},
  pages={311-320},
  abstract={Social priming is the idea that observations of a virtual human (VH) engaged in short social interactions with a real or virtual human bystander can positively influence users’ subsequent interactions with that VH. In this paper we investigate the question of whether the positive effects of social priming are limited to interactions with humanoid entities. For instance, virtual dogs offer an attractive candidate for non-humanoid entities, as previous research suggests multiple positive effects. In particular, real human dog owners receive more positive attention from strangers than non-dog owners.To examine the influence of such social priming we carried out a human-subjects experiment with four conditions: three social priming conditions where a participant initially observed a VH interacting with one of three virtual entities (another VH, a virtual pet dog, or a virtual personal robot), and a non-social priming condition where a VH (alone) was intently looking at her phone as if reading something. We recruited 24 participants and conducted a mixed-methods analysis. We found that a VH’s prior social interactions with another VH and a virtual dog significantly increased participants’ perceptions of the VHs’ affective attraction. Also, participants felt more inclined to interact with the VH in the future in all of the social priming conditions. Qualitatively, we found that the social priming conditions resulted in a more positive user experience than the non-social priming condition. Also, the virtual dog and the virtual robot were perceived as a source of positive surprise, with participants appreciating the non-humanoid interactions for various reasons, such as the avoidance of social anxieties sometimes associated with humans.},
  keywords={Three-dimensional displays;Design methodology;Conferences;Anxiety disorders;Humanoid robots;Dogs;User interfaces;Human-centered computing;Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality},
  doi={10.1109/VR51125.2022.00050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756733,
  author={Chen, Yuzhong and Shen, Qijin and Niu, Yuzhen and Liu, Wenxi},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Continuous Transformation Superposition for Visual Comfort Enhancement of Casual Stereoscopic Photography}, 
  year={2022},
  volume={},
  number={},
  pages={321-329},
  abstract={Casual stereoscopic photography allows ordinary users to create a stereoscopic photo using two photos taken casually by a monocular camera. The visual comfort of a casual stereoscopic photo can greatly affect its visual experience. In this paper, we present a novel visual comfort enhancement method for casual stereoscopic photography via reinforcement learning based on continuous transformation superposition. We consider the transformation, in a continuous transformation space, to transform each view as superpositions of several basic continuous transformations, enabling more subtle and flexible image transformation operations to approach better solutions. To achieve the continuous transformation superposition, we prepare a collection of continuous transformation models for translation, rotation, and perspective transformations. Then we train a policy model to determine an optimal transformation chain to recurrently handle both the geometric constraints and disparity adjustment, and thereby enhance the visual comfort of casual stereoscopic images. We further propose an attention-based stereo feature fusion module that enhances and integrates the binocular information between the left and right views. Experimental results on three datasets demonstrate that our proposed method achieves superior performance to state-of-the-art methods.},
  keywords={Photography;Visualization;Solid modeling;Three-dimensional displays;Stereo image processing;Conferences;Virtual reality;Human-centered computing;3D authoring;Image and video acquisition;Computer vision},
  doi={10.1109/VR51125.2022.00051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756732,
  author={Liu, Kuan-Yu and Wong, Sai-Keung and Volonte, Matias and Ebrahimi, Elham and Babu, Sabarish V.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Effects of Leading and Following Behaviors of Virtual Humans in Collaborative Fine Motor Tasks in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={330-339},
  abstract={We examined the effects on users during collaboration with two types of virtual human (VH) agents in object transportation in an immersive virtual environment or virtual reality (VR). The two types of virtual humans we examined are leader and follower agents. The goal of the users is to interact with the agents using natural language and carry objects from initial locations to destinations. In each trial, the follower agent follows a user’s instructions to perform actions to manipulate the object. The leader agent determines the appropriate actions that the agent and the user should perform. We developed a system which enabled users and virtual agents to carry objects in an intuitive manner. We conducted a within-subjects study to evaluate the user behaviors under two conditions: (LVH) interaction with a leader virtual human and (FVH) interaction with a follower virtual human. We found that the participants in LVH required a higher workload than that in FVH. However, the users’ experiences, game experiences, and users’ impressions between the two conditions were not significantly different.},
  keywords={Solid modeling;Three-dimensional displays;Multimedia systems;Natural languages;Collaboration;Virtual environments;Transportation;Virtual Humans;Virtual Reality;Animation;Behavior Modeling;Human Computer Interaction},
  doi={10.1109/VR51125.2022.00052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756741,
  author={Wang, Chang-Chun and Volonte, Matias and Ebrahimi, Elham and Liu, Kuan-Yu and Wong, Sai-Keung and Babu, Sabarish V.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Evaluation of Native versus Foreign Communicative Interactions on Users’ Behavioral Reactions towards Affective Virtual Crowds}, 
  year={2022},
  volume={},
  number={},
  pages={340-349},
  abstract={This investigation compared the impact on the users’ non-verbal behaviors elicited by interacting with a crowd of emotional virtual humans (VHs) in native and non-native language settings. In a between-subject experiment, we collected objective metrics regarding the users’ behaviors during interaction with a crowd of VHs who expressed verbal and non-verbal communicative behaviors in response to users’ speech-based interaction. The study presented four VH crowd conditions based on their emotional disposition, namely Positive, Negative, Neutral, and Mixed, in accordance with which VHs exhibited the appropriate behaviors. Participants were tasked with collecting items in a virtual flea market, and they interacted using natural speech-based dialogue with the VHs. The language conditions were collected in the USA and under two different conditions in Taiwan. The participants in the USA group interacted with the VHs in English (a native language for the USA setting); and two different groups in Taiwan interacted with the VHs in either a foreign (English) or native (Mandarin) language, respectively. Results reveal that in the (TMA, TEN) and (UEN, TEN) conditions, native versus non-native language communication can alter social behaviors (e.g., interaction time) of participants towards virtual interlocutors. Our results also revealed strong effects of cultural differences on participants’ non-verbal social behaviors with VHs in the UEN and TMA conditions.},
  keywords={Measurement;Three-dimensional displays;Multimedia systems;Conferences;Virtual reality;User interfaces;Cultural differences;Virtual Humans;Virtual Reality;Human-Computer Interaction},
  doi={10.1109/VR51125.2022.00053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756775,
  author={Wolf, Erik and Fiedler, Marie Luisa and Döllinger, Nina and Wienrich, Carolin and Latoschik, Marc Erich},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Presence, Avatar Embodiment, and Body Perception with a Holographic Augmented Reality Mirror}, 
  year={2022},
  volume={},
  number={},
  pages={350-359},
  abstract={The embodiment of avatars in virtual reality (VR) is a promising tool for enhancing the user’s mental health. A great example is the treatment of body image disturbances, where eliciting a full-body illusion can help identify, visualize, and modulate persisting misperceptions. Augmented reality (AR) could complement recent advances in the field by incorporating real elements, such as the therapist or the user’s real body, into therapeutic scenarios. However, research on the use of AR in this context is very sparse. Therefore, we present a holographic AR mirror system based on an optical see-through (OST) device and markerless body tracking, collect valuable qualitative feedback regarding its user experience, and compare quantitative results regarding presence, embodiment, and body weight perception to similar systems using video see-through (VST) AR and VR. For our OST AR system, a total of 27 normal-weight female participants provided predominantly positive feedback on display properties (field of view, luminosity, and transparency of virtual objects), body tracking, and the perception of the avatar’s appearance and movements. In the quantitative comparison to the VST AR and VR systems, participants reported significantly lower feelings of presence, while they estimated the body weight of the generic avatar significantly higher when using our OST AR system. For virtual body ownership and agency, we found only partially significant differences. In summary, our study shows the general applicability of OST AR in the given context offering huge potential in future therapeutic scenarios. However, the comparative evaluation between OST AR, VST AR, and VR also revealed significant differences in relevant measures. Future work is mandatory to corroborate our findings and to classify the significance in a therapeutic context.},
  keywords={Visualization;Three-dimensional displays;Tracking;Avatars;Optical feedback;User interfaces;Optical imaging;Virtual reality;virtual human;virtual body ownership;agency;body image distortion;body weight perception},
  doi={10.1109/VR51125.2022.00054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756790,
  author={Bouzbib, Elodie and Bailly, Gilles},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={"Let’s Meet and Work it Out": Understanding and Mitigating Encountered-Type of Haptic Devices Failure Modes in VR}, 
  year={2022},
  volume={},
  number={},
  pages={360-369},
  abstract={Encountered-type of Haptic devices (ETHD) are robotic interfaces physically overlaying virtual counterparts prior to a user interaction in Virtual Reality. They theoretically reliably provide haptics in Virtual environments, yet they raise several intrinsic design challenges to properly display rich haptic feedback and interactions in VR applications. In this paper, we use a Failure Mode and Effects Analysis (FMEA) approach to identify, organise and analyse the failure modes and their causes in the different stages of an ETHD scenario and highlight appropriate solutions from the literature to mitigate them. We help justify these interfaces’ lack of deployment, to ultimately identify guidelines for future ETHD designers.},
  keywords={Three-dimensional displays;Conferences;Virtual environments;User interfaces;Reliability theory;Reliability engineering;Hardware;Encountered-type of Haptic Devices;Haptics;Virtual Reality;Design;FMEA;Theoretical Framework;Robotic Graphics},
  doi={10.1109/VR51125.2022.00055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756767,
  author={Liang, Jiadong and Liu, Yunfei and Lu, Feng},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reconstructing 3D Virtual Face with Eye Gaze from a Single Image}, 
  year={2022},
  volume={},
  number={},
  pages={370-378},
  abstract={Reconstructing 3D virtual face from a single image has a wide range of applications in virtual reality. Existing approaches synthesize plausible reconstructed virtual faces, however, eye gaze information is usually ignored, which is critical in human-computer interaction. In this paper, we propose to reconstruct 3D virtual face with eye gaze information from a single image. The main challenges lie in two aspects, one is the low reconstruction quality in the eye region, the other one is the lack of an efficient method to obtain precise eye gaze information. To address these problems, we decompose this task into two key steps, i.e., 3D face reconstruction with precise eye region and eye contact guided facial-rotation for eye gaze information. The first step is designed for precise eye region reconstruction through joint optimization on 3D face/eye shapes and textures. The second step consists of two parts: eye contact discriminator and automatic eye contact search algorithm via gradient-based optimization to perform both eye contact and gaze estimation simultaneously. Extensive experiments on different tasks demonstrate the significant gain of the proposed approach, achieving an MSE of (30%), an SSIM of (17.85%), and a PSNR of (8.4%). It also produces lower angular errors (63.01%) in the gaze estimation task compared with human annotations.},
  keywords={Human computer interaction;Three-dimensional displays;Shape;Conferences;Estimation;Virtual reality;Task analysis;3D face reconstruction;eye contact;gaze estimation},
  doi={10.1109/VR51125.2022.00056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756730,
  author={Kim, Dooyoung and Kim, Jinwook and Shin, Jae-Eun and Yoon, Boram and Lee, Jeongmi and Woo, Woontack},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Virtual Room Size and Objects on Relative Translation Gain Thresholds in Redirected Walking}, 
  year={2022},
  volume={},
  number={},
  pages={379-388},
  abstract={This paper investigates how the size of virtual space and objects within it affect the threshold range of relative translation gains, a Redirected Walking (RDW) technique that scales the user’s movement in virtual space in different ratios for the width and depth. While previous studies assert that a virtual room’s size affects relative translation gain thresholds on account of the virtual horizon’s location, additional research is needed to explore this assumption through a structured approach to visual perception in Virtual Reality (VR). We estimate the relative translation gain thresholds in six spatial conditions configured by three room sizes and the presence of virtual objects (3 × 2), which were set according to differing Angles of Declination (AoDs) between eye-gaze and the forward-gaze. Results show that both size and virtual objects significantly affect the threshold range, it being greater in the large-sized condition and furnished condition. This indicates that the effect of relative translation gains can be further increased by constructing a perceived virtual movable space that is even larger than the adjusted virtual movable space and placing objects in it. Our study can be applied to adjust virtual spaces in synchronizing heterogeneous spaces without coordinate distortion where real and virtual objects can be leveraged to create realistic mutual spaces.},
  keywords={Legged locomotion;Three-dimensional displays;Conferences;Virtual reality;User interfaces;Distortion;Visual perception;Virtual Reality;relative translation gains;threshold;redirected walking;angle of declination;virtual object},
  doi={10.1109/VR51125.2022.00057},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756780,
  author={Künz, Andreas and Rosmann, Sabrina and Loria, Enrica and Pirker, Johanna},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Potential of Augmented Reality for Digital Twins: A Literature Review}, 
  year={2022},
  volume={},
  number={},
  pages={389-398},
  abstract={As the boundary between real and virtual life is becoming increasingly blurred, researchers and practitioners are looking for ways to integrate the two intending to improve human lives in a plethora of domains. A cutting-edge concept is the design of Digital Twins (DT), having a broad range of implications and applications, spanning from education, training, as well as safety and productivity in the workplace. An emergent approach for implementing DTs is the usage of mixed reality (MR) and augmented reality (AR), which are well aligned with merging real and virtual objects to enhance the human’s ability to interact with and manage DTs. Yet, this is still a novel area of research and, as such, a grounded understanding of the current state, challenges, and open questions is still lacking. Towards this, we conducted a PRISMA-based literature review of scientific articles and book chapters dealing with the use of MR and AR for digital twins. After a thorough screening phase and eligibility check, 25 papers were analyzed, sorted and compared by different categories like research topic (e.g., visualization, guidance), domain (e.g., manufacturing, education), paper type (e.g., design study, evaluation), evaluation type (user study, case study or none), used hardware (e.g., Microsoft HoloLens, mobile devices) as well as the different outcomes (result type and topic, problems, outlook). The major finding of this research survey is the predominant focus of the reviewed papers on the technology itself and the neglect of factors regarding the users. We, therefore, encourage researchers in this area to keep the importance of ease and joy of use in mind and include users in multiple stages of their work.},
  keywords={Training;Three-dimensional displays;Systematics;Digital twin;Bibliographies;Mixed reality;Manufacturing;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality;Computer systems organization;Embedded and cyber-physical systems;General and reference;Document types;Surveys and overviews},
  doi={10.1109/VR51125.2022.00058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756796,
  author={Feng, Yu and Goulding-Hotta, Nathan and Khan, Asif and Reyserhove, Hans and Zhu, Yuhao},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Time Gaze Tracking with Event-Driven Eye Segmentation}, 
  year={2022},
  volume={},
  number={},
  pages={399-408},
  abstract={Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1°–0.5° gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.},
  keywords={Solid modeling;Three-dimensional displays;Software algorithms;Gaze tracking;Virtual reality;User interfaces;Prediction algorithms;Gaze;eye tracking;event camera;segmentation},
  doi={10.1109/VR51125.2022.00059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756791,
  author={Miller, Robert and Banerjee, Natasha Kholgade and Banerjee, Sean},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics}, 
  year={2022},
  volume={},
  number={},
  pages={409-418},
  abstract={Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.},
  keywords={Performance evaluation;Deep learning;Smoothing methods;Biometrics (access control);Neural networks;Authentication;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Security and privacy;Security services;Authentication;Biometrics},
  doi={10.1109/VR51125.2022.00060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756769,
  author={Williams, Niall L. and Bera, Aniket and Manocha, Dinesh},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={ENI: Quantifying Environment Compatibility for Natural Walking in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={419-427},
  abstract={We present a novel metric to analyze the similarity between the physical environment and the virtual environment for natural walking in virtual reality. Our approach is general and can be applied to any pair of physical and virtual environments. We use geometric techniques based on conforming constrained Delaunay triangulations and visibility polygons to compute the Environment Navigation Incompatibility (ENI) metric that can be used to measure the complexity of performing simultaneous navigation. We demonstrate applications of ENI for highlighting regions of incompatibility for a pair of environments, guiding the design of the virtual environments to make them more compatible with a fixed physical environment, and evaluating the performance of different redirected walking controllers. We validate the ENI metric using simulations and two user studies. Results of our simulations and user studies show that in the environment pair that our metric identified as more navigable, users were able to walk for longer before colliding with objects in the physical environment. Overall, ENI is the first general metric that can automatically identify regions of high and low compatibility in physical and virtual environments. Our project website is available at https://gamma.umd.edu/eni/.},
  keywords={Measurement;Legged locomotion;Solid modeling;Three-dimensional displays;Navigation;Conferences;Virtual environments;Navigability Metric;Locomotion Interfaces;Visibility Polygon;Isovist},
  doi={10.1109/VR51125.2022.00061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756773,
  author={Ang, Samuel and Quarles, John},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={You’re in for a Bumpy Ride! Uneven Terrain Increases Cybersickness While Navigating with Head Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={428-435},
  abstract={Cybersickness (i.e., visually induced motion sickness) serves as a significant obstacle to the usage and broader adoption of virtual reality (VR) technologies. This collection of symptoms akin to motion sickness can be impacted by different characteristics of a virtual experience, such as visual realism and optical flow. However, relatively little is known regarding how cybersickness is influenced by traversing uneven virtual terrain. In this study, we aim to better understand the impacts of different virtual terrain types on cybersickness in VR. We recruited 38 participants to navigate a virtual forest environment with three terrain variants: flat surface, terrain with regular bumps, and irregular terrain generated from Perlin noise. We collected cybersickness data using the Fast Motion Sickness Scale (FMSS) and Simulator Sickness Questionnaire (SSQ) in addition to galvanic skin response data. Our results indicate that users felt greater levels of cybersickness in the presence of regular bumps and irregular terrain than they did when traversing flat geometry. We recommend that designers exercise caution when incorporating uneven terrain into their virtual experiences, and maintain awareness of the risks carried by these design decisions.},
  keywords={Visualization;Three-dimensional displays;Cybersickness;Navigation;Multimedia systems;Virtual environments;User interfaces;H.5.1 [INFORMATION INTERFACES AND PRESENTATION (e.g., HCI)]: Multimedia Information Systems;Artificial, augmented, and virtual realities},
  doi={10.1109/VR51125.2022.00062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756789,
  author={Davari, Shakiba and Lu, Feiyu and Bowman, Doug A.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Validating the Benefits of Glanceable and Context-Aware Augmented Reality for Everyday Information Access Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={436-444},
  abstract={Glanceable Augmented Reality interfaces have the potential to provide fast and efficient information access for the user. However, where to place the virtual content and how to access them depend on the user context. We designed a Context-Aware AR interface that can intelligently adapt for two different contexts: solo and social. We evaluated information access using Context-Aware AR compared to current mobile phones and non-adaptive Glanceable AR interfaces. We found that in a solo scenario, compared to a mobile phone, the Context-Aware AR interface was preferred, easier, and significantly faster; it improved the user experience; and it allowed the user to better focus on their primary task. In the social scenario, we discovered that the mobile phone was slower, more intrusive, and perceived as the most difficult. Meanwhile, Context-Aware AR was faster for responding to information needs triggered by the conversation; it was preferred and perceived as the easiest for resuming conversation after information access; and it improved the user’s awareness of the other person’s facial expressions.},
  keywords={Human computer interaction;Three-dimensional displays;Conferences;User experience;Task analysis;Augmented reality;Smart phones;Human-centered computing;Mixed/augmented reality;Interaction techniques;Empirical Studies in HCI},
  doi={10.1109/VR51125.2022.00063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756771,
  author={Luo, Shiqing and Hu, Xinyu and Yan, Zhisheng},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HoloLogger: Keystroke Inference on Mixed Reality Head Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={445-454},
  abstract={When using personal computing services in mixed reality (MR) such as online payment and social media, sensitive information and account passwords must be typed in MR. To design secure MR systems and build up user trust, it is imperative to first understand the security threat to the sensitive MR input. Although keystroke inference attacks by analyzing human-computer interaction in videos or via wireless signals have been successful, they require placing extra hardware near the user which is easily noticeable in practice. In this paper, we expose a more dangerous malware-based attack through the vulnerability that no permission is required for accessing MR motion data. We aim to monitor MR headset motion and infer the user input through a benign App. Realizing the attack system requires addressing unique challenges in MR such as six-degree-of-freedom (6DoF) device motion and no explicit motion signal for keystroke identification. To this end, we present HoloLogger, the first malware-based keystroke inference attack system on HoloLens. HoloLogger is empowered by a 6DoF-head-motion-driven key tracking scheme and an air-tap-pattern-based keystroke inference framework. Extensive evaluations with 25 users and 750 inference trials of passwords consisting of 4–8 lowercase English letters demonstrate that HoloLogger successfully achieves a top-5 accuracy of 93%. HoloLogger is also robust in various environments such as different user positions and input categories.},
  keywords={Wireless communication;Wireless sensor networks;Solid modeling;Three-dimensional displays;Tracking;Social networking (online);Mixed reality;Human-centered computing;Human computer interaction;Mixed/augmented reality;Security and privacy;Human and societal aspects of security and privacy;Privacy protection},
  doi={10.1109/VR51125.2022.00064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756762,
  author={Stuart, Jacob and Aul, Karen and Bumbach, Michael D. and Stephen, Anita and de Siqueira, Alexandre Gomes and Lok, Benjamin},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Virtual Humans Making Verbal Communication Mistakes on Learners’ Perspectives of their Credibility, Reliability, and Trustworthiness}, 
  year={2022},
  volume={},
  number={},
  pages={455-463},
  abstract={Simulating real-world experiences in a safe environment has made virtual human medical simulations a common use case for research and interpersonal communication training. Despite the benefits virtual human medical simulations provide, previous work suggests that users struggle to notice when virtual humans make potentially life-threatening verbal communication mistakes inside virtual human medical simulations. In this work, we performed a 2x2 mixed design user study that had learners (n = 80) attempt to identify verbal communication mistakes made by a virtual human acting as a nurse in a virtual desktop environment. A virtual desktop environment was used instead of a head-mounted virtual reality environment due to Covid-19 limitations. The virtual desktop environment experience allowed us to explore how frequently learners identify verbal communication mistakes in virtual human medical simulations and how perceptions of credibility, reliability, and trustworthiness in the virtual human affect learner error recognition rates. We found that learners struggle to identify infrequent virtual human verbal communication mistakes. Additionally, learners with lower initial trustworthiness ratings are more likely to overlook potentially life-threatening mistakes, and virtual human mistakes temporarily lower learner credibility, reliability, and trustworthiness ratings of virtual humans. From these findings, we provide insights on improving virtual human medical simulation design. Developers can use these insights to design virtual simulations for error identification training using virtual humans.},
  keywords={Training;Human computer interaction;Solid modeling;Three-dimensional displays;Medical simulation;Conferences;Computational modeling;Human-centered computing;Empirical studies in HCI},
  doi={10.1109/VR51125.2022.00065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756802,
  author={Hoshikawa, Yukai and Fujita, Kazuyuki and Takashima, Kazuki and Fjeld, Morten and Kitamura, Yoshifumi},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={RedirectedDoors: Redirection While Opening Doors in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={464-473},
  abstract={We propose RedirectedDoors, a novel space-efficient technique for redirection in VR focused on door-opening behavior. This technique manipulates the user’s walking direction by rotating the entire virtual environment (VE) at a certain angular ratio of the door being opened. This ratio is called door rotation gain. At the same time, the virtual door’s position is kept unmanipulated so that a realistic door-opening user experience can be ensured. We designed and implemented the rotational manipulation algorithm and two types of door-opening interfaces; with and without a doorknob-type passive haptic prop. We then conducted a user study (N = 12) to investigate redirection performance and user feedback as we examined three independent variables: door rotation gain, door-opening interface, and door-opening direction (push/pull). From the results, the estimated detection thresholds generally showed a higher space efficiency of redirection with our technique. Our results also showed that providing the haptic feedback led to a higher noticeability of redirection, but at the same time supported a higher subjective sense of realism and less discomfort. Following our results, we discuss which combinations of gain and door-opening direction can jointly provide lower noticeability and higher acceptability.},
  keywords={Resistance;Legged locomotion;Three-dimensional displays;Conferences;Force feedback;Virtual environments;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality},
  doi={10.1109/VR51125.2022.00066},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756744,
  author={Zou, Shangyin and Hu, Xianyin and Ban, Yuki and Warisawa, Shin’ichi},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simulating Olfactory Cocktail Party Effect in VR: A Multi-odor Display Approach Based on Attention}, 
  year={2022},
  volume={},
  number={},
  pages={474-482},
  abstract={Odor display has been a popular approach in virtual reality (VR) to enhance users’ multi-sensory experience. The existing multi-odor presentation methods in VR are mostly based on spatiality of scent sources to produce mixed scents, which will possibly compromise users’ olfactory experience because humans normally have poor ability to analyze distinct odorant components from a mixture. To tackle this problem, we present a VR multi-odor display approach that dynamically changes the intensity combinations of different scent sources in the virtual environment according to the user’s attention, hence simulating a virtual cocktail party effect of smell. We acquire the user’s gaze information as attention from the eye-tracking sensors embedded in the head mounted display (HMD), and increase the display intensity of the scent that the user is focusing on to simulate the cocktail party effect of smell, enabling the user to distinguish their desired scent source. We conducted a user study to validate the perception and experience of 2 ways of intensity settings in response to the user’s attention shift, which were a strong level of focused scent mixed with weak levels of non-focused scents and strong focused scent only. The results showed that both of the two intensity settings were able to improve olfactory experience in VR compared to the non-dynamic odor display method. Meanwhile, only the method of presenting strengthened focused scent while maintaining the weaker mixture of background scents showed significant improvement on simulating the olfactory cocktail party effect by giving the users an enhanced sense of their own olfactory sensitivity.},
  keywords={Sensitivity;Three-dimensional displays;Conferences;Olfactory;Virtual environments;Focusing;Resists;virtual reality;odor presentation;attention},
  doi={10.1109/VR51125.2022.00067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756772,
  author={Dai, Danqing and Shi, Xuehuai and Wang, Lili and Li, Xiangyu},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interactive Mixed Reality Rendering on Holographic Pyramid}, 
  year={2022},
  volume={},
  number={},
  pages={483-492},
  abstract={Currently, ray tracing and image-based lighting (IBL) have shortcomings when rendering the metallic virtual object displayed in the holographic pyramid in mixed reality. Ray tracing can hardly achieve the interactive frame rates, and IBL cannot accurately render the reflection result of the foreground near the virtual object. In this paper, we propose a mixed reality rendering method to render glossy and specular reflection effects on metallic virtual objects displayed in the holographic pyramid based on the surrounding real environment at interactive frame rates. First, we acquire the real environment data with four RGBD cameras and a panoramic camera; then, we introduce a foreground point cloud generation method to extract a temporally stable foreground point cloud from RGBD videos captured in real time; after this, we propose an efficient ray tracing method to render the dynamic glossy and specular reflections on the virtual objects that are displayed in the holographic pyramid. We test our method on several real and synthetic scenes. Compared with IBL and screen-space ray tracing, our method can generate the rendering results closer to the ground truth at the same time cost. Compared with Monte Carlo path tracing, our method is 2.5-4.5× faster in generating rendering results of the comparable quality.},
  keywords={Point cloud compression;Three-dimensional displays;Mixed reality;Virtual reality;Ray tracing;User interfaces;Rendering (computer graphics);Mixed Reality;Virtual Reality;Reflection Rendering},
  doi={10.1109/VR51125.2022.00068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756758,
  author={Stein, Niklas and Bremer, Gianni and Lappe, Markus},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Eye Tracking-based LSTM for Locomotion Prediction in VR}, 
  year={2022},
  volume={},
  number={},
  pages={493-503},
  abstract={Virtual Reality (VR) allows users to perform natural movements such as hand movements, turning the head and natural walking in virtual environments. While such movements enable seamless natural interaction, they come with the need for a large tracking space, particularly in the case of walking. To optimise use of the available physical space, prediction models for upcoming behavior are helpful. In this study, we examined whether a user’s eye movements tracked by current VR hardware can improve such predictions. Eighteen participants walked through a virtual environment while performing different tasks, including walking in curved paths, avoiding or approaching objects, and conducting a search. The recorded position, orientation and eye-tracking features from 2.5 s segments of the data were used to train an LSTM model to predict the user’s position 2.5 s into the future. We found that future positions can be predicted with an average error of 65 cm. The benefit of eye movement data depended on the task and environment. In particular, situations with changes in walking speed benefited from the inclusion of eye data. We conclude that a model utilizing eye tracking data can improve VR applications in which path predictions are helpful.},
  keywords={Legged locomotion;Deep learning;Solid modeling;Tracking;Virtual environments;Gaze tracking;Predictive models;Virtual Reality;Eye Tracking;Locomotion;LSTM;Path Prediction;Machine Learning;Gaze},
  doi={10.1109/VR51125.2022.00069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756792,
  author={Wang, Qisong and Kang, Bo and Kristensson, Per Ola},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Supporting Playful Rehabilitation in the Home using Virtual Reality Headsets and Force Feedback Gloves}, 
  year={2022},
  volume={},
  number={},
  pages={504-513},
  abstract={Virtual Reality (VR) is a promising platform for home rehabilitation with the potential to completely immerse users within a playful experience. To explore this area we design, implement, and evaluate a system that uses a VR headset in conjunction with force feed-back gloves to present users with a playful experience for home rehabilitation. The system immerses the user within a virtual cat bathing simulation that allows users to practice fine motor skills by progressively completing three cat-care tasks. The study results demonstrate the positive role that playfulness may play in the user experience of VR rehabilitation.},
  keywords={Headphones;Solid modeling;Three-dimensional displays;Force feedback;Force;Exoskeletons;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Interaction devices;Haptic devices},
  doi={10.1109/VR51125.2022.00070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756759,
  author={Hombeck, Jan and Meuschke, Monique and Zyla, Lennert and Heuser, André-Joel and Toader, Justus and Popp, Felix and Bruns, Christiane J. and Hansen, Christian and Datta, Rabi R. and Lawonn, Kai},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Perceptional Tasks for Medicine: A Comparative User Study Between a Virtual Reality and a Desktop Application}, 
  year={2022},
  volume={},
  number={},
  pages={514-523},
  abstract={Since for most consumers the Virtual Reality (VR) experience exceeds that of desktop applications, an increasing number of applications is being transferred from desktop to VR. Industrial and entertainment applications primarily expect for a richer consumer experience, while others, such as surgical applications, seek for improved precision over their desktop counterparts. One way to improve the performance of precision-based VR applications is to provide suitable visualizations. Today, these "suitable" visualizations are mostly transferred from desktop to VR without considering their spatial and temporal performance might change in VR. This may not lead to an optimal solution, which can be crucial for precision-based tasks. Misinterpretation of shape or distance in a surgical or pre-operative simulation can affect the chosen treatment and thus directly impact the outcome. Therefore, we evaluate the performance differences of multiple visualizations for 3D surfaces based on their shape and distance estimation for desktop and VR applications. We conducted a quantitative user study with 56 participants evaluating seven visualizations (Phong, Toon, Fresnel, Pseudo-Chromadepth, Heatmap, Isolines, and Arrow Glyphs). Our results show that the performance of each visualization varies depending on the task, system, and surface type, with VR generally providing improved results. While Isolines are able to improve distance estimation, Phong and Heatmaps are beneficial for shape estimation.},
  keywords={Heating systems;Visualization;Solid modeling;Three-dimensional displays;Shape;Estimation;Surgery;Computer Graphics;Distance and Shape Estimation;Virtual Reality;Visualization},
  doi={10.1109/VR51125.2022.00071},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756764,
  author={Hirt, Christian and Kompis, Yves and Holz, Christian and Kunz, Andreas},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Chaotic Behavior of Redirection – Revisiting Simulations in Redirected Walking}, 
  year={2022},
  volume={},
  number={},
  pages={524-533},
  abstract={Redirected Walking (RDW) is a common technique leveraged to allow real walking for exploring large virtual environments in constrained physical tracking spaces. Effective RDW is challenging due to its complexity and disturbance factors (e.g., spontaneous user behavior). Existing techniques range from combinations of simple motion scaling to more elaborate curvature injections and reactive, predictive, or scripted steering concepts. However, many of these approaches were evaluated in simulation only, and researchers argued that the findings would translate to real scenarios to motivate the effectiveness of their algorithms. Using the Redirected Walking Toolkit and its virtual path generator, a randomized waypoint-based path generator has been common practice, although its built-in simplifications assume sequential user behavior regarding translation and rotation.In this paper, we argue that pure simulation-based evaluations employing such simplified path generators require critical reflection. We demonstrate RDW simulations that show the chaotic process fundamental to RDW, in which altering the initial user’s position by mere millimeters can drastically change the resulting steering behavior. This insight suggests that RDW is more sensitive to the underlying data than previously assumed. Thus, we rigorously analyze the influence of commonly used synthetically generated paths on multiple state-of-the-art steering concepts and compare them against previously recorded real paths.},
  keywords={Legged locomotion;Solid modeling;Three-dimensional displays;Tracking;Conferences;Virtual environments;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR51125.2022.00072},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756739,
  author={Huang, Bingyao and Ling, Haibin},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers}, 
  year={2022},
  volume={},
  number={},
  pages={534-542},
  abstract={Light-based adversarial attacks use spatial augmented reality (SAR) techniques to fool image classifiers by altering the physical light condition with a controllable light source, e.g., a projector. Compared with physical attacks that place hand-crafted adversarial objects, projector-based ones obviate modifying the physical entities, and can be performed transiently and dynamically by altering the projection pattern. However, subtle light perturbations are insufficient to fool image classifiers, due to the complex environment and project-and-capture process. Thus, existing approaches focus on projecting clearly perceptible adversarial patterns, while the more interesting yet challenging goal, stealthy projector-based attack, remains open. In this paper, for the first time, we formulate this problem as an end-to-end differentiable process and propose a Stealthy Projector-based Adversarial Attack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture process using a deep neural network named PCNet, then we include PCNet in the optimization of projector-based attacks such that the generated adversarial projection is physically plausible. Finally, to generate both robust and stealthy adversarial projections, we propose an algorithm that uses minimum perturbation and adversarial confidence thresholds to alternate between the adversarial loss and stealthiness loss optimization. Our experimental evaluations show that SPAA clearly outperforms other methods by achieving higher attack success rates and meanwhile being stealthier, for both targeted and untargeted attacks.},
  keywords={Deep learning;Solid modeling;Three-dimensional displays;Sensitivity;Perturbation methods;Neural networks;Spatial augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Security and privacy;Human and societal aspects of security and privacy;Privacy protections Computing methodologies;Artificial intelligence;Computer vision;Object recognition},
  doi={10.1109/VR51125.2022.00073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756811,
  author={Riegler, Andreas and Riener, Andreas and Holzmann, Clemens},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Content Presentation on 3D Augmented Reality Windshield Displays in the Context of Automated Driving}, 
  year={2022},
  volume={},
  number={},
  pages={543-552},
  abstract={Increasing vehicle automation presents challenges as drivers of automated vehicles become more disengaged from the primary driving task, as there will still be activities that require interfaces for vehicle-passenger interactions. Windshield displays provide large-content areas supporting drivers in non-driving related tasks. This work addresses user preferences as well as task and safety aspects for 3D augmented reality (AR) windshield displays in automated driving. Participants of a user study (N = 24) were presented with two modes of content presentation (multiple content-specific windows vs. one main window), and could freely choose their preferred positions, content types, as well as size, and transparency levels for these content windows using a simulated "ideal" windshield display in a virtual reality driving simulator. We found that using one main content window resulted in better task performance and lower take-over times, however, subjective user experience was higher for the multi-window user interface. These insights help designers of in-vehicle applications to provide a rich user experience in automated vehicles.},
  keywords={Productivity;Three-dimensional displays;Automation;User interfaces;User experience;Safety;Task analysis;Human-centered computing;Visualization;Visualization techniques;Visualization design and evaluation methods;User Interfaces;Graphical user interfaces (GUI);Information Interfaces and Presentation;Miscellaneous},
  doi={10.1109/VR51125.2022.00074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756748,
  author={Jabbireddy, Susmija and Zhang, Yang and Peckerar, Martin and Dagenais, Mario and Varshney, Amitabh},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sparse Nanophotonic Phased Arrays for Energy-Efficient Holographic Displays}, 
  year={2022},
  volume={},
  number={},
  pages={553-562},
  abstract={The Nanophotonic Phased Array (NPA) is an emerging holographic display technology. With chip-scaled sizes, high refresh rates, and integrated light sources, a large-scale NPA can enable high-resolution real-time dynamic holographic displays. However, one of the critical challenges impeding the development of such large-scale NPAs is the high electrical power consumption required to modulate the amplitude and phase of each of the pixel elements. We argue that the modulation of all the elements on the array is, in fact, not necessary to produce a high-quality image. We propose a simple method that outputs the configuration of a sparse NPA, along with the amplitude and the phase required at each active pixel to generate the desired image at the observation plane. We identify the set of active pixels according to their optimized intensities. We observe that the brighter pixels have a greater influence on the target image, and it is these that we must focus on in image formation. Using as few as 10% of the total pixels from a dense 2D array of light-emitting elements, we show that a perceptually acceptable holographic image can be generated. We compare various sparse sampling methods through computational simulations and show that our proposed method gives superior qualitative and quantitative results. We believe our study will help advance research on sparse NPAs and facilitate the use of large-scale NPAs to display high-resolution 3D holographic images.},
  keywords={Phased arrays;Solid modeling;Three-dimensional displays;Power demand;Modulation;Virtual reality;User interfaces;Computing methodologies;Image processing;Hardware;Displays and imagers;Mixed / augmented reality;Virtual reality},
  doi={10.1109/VR51125.2022.00075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756745,
  author={Miller, Robert and Banerjee, Natasha Kholgade and Banerjee, Sean},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Temporal Effects in Motion Behavior for Virtual Reality (VR) Biometrics}, 
  year={2022},
  volume={},
  number={},
  pages={563-572},
  abstract={Using the motion behavior of users in virtual reality (VR) as a biometric signature has the potential to enable continuous identification and authentication of users without compromising VR applications if traditional passwords are acquired by malicious agents. Users exhibit natural variabilities in behavior over time that influence their body motions and can alter the trajectories of VR devices such as the headset and the controllers. Behavior variabilities may negatively impact the success rate of VR biometrics. In this work, we evaluate how deep learning approaches to match input and enrollment trajectories are influenced by user behavior variation over varying time scales. We demonstrate that over short timescales on the order of seconds to minutes, no statistically significant relationship is found in the temporal placement of enrollment trajectories and their matches to input trajectories. We find that on medium-scale separation between enrollment and input trajectories, on the order of days to weeks, median accuracy is similar within users who provide input close and distant to enrollment data. Over long timescales on the order of 7 to 18 months, we obtain optimal performance for short and long enrollment/input separations by using training sets from users providing long-timescale data, as these sets encompass coarse and fine-scale changes in behavior.},
  keywords={Training;Headphones;Deep learning;Three-dimensional displays;Biometrics (access control);Impedance matching;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Security and privacy;Security services;Authentication;Biometrics},
  doi={10.1109/VR51125.2022.00076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756800,
  author={Hoang, Thuong and Greuter, Stefan and Taylor, Simeon},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={An Evaluation of Virtual Reality Maintenance Training for Industrial Hydraulic Machines}, 
  year={2022},
  volume={},
  number={},
  pages={573-581},
  abstract={Virtual reality applications for industrial training have widespread benefits for simulating various scenarios and conditions. We present an empirical evaluation of VR training approach using kinesthetics learning strategy in industrial maintenance training, specifically the hydraulic manufacturing industry. Through our collaboration with a leading industry partner, a remote multi-user training platform using head-mounted display was designed and implemented. We present the evaluation of the platform with two diverse cohorts of novice users and industry contractors, in comparison to traditional training using slides, photos, and videos. The results show that VR training is engaging and effective in boosting trainee’s confidence, especially for novice users. Our studies highlight the impact of virtual reality training on trainee experience, performance, and skills transfer, with reflections on the differences between novice and industry trainees.},
  keywords={Training;Three-dimensional displays;Collaboration;Virtual environments;Virtual reality;Hydraulic systems;Maintenance engineering;Virtual Reality;Industrial Training;Multimedia Learning in Virtual Reality;Kinesthetics Learning},
  doi={10.1109/VR51125.2022.00077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756770,
  author={Fujimoto, Yuichiro and Sawabe, Taishi and Kanbara, Masayuki and Kato, Hirokazu},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Structured Light of Flickering Patterns Having Different Frequencies for a Projector-Event-Camera System}, 
  year={2022},
  volume={},
  number={},
  pages={582-588},
  abstract={Projector-camera systems have long been used in measuring three-dimensional shapes. Most projector-camera systems can only be used in dark rooms because frame-based cameras are not robust against strong ambient light and are difficult to obtain correspondence to the image pixels of the projector. Recently, event cameras, which can detect the direction of luminance change, have received attention in the field of computer vision. When considering the many advantages of event cameras, this study focuses on their wide dynamic range (120 vs. 40 dB of a frame-based camera) and their ability to detect fast luminance changes. Our objective is to realize a projector-camera system that combines the event camera with a projector under the strong ambient light. Specifically, this study proposes a new structured light that combines different frequencies of flickers to acquire the correspondence between the image pixels of the event camera and the projector. This method does not rely on the co-axial frame-based measurement and synchronization mechanism between projector and camera and is thus applicable to most general event cameras. Experiments confirm that the proposed method obtains the correspondence robustly with reasonable accuracy in a bright room (up to 2,600 lux) under general indoor lighting and additional light projection.},
  keywords={Three-dimensional displays;Shape;Shape measurement;Lighting;Virtual reality;User interfaces;Dynamic range;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality;Artificial intelligence;Computer vision;Computer vision problems},
  doi={10.1109/VR51125.2022.00078},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756823,
  author={Volmer, Benjamin and Baumeister, James and Matthews, Raymond and Grosser, Linda and Von Itzstein, Stewart and Banks, Siobhan and Thomas, Bruce H.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparison of Spatial Augmented Reality Predictive Cues and their Effects on Sleep Deprived Users}, 
  year={2022},
  volume={},
  number={},
  pages={589-598},
  abstract={Spatial Augmented Reality (SAR) is a useful tool for procedural tasks as virtual instructions can be co-located with the physical task. Predictive cue annotations have been utilized to further enhance user performance through SAR. These predictive cues have only been measured under ordinary circumstances. This paper aims to investigate predictive cues under a sub-optimal scenario by depriving users of sleep. Sleep deprivation is a common form of fatigue, which is known to have serious detriments on user performance. Based upon existing sleep literature, we expected predictive cue performance to degrade over time with sharp declines during the early hours of the morning. Despite these drops in performance, we hypothesized that having a predictive cue would benefit user performance in comparison to no cue. Results from a 62-hour sleep deprivation experiment indicated that providing SAR predictive cues was beneficial throughout sleep deprivation. Furthermore, having no predictive cue caused accuracy to decline earlier in the sleep deprivation period. From the predictive cues outlined in this paper, the line cue maintained the fastest response time and was least impacted by early morning performance declines.},
  keywords={Performance evaluation;Uncertainty;Spatial augmented reality;Resists;Virtual reality;User interfaces;Fatigue;Spatial augmented reality;sleep deprivation;circadian rhythm;predictive cues;procedural task},
  doi={10.1109/VR51125.2022.00079},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756734,
  author={Zhang, Yidan and Ens, Barrett and Satriadi, Kadek Ananta and Prouzeau, Arnaud and Goodwin, Sarah},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={TimeTables: Embodied Exploration of Immersive Spatio-Temporal Data}, 
  year={2022},
  volume={},
  number={},
  pages={599-605},
  abstract={We propose TimeTables, a novel prototype system that aims to support data exploration, using embodiment with space-time cubes in virtual reality. TimeTables uses multiple space-time cubes on virtual tabletops, which users can manipulate by extracting time layers or individual buildings to create new tabletop views. The surrounding environment includes a large space for multiple linked tabletops and a storage wall. TimeTables presents information at different time scales by stretching layers to drill down in time. Users can also jump into tabletops to inspect data from an egocentric perspective. We present a use case scenario of energy consumption displayed on a university campus to demonstrate how our system could support data exploration and analysis over space and time. From our experience and analysis we believe the system has a high potential in assisting spatio-temporal data exploration and analysis.},
  keywords={Energy consumption;Three-dimensional displays;Data analysis;Conferences;Buildings;Prototypes;Data visualization;Human-centered computing;Visualization;Visualization application domains;Information visualization;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques},
  doi={10.1109/VR51125.2022.00080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756834,
  author={Kiuchi, Shunji and Koizumi, Naoya},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Distortion-free Mid-air Image Inside Refractive Surface and on Reflective Surface}, 
  year={2022},
  volume={},
  number={},
  pages={606-614},
  abstract={In this study, we propose an approach to display a distortion-free mid-air image inside a transparent refractive object and on a curved reflective surface. We compensate for the distortion by generating a light source image that cancels the distortions in the mid-air image caused by refraction and reflection. The light source image is generated via ray-tracing simulation by transmitting the desired view image to the mid-air imaging system, which includes distortive surfaces, and by receiving the transmitted image at the light source position. The proposed methods can be applied to dynamic images using a light source image as a UV map in texture mapping. Finally, we present the results of an evaluation of our method performed in an actual optical system using the generated light source image, which visually demonstrate the effectiveness of the proposed approach.},
  keywords={Solid modeling;Optical distortion;Virtual reality;Ray tracing;User interfaces;Distortion;Optical imaging;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Rendering;Ray tracing},
  doi={10.1109/VR51125.2022.00081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756754,
  author={Reichherzer, Carolin and Cunningham, Andrew and Barr, Jason and Coleman, Tracey and McManus, Kurt and Sheppard, Dion and Coussens, Scott and Kohler, Mark and Billinghurst, Mark and Thomas, Bruce H.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Supporting Jury Understanding of Expert Evidence in a Virtual Environment}, 
  year={2022},
  volume={},
  number={},
  pages={615-624},
  abstract={This work investigates the use of Virtual Reality (VR) to present forensic evidence to the jury in a courtroom trial. The findings of a between-participant user study on comprehension of an expert statement are presented, examining the benefits and issues of using VR compared to traditional courtroom presentation (being still images). Participants listened to a forensic scientist explain bloodstain spatter patterns while viewing a mock crime scene in either VR or as still images in video format. Under these conditions, we compared understanding of the expert domain, mental effort and content recall. We found that VR significantly improves the understanding of spatial information and knowledge acquisition. We also identify different patterns of user behaviour depending on the display method. We conclude with suggestions on how to best adapt evidence presentation to VR.},
  keywords={Visualization;Three-dimensional displays;Forensics;Knowledge acquisition;Conferences;Virtual environments;Information processing;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Interaction paradigms;Virtual reality},
  doi={10.1109/VR51125.2022.00082},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756806,
  author={Matthews, Brandon J. and Thomas, Bruce H. and Von Itzstein, G. Stewart and Smith, Ross T.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Shape Aware Haptic Retargeting for Accurate Hand Interactions}, 
  year={2022},
  volume={},
  number={},
  pages={625-634},
  abstract={This paper presents Shape Aware Haptic Retargeting, an extension of "state-of-the-art" haptic retargeting that is the first to support retargeted interaction between any part of the user’s hand and any part of the target object. In previous haptic retargeting algorithms, the maximum retargeting is applied only when the hand position aligns with the target position. Shape Aware Haptic Retargeting generalizes the distance computation process to instead consider the hand and target geometry. The shortest hand-target distance is then used to calculate the applied retargeting offset. This ensures the full amount of haptic retargeting is applied at the point of contact with the passive haptic regardless of contact position on the hand or target. We leverage existing geometry algorithms to implement three distance computation methods: Multi-Point, Primitive and Mesh Geometry, in addition to conventional single position approaches. These are evaluated through a set of simulated interactions instead of the single position representation used in previous haptic retargeting systems. The evaluation demonstrated all three approaches can provide improved interaction accuracy over a Point distance computation method, with Mesh Geometry being the most accurate and Primitive being the preferred method for combined performance and interaction accuracy.},
  keywords={Geometry;Visualization;Solid modeling;Three-dimensional displays;Target tracking;Shape;Conferences;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computer graphics;Shape modeling;Shape analysis},
  doi={10.1109/VR51125.2022.00083},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756765,
  author={Ogawa, Kumpei and Fujita, Kazuyuki and Takashima, Kazuki and Kitamura, Yoshifumi},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={PseudoJumpOn: Jumping onto Steps in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={635-643},
  abstract={Jumping onto steps is a promising action for creating an instant height movement in VR, but installing physical steps is impractical. We propose PseudoJumpOn, a novel locomotion technique using a common VR setup that allows the user to experience virtual step-up jumping motion by applying two types of viewpoint-manipulation methods to a physical jump on a flat floor. The core idea is to replicate the physical characteristics of ascending jumps, and thus we designed two viewpoint-manipulation methods: gain manipulation, which differentiates the ascent and descent height, and peak shifting, which delays the peak timing. We conducted a user study asking participants (N = 20) to experience two-legged step-up jumps onto 0.2–0.8-m heights in VR as the two methods were applied in combination (gain manipulation: five conditions where the ascending gain was 1.0–5.0; peak shifting: four conditions where the peak timing in VR was delayed by 0–1.0 ratios of the original timing). The results showed that the participants in most conditions felt positively in terms of reality and naturalness of actually jumping onto steps, even though knowing no physical steps existed. In addition, subsequent analyses also derived practical guidelines for determining the appropriate gains and the potential use of peak shifting to achieve a natural step-up jumping experience.},
  keywords={Three-dimensional displays;Conferences;Buildings;Virtual reality;User interfaces;Prediction algorithms;Delays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality},
  doi={10.1109/VR51125.2022.00084},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756827,
  author={Hamada, Takeo and Hautasaari, Ari and Kitazaki, Michiteru and Koshizuka, Noboru},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Solitary Jogging with A Virtual Runner using Smartglasses}, 
  year={2022},
  volume={},
  number={},
  pages={644-654},
  abstract={Group exercise is more effective for gaining motivation than exercising alone, but it can be difficult to always find such partners. In this paper, we explore the experiences that joggers have with a virtual partner instead of a human partner and report on the results of two controlled experiments evaluating our approach. In Study 1, we investigated how participants felt and how their behav-ior changed when they jogged indoors with a human partner or with a virtual partner compared to solitary jogging. The virtual partner was represented either as a full-body, limb-only, or a point-light avatar displayed on smartglasses. In Study 2, we investigated the differences between the three representations as virtual partners for casual joggers in an outdoor setting. Based on our results, we propose implications for the design of virtual runners as casual jogging partners and speculate on their relationship with human users.},
  keywords={Three-dimensional displays;Statistical analysis;Avatars;Design methodology;Conferences;User interfaces;Safety;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Applied computing;Life and medical sciences;Consumer health},
  doi={10.1109/VR51125.2022.00085},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756797,
  author={Xu, Sen-Zhe and Lv, Tian and He, Guangrong and Chen, Chia-Hao and Zhang, Fang-Lue and Zhang, Song-Hai},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimal Pose Guided Redirected Walking with Pose Score Precomputation}, 
  year={2022},
  volume={},
  number={},
  pages={655-663},
  abstract={Redirected walking (RDW) aims to reduce the collisions in the physical space for VR applications. However, most of the previous RDW methods do not consider future possibilities of collisions after imperceptibly redirecting users. In this paper, we combine the subtle RDW methods and reset strategy in our method design and propose a novel solution for RDW that can make better use of physical space and trigger fewer resets. The key idea of our method is to discretize the representation of possible user positions and orientations by a series of standard poses and rate them based on the possibilities of hitting obstacles of their reachable poses. A transfer path algorithm is proposed to measure the accessibility among standard poses and is used to support the calculation of the scores of standard poses. Using our method, the user can be redirected imperceptibly to the optimal pose with the best score among all the reachable poses from the user’s current pose during walking. Experiments demonstrate that our method outperforms state-of-the-art methods in various environment sizes and obstacle layouts.},
  keywords={Legged locomotion;Three-dimensional displays;Design methodology;Current measurement;Conferences;Layout;Virtual reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR51125.2022.00086},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756738,
  author={Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={360 Depth Estimation in the Wild - the Depth360 Dataset and the SegFuse Network}, 
  year={2022},
  volume={},
  number={},
  pages={664-673},
  abstract={Single-view depth estimation from omnidirectional images has gained popularity with its wide range of applications such as autonomous driving and scene reconstruction. Although data-driven learning-based methods demonstrate significant potential in this field, scarce training data and ineffective 360 estimation algorithms are still two key limitations hindering accurate estimation across diverse domains. In this work, we first establish a large-scale dataset with varied settings called Depth360 to tackle the training data problem. This is achieved by exploring the use of a plenteous source of data, 360 videos from the internet, using a test-time training method that leverages unique information in each omnidirectional sequence. With novel geometric and temporal constraints, our method generates consistent and convincing depth samples to facilitate single-view estimation. We then propose an end-to-end two-branch multi-task learning network, SegFuse, that mimics the human eye to effectively learn from the dataset and estimate high-quality depth maps from diverse monocular RGB images. With a peripheral branch that uses equirectangular projection for depth estimation and a foveal branch that uses cubemap projection for semantic segmentation, our method predicts consistent global depth while maintaining sharp details at local regions. Experimental results show favorable performance against the state-of-the-art methods.},
  keywords={Training;Three-dimensional displays;MIMICs;Semantics;Estimation;Training data;Virtual reality;Computing methodologies;Computer graphics;Image manipulation;Image-based rendering;Artificial intelligence;Computer vision;Reconstruction},
  doi={10.1109/VR51125.2022.00087},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756747,
  author={Zhang, Yiran and Nguyen, Huyen and Ladevèze, Nicolas and Fleury, Cédric and Bourdot, Patrick},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Workspace Positioning Techniques during Teleportation for Co-located Collaboration in Virtual Reality using HMDs}, 
  year={2022},
  volume={},
  number={},
  pages={674-682},
  abstract={In many collaborative virtual reality applications, co-located users often have their relative position in the virtual environment matching the one in the real world. The resulting spatial consistency facilitates the co-manipulation of shared tangible props and enables the users to have direct physical contact with each other. However, these applications usually exclude their individual virtual navigation capability, such as teleportation, as it may break the spatial configuration between the real and virtual world. As a result, the users can only explore the virtual environment of approximately similar size and shape compared to their physical workspace. Moreover, their individual tasks with unlimited virtual navigation capability, which often take part in a continuous workflow of a complex collaborative scenario, have to be removed due to this constraint. This work aims to help overcome these limits by allowing users to recover spatial consistency after individual teleportation in order to re-establish their position in the current context of the collaborative task. We use a virtual representation of the user’s shared physical workspace and develop two different techniques to position it in the virtual environment. The first technique allows one user to fully position the virtual workspace, and the second approach enables concurrent positioning by equally integrating the input from all the users. We compared these two techniques in a controlled experiment in a virtual assembly task. The results show that allowing two users to manipulate the workspace significantly reduced the time they spent negotiating the position of the future workspace. However, the inevitable conflicts in simultaneous co-manipulation were also a little confusing to them.},
  keywords={Training;Three-dimensional displays;Navigation;Shape;Avatars;Collaboration;Virtual environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Collaborative interaction},
  doi={10.1109/VR51125.2022.00088},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756736,
  author={Gründling, Jan P. and Zeiler, Daniel and Weyers, Benjamin},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Answering With Bow and Arrow: Questionnaires and VR Blend Without Distorting the Outcome}, 
  year={2022},
  volume={},
  number={},
  pages={683-692},
  abstract={Psychological factors to be assessed by questionnaires, such as presence, may be influenced by variables like a break in presence induced by transitioning between physical and virtual reality - or time elapsed between the actual sensation and its measurement when questionnaires are used. Moreover, participants of previous experiments stated discomfort due to changing repeatedly between VR and non-VR. These aspects motivate the investigation of new ways of administering questionnaires in VR research. Previous attempts to integrate questionnaires into VR as world- or user-anchored canvases still lead to a clear separation between interaction for task and data acquisition in the virtual environment, which has been shown as problematic due to interruption of experience. Therefore, this work goes one step further by integrating the answering of the questionnaire into the actual task using the same interaction technique and metaphor. We implemented a bow and arrow game in which the player had to shoot at randomly spawning targets as fast and accurate as possible. Subsequently, the player had to answer each item of a questionnaire by actually shooting at a target that represented the rating with the same type of interaction method. No difference in presence (SUS-PQ), satisfaction (ASQ) or workload (SMEQ) ratings was found between questionnaires presented either embedded in VR (as described above), as text panel in VR or as desktop PC version. In conclusion, the embedded questionnaires may render less negative effects of transitioning between VR and non-VR without impairing the questionnaire results.},
  keywords={Human computer interaction;Three-dimensional displays;Design methodology;Virtual environments;Psychology;Resists;Games;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Empirical Studies in HCI},
  doi={10.1109/VR51125.2022.00089},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756817,
  author={Zielasko, Daniel and Heib, Jonas and Weyers, Benjamin},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Systematic Design Space Exploration of Discrete Virtual Rotations in VR}, 
  year={2022},
  volume={},
  number={},
  pages={693-702},
  abstract={Continuous virtual rotation is likely one of the biggest contributors to cybersickness, while simultaneously being necessary for many VR scenarios where the user, for instance, is sitting in a bus, at an office desk, or on a couch and therefore, limited in physical body rotation. A possible solution is discrete virtual rotation, such as already broadly accepted in translational movements (teleportation). In this work, we want to help increase the knowledge about discrete virtual rotations. We classify existing work and systematically investigate the two dimensionstarget(rotation)acquisition(selectionvs. directional)and body-based (yes vs. no) regarding their impact on the performance in a naive and a primed rotational search task, spatial orientation, and usability. We do find the novel virtual rotation selection most successful in both search tasks and no difference in the factor bodybased on spatial orientation.},
  keywords={Human computer interaction;Three-dimensional displays;Systematics;Cybersickness;Conferences;Teleportation;Space exploration;Human-centered computing;Mixed / augmented reality;Interaction techniques;Empirical studies in HCI},
  doi={10.1109/VR51125.2022.00090},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756784,
  author={Mori, Shohei and Kataoka, Yuta and Hashiguchi, Satoshi},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Pseudo-Weight in Augmented Reality Extended Displays}, 
  year={2022},
  volume={},
  number={},
  pages={703-710},
  abstract={Augmented reality (AR) allows us to wear virtual displays that are registered to our bodies and devices. Such virtually extendable displays, or AR extended displays (AREDs), provide personal display space and are free from physical restrictions. Existing work has explored the new design space to improve user experience and efficiency. Contrary to this direction, we focus on the weight that the user perceives from AREDs, even though they are virtual and have no physical weight. Our user study results show evidence that AREDs can be a source of pseudo-weight, in addition to that of a handheld physical display device. We also systematically evaluate the perceived weight changes depending on the layout and delay in the visualization system. These findings are similar to those in existing pseudo-haptics research. However, we found such behavior in pseudo-weight for a real device and virtual visual stimuli in the air, which differentiates our research from previous work.},
  keywords={Visualization;Three-dimensional displays;Handheld computers;Conferences;Layout;User interfaces;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception},
  doi={10.1109/VR51125.2022.00091},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756749,
  author={Bonfert, Michael and Lemke, Stella and Porzel, Robert and Malaka, Rainer},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Kicking in Virtual Reality: The Influence of Foot Visibility on the Shooting Experience and Accuracy}, 
  year={2022},
  volume={},
  number={},
  pages={711-718},
  abstract={When playing sports in virtual reality foot interaction is crucial for many disciplines. We investigated how the visibility of the foot influences penalty shooting in soccer. In a between-group experiment, we asked 28 players to hit eight targets with a virtual ball. We measured the performance, task load, presence, ball control, and body ownership of inexperienced to advanced soccer players. In one condition, the players saw a visual representation of their tracked foot which improved the accuracy of the shots significantly. Players with invisible foot needed 58% more attempts. Further, with foot visibility the self-reported body ownership was higher.},
  keywords={Human computer interaction;Visualization;Three-dimensional displays;Target tracking;Sensitivity;Conferences;Virtual reality;Human-centered computing—Virtual reality;Human-centered computing—Empirical studies in HCI},
  doi={10.1109/VR51125.2022.00092},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756778,
  author={Breitkreutz, Christiane and Brade, Jennifer and Winkler, Sven and Bendixen, Alexandra and Klimant, Philipp and Jahn, Georg},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatial Updating in Virtual Reality – Auditory and Visual Cues in a Cave Automatic Virtual Environment}, 
  year={2022},
  volume={},
  number={},
  pages={719-727},
  abstract={When we move through a real environment, egocentric location representations are effortlessly and automatically updated. While moving in synthetic environments, this effortless, continuous spatial updating is often disrupted or incomplete due to a lack of sensory, especially body-based, movement information. To prevent disorientation in virtual reality caused by missing body-based information, the support of spatial updating via other sensory movement cues is necessary. In the presented experiment, participants performed a spatial updating task in a sparse virtual scene presented inside a CAVE (Cave Automatic Virtual Environment). The task was to navigate back to a starting position after simulated movements with either no orientation cues, three visible distant landmarks or one continuous auditory cue present. The focus was not to compare visual and auditory cues but to explore the viability of auditory cueing with visual cues as a reference. Overall, the data showed improved task performance when an orientation cue was present, with auditory cues providing at least as much improvement as visual cues. Our results indicate that auditory cues in virtual environments can support spatial updating when body-based information is missing.},
  keywords={Visualization;Three-dimensional displays;Navigation;Conferences;Virtual environments;User interfaces;Task analysis;Spatial updating;virtual reality;auditory and visual cues;triangle completion task;CAVE;spatial orientation},
  doi={10.1109/VR51125.2022.00093},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756755,
  author={Flemming, Carlo and Weyers, Benjamin and Zielasko, Daniel},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={How to Take a Brake from Embodied Locomotion – Seamless Status Control Methods for Seated Leaning Interfaces}, 
  year={2022},
  volume={},
  number={},
  pages={728-736},
  abstract={Embodied locomotion, especially leaning, has one major problem. Effectively the regular functionality of the utilized body parts is overwritten. Thus, in this work, we propose 6 different status control methods that seamlessly switch off (brake) a seated leaning locomotion interface. Different input modalities, such as a physical button, voice, and gestures/metaphors, are used and evaluated against a baseline condition and a leaning interface with a bilateral transfer function. In a user study, participants were encouraged, but not forced to use these interfaces. They did, and the most diegetic interface, a hover-board metaphor, was the most preferred. However, the overall results are more heterogeneous and the interfaces vary in their suitability for different applications/scenarios. Therefore, we summarized the results in a guideline.},
  keywords={Human computer interaction;Three-dimensional displays;Conferences;Transfer functions;Switches;Virtual reality;Muscles;Human-centered computing;Mixed/augmented reality;Interaction techniques;Empirical studies in HCI},
  doi={10.1109/VR51125.2022.00094},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756737,
  author={Jung, Sungchul and Wu, Yuanjie and McKee, Ryan and Lindeman, Robert W.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={All Shook Up: The Impact of Floor Vibration in Symmetric and Asymmetric Immersive Multi-user VR Gaming Experiences}, 
  year={2022},
  volume={},
  number={},
  pages={737-745},
  abstract={This paper investigates the influence of floor-vibration tactile feedback on immersed users. Under symmetric and asymmetric tactile sensory cue conditions, we explore how multi-user Virtual Reality (VR) experiences are impacted by these cues in terms of illusion and coherence. Based on the reported positive impact of tactile cues in solo VR experiences, we posit that if context-matched perceptual tactile feedback is exchanged between users, they will report a significantly enhanced VR experience compared to not receiving the sensory stimuli, even within the same immersive VR experience. With our custom-built, computer-controlled vibration floor, we implemented a cannonball shooting game for two physically-separated players. In the VR game, the two players shoot cannonballs to destroy their opponent’s protective wall and cannon, while the programmed floor platform generates vertical vibrations depending on the experimental condition. We used a mixed-factorial design with four conditions for each pair of participants: 1) both A and B had vibration, and 2) neither A nor B had vibration (the Symmetric group), or 3) A had vibration, but B did not, and 4) B had vibration, but A did not (the Asymmetric group). We collected subjective and objective data for variables previously shown to be related to levels of illusion, coherence, and usability, including Presence, Co-Presence, Social Presence, Plausibility Illusion, Engagement, Embodiment, Coherence, Gaming Performance, and Overall Preference. A total of 39 pairs of participants were involved in the study. We found statistically significant differences for the vibration conditions on Co-Presence, Social Presence, Engagement, and Coherence, and for the symmetric conditions on the Plausibility Illusion and Coherence, but only with trivial or small effect sizes. The results indicate that vibration provided to a pair of game players in immersive VR can significantly enhance the VR experience, but sensory symmetry does not guarantee improved gaming performance.},
  keywords={Vibrations;Three-dimensional displays;Conferences;Tactile sensors;Games;Coherence;Virtual reality;Floor-vibration;Whole-body tactile;tactile;vibration;VR Game;Shared VR;Multiuser VR;Competition Game;Symmetric;Asymmetric},
  doi={10.1109/VR51125.2022.00095},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756777,
  author={Walton, David R. and Kavaklı, Koray and Dos Anjos, Rafael Kuffner and Swapp, David and Weyrich, Tim and Urey, Hakan and Steed, Anthony and Ritschel, Tobias and Akşit, Kaan},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Metameric Varifocal Holograms}, 
  year={2022},
  volume={},
  number={},
  pages={746-755},
  abstract={Computer-Generated Holography (CGH) offers the potential for genuine, high-quality three-dimensional visuals. However, fulfilling this potential remains a practical challenge due to computational complexity and visual quality issues. We propose a new CGH method that exploits gaze-contingency and perceptual graphics to accelerate the development of practical holographic display systems. Firstly, our method infers the user’s focal depth and generates images only at their focus plane without using any moving parts. Second, the images displayed are metamers; in the user’s peripheral vision, they need only be statistically correct and blend with the fovea seamlessly. Unlike previous methods, our method prioritises and improves foveal visual quality without causing perceptually visible distortions at the periphery. To enable our method, we introduce a novel metameric loss function that robustly compares the statistics of two given images for a known gaze location. In parallel, we implement a model representing the relation between holograms and their image reconstructions. We couple our differentiable loss function and model to metameric varifocal holograms using a stochastic gradient descent solver. We evaluate our method with an actual proof-of-concept holographic display, and we show that our CGH method leads to practical and perceptually three-dimensional image reconstructions.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Stochastic processes;Virtual reality;Holography;User interfaces;Computer-Generated Holography;Foveated Rendering;Metamerisation;Varifocal Near-Eye Displays;Virtual Reality;Augmented Reality},
  doi={10.1109/VR51125.2022.00096},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756831,
  author={Ye, Jiannan and Xie, Anqi and Jabbireddy, Susmija and Li, Yunchuan and Yang, Xubo and Meng, Xiaoxu},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Rectangular Mapping-based Foveated Rendering}, 
  year={2022},
  volume={},
  number={},
  pages={756-764},
  abstract={With the speedy increase of display resolution and the demand for interactive frame rate, rendering acceleration is becoming more critical for a wide range of virtual reality applications. Foveated rendering addresses this challenge by rendering with a non-uniform resolution for the display. Motivated by the non-linear optical lens equation, we present rectangular mapping-based foveated rendering (RMFR), a simple yet effective implementation of foveated rendering framework. RMFR supports varying level of foveation according to the eccentricity and the scene complexity. Compared with traditional foveated rendering methods, rectangular mapping-based foveated rendering provides a superior level of perceived visual quality while consuming minimal rendering cost.},
  keywords={Visualization;Costs;Three-dimensional displays;Pipelines;Optical buffering;Virtual reality;User interfaces;Computing methodologies;Computer graphics;Rendering;Visibility;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR51125.2022.00097},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756766,
  author={Meteriz-Yıldıran, Ülkü and Yıldıran, Necip Fazıl and Awad, Amro and Mohaisen, David},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Keylogging Inference Attack on Air-Tapping Keyboards in Virtual Environments}, 
  year={2022},
  volume={},
  number={},
  pages={765-774},
  abstract={Enabling users to push the physical world’s limits, augmented and virtual reality platforms opened a new chapter in perception. Novel immersive experiences resulted in the emergence of new interaction methods for virtual environments, which came with unprecedented security and privacy risks. This paper presents a keylogging inference attack to infer user inputs typed with in-air tapping keyboards. We observe that hands follow specific patterns when typing in the air and exploit this observation to carry out our attack. Starting with three plausible attack scenarios where the adversary obtains the hand trace patterns of the victim, we build a pipeline to reconstruct the user input. Our attack pipeline takes the hand traces of the victim as an input and outputs a set of input inferences ordered from the best to worst. Through various experiments, we showed that our inference attack achieves a pinpoint accuracy ranging from 40% to 87% within at most the top-500 candidate reconstructions. Finally, we discuss countermeasures, while the results presented provide a cautionary tale of the security and privacy risk of the immersive mobile technology.},
  keywords={Solid modeling;Privacy;Three-dimensional displays;Pipelines;Keyboards;Virtual environments;User interfaces;Security and privacy—Privacy protections;Human-centered computing—Text input;Human-centered computing—Ubiquitous and mobile devices},
  doi={10.1109/VR51125.2022.00098},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756804,
  author={Young, Gareth W. and O’Dwyer, Néill and Moynihan, Matthew and Smolic, Aljosa},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Audience Experiences of a Volumetric Virtual Reality Music Video}, 
  year={2022},
  volume={},
  number={},
  pages={775-781},
  abstract={Music videos are short films that integrate songs and imagery and are produced for artistic and promotional purposes. Modern music videos apply various media capture techniques and creative postproduction technologies to provide a myriad of stimulating and artistic approaches to audience entertainment and engagement for viewing across multiple devices. Within this domain, volumetric technologies are becoming a popular means of recording and reproducing musical performances for new audiences to access via traditional 2D screens and emergent virtual reality platforms. However, the precise impact of volumetric video in virtual reality music video entertainment has yet to be fully explored from a user’s perspective. Here we show how users responded to volumetric representations of music performance in virtual reality. Our results preliminarily demonstrate how audiences are likely to respond to music videos and offer insight into how future music videos may be developed for different user types. We anticipate our essay as a formative starting point for more sophisticated, interactive music videos that can be accessed and presented via extended-reality technologies.},
  keywords={Human computer interaction;Three-dimensional displays;Music;Entertainment industry;Media;User experience;Real-time systems;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI},
  doi={10.1109/VR51125.2022.00099},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756779,
  author={Mahmud, M. Rasel and Stewart, Michael and Cordova, Alberto and Quarles, John},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Auditory Feedback for Standing Balance Improvement in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={782-791},
  abstract={Virtual Reality (VR) users often experience postural instability, i.e., balance problems, which could be a major barrier to universal usability and accessibility for all, especially for persons with balance impairments. Prior research has confirmed the imbalance effect, but minimal research has been conducted to reduce this effect. We recruited 42 participants (with balance impairments: 21, without balance impairments: 21) to investigate the impact of several auditory techniques on balance in VR, specifically spatial audio, static rest frame audio, rhythmic audio, and audio mapped to the center of pressure (CoP). Participants performed two types of tasks - standing visual exploration and standing reach and grasp. Within-subject results showed that each auditory technique improved balance in VR for both persons with and without balance impairments. Spatial and CoP audio improved balance significantly more than other auditory conditions. The techniques presented in this research could be used in future virtual environments to improve standing balance and help push VR closer to universal usability.},
  keywords={Human computer interaction;Visualization;Three-dimensional displays;Design methodology;Conferences;Spatial audio;Virtual environments;Virtual Reality;balance;postural stability;auditory feedback;VR accessibility;VR usability;Head-Mounted Display},
  doi={10.1109/VR51125.2022.00100},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756824,
  author={Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah and Bodenheimer, Bobby},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Depth Perception in Augmented Reality: The Effects of Display, Shadow, and Position}, 
  year={2022},
  volume={},
  number={},
  pages={792-801},
  abstract={Although it is commonly accepted that depth perception in augmented reality (AR) displays is distorted, we have yet to isolate which properties of AR affect people’s ability to correctly perceive virtual objects in real spaces. From prior research on depth perception in commercial virtual reality, it is likely that ergonomic properties and graphical limitations impact visual perception in head-mounted displays (HMDs). However, an insufficient amount of research has been conducted in augmented reality HMDs for us to begin isolating pertinent factors in this family of displays. To this end, in the current research, we evaluate absolute measures of distance perception in the Microsoft HoloLens 2, an optical see-through AR display, and the Varjo XR-3, a video see-through AR display. The current work is the first to evaluate either device using absolute distance perception as a measure. For each display, we asked participants to verbally report distance judgments to both grounded and floating targets that were rendered either with or without a cast shadow along the ground. Our findings suggest that currently available video see-through displays may induce more distance underestimation than their optical see-through counterparts. We also find that the vertical position of an object and the presence of a cast shadow influence depth perception.},
  keywords={Three-dimensional displays;Atmospheric measurements;Current measurement;Optical distortion;Optical variables measurement;User interfaces;Particle measurements;OST AR;VST AR;distance;perception;shadow;depth;surface contact},
  doi={10.1109/VR51125.2022.00101},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756757,
  author={Lee, Jaewook and Jin, Fanjie and Kim, Younsoo and Lindlbauer, David},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={User Preference for Navigation Instructions in Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={802-811},
  abstract={Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users’ visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users’ contexts.},
  keywords={Visualization;Three-dimensional displays;Navigation;Avatars;Multimedia systems;Mixed reality;User interfaces;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
  doi={10.1109/VR51125.2022.00102},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756808,
  author={Bozgeyikli, Lal Lila and Bozgeyikli, Evren},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Tangiball: Foot-Enabled Embodied Tangible Interaction with a Ball in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={812-820},
  abstract={Interaction with tangible user interfaces (TUIs) in virtual reality (VR) is known to offer several benefits in terms of user experience. Incorporating identical-formed tangible objects for foot-enabled embodied interaction in VR is not a well-researched area. To address this gap, in this study, we explored foot-enabled embodied interaction in VR through a room-scale tangible soccer game (Tangiball). Users interacted with a physical ball with their feet in real time by seeing its virtual counterpart inside a VR head mounted display (HMD). Tangiball included a custom-built transparent physical ball, inside which motion trackers were secured using custom 3D-printed attachments. A between-subjects user study was performed with 40 participants, in which Tangiball was compared with the control condition of foot-enabled embodied interaction with a purely virtual ball. The results revealed that tangible interaction improved user performance and presence significantly, while no difference in terms of motion sickness was detected between the tangible and virtual versions. This paper discusses the development and evaluation of Tangiball along with implications of the user study results.},
  keywords={Three-dimensional displays;Tracking;Virtual reality;Resists;Games;User interfaces;Motion sickness;User experience;Real-time systems;Sports;Virtual reality;tangible user interfaces;foot-enabled interaction;embodied interaction;direct manipulation;user experience;evaluation},
  doi={10.1109/VR51125.2022.00103},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756825,
  author={Paris, Richard A. and Buck, Lauren E. and McNamara, Timothy P. and Bodenheimer, Bobby},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Impact of Limited Physical Space on the Navigation Performance of Two Locomotion Methods in Immersive Virtual Environments}, 
  year={2022},
  volume={},
  number={},
  pages={821-831},
  abstract={Consumer level virtual experiences almost always occur when physical space is limited, either by the constraints of an indoor space or of a tracked area. This observation coupled with the need for movement through large virtual spaces has resulted in a proliferation of research into locomotion interfaces that decouples movement through the virtual environment from movement in the real world. While many locomotion interfaces support movement of some kind in the real world, some do not. This paper examines the effect of the amount of physical space used in the real world on one popular locomotion interface, resetting, when compared to a locomotion interface that requires minimal physical space, walking in place. The metric used to compare the two locomotion interfaces was navigation performance, specifically, the acquisition of survey knowledge. We find that, while there are trade-offs between the two methods, walking in place is preferable in small spaces.},
  keywords={Legged locomotion;Measurement;Costs;Three-dimensional displays;Navigation;Tracking;Sociology;Virtual Reality;Locomotion Methods;Walking in Place;Resetting},
  doi={10.1109/VR51125.2022.00104},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756763,
  author={Yi, Xin and Lu, Yiqin and Cai, Ziyin and Wu, Zihan and Wang, Yuntao and Shi, Yuanchun},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={GazeDock: Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu}, 
  year={2022},
  volume={},
  number={},
  pages={832-842},
  abstract={Gaze-only input techniques in VR face the challenge of avoiding false triggering due to continuous eye tracking while maintaining interaction performance. In this paper, we proposed GazeDock, a technique for enabling fast and robust gaze-based menu selection in VR. GazeDock features a view-fixed peripheral menu layout that automatically triggers appearing and selection when the user’s gaze approaches and leaves the menu zone, thus facilitating interaction speed and minimizing the false triggering rate. We built a dataset of 12 participants’ natural gaze movements in typical VR applications. By analyzing their gaze movement patterns, we designed the menu UI personalization and optimized selection detection algorithm of GazeDock. We also examined users’ gaze selection precision for targets on the peripheral menu and found that 4–8 menu items yield the highest throughput when considering both speed and accuracy. Finally, we validated the usability of GazeDock in a VR navigation game that contains both scene exploration and menu selection. Results showed that GazeDock achieved an average selection time of 471ms and a false triggering rate of 3.6%. And it received higher user preference ratings compared with dwell-based and pursuit-based techniques.},
  keywords={Three-dimensional displays;Navigation;Layout;Virtual reality;Gaze tracking;Games;User interfaces;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Human-centered computing—Human computer interaction (HCI)— Interaction paradigms—Virtual reality},
  doi={10.1109/VR51125.2022.00105},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9756781,
  author={Woodworth, Jason W. and Broussard, David and Borst, Christoph W.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Redirecting Desktop Interface Input to Animate Cross-Reality Avatars}, 
  year={2022},
  volume={},
  number={},
  pages={843-851},
  abstract={We present and evaluate methods to redirect desktop inputs such as eye gaze and mouse pointing to a VR-embedded avatar. We use these methods to build a novel interface that allows a desktop user to give presentations in remote VR meetings such as conferences or classrooms. Recent work on such VR meetings suggests a substantial number of users continue to use desktop interfaces due to ergonomic or technical factors. Our approach enables desk-top and immersed users to better share virtual worlds, by allowing desktop-based users to have more engaging or present "cross-reality" avatars. The described redirection methods consider mouse pointing and drawing for a presentation, eye-tracked gaze towards audience members, hand tracking for gesturing, and associated avatar motions such as head and torso movement. A study compared different levels of desktop avatar control and headset-based control. Study results suggest that users consider the enhanced desktop avatar to be human-like and lively and draw more attention than a conventionally animated desktop avatar, implying that our interface and methods could be useful for future cross-reality remote learning tools.},
  keywords={Torso;Three-dimensional displays;Tracking;Distance learning;Avatars;Design methodology;Ergonomics;Human-centered computing;Human computer interaction (HCI);Interaction Paradigms;Virtual Reality;Human-centered computing;Interaction design;Interaction design pro-cess and methods;User interface design},
  doi={10.1109/VR51125.2022.00106},
  ISSN={2642-5254},
  month={March},}

