@INPROCEEDINGS{9417677,
  author={Eroglu, Sevinc and Stefan, Frederic and Chevalier, Alain and Roettger, Daniel and Zielasko, Daniel and Kuhlen, Torsten W. and Weyers, Benjamin},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Evaluation of a Free-Hand VR-based Authoring Environment for Automated Vehicle Testing}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={Virtual Reality is increasingly used for safe evaluation and validation of autonomous vehicles by automotive engineers. However, the design and creation of virtual testing environments is a cumbersome process. Engineers are bound to utilize desktop-based authoring tools, and a high level of expertise is necessary. By performing scene authoring entirely inside VR, faster design iterations become possible. To this end, we propose a VR authoring environment that enables engineers to design road networks and traffic scenarios for automated vehicle testing based on free-hand interaction. We present a 3D interaction technique for the efficient placement and selection of virtual objects that is employed on a 2D panel. We conducted a comparative user study in which our interaction technique outperformed existing approaches regarding precision and task completion time. Furthermore, we demonstrate the effectiveness of the system by a qualitative user study with domain experts.},
  keywords={Three-dimensional displays;Roads;Design methodology;Fingers;Virtual environments;User interfaces;Usability;Human-centered computing-Human computer interaction (HCI)- Virtual Reality;Human-centered computing-Interaction design and evaluation methods-User interface design-User studies},
  doi={10.1109/VR50410.2021.00020},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417716,
  author={Shimizu, Shuntaro and Hashimoto, Takeru and Yoshida, Shigeo and Matsumura, Reo and Narumi, Takuji and Kuzuoka, Hideaki},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Unident: Providing Impact Sensations on Handheld Objects via High-Speed Change of the Rotational Inertia}, 
  year={2021},
  volume={},
  number={},
  pages={11-20},
  abstract={Several virtual reality (VR) proxies have been developed that can emulate impact sensations by generating actual forces on the hand. Although these proxies contribute to increasing the reality of VR, they still have some limitations, such as high latency, high power consumption, and low frequency to provide impact sensations. To overcome these limitations, we first propose a method to provide an impact sensation without actual force generation by quickly changing the rotational inertia of a handheld proxy while users are swinging it. Then, we developed Unident, a handheld proxy capable of changing its rotational inertia by moving a weight along one axis at a high speed. Two experiments were conducted to evaluate the ability of Unident to provide users with impact sensations. In the first experiment, we demonstrate that Unident can physically provide an impact sensation applied to a handheld object by analyzing the pressure on the user's palm. The second experiment shows that Unident can provide an impact sensation with various magnitudes depending on the amount of rotational inertia to be changed. Finally, we present an application that can be enabled by Unident.},
  keywords={Solid modeling;Three-dimensional displays;Power demand;Force;Virtual reality;User interfaces;Kinetic theory;Human-centered computing-Human computer interaction-Interaction devices-Haptic devices},
  doi={10.1109/VR50410.2021.00021},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417718,
  author={Medeiros, Daniel and Anjos, Rafael dos and Pantidi, Nadia and Huang, Kun and Sousa, Maurício and Anslow, Craig and Jorge, Joaquim},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Promoting Reality Awareness in Virtual Reality through Proxemics}, 
  year={2021},
  volume={},
  number={},
  pages={21-30},
  abstract={Head-Mounted Virtual reality (VR) systems provide full-immersive experiences to users and completely isolate them from the outside world, placing them in unsafe situations. Existing research proposed different alert-based solutions to address this. Our work builds on these studies on notification systems for VR environments from a different perspective. We focus on: (i) exploring alert systems to notify VR users about non-immersed bystanders' in socially related, non-critical interaction contexts; (ii) understanding how best to provide awareness of non-immersed bystanders while maintaining presence and immersion within the Virtual Environment(VE). To this end, we developed single and combined alert cues - leveraging proxemics, perception channels, and push/pull approaches and evaluated those via two user studies. Our findings indicate a strong preference towards maintaining immersion and combining audio and visual cues, push and pull notification techniques that evolve dynamically based on proximity.},
  keywords={Wrist;Pervasive computing;Human computer interaction;Visualization;Three-dimensional displays;Virtual reality;Safety;Notifications;Virtual Reality;Human Computer Inter-action;Context Awareness;Reality Awareness},
  doi={10.1109/VR50410.2021.00022},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417691,
  author={Guefrech, Fatma Ben and Berthaut, Florent and Plénacoste, Patricia and Peter, Yvan and Grisoni, Laurent},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Revealable Volume Displays: 3D Exploration of Mixed-Reality Public Exhibitions}, 
  year={2021},
  volume={},
  number={},
  pages={31-39},
  abstract={In this paper, we present a class of mixed-reality displays which allow for the 3D exploration of content in public exhibitions. The shared experience of the exhibition and the preservation of artworks are two very important aspects of these contexts, in particular for museum exhibits. The use of display cases as a protection tool is substantially accepted. It decreases the risks of damages to artworks and cultural materials hosted in museums. In addition, the transparent panels create a reflection of the visitors inside the display case. This reflection can be used to augment and interact in 3D with the exhibited content, by coupling Spatial Augmented-Reality and Optical Combiners. We call such a combination a Revealable Volume Display (RVD). It allows visitors to reveal information placed freely inside or around protected artefacts, visible by all, using their reflection in the panel. However, it may also suffer from unfamiliar gestures and disrupted depth perception cues, making 3D exploration of content difficult. In this paper, we first discuss the implementation of RVDs, providing both projector-based and mobile versions. We then present a design space that describes the interaction possibilities that it offers. Drawing on insights from a field study during a first exhibition, we finally propose and evaluate techniques for facilitating 3D exploration with RVDs.},
  keywords={Couplings;Three-dimensional displays;Input devices;User interfaces;Tools;Reflection;Optical coupling;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Graphics input devices Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
  doi={10.1109/VR50410.2021.00023},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417806,
  author={Mousas, Christos and Koilias, Alexandros and Rekabdar, Banafsheh and Kao, Dominic and Anastaslou, Dimitris},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Toward Understanding the Effects of Virtual Character Appearance on Avoidance Movement Behavior}, 
  year={2021},
  volume={},
  number={},
  pages={40-49},
  abstract={This virtual reality study was conducted to assess the impact of the appearance of virtual characters on the avoidance movement behavior of participants. Five experimental conditions were examined. Under each condition, one of the five different virtual characters (classified as mannequin, human, cartoon, robot, and zombie) was studied. Each participant had to experience only one condition and was asked to perform the collision avoidance tasks two times. During the walking task, the motion of participants was recorded. After finishing the collision avoidance segment of the study, a questionnaire that examined different concepts (emotional reactivity, emotional contagion, attentional allocation, behavioral independence, perceived skill, presence, immersion, virtual character realism, and virtual character unpleasantness) was distributed to the participants. Based on the collected measurements (avoidance movement behavior and self-reported ratings), we tried to understand the effects of the appearance of a virtual character on the avoidance movement behavior, and its possible correlation to subjective ratings. The results obtained from this study indicated that the appearance of the virtual characters did affect the avoidance movement behavior and also some of the examined concepts. Additionally, participant avoidance movement behavior correlates with some subjective ratings.},
  keywords={Three-dimensional displays;Motion segmentation;Virtual reality;User interfaces;Particle measurements;Resource management;Motion measurement;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR50410.2021.00024},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417791,
  author={Cheng, Jen-Hao and Chen, Yi and Chang, Ting-Yi and Lin, Hsu-En and Wang, Po-Yao Cosmos and Cheng, Lung-Pan},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Impossible Staircase: Vertically Real Walking in an Infinite Virtual Tower}, 
  year={2021},
  volume={},
  number={},
  pages={50-56},
  abstract={We present Impossible Staircase, a real-walking virtual reality system that allows users to climb an infinite virtual tower. Our set-up consists of an one-level scaffold and a lifter. A user climbs up the scaffold by real walking on a stairway while wearing a head-mounted display, and gets reset to the ground level by a lifter imperceptibly. By repeating this process, the user perceives an illusion of climbing an infinite number of levels. Our system achieves the illusion by (1) controlling the movement of the lifter to generate reverse and imperceptible motion, (2) guiding the user through the scaffold with delay mechanisms to reset the lifter in time, and (3) procedural generating overlapping structures to enlarge perceived height of each level. We built a working system and demonstrated it with a 15-min experience. With the working system, we conducted user studies to gain deeper insights into vertical motion simulation and vertical real walking in virtual reality.},
  keywords={Legged locomotion;Solid modeling;Three-dimensional displays;Head-mounted displays;Multimedia systems;Poles and towers;Virtual reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;Virtual Realities},
  doi={10.1109/VR50410.2021.00025},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417796,
  author={Hou, Sifan and Wang, Yujia and Ning, Bing and Liang, Wei},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Climaxing VR Character with Scene-Aware Aesthetic Dress Synthesis}, 
  year={2021},
  volume={},
  number={},
  pages={57-64},
  abstract={Like real humans, virtual characters also need to dress up according to different application scenarios so that the virtual character appears professionally, harmoniously, and naturally. However, manual selection is tedious, and the appearances of virtual characters usually lack variety. In this paper, we propose a new problem of synthesizing appropriate dress for a virtual character based on the scenario analysis where he/she shows up. We come up with a pipeline to tackle the scenario-aware dress synthesis problem. Firstly, given a scene, our approach predicts a dress code from the extracted high-level information in the scene, consisting of season, occasion, and scene category. Then our approach tunes the dress details to fit the aesthetic criteria and the virtual character's attributes. An optimization of a cost function implements the tuning process. We carried out experiments to validate the efficacy of the proposed approach. The perceptual study results show the good performance of our approach.},
  keywords={Visualization;Three-dimensional displays;Semantics;Clothing;Color;Virtual reality;Lakes;Digital Fashion;Visualization Design and Evaluation Methods;Fashion Outfit Generation},
  doi={10.1109/VR50410.2021.00026},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417692,
  author={Wolf, Erik and Merdan, Nathalie and Dölinger, Nina and Mal, David and Wienrich, Carolin and Botsch, Mario and Latoschik, Marc Erich},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Embodiment of Photorealistic Avatars Influences Female Body Weight Perception in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={65-74},
  abstract={Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.},
  keywords={Three-dimensional displays;Avatars;Estimation;Medical treatment;User interfaces;Tools;Mirrors;Virtual human;presence;virtual body ownership;agency;body image;eating- and body weight disorders.: Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI;Human-centered computing-Human computer interaction (HCI)- Virtual reality},
  doi={10.1109/VR50410.2021.00027},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417723,
  author={Zenner, André and Regitz, Kora Persephone and Krüger, Antonio},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Blink-Suppressed Hand Redirection}, 
  year={2021},
  volume={},
  number={},
  pages={75-84},
  abstract={Many interaction techniques in virtual reality break with the 1-to-1 mapping from real to virtual space. Instead, specialized techniques for 3D interaction and haptic retargeting leverage hand redirection, offsetting the virtual hand rendering from the real hand position. To achieve unnoticeable hand redirection, however, the utilization of change blindness phenomena has not been systematically explored. Inspired by recent advances in the domain of redirected walking, we present the first hand redirection technique that makes use of blink-induced visual suppression and corresponding change blindness. We introduce Blink-Suppressed Hand Redirection (BSHR) to study the feasibility and detectability of hand redirection based on blink suppression. Our technique is based on Cheng et al.'s (2017) [9] body warping algorithm and instantaneously shifts the virtual hand when the user's vision is suppressed during a blink. Additionally, it can be configured to continuously increment hand offsets when the user's eyes are opened, limited to an extent below detection thresholds. In a psychophysical experiment, we verify that unnoticeable blink-suppressed hand redirection is possible even in worst -case scenarios, and derive the corresponding conservative detection thresholds (CDTs). Moreover, our results show that the range of unnoticeable redirection can be increased by combining continuous warping and blink-suppressed instantaneous shifts. As an additional contribution, we derive the CDTs for Cheng et al.'s (2017) [9] redirection technique that does not leverage blinks.},
  keywords={Legged locomotion;Visualization;Three-dimensional displays;Blindness;Virtual reality;User interfaces;Rendering (computer graphics);Virtual reality;hand redirection;haptic retargeting;blink suppression;detection thresholds},
  doi={10.1109/VR50410.2021.00028},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417777,
  author={Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Pitch in Auditory Error Feedback for Fitts' Tasks in Virtual Reality Training Systems}, 
  year={2021},
  volume={},
  number={},
  pages={85-94},
  abstract={Fitts' law and the associated throughput measure characterize user pointing performance in virtual reality (VR) training systems and simulators well. Yet, pointing performance can be affected by the feedback users receive from a VR application. This work examines the effect of the pitch of auditory error feedback on user performance in a Fitts' task through a distributed experiment. In our first study, we used middle- and high-frequency sound feedback and demonstrated that high-pitch error feedback significantly decreases user performance in terms of time and throughput. In the second study, we used adaptive sound feedback, where we increased the frequency with the error rate, while asking subjects to execute the task “as fast/as precise/as fast and precise as possible”. Results showed that adaptive sound feedback decreases the error rate for “as fast as possible” task execution without affecting the time. The results can be used to enhance and design various VR systems.},
  keywords={Training;Three-dimensional displays;Error analysis;Virtual reality;User interfaces;Throughput;Task analysis;Human-centered computing-Human Computer Interaction (HCI);Human-centered computing-Virtual Reality;Human-centered computing-Pointing},
  doi={10.1109/VR50410.2021.00029},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417804,
  author={Li, Yi-Jun and Jin, De-Rong and Wang, Miao and Chen, Jun-Long and Steinicke, Frank and Hu, Shi-Min and Zhao, Qinping},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Detection Thresholds with Joint Horizontal and Vertical Gains in Redirected Jumping}, 
  year={2021},
  volume={},
  number={},
  pages={95-102},
  abstract={Redirected jumping (RDJ) is a locomotion technique that allows users to explore a virtual space that is larger than the available physical space by imperceptibly manipulating users' virtual viewpoints according to different gains. In previous redirected jumping work, different types of gains were imposed separately, without considering the possible interaction effects of horizontal and vertical gains on the jumping distance perception. To figure out how humans perceive distance manipulation when more than one gain is used, in this paper, we explored joint horizontal and vertical gains that manipulate horizontal and vertical distances at the same time during two-legged takeoff jumping in the virtual space. We estimated and analyzed horizontal and vertical detection thresholds by conducting a user study, fitting the data to two-dimensional psychometric functions, and visualizing the fitted 3D plots. We provided quantitative insights into the effects of joint gains on detection thresholds, where the imperceptible range for one gain can be affected by the variation of the other gain. Finally, we designed redirected jumping-based games as applications with joint horizontal and vertical gains and demonstrated the effectiveness of the redirected jumping technique.},
  keywords={Three-dimensional displays;Fitting;Estimation;Data visualization;Games;Virtual reality;Space exploration;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR50410.2021.00030},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417697,
  author={Ricca, Aylen and Chellali, Amine and Otrnane, Samir},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The influence of hand visualization in tool-based motor-skills training, a longitudinal study}, 
  year={2021},
  volume={},
  number={},
  pages={103-112},
  abstract={Mastering motor skills requires performing the task unconsciously with great speed and accuracy. This is acquired slowly through practice over time. Nonetheless, in domains such as surgery, the training of these skills in the field introduces safety, ethical and economic issues. In this context, immersive VR technologies offer the possibility to recreate real-world situations and allow the trainees to improve their skills in a safe and controlled environment. However, the design of such systems raises new research questions, such as how to represent the user in the virtual environment, and whether this representation can influence motor skills automaticity. In this work, we focus on how the user's hand representation can impact the training of tool-based motor skills in immersive VR. To investigate this question, we have created a VR simulator for training a tool-based pick and place task, and conducted a user study to evaluate how the user's hand visualization can influence participants' learning performance after a two-week training period. For that purpose, two groups of participants were trained in the VR simulator under one of the two experimental conditions: the presence and the absence of their virtual hands' representation, while a control group received no training. The results of the study show that training on the VR simulator improves the participants' motor task performance when compared with the control group. On the other hand, no difference was observed between the two training groups. This suggests that the user's hand visualization does not always impact tool-based motor tasks training in immersive VR simulators. Indeed, for short-term motor training there was no difference in performance between having a partial embodiment of the user's hands and only the tools representation.},
  keywords={Training;Visualization;Solid modeling;Three-dimensional displays;Surgery;Prototypes;Virtual environments;Avatar;Motor-skills training;Immersive virtual reality},
  doi={10.1109/VR50410.2021.00031},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417642,
  author={Asahina, Ray and Nomoto, Takashi and Yoshida, Takatoshi and Watanabe, Yoshihiro},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Realistic 3D Swept-Volume Display with Hidden-Surface Removal Using Physical Materials}, 
  year={2021},
  volume={},
  number={},
  pages={113-121},
  abstract={Conventional swept-volume displays can provide accurate physical cues for depth perception. However, the corresponding texture reproduction does not have high quality because such displays employ high-speed projectors with low bit-depth and low resolution. In this study, to address the limitation of swept-volume displays while retaining their advantages, a novel swept-volume three-dimensional (3D) display is proposed by incorporating physical materials as screens. Physical materials such as wool, felt, and so on are directly used for reproducing textures on a displayed 3D surface. Furthermore, we introduce the adaptive pattern generation based on real-time viewpoint tracking to perform the hidden-surface removal. Our algorithm leverages the ray-tracing concept and can run at high speed on GPU.},
  keywords={Three-dimensional displays;Shape;Graphics processing units;Virtual reality;User interfaces;Ray tracing;Observers;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers},
  doi={10.1109/VR50410.2021.00032},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417722,
  author={Hertel, Julia and Steinicke, Frank},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Reality for Maritime Navigation Assistance - Egocentric Depth Perception in Large Distance Outdoor Environments}, 
  year={2021},
  volume={},
  number={},
  pages={122-130},
  abstract={Augmented reality (AR) provides enormous potential to improve navigation assistance interfaces by displaying information directly into the user's field of view. In maritime contexts, such AR interfaces could supplement conventional solutions that require users to interpret spatial positions visualized on two-dimensional maps. In order to design useful navigation assistance, it is crucial to understand how egocentric distances of displayed objects are perceived and how different design attributes influence depth estimation. While previous research mainly focused on depth perception in indoor environments and rather short distances, this paper presents an investigation of the perceived egocentric distance of virtual objects in distances up to 75 meters in an open outdoor environment. In a perceptual matching task experiment using the Microsoft HoloLens 2, participants had to move objects with different (i) shape, (ii) coloration, and (iii) relation to floor to various target distances. Our results suggest that participants overestimated the distance to virtual objects across all tested distances since they significantly underproduced target distances for all investigated design factors. Based on these results, we explored potential design implications for a maritime AR navigation assistance on a ship in the local port area.},
  keywords={Performance evaluation;Visualization;Navigation;Shape;Seaports;Rendering (computer graphics);Indoor environment;Human-centered computing-Visualization-Empirical studies in visualization-;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality},
  doi={10.1109/VR50410.2021.00033},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417787,
  author={Dickinson, Patrick and Cardwell, Andrew and Parke, Adrian and Gerling, Kathrin and Murray, John},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Diegetic Tool Management in a Virtual Reality Training Simulation}, 
  year={2021},
  volume={},
  number={},
  pages={131-139},
  abstract={Researchers and developers have suggested that the use of diegetic interfaces can enhance users' sense of presence and immersion in virtual reality (VR) applications. While concepts of diegetic interfaces in VR are analogous to those seen on 2D displays, little work has considered how they might integrate with the movement-based controllers commonly used in consumer VR systems, to create higher fidelity interactions. In this paper we present a study (N = 58) in which we compare participants' experiences of diegetic and non-diegetic tool management interfaces, in a prototype VR crime scene investigation (CSI) training application. Contrary to expectations, we do not find evidence that participants' sense of presence is elevated when using the diegetic interface; however, we suggest that this may be due to reported higher levels of perceived workload, which can act to degrade user experience and engagement. We conclude by discussing the relationship between diegetic interface design and interaction fidelity, and highlighting trade-offs between fidelity, engagement, and learning outcomes in VR training applications.},
  keywords={Training;Solid modeling;Three-dimensional displays;Two dimensional displays;Prototypes;Virtual reality;Tools;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Interaction design-Interaction design process and methods-User interface design;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/VR50410.2021.00034},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417775,
  author={Miller, Robert and Banerjee, Natasha Kholgade and Banerjee, Sean},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Siamese Neural Networks to Perform Cross-System Behavioral Authentication in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={140-149},
  abstract={In this paper, we provide an approach on using behavioral biometrics to perform cross-system high-assurance authentication of users in virtual reality (VR) environments. VR is currently being explored as a critical tool to ensure seamless delivery of essential services, such as education, healthcare, and personal finance, while enabling users to work from home environments. Due to the sensitive nature of personal data generated, VR applications for essential services need to provide secure access. Traditional PIN or password-based credentials can be breached by malicious impostors, or be handed over by an intended user of a VR system to a confederate to assist the intended user in completing a task, e.g., an exam or a physical therapy routine. Existing approaches that use the behavior of the user in VR as a biometric signature fail when users provide enrollment and use-time data on different VR systems. We use Siamese neural networks to learn a distance function that characterizes the systematic differences between data provided across pairs of dissimilar VR systems. Our approach provides average equal error rates (EERs) ranging from 1.38% to 3.86% for authentication using a benchmark dataset that consists of 41 users performing a ball-throwing task with 3 VR systems-an Oculus Quest, an HTC Vive, and an HTC Vive Cosmos. To compare to prior approaches in VR biometrics, we also obtain average accuracies for the task of identification, where given an input user's trajectory in a use-time VR system, we use Siamese networks to return the user with the top matching trajectory in an enrollment VR system as the label. We report identification results ranging from 87.82% to 98.53% with average improvements of 29.78%±8.58% and 30.78%±3.68% over existing approaches that use generic distance matching and fully convolutional networks on the enrollment dataset respectively.},
  keywords={Systematics;Biometrics (access control);Neural networks;Authentication;Virtual reality;User interfaces;Tools;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Security and privacy-Security services-Authentication-Biometrics},
  doi={10.1109/VR50410.2021.00035},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417717,
  author={Clarence, Aldrich and Knibbe, Jarrod and Cordeil, Maxime and Wybrow, Michael},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Unscripted Retargeting: Reach Prediction for Haptic Retargeting in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={150-159},
  abstract={Research is exploring novel ways of adding haptics to VR. One popular technique is haptic retargeting, where real and virtual hands are decoupled to enable the reuse of physical props. However, this technique requires the system to know the users' intended interaction target, or requires additional hardware for prediction. We explore software-based reach prediction as a means of facilitating responsive, unscripted retargeting. We trained a Long Short-Term Memory network on users' reach trajectories to predict intended targets. We achieved an accuracy of 81.1 % at approximately 65% of movement. This could enable haptic retargeting during the last 35% of movement. We discuss the implications for possible physical proxy locations.},
  keywords={Solid modeling;Three-dimensional displays;Neural networks;Virtual reality;Predictive models;User interfaces;Hardware;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Machine learning-Machine learning approaches-Neural networks},
  doi={10.1109/VR50410.2021.00036},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417647,
  author={Bhattacharya, Uttaran and Rewkowski, Nicholas and Banerjee, Abhishek and Guhan, Pooja and Bera, Aniket and Manocha, Dinesh},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.},
  keywords={Learning systems;Three-dimensional displays;Correlation;Databases;Natural languages;Pipelines;Graphics processing units;Computing methodologies-Virtual reality;Computing methodologies-Intelligent agents;Computer systems organization-Neural networks},
  doi={10.1109/VR50410.2021.00037},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417794,
  author={Wang, Lili and Wang, Hao and Dai, Danqing and Leng, Jiaye and Han, Xiaoguang},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Bidirectional Shadow Rendering for Interactive Mixed 360° Videos}, 
  year={2021},
  volume={},
  number={},
  pages={170-178},
  abstract={In this paper, we provide a bidirectional shadow rendering method to render shadows between real and virtual objects in the 360° videos in real time. We construct a 3D scene approximation from the current output viewpoint to approximate the real scene geometry nearby in the video. Then, we propose a ray casting based algorithm to determine the shadow regions on the virtual objects cast by the real objects. After that, we introduce an object-aware shadow mapping method to cast shadows from virtual objects to real objects. Finally, we use a shadow intensity estimation algorithm to determine the shadow intensity of virtual objects and real objects to obtain shadows consistent with the input 360° video. The experiment results prove the effectiveness of our bidirectional shadow rendering method for mixed 360° videos. Our method can generate visually realistic shadows for virtual objects and real objects in 360° video in realtime, and make virtual objects more natural to integrate with real scenes in 360° videos of the mixed reality applications.},
  keywords={Geometry;Three-dimensional displays;Mixed reality;Estimation;Virtual reality;User interfaces;Rendering (computer graphics);Mixed reality-360°;videos-Shadow rendering},
  doi={10.1109/VR50410.2021.00038},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417714,
  author={Xiong, Yuan and Chen, Hongrui and Wang, Jingru and Zhu, Zhe and Zhou, Zhong},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={DSNet: Deep Shadow Network for Illumination Estimation}, 
  year={2021},
  volume={},
  number={},
  pages={179-187},
  abstract={Illumination consistency has applications to modeling and rendering in virtual reality. In 3D reconstruction and Mixed Reality(MR) fusion, the appearance of a large-scale outdoor scene may change in response to lighting and seasons, for example. Since 3D reconstruction from scratch is costly, it is helpful to be able to update existing models with recently captured photographs. However, the illumination conditions of the captured photograph can be arbitrary, making it challenging to fit to the existing model. To tackle this problem, this paper proposes a novel approach that can precisely estimate the illumination of the input image. Our Deep Shadow Network (DSNet) collaboratively utilizes illumination-based data augmentation for sun position estimation, along with a dataset of illumination-based augmented renderings. Our run-time rendering and optimization strategy is also discussed. We show that accurate simulation of illumination can improve the performance of visual applications including place recognition and long-term localization. Experimental results validate the effectiveness of the proposed approach, and show its superiority over the state-of-the-art.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Computational modeling;Lighting;Estimation;Virtual reality;Computing methodologies-Modeling and simulation-Model development and analysis-Modeling methodologies;Artificial intelligence-Computer vision-Computer vision problems-Reconstruction},
  doi={10.1109/VR50410.2021.00039},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417713,
  author={Volonte, Matias and Wang, Chang-Chun and Ebrahimi, Elham and Hsu, Yu-Chun and Liu, Kuan-Yu and Wong, Sai-Keung and Babu, Sabarish V.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Effects of Language Familiarity in Simulated Natural Dialogue with a Virtual Crowd of Digital Humans on Emotion Contagion in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={188-197},
  abstract={This investigation compared the emotional impact caused by a crowd of affective virtual humans (VHs) that communicated in the users' native or foreign language. We evaluated the users' affective reactions to a crowd of VHs that exhibited distinct emotional expressions. A total of four emotions were presented, which were Positive, Negative, Neutral, and Mixed. The VHs performed verbal and non-behaviors accordingly. Under the Mixed condition, the VHs were divided into three groups equally and each group was uniquely assigned one of the three emotions (i.e., positive, negative, and neutral). Users collected ten items from a virtual reality market. To complete the tasks, they interacted using natural speech with the emotional VHs. Three language conditions were investigated: one condition in USA and another two conditions in Taiwan. The group of participants in USA interacted with the VHs in English; and the two groups of participants in Taiwan interacted with the VHs using a foreign (English) language and a native (Mandarin) language respectively. We discovered that the medium of communication or language familiarity had a strong influence on participants' emotional reactions. When participants interacted in a foreign language with VHs with a positive emotional disposition, we found their positive emotional reactions were subdued and negative reactions were elevated. However, this was not the case when participants interacted with VHs in their native language, as their emotional reactions were contingent on the emotional disposition of the VHs.},
  keywords={Learning systems;Visualization;Solid modeling;Three-dimensional displays;Natural languages;Virtual reality;Writing;Virtual Humans;Virtual Reality;Virtual Crowds;Emotional Contagion},
  doi={10.1109/VR50410.2021.00040},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417683,
  author={Singla, Ashutosh and Göring, Steve and Keller, Dominik and Ramachandra Rao, Rakesh Rao and Fremerey, Stephan and Raake, Alexander},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Assessment of the Simulator Sickness Questionnaire for Omnidirectional Videos}, 
  year={2021},
  volume={},
  number={},
  pages={198-206},
  abstract={Virtual Reality/360° videos provide an immersive experience to users. Besides this, 360° videos may lead to an undesirable effect when consumed with Head-Mounted Displays (HMDs), referred to as simulator sickness/cybersickness. The Simulator Sickness Questionnaire (SSQ) is the most widely used questionnaire for the assessment of simulator sickness. Since the SSQ with its 16 questions was not designed for 360° video related studies, our research hypothesis in this paper was that it may be simplified to enable more efficient testing for 360° video. Hence, we evaluate the SSQ to reduce the number of questions asked from subjects, based on six different previously conducted studies. We derive the reduced set of questions from the SSQ using Principal Component Analysis (PCA) for each test. Pearson Correlation is analysed to compare the relation of all obtained reduced questionnaires as well as two further variants of SSQ reported in the literature, namely Virtual Reality Sickness Questionnaire (VRSQ) and Cybersickness Questionnaire (CSQ). Our analysis suggests that a reduced questionnaire with 9 out of 16 questions yields the best agreement with the initial SSQ, with less than 44% of the initial questions. Exploratory Factor Analysis (EFA) shows that the nine symptom-related attributes determined as relevant by PCA also appear to be sufficient to represent the three dimensions resulting from EFA, namely, Uneasiness, Visual Discomfort and Loss of Balance. The simplified version of the SSQ has the potential to be more efficiently used than the initial SSQ for 360° video by focusing on the questions that are most relevant for individuals, shortening the required testing time.},
  keywords={Visualization;Three-dimensional displays;Head-mounted displays;Focusing;Immersive experience;User interfaces;Tools;360° video-simulator sickness-questionnaire-cybersickness;PCA-factor analysis},
  doi={10.1109/VR50410.2021.00041},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417783,
  author={Ye, Weicai and Li, Hai and Zhang, Tianxiang and Zhou, Xiaowei and Bao, Hujun and Zhang, Guofeng},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={SuperPlane: 3D Plane Detection and Description from a Single Image}, 
  year={2021},
  volume={},
  number={},
  pages={207-215},
  abstract={We present a novel end-to-end plane detection and description network named SuperPlane to detect and match planes in two RGB images. SuperPlane takes a single image as input and extracts 3D planes and generates corresponding descriptors simultaneously. A mask-attention module and an instance-triplet loss are proposed to improve the distinctiveness of the plane descriptor. For image matching, we also propose an area-aware Kullback-Leibler (KL) divergence retrieval method. Extensive experiments show that the proposed method outperforms state-of-the-art methods and retains good generalization capacity. The applications in image-based localization and augmented reality also demonstrate the effectiveness of SuperPlane.},
  keywords={Location awareness;Three-dimensional displays;Image matching;User interfaces;Augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Computing methodologies-Artificial intelligence-Computer vision-Computer vision problems},
  doi={10.1109/VR50410.2021.00042},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417655,
  author={Wang, Lili and Chen, Jianjun and Ma, Qixiang and Popescu, Voicu},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Disocclusion Headlight for Selection Assistance in VR}, 
  year={2021},
  volume={},
  number={},
  pages={216-225},
  abstract={We introduce the disocclusion headlight, a method for VR selection assistance based on alleviating occlusions at the center of the user's field of view. The user's visualization of the VE is modified to reduce overlap between objects. This way, selection candidate objects have larger image footprints, which facilitates selection. The modification is confined to the center of the frame, with continuity to the periphery of the frame which is rendered conventionally. The selection assistance is provided automatically, without any interaction from the user. Furthermore, our method disoccludes without destroying the local spatial relationships between selection candidates, which allows solving complex selection queries based on the relative position of objects. We have tested our method on three selection tasks, where we compared it to two state-of-the-art VR selection techniques, i.e., the alpha cursor and the flower cone. Our method showed significant advantages in terms of shorter task completion times, and of fewer selection errors.},
  keywords={Visualization;Three-dimensional displays;Virtual reality;User interfaces;Rendering (computer graphics);Task analysis;Virtual reality-Pointing and selection-Disocclusion-Multiperspective rendering},
  doi={10.1109/VR50410.2021.00043},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417666,
  author={Leng, Zhiying and Chen, Jiaying and Shum, Hubert P. H. and Li, Frederick W. B. and Liang, Xiaohui},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Stable Hand Pose Estimation under Tremor via Graph Neural Network}, 
  year={2021},
  volume={},
  number={},
  pages={226-234},
  abstract={Hand pose estimation, which predicts the spatial location of hand joints, is a fundamental task in VR/AR applications. Although existing methods can recover hand pose competently, the tremor issue occurring in hand motion has not been completely solved. Tremor is an involuntary motion accompanied by a desired gesture or hand motion, leading to hand pose that deviates from user's intentions. Considering the characteristic of tremor motion, we present a novel Graph Neural Network for stable 3D hand pose estimation. The input is depth images. The constraint adjacency matrix is devised in Graph Neural Network for dynamically adjusting the topology of a hand graph during message passing and aggregation. Firstly, since there are rich potential constraints among hand joints, we utilize the constraint adjacency matrix to mine the suitable topology, modeling spatial-temporal constraints of joints and outputting the precise tremor hand pose as the pre-estimation result. Then, for obtaining a stable hand pose, we provide a tremor compensation module based on the constraint adjacency matrix, which exploits the constraint between control points and tremor hand pose. Concretely, the control points represented the voluntary motion are employed as constraints to edit the tremor hand pose. Our extensive quantitative and qualitative experiments show that the proposed method has achieved decent performance for 3D tremor hand pose estimation.},
  keywords={Training;Human computer interaction;Three-dimensional displays;Network topology;Message passing;Pose estimation;Virtual reality;Computing methodologies-Artificial intelligence-Computer vision-Computer vision problems;Human-centered computing-Human computer interaction-Interaction techniques-Gestural input},
  doi={10.1109/VR50410.2021.00044},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417774,
  author={Wang, Miao and Ye, Zi-Ming and Shi, Jin-Chuan and Yang, Yang-Liang},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Scene-Context-Aware Indoor Object Selection and Movement in VR}, 
  year={2021},
  volume={},
  number={},
  pages={235-244},
  abstract={Virtual reality (VR) applications such as interior design typically require accurate and efficient selection and movement of indoor objects. In this paper, we present an indoor object selection and movement approach by taking into account scene contexts such as object semantics and interrelations. This provides more intelligence and guidance to the interaction, and greatly enhances user experience. We evaluate our proposals by comparing them with traditional approaches in different interaction modes based on controller, head pose, and eye gaze. Extensive user studies on a variety of selection and movement tasks are conducted to validate the advantages of our approach. We demonstrate our findings via a furniture arrangement application.},
  keywords={Three-dimensional displays;Semantics;Virtual reality;User interfaces;User experience;Proposals;Task analysis;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Pointing},
  doi={10.1109/VR50410.2021.00045},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417803,
  author={Hawes, Dan and Arya, Ali},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-based Student Priming to Reduce Anxiety and Increase Cognitive Bandwidth}, 
  year={2021},
  volume={},
  number={},
  pages={245-254},
  abstract={Recent research indicates that many post-secondary students feel overwhelming anxiety, negatively impacting academic performance and overall well-being. In this paper, based on multidisciplinary literature analysis and innovative ideas in cognitive science, learning models, and emerging technologies, we introduce a theoretical framework that shows how and when priming activities can be introduced into the experiential learning cycle to reduce anxiety and increase cognitive bandwidth. This framework proposes a Virtual Reality based priming approach that uses games and meditative interventions. Our results show this approach's potential compared to no-priming scenarios for reducing anxiety and significance for VR gaming in improving cognitive bandwidth.},
  keywords={Solid modeling;Analytical models;Epidemics;Three-dimensional displays;Buildings;Bandwidth;Virtual reality;Anxiety;scarcity;priming;virtual reality;games},
  doi={10.1109/VR50410.2021.00046},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417665,
  author={He, Wennan and Xi, Mingze and Gardner, Henry and Swift, Ben and Adcock, Matt},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spatial Anchor Based Indoor Asset Tracking}, 
  year={2021},
  volume={},
  number={},
  pages={255-259},
  abstract={Indoor asset tracking is an essential task in many areas of industry such as shipping and warehousing. Widely-used asset tracking technologies typically require supporting infrastructure (signal transponders) to communicate with active tags on the assets. Continuous asset tracking in large indoor spaces like warehouses can be costly, and reliable reference points are needed to calculate asset positions accurately. In this paper, we describe an indoor asset tracking technique for augmented reality (AR) that combines the use of (passive) fiducial markers together with flexible spatial anchors. Our approach, called SABIAT, continuously tracks the approximate location of an asset using spatial anchors and, when needed, the precise location of that asset using fiducial markers. We have applied our SABIAT technique to build a demonstrator system, AR-IPS, to show how assets can be tracked and located inside of a large, multi-level building.},
  keywords={Industries;Three-dimensional displays;Warehousing;Buildings;User interfaces;Fiducial markers;Transponders;Human-centered computing-Mixed / augmented reality;Information systems-Location based services},
  doi={10.1109/VR50410.2021.00047},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417696,
  author={Horie, Arata and Saraiji, MHD Yamen and Kashino, Zendai and Inami, Masahiko},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={EncounteredLimbs: A Room-scale Encountered-type Haptic Presentation using Wearable Robotic Arms}, 
  year={2021},
  volume={},
  number={},
  pages={260-269},
  abstract={Haptic information significantly improves human awareness of objects in virtual reality. One way of presenting this information is via encountered-type haptic feedback. An advantage of encounter type feedback is that it enables physical interaction with virtual environments without the need for specialized haptic devices on the hand. Additionally, encountered-type haptics are known for being able to provide high quality contact feedback to the user. However, such systems are typically designed to be grounded (i.e., fixed to the floor). As such, they typically have bounded workspace and a limited range of possible applications. In this work, we present a novel, wearable approach to presenting a user with encountered-type haptic feedback. We realize this feedback using a wearable robotic limb that holds a plate where the user might interact with their environment. An appropriate location for the plate is determined by a novel haptic solver while control of the arm is made possible using motion trackers. The system was designed to be stable, for presenting consistent haptic feedback, while also being safe and lightweight for wearability. By making the feedback system wearable, we enable the presentation of stiff feedback while maintaining the spatial freedom and unbounded workspace of natural hand interaction. Herein, we present the design of the novel system, mechanical and safety considerations when designing a wearable encountered-type system, and an evaluation of the system. A technical evaluation of the implemented system showed that the system provides a stiffness over 25 N/m and slant angle errors under 3°. Three user studies show the limitations of haptic slant perception in humans and the quantitative and qualitative effectiveness of the current prototype system. We conclude the paper by discussing various potentialapplications and possible improvements that could be made to the system.},
  keywords={Visualization;Three-dimensional displays;Tracking;Virtual environments;Tactile sensors;User interfaces;End effectors;Human-centered computing;Human computer interaction(HCI);Interaction devices;Haptic devices},
  doi={10.1109/VR50410.2021.00048},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417801,
  author={Liu, Jingjing and Liane, Wei and Ning, Bing and Mao, Ting},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Work Surface Arrangement Optimization Driven by Human Activity}, 
  year={2021},
  volume={},
  number={},
  pages={270-278},
  abstract={In this paper, we aim at guiding people to accomplish a personalized task, work surface organizing, in mixed reality environment, which can also be applied to intelligent robots. Through the cameras mounted in a MR device, e.g., Hololens, we firstly capture a person's daily activities in real scene when he uses the work surface. From such activities, we model the individual behavior habits and apply them to optimize the arrangement of the work surface. A cost function is defined for the optimization, considering general arrangement rules and human habitual behavior. The optimized arrangement is suggested to the user by augmenting the virtual arrangement on the real scene. To evaluate the effectiveness of our approach, we conducted experiments on a variety of scenes.},
  keywords={Solid modeling;Three-dimensional displays;Robot vision systems;Mixed reality;Virtual reality;User interfaces;Cost function;Human-centered Design;Mixed Reality;Work Surface Design;Remodeling},
  doi={10.1109/VR50410.2021.00049},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417703,
  author={Tsai, Shou-En and Tsai, Wan-Lun and Pan, Tse-Yu and Kuo, Chia-Ming and Hu, Min-Chun},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Does Virtual Odor Representation Influence the Perception of Olfactory Intensity and Directionality in VR?}, 
  year={2021},
  volume={},
  number={},
  pages={279-285},
  abstract={Introducing olfactory display in the virtual reality (VR) system brings the immersive experience to new heights. However, it is intractable to simulate olfactory features (such as the intensity and the direction) with multiple levels. Visual stimuli have been proved to dominate human perception among multiple sensors in virtual environments. If visual stimuli can be used to guide the olfactory sense in VR, the design of the olfactory display can be simpler but still able to provide olfactory experience with more diversity. To understand the visual-olfactory effect on different olfactory characteristics, a portable olfactory display that can control the intensity and direction of odors was developed. An experimental study was conducted to investigate cross-modal human perception, i.e. how the visually virtual odor representation in VR influences human perception of real odor produced by the proposed olfactory display. The results showed that the perception of odor intensity and directionality can be modulated by visually virtual odor representation.},
  keywords={Visualization;Three-dimensional displays;Olfactory;Virtual environments;Prototypes;Immersive experience;User interfaces;virtual reality;olfactory display: Human-centered computing-Virtual reality-},
  doi={10.1109/VR50410.2021.00050},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417680,
  author={Handali, Joshua Peter and Schneider, Johannes and Gau, Michael and Holzwarth, Valentin and Brocke, Jan vom},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visual Complexity and Scene Recognition: How Low Can You Go?}, 
  year={2021},
  volume={},
  number={},
  pages={286-295},
  abstract={Visual realism in Virtual Reality (VR) increases both immersion and development costs. Consequently, it is important to understand the cost-benefit trade-off of specific aspects of visual realism. Determining the extent of visual realism often leads to decisions on the level of visual complexity of a Virtual Environment (VE). In this paper, we investigate the impact of visual complexity on a user's spatial orientation through a user study. To do so, we created a VE containing parts of a real-world place. Participants were asked to map their location within the VE to the corresponding real-world location. They were provided by a pop-up map of the entire VE, on which they were required to choose one named location out of a predefined set of locations. We manipulated the VE's visual complexity by varying the visual elements used to provide cartographic information, namely a map overlay and 3D blocks as buildings. This results in four scene types: i) landscape contour, ii) landscape contour with 3D buildings, iii) landscape contour overlaid with a map, and iv) landscape contour with both 3D buildings and the map overlay. Each participant performed our location recall task for each scene type. Our findings provide empirical evidence that addition of each of these two visual elements individually improved spatial orientation, while their combination only adds a slight improvement.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Scalability;Buildings;Urban planning;Virtual environments;Human-centered computing-Visualization-Empirical studies in visualization;Human-centered computing-Visualization-Visualization design and evaluation methods},
  doi={10.1109/VR50410.2021.00051},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417662,
  author={Schott, Danny and Saalfeld, Patrick and Schmidt, Gerd and Joeres, Fabian and Boedecker, Christian and Huettl, Florentine and Lang, Hauke and Huber, Tobias and Preim, Bernhard and Hansen, Christian},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={A VR/AR Environment for Multi-User Liver Anatomy Education}, 
  year={2021},
  volume={},
  number={},
  pages={296-305},
  abstract={We present a Virtual and Augmented Reality multi-user prototype of a learning environment for liver anatomy education. Our system supports various training scenarios ranging from small learning groups to classroom-size education, where students and teachers can participate in virtual reality, augmented reality, or via desktop PCs. In an iterative development process with surgeons and teachers, a virtual organ library was created. Nineteen liver data sets were used comprising 3D surface models, 2D image data, pathology information, diagnosis and treatment decisions. These data sets can interactively be sorted and investigated individually regarding their volumetric and meta information. The three participation modes were evaluated within a user study with surgery lecturers (5) and medical students (5). We assessed the usability and presence using questionnaires. Additionally, we collected qualitative data with semistructured interviews. A total of 435 individual statements were recorded and summarized to 49 statements. The results show that our prototype is usable, induces presence, and potentially support the teaching of liver anatomy and surgery in the future.},
  keywords={Training;Three-dimensional displays;Liver;Surgery;Prototypes;User interfaces;Systems support;Human-centered computing-Human computer interaction (HCI);Human-centered computing-Interactive systems and tools;Human-centered computing-Collaborative and social computing systems and tools;Applied computing-Life and medical sciences;Applied computing-Interactive learning environments},
  doi={10.1109/VR50410.2021.00052},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417660,
  author={Romat, Hugo and Fender, Andreas and Meier, Manuel and Holz, Christian},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Flashpen: A High-Fidelity and High-Precision Multi-Surface Pen for Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={306-315},
  abstract={Digital pen interaction has become a first-class input modality for precision tasks such as writing, annotating, drawing, and 2D manipulation. The key enablers of digital inking are the capacitive or resistive sensors that are integrated in contemporary tablet devices. In Virtual Reality (VR), however, users typically provide input across large regions, hence limiting the suitability of using additional tablet devices for accurate pen input. In this paper, we present Flashpen, a digital pen for VR whose sensing principle affords accurately digitizing hand writing and intricate drawing, including small and quick turns. Flashpen re-purposes an inexpensive gaming mouse sensor that digitizes extremely fine grained motions in the micrometer range at over 8 kHz when moving on a surface. We combine Flashpen's high-fidelity relative input with the absolute tracking cues from a VR headset to enable pen interaction across a variety of VR applications. In our two-block evaluation, which consists of a tracing task and a writing task, we compare Flashpen to a professional drawing tablet (Wacom). With this, we demonstrate that Flashpen's fidelity matches the performance of state-of-the-art digitizers and approaches the fidelity of analog pens, while adding the flexibility of supporting a wide range of flat surfaces.},
  keywords={Surface reconstruction;Three-dimensional displays;Tracking;Input devices;Virtual reality;Writing;Tools;Human-centered computing;Human computer interaction (HCI);Interaction devices;Graphics input devices},
  doi={10.1109/VR50410.2021.00053},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417784,
  author={Benvegnù, Giulia and Pluchino, Patrik and Garnberini, Luciano},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Morality: Using Virtual Reality to Study Moral Behavior in Extreme Accident Situations}, 
  year={2021},
  volume={},
  number={},
  pages={316-325},
  abstract={Virtual Reality (VR) technologies are widely employed to investigate human behavior in dangerous situations that cannot be safely reproduced in the real world, allowing researchers to study in an ecological way complex scenarios such as training for risky jobs, safety procedures, emergencies and, more recently, moral dilemmas in driving context. Understanding how people act when facing severe accidents involving unavoidable collisions has extremely important implications for the design and development of the “decisional system” of Autonomous Vehicles (AV s). However, previous studies have not focused on the differences between being the driver acting in a complex moral situation or being in a self-driving car that chooses for you. In the present paper, we described a case study that uses a first-person virtual reality simulation to investigate people's emotional reactions, perceived sense of responsibility, and acceptability of moral behavior in human and autonomous driving modalities. The main findings showed that participants experienced a high sense of presence in our simulation and react differently to the two driving conditions, showing a greater arousal, a more negative valence, and an increased sense of responsibility when faced moral dilemmas as drivers. Instead, in scenarios that did not involve killing someone (non-moral dilemmas), being in a fully autonomous vehicle was judged less pleasant than being the actual driver. These results suggest that people prefer to be in control only in common driving situations and not when their actions have deadly consequences on other people, suggesting the need to consider emotional factors in studying decision-making applied to autonomous vehicles, as a mean to reach a more complete understanding of people's reactions to this new technology, and to possibly gain insights for the design of autonomous driving systems and, more generally, AI-driven machines.},
  keywords={Training;Ethics;Solid modeling;Three-dimensional displays;Biological system modeling;Virtual reality;Safety;Virtual reality;Moral dilemma;Autonomous vehicles;Emotion;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI) -HCI design and evaluation methods-Laboratory experiments},
  doi={10.1109/VR50410.2021.00054},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417780,
  author={Wirth, Markus and Gradl, Stefan and Prosinger, Georg and Kluge, Felix and Roth, Daniel and Eskofier, Bjoern M.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Avatar Appearance, Perspective and Context on Gait Variability and User Experience in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={326-335},
  abstract={Gait supervision plays an important role in the diagnosis, analysis and rehabilitation of motor impairments and neurodegenerative disorders. For example, in Parkinson's disease, gait assessment is used for progression observation and medication guidance. Previous work has presented the potential of virtual reality (VR) supported gait applications. While virtual environments and user representation strategies are used for gait applications, the influence of appearance and context cues on gait performance is not extensively researched. In this paper, we analyzed the influence of avatar appearance, environment awareness, and camera perspective on gait parameters relevant for clinical application. Four different avatar appearances, varying in abstraction, two environmental settings, as well as an egocentric and exocentric camera perspective were compared in three walking tasks on a treadmill. Our results show that variability, as an indicator for gait stability, is significantly impacted by VR exposure in comparison to a real world (in vivo) baseline. Further, our results revealed that walking tasks influence gait behavior significantly different in VR compared to in vivo. Overall, these findings suggest that particular care has to be taken when assessing gait characteristics acquired from subjects immersed in VR and that equivalence of results with in vivo may not be blindly assumed.},
  keywords={Legged locomotion;In vivo;Three-dimensional displays;Avatars;Virtual environments;User interfaces;Cameras;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
  doi={10.1109/VR50410.2021.00055},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417656,
  author={Vaziri, Koorosh and Bondy, Maria and Bui, Amanda and Interrante, Victoria},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Egocentric Distance Judgments in Full-Cue Video-See-Through VR Conditions are No Better than Distance Judgments to Targets in a Void}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  abstract={Understanding the extent to which, and conditions under which, scene detail affects spatial perception accuracy can inform the responsible use of sketch-like rendering styles in applications such as immersive architectural design walkthroughs using 3D concept drawings. This paper reports the results of an experiment that provides important new insight into this question using a custom-built, portable video-see-through (VST) conversion of an optical-see-through head-mounted display (HMD). Participants made egocentric distance judgments by blind walking to the perceived location of a real physical target in a real-world outdoor environment under three different conditions of HMD-mediated scene detail reduction: full detail (raw camera view), partial detail (Sobel-filtered camera view), and no detail (complete background subtraction), and in a control condition of unmediated real world viewing through the same HMD. Despite the significant differences in participants' ratings of visual and experiential realism between the three different video-see-through rendering conditions, we found no significant difference in the distances walked between these conditions. Consistent with prior findings, participants underestimated distances to a significantly greater extent in each of the three VST conditions than in the real world condition. The lack of any clear penalty to task performance accuracy not only from the removal of scene detail, but also from the removal of all contextual cues to the target location, suggests that participants may be relying nearly exclusively on context - independent information such as angular declination when performing the blind-walking task. This observation highlights the limitations in using blind walking to the perceived location of a target on the ground to make inferences about people's understanding of the 3D space of the virtual environment surrounding the target. For applications like immersive architectural design, where we seek to verify the equivalence of the 3D spatial understanding derived from virtual immersion and real world experience, additional measures of spatial understanding should be considered.},
  keywords={Legged locomotion;Visualization;Three-dimensional displays;Virtual environments;Resists;User interfaces;Rendering (computer graphics);Virtual reality;spatial perception;non-photorealistic rendering},
  doi={10.1109/VR50410.2021.00056},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417634,
  author={Englmeier, David and Sajko, Wanja and Butz, Andreas},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Spherical World in Miniature: Exploring the Tiny Planets Metaphor for Discrete Locomotion in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={345-352},
  abstract={We explore the concept of a Spherical World in Miniature (SWIM) for discrete locomotion in Virtual Reality (VR). A SWIM wraps a planar WIM around a physically embodied sphere and thereby implements the metaphor of a tangible Tiny Planet that can be rotated and moved, enabling scrolling, scaling, and avatar teleportation. The scaling factor is set according to the sphere's distance from the head-mounted display (HMD), while rotation moves the current viewing window. Teleportation is triggered with a dwell time when looking at the sphere and keeping it still. In a lab study (N=20), we compare our SWIM implementation to a planar WIM with an established VR controller technique using physical buttons. We test both concepts in a navigation task and also investigate the effects of two different screen sizes. Our results show that the SWIM, despite its less direct geometrical transformation, performed superior in most evaluations. It outperformed the planar WIM not only in terms of task completion time (TCT) and accuracy but also in subjective ratings.},
  keywords={Three-dimensional displays;Head-mounted displays;Planets;Navigation;Avatars;Resists;Teleportation;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Haptic devices;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR50410.2021.00057},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417643,
  author={Sun, Xuetong and Zhang, Yang and Huang, Po-Chun and Acharjee, Niloy and Dagenais, Mario and Peckerar, Martin and Varshney, Amitabh},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Proximity Effect Correction for Fresnel Holograms on Nanophotonic Phased Arrays}, 
  year={2021},
  volume={},
  number={},
  pages={353-362},
  abstract={Holographic displays and computer-generated holography offer a unique opportunity in improving optical resolutions and depth characteristics of near-eye displays. The thermally-modulated Nanopho-tonic Phased Array (NPA), a new type of holographic display, affords several advantages, including integrated light source and higher refresh rates, over other holographic display technologies. However, the thermal phase modulation of the NPA makes it susceptible to the thermal proximity effect where heating one pixel affects the temperature of nearby pixels. Proximity effect correction (PEC) methods have been proposed for 2D Fourier holograms in the far field but not for Fresnel holograms at user-specified depths. Here we extend an existing PEC method for the NPA to Fresnel holograms with phase-only hologram optimization and validate it through computational simulations. Our method is not only effective in correcting the proximity effect for the Fresnel holograms of 2D images at desired depths but can also leverage the fast refresh rate of the NPA todisplay 3D scenes with time-division multiplexing.},
  keywords={Phased arrays;Multiplexing;Solid modeling;Three-dimensional displays;Phase modulation;Proximity effects;Holography;Nanophotonic phased array;proximity effect correction;proximal algorithms;phase-only hologram;Fresnel hologram;Computing methodologies;Image processing;Computing methodologies-Mixed / augmented reality Computing methodologies;Virtual reality;Hardware-Displays and imagers},
  doi={10.1109/VR50410.2021.00058},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417797,
  author={Onishi, Yuki and Takashima, Kazuki and Fujita, Kazuyuki and Kitamura, Yoshifumi},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={BouncyScreen: Physical Enhancement of Pseudo-Force Feedback}, 
  year={2021},
  volume={},
  number={},
  pages={363-372},
  abstract={We explore BouncyScreen, an actuated 1D display system that enriches indirect interaction with a virtual object by pseudo-haptic feedback mechanics enhanced through the screen's physical movements. We configured a proof-of-concept prototype of BouncyScreen with a flat-screen mounted on a mobile robot. When the user manipulates a virtual object using virtual reality (VR) controllers, the screen moves in accordance with the virtual object. We conducted psychophysical studies to examine how BouncyScreen's physical movements would affect users' pseudo-haptic perceptions and interaction experiences. Our preliminary study using a weight discrimination task for object pushing interaction showed that BouncyScreen offers identical pseudo-force feedback to the vision-based pseudo-haptic technique. We conducted a follow-up psychophysical study using a weight magnitude estimation task for object pushing and bumping interactions. The results reveal that a users' perceived weight magnitude is enhanced by the screen's physical motion under different characteristics depending on interaction styles (i.e., pushing and bumping). Our study also confirmed that the screen's synchronous physical motions significantly enhance the reality of the interaction and the sense of presence. We close this paper with some applications and use suggestions for BouncyScreen in future HMD-free flat-screen 3D user interface systems.},
  keywords={Three-dimensional displays;Force feedback;Display systems;Force;Prototypes;Estimation;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Haptic devices},
  doi={10.1109/VR50410.2021.00059},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417785,
  author={Wang, Yuyang and Chardonnet, Jean-Rémy and Merienne, Frédéric and Ovtcharova, Jivka},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Fuzzy Logic to Involve Individual Differences for Predicting Cybersickness during VR Navigation}, 
  year={2021},
  volume={},
  number={},
  pages={373-381},
  abstract={Many studies have explored how individual differences can affect users' susceptibility to cybersickness in a VR application. However, the lack of strategy to integrate the influence of each factor on cybersickness makes it difficult to utilize the results of existing research. Based on the fuzzy logic theory that can represent the effect of different factors as a single value containing integrated information, we developed two approaches including the knowledge-based Mamdani-type fuzzy inference system and the data-driven Adaptive neuro-fuzzy inference system (ANFIS) to involve three individual differences (Age, Gaming experience and Ethnicity). We correlated the corresponding outputs with the simulator sickness questionnaire (SSQ) scores in a simple navigation scenario. The correlation coefficients obtained through a 4- fold cross validation were found statistically significant with both fuzzy logic approaches, indicating their effectiveness to influence the occurrence and the level of cybersickness. Our work provides insights to establish customized experiences for VR navigation by involving individual differences.},
  keywords={Fuzzy logic;Correlation coefficient;Visualization;Solid modeling;Three-dimensional displays;Cybersickness;Navigation;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Artificial intelligence-Knowledge representation and reasoning-Vagueness and fuzzy logic;Information systems-Information retrieval-Users and interactive retrieval-Personalization},
  doi={10.1109/VR50410.2021.00060},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417689,
  author={Hamzeheinejad, Negin and Roth, Daniel and Monty, Samantha and Breuer, Julian and Rodenberg, Anuschka and Latoschik, Marc Erich},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Implicit and Explicit Feedback on Performance and Experience during VR-Supported Motor Rehabilitation}, 
  year={2021},
  volume={},
  number={},
  pages={382-391},
  abstract={This paper examines the impact of implicit and explicit feedback in Virtual Reality (VR) on performance and user experience during motor rehabilitation. In this work, explicit feedback consists of visual and auditory cues provided by a virtual trainer, compared to traditional feedback provided by a real physiotherapist. Implicit feedback was generated by the walking motion of the virtual trainer accompanying the patient during virtual walks. Here, the potential synchrony of movements between the trainer and trainee is intended to create an implicit visual affordance of motion adaption. We hypothesize that this will stimulate the activation of mirror neurons, thus fostering neuroadaptive processes. We conducted a clinical user study in a rehabilitation center employing a gait robot. We investigated the performance outcome and subjective experience of four resulting VR-supported rehabilitation conditions: with/without explicit feedback, and with/without implicit (synchronous motion) stimulation by a virtual trainer. We further included two baseline conditions reflecting the current NonVR procedure in the rehabilitation center. Our results show that additional feedback generally resulted in better patient performance, objectively assessed by the necessary applied support force of the robot. Additionally, our VR-supported rehabilitation procedure improved enjoyment and satisfaction, while no negative impacts could be observed. Implicit feedback and adapted motion synchrony by the virtual trainer led to higher mental demand, giving rise to hopes of increased neural activity and neuroadaptive stimulation.},
  keywords={Legged locomotion;Visualization;Three-dimensional displays;Neurons;Virtual reality;User interfaces;Synchronous motors;Human-centered computing-Visualization-Virtual reality},
  doi={10.1109/VR50410.2021.00061},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417638,
  author={Yu, Kevin and Winkler, Alexander and Pankratz, Frieder and Lazarovici, Marc and Wilhelm, Dirk and Eck, Ulrich and Roth, Daniel and Navab, Nassir},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Magnoramas: Magnifying Dioramas for Precise Annotations in Asymmetric 3D Teleconsultation}, 
  year={2021},
  volume={},
  number={},
  pages={392-401},
  abstract={When users create hand-drawn annotations in Virtual Reality they often reach their physical limits in terms of precision, especially if the region to be annotated is small. One intuitive solution employs magnification beyond natural scale. However, scaling the whole environment results in wrong assumptions about the coherence between physical and virtual space. In this paper, we introduce Mag-noramas, a novel interaction method for selecting and extracting a region of interest that the user can subsequently scale and transform inside the virtual space. Our technique enhances the user's capabilities to perform supernaturally precise virtual annotations on virtual objects. We explored our technique in a user study within asimplified clinical scenario of a teleconsultation-supported craniectomy procedure that requires accurate annotations on a human head. Teleconsultation was performed asymmetrically between a remote expert in Virtual Reality that collaborated with a local user through Augmented Reality. The remote expert operates inside a reconstructed environment, captured from RGB-D sensors at the local site, and is embodied by an avatar to establish co-presence. The results show that Magnoramas significantly improve the precision of annotations while preserving usability and perceived presence measures compared to the baseline method. By hiding the 3D reconstruction while keeping the Magnorama, users can intentionally choose to lower their perceived social presence and focus on their tasks.},
  keywords={Manifolds;Three-dimensional displays;Telepresence;Annotations;Transforms;User interfaces;Real-time systems;Interaction Techniques;Medical Information System;Virtual Reality},
  doi={10.1109/VR50410.2021.00062},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417778,
  author={Kammerlander, Robin K. and Pereira, André and Alexanderson, Simon},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Virtual Reality to Support Acting in Motion Capture with Differently Scaled Characters}, 
  year={2021},
  volume={},
  number={},
  pages={402-410},
  abstract={Motion capture is a well-established technology for capturing actors' movements and performances within the entertainment industry. Many actors, however, witness the poor acting conditions associated with such recordings. Instead of detailed sets, costumes and props, they are forced to play in empty spaces wearing tight suits. Often, their co-actors will be imaginary, replaced by placeholder props, or they would be out of scale with their virtual counterparts. These problems do not only affect acting, they also cause an abundance of laborious post-processing clean-up work. To solve these challenges, we propose using a combination of virtual reality and motion capture technology to bring differently proportioned virtual characters into a shared collaborative virtual environment. A within-subjects user study with trained actors showed that our proposed platform enhances their feelings of body ownership and immersion. This in turn changed actors' performances which narrowed the gap between virtual performances and final intended animations.},
  keywords={Three-dimensional displays;Interactive systems;Collaboration;Virtual environments;Production;User interfaces;Tools;Collaborative virtual production;acting;motion capture;body ownership;presence},
  doi={10.1109/VR50410.2021.00063},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417687,
  author={Park, Hyerim and Woo, Woontack},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Video Content Representation to Support the Hyper-reality Experience in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={411-419},
  abstract={Most research on providing location-based content in 3D interactive virtual reality has been limited to social media content. Few studies have suggested how to represent the video clip of movies or TV shows in virtual reality. This paper investigates a video content representation method to provide a hyper-reality experience of the narrative world in virtual reality. We reflect the time and place settings of the video content in virtual reality and have participants watch the video in four different virtual reality environments. We reveal that reflecting the story's environment settings to the virtual reality environment significantly improves the spatial presence and narratives engagement. We also confirm a positive correlation between spatial presence and narrative engagement, including subscales such as emotional engagement and narrative presence. Based on the study results, we discuss how to provide the hyper-reality experience in content-adaptive virtual reality.},
  keywords={Three-dimensional displays;Correlation;TV;Social networking (online);Virtual reality;User interfaces;Motion pictures;Cinematic virtual reality;hyper-reality;video representation;narrative engagement;spatial presence: Human-centered computing-Human Computer Interaction (HCI)-Empirical studies in HCI-;Human-centered computing-Interaction design-Empirical studies in interaction design},
  doi={10.1109/VR50410.2021.00064},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417686,
  author={Vailland, Guillaume and Devigne, Louise and Pasteau, François and Nouviale, Florian and Fraudet, Bastien and Leblong, Émilie and Babel, Marie and Gouranton, Valérie},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR based Power Wheelchair Simulator: Usability Evaluation through a Clinically Validated Task with Regular Users}, 
  year={2021},
  volume={},
  number={},
  pages={420-427},
  abstract={Power wheelchairs are one of the main solutions for people with reduced mobility to maintain or regain autonomy and a comfortable and fulfilling life. However, driving a power wheelchair in a safe way is a difficult task that often requires training methods based on real-life situations. Although these methods are widely used in occupational therapy, they are often too complex to implement and unsuitable for some people with major difficulties. In this context, we collaborated with clinicians to develop a Virtual Reality based power wheelchair simulator. This simulator is an innovative training tool adapted to any type of situations and impairments. In this paper, we present a clinical study in which 29 power wheelchair regular users were asked to complete a clinically validated task designed by clinicians within two conditions: driving in a virtual environment with our simulator and driving in real conditions with a real power wheelchair. The objective of this study is to compare performances between the two conditions and to evaluate the Quality of Experience provided by our simulator in terms of Sense of Presence and Cybersickness. Results show that participants complete the tasks in a similar amount of time for both real and virtual conditions, using respectively a real power wheelchair and our simulator. Results also show that our simulator provides a high level of Sense of Presence and provokes only slight to moderate Cybersickness discomforts resulting in a valuable Quality of Experience.},
  keywords={Training;Cybersickness;Wheelchairs;Urban planning;Virtual environments;User interfaces;Tools},
  doi={10.1109/VR50410.2021.00065},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417645,
  author={Gold, Lauren and Bahremand, Alireza and Richards, Connor and Hertzberg, Justin and Sese, Kyle and Gonzalez, Alexander and Purcell, Zoe and Powell, Kathryn and LiKamWa, Robert},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Visualizing Planetary Spectroscopy through Immersive On-site Rendering}, 
  year={2021},
  volume={},
  number={},
  pages={428-437},
  abstract={Remote sensing is currently the primary method of obtaining knowledge about the composition and physical properties of the surface of other planets. In a commonly used technique, visible and near-infrared (VNIR) spectrometers onboard orbiting satellites capture reflectance data at different wavelengths, which in turn gives insight about the minerals present and the overall composition of the terrain. In select locations on Mars, rovers have also conducted up close in-situ investigation of the same terrains examined by orbiters, allowing direct comparisons at different spatial scales. In this work, we build Planetary Visor, a virtual reality tool to visualize orbital and ground data around NASA's Mars Science Laboratory Curiosity rover's ongoing traverse in Gale Crater. We have built a 3D terrain along Curiosity's traverse using rover images, and within it we visualize satellite data as polyhedrons, superimposed on that terrain. This system provides perspectives of VNIR spectroscopic data from a satellite aligned with ground images from the rover, allowing the user to explore both the physical aspects of the terrain and their relation to the mineral composition. The result is a system that provides seamless rendering of datasets at vastly different scales. We conduct a user study with subject matter experts to evaluate the success and potential of our tool. The results indicate that Visor assists with geometric understanding of spectral data, improved geological context, a better sense of scale while navigating terrain, and new insights into spectral data. The result is not only an immersive environment in a scientifically interesting area on Mars, but a robust tool for analysis and visualization of data that can yield improved scientific discovery. This technology is relevant to the ongoing operations of the Curiosity rover and will directly be able to represent the data collected in the upcoming Mars 2020 Perseverance rover mission.},
  keywords={Space vehicles;Mars;Satellites;Three-dimensional displays;Data visualization;Virtual reality;Tools;Spectroscopy;Remote Sensing;Vitrual reality},
  doi={10.1109/VR50410.2021.00066},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417667,
  author={Ke, Pinachuan and Zhu, Kenina},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Larger Step Faster Speed: Investigating Gesture-Amplitude-based Locomotion in Place with Different Virtual Walking Speed in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={438-447},
  abstract={In this paper, we present a series of user studies to investigate the technique of gesture-amplitude-based walking-speed control for locomotion in place (LIP) in virtual reality (VR). Our 1st study suggested that compared to tapping and goose-stepping, the gesture of marching in place was significantly preferred by users across three different virtual walking speed (i.e., 1 ×, 3 ×, and 10 ×) while sitting and standing, and it yielded larger motion difference across the three speed levels. With the tracker data recorded in the 1st study, we trained a Support- Vector-Machine classification model for LIP speed control based on users' leg/foot gestures in marching in place. The overall accuracy for classifying three speed levels was above 90% for sitting and standing. With the classification model, we then compared the marching-in-place speed-control technique with the controller-based teleportation approach on a target-reaching task where users were sitting and standing. We found no significant difference between the two conditions in terms of target-reaching accuracy. More importantly, the technique of marching in place yielded significantly higher user ratings in terms of naturalness, realness, and engagement than the controller-based teleportation did.},
  keywords={Legged locomotion;Solid modeling;Three-dimensional displays;Tracking;Lips;Velocity control;Virtual reality;H.5 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems;Artificial;augmented;virtual realities},
  doi={10.1109/VR50410.2021.00067},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417724,
  author={Cho, Yong-Hun and Min, Dae-Hong and Huh, Jin-Suk and Lee, Se-Hee and Yoon, June-Seop and Lee, In-Kwon},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Walking Outside the Box: Estimation of Detection Thresholds for Non-Forward Steps}, 
  year={2021},
  volume={},
  number={},
  pages={448-454},
  abstract={Most virtual reality (VR) experiences are held in limited physical space; therefore, increasing the physical space's spatial efficiency is an essential task for the VR industry. Redirected walking maps a virtual path and a real path with unnoticeable distortion, enabling users to walk through a much bigger virtual space than physical space. To hide the distortion from the user, detection thresholds have been measured, entirely focusing on forward steps. However, it is not uncommon for the user to walk non-forward, that is, sideward and backward in VR. In addition to a forward step, adding options for a non-forward step can expand the VR locomotion in any direction. In this work, we measure the translation and curvature detection thresholds for non-forward steps. The results show similar translation detection thresholds with forward-step and wider detection thresholds for the curvature gain in both backward and sideward step experiments. Having sideward and backward steps in the redirected walking arsenal can add freedom to virtual world design and lead to efficient space usage.},
  keywords={Legged locomotion;Industries;Three-dimensional displays;Focusing;Estimation;Virtual reality;User interfaces;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR50410.2021.00068},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417640,
  author={Lilija, Klemen and Kyllingsbæk, Søren and Hornbæk, Kasper},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Correction of Avatar Hand Movements Supports Learning of a Motor Skill}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Learning to move the hands in particular ways is essential in many training andleisure virtual reality applications, yet challenging. Existing techniques that support learning of motor movement in virtual reality rely on external cues such as arrows showing where to move or transparent hands showing the target movement. We propose a technique where the avatar's hand movement is corrected to be closer to the target movement. This embeds guidance in the user's avatar, instead of in external cues and minimizes visual distraction. Through two experiments, we found that such movement guidance improves the short-term retention of the target movement when compared to a control condition without guidance.},
  keywords={Training;Visualization;Three-dimensional displays;Avatars;User interfaces;Human-centered computing—Empirical studies in HCI;Human-centered computing—Virtual reality},
  doi={10.1109/VR50410.2021.00069},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417808,
  author={Palmas, Fabrizio and Reinelt, Ramona and Cichor, Jakub E. and Plecher, David A. and Klinker, Gudrun},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Reality Public Speaking Training: Experimental Evaluation of Direct Feedback Technology Acceptance}, 
  year={2021},
  volume={},
  number={},
  pages={463-472},
  abstract={Virtual Reality (VR) offers significant potential for public speaking training. Virtual Reality Speech Training (VR-ST) helps trainees develop presentation skills and practice their application in the real world. Additionally, participants with public speaking anxiety can improve their presentation skills in a safe virtual environment without fear of judgment. Another benefit is direct feedback based on gamification principles, which provides users with information about their performance during training and allows for the adjustment of behavior in real-time. However, it is not yet clear if direct feedback based on visualization through icons is accepted by participants, such that it may support learning transfer in VR training applications. As a result, we set out to investigate how direct feedback in a VR -ST affects the participants' technology acceptance based on the Technology Acceptance Model (TAM). We conducted a between-subjects experimental study in order to compare a VR-ST with direct feedback (n = 100) with a simulation-based VR-ST (n = 100). The resulting MANOVAs demonstrated a preference for the direct feedback version for all TAM determinations, showing that direct feedback offers benefits to trainees by improving technology acceptance, independent of location and without supervision by trainers. Further results show that VR-ST is generally more accepted by participants without public speaking anxiety. Our findings indicate that developers of VR public speaking applications should focus on the inclusion of meaningful direct feedback and consider individual differences between users in order to optimally implement training measures.},
  keywords={Training;Visualization;Three-dimensional displays;Technology acceptance model;Atmospheric measurements;Virtual environments;Tools;Virtual Reality;Public Speaking;Gamification;Virtual Training;Direct Feedback;Experimental Study},
  doi={10.1109/VR50410.2021.00070},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417668,
  author={Cmentowski, Sebastian and Krekhov, Andrey and Zenner, André and Kucharski, Daniel and Krüger, Jens},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Towards Sneaking as a Playful Input Modality for Virtual Environments}, 
  year={2021},
  volume={},
  number={},
  pages={473-482},
  abstract={Using virtual reality setups, users can fade out of their surroundings and dive fully into a thrilling and appealing virtual environment. The success of such immersive experiences depends heavily on natural and engaging interactions with the virtual world. As developers tend to focus on intuitive hand controls, other aspects of the broad range of full-body capabilities are easily left vacant. One repeatedly overlooked input modality is the user's gait. Even though users may walk physically to explore the environment, it usually does not matter how they move. However, gait-based interactions, using the variety of information contained in human gait, could offer interesting benefits for immersive experiences. For instance, stealth VR-games could profit from this additional range of interaction fidelity in the form of a sneaking-based input modality. In our work, we explore the potential of sneaking as a playful input modality for virtual environments. Therefore, we discuss possible sneaking-based gameplay mechanisms and develop three technical approaches, including precise foot-tracking and two abstraction levels. Our evaluation reveals the potential of sneaking-based inter-actions in IVEs, offering unique challenges and thrilling gameplay. For these interactions, precise tracking of individual footsteps is unnecessary, as a more abstract approach focusing on the players' intention offers the same experience while providing better comprehensible feedback. Based on these findings, we discuss the broader potential and individual strengths of our gait-centered interactions.},
  keywords={Three-dimensional displays;Virtual environments;Focusing;Games;User interfaces;Human-centered computing-Virtual reality;Software and its engineering-Interactive games},
  doi={10.1109/VR50410.2021.00071},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417770,
  author={Paik, Seungwon and Jeon, Youngseung and Shih, Patrick C. and Han, Kyungsik},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={I Feel More Engaged When I Move!: Deep Learning-based Backward Movement Detection and its Application}, 
  year={2021},
  volume={},
  number={},
  pages={483-492},
  abstract={Movement is one of the key elements in virtual reality (VR) and significantly influences user experience. In particular, walking-in-place is a method of supporting movement in a limited space, and many studies are being conducted on its effective support. However, most studies have focused on forward movement despite many situations in which backward movement is needed. In this paper, we present the development of a prediction model for forward/backward movement while considering a user's orientation and the verification of the model's effectiveness. We built a deep learning-based model by collecting sensor data on the movement of the user's head, waist, and feet. We developed three realistic VR scenarios that involve backward movement, set three conditions (controller-based, treadmill-based, and model-based) for movement, and evaluated user experience in each condition through a study of 36 participants. As a result, the model-based condition showed the highest sensory sensitivity, effectiveness, and satisfaction and similar cognitive burden compared with the other two conditions. The results of our study demonstrated that movement support through modeling is possible, suggesting its potential for use in many VR applications.},
  keywords={Solid modeling;Three-dimensional displays;Sensitivity;Navigation;Virtual reality;Predictive models;User interfaces;Human-centered computing-Human computer interaction (HCI)-;Human-centered computing-Virtual reality-;Computing methodologies-Machine learning approaches-},
  doi={10.1109/VR50410.2021.00072},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417789,
  author={Gao, BoYu and Mai, Zijun and Tu, Huawei and Duh, Henry Been-Lirn},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Body-centric Locomotion with Different Transfer Functions in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={493-500},
  abstract={Body-centric locomotion allows users to navigate virtual environments with body parts (e.g. head tilt, arm swing or torso lean). Transfer functions are an important determinant of the locus of such a locomotion method. However, there is little known about the effects of transfer functions on virtual locomotion with different body parts. In this work, we selected four typical transfer functions (linear function: L, power function: P, a piecewise function with constant and linear functions: CL, and a piecewise function with constant and power functions: CP) and four body parts (head, arm, torso, and knee) from existing works, and conducted an experiment to evaluate their effects on virtual locomotion under three distances (5, 10, and 15 m) in Virtual Reality (VR). Results show that (1) CP function generally led to the longest task time with a low rate of failed trials, while CL function had the shortest task time with a high rate of failed trials; (2) body parts significantly affected the rate of failed trials, but not task time and final position offset. Head and torso resulted in the lowest and highest rate of failed trials respectively; (3) body parts did not differ in User Experience Questionnaire-Short (UEQ-S), UEQ-S Pragmatic and UEQ-S Hedonic. L was rated as the highest score for UEQ-S, UEQ-S Pragmatic and UEQ-S Hedonic, but CP had the lowest score. According to the results, we provide implications of designing body-centric locomotion with different transfer functions in VR.},
  keywords={Torso;Three-dimensional displays;Navigation;Transfer functions;Virtual environments;User interfaces;Market research;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR50410.2021.00073},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417671,
  author={Malinov, Yoan-Daniel and Millard, David E. and Blount, Tom},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={StuckInSpace: Exploring the Difference Between Two Different Mediums of Play in a Multi-Modal Virtual Reality Game}, 
  year={2021},
  volume={},
  number={},
  pages={501-510},
  abstract={With the rising popularity of Virtual Reality (VR), there is also a rising interest in co-located multiplayer experiences, as people want to play VR games together with their friends. As having multiple VR headsets is out of reach to the average consumer, we need to look into different possible ways of including multiple people in this play space. We have created a multi-modal co-located multiplayer VR game, Stuck in Space, that introduces a second player in two ways - one with a PC (the baseline that a lot of current games do), as well as a tracked Phone that can be used as a ‘window into the virtual world’. We have conducted a user study (n = 24) where we explore the difference in immersion and co-presence between the two versions using two questionnaires (IPQ and NMMoSP), as well as a thematic analysis of the subsequent interview data, from which 5 themes emerged. Surprisingly, we found no significant difference in co-presence or immersion based on the quantitative data. However, the qualitative analysis helps reveal one of the main reasons why that is - maintaining a mental model of the real world while also being in the virtual world makes it harder for the person wearing the headset to immerse themselves and feel co-present. From these themes and sub-themes we theorize that each of the two versions has positives and negatives that cancel each other out in the quantitative data, and for there to be a difference we would need to accentuate or change certain elements of the game. The results show that introducing a second player through a Phone is not detrimental in terms of co-presence and immersion and that it is a viable way of doing so, although certain design considerations would have to be taken into account to minimize the negatives.},
  keywords={Headphones;Three-dimensional displays;Games;Resists;Virtual reality;User interfaces;Mobile handsets;Human-centered computing-User studies;Human-centered computing-Virtual reality;Human-centered computing-Collaborative interaction;Applied computing-Computer games},
  doi={10.1109/VR50410.2021.00074},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417809,
  author={Macchini, Matteo and Lortkipanidze, Manana and Schiano, Fabrizio and Floreano, Dario},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Impact of Virtual Reality and Viewpoints in Body Motion Based Drone Teleoperation}, 
  year={2021},
  volume={},
  number={},
  pages={511-518},
  abstract={The operation of telerobotic systems can be a challenging task, requiring intuitive and efficient interfaces to enable inexperienced users to attain a high level of proficiency. Body-Machine Interfaces (BoMI) represent a promising alternative to standard control devices, such as joysticks, because they leverage intuitive body motion and gestures. It has been shown that the use of Virtual Reality (VR) and first-person view perspectives can increase the user's sense of presence in avatars. However, it is unclear if these beneficial effects occur also in the teleoperation of non-anthropomorphic robots that display motion patterns different from those of humans. Here we describe experimental results on teleoperation of a non-anthropomorphic drone showing that VR correlates with a higher sense of spatial presence, whereas viewpoints moving coherently with the robot are associated with a higher sense of embodiment. Furthermore, the experimental results show that spontaneous body motion patterns are affected by VR and viewpoint conditions in terms of variability, amplitude, and robot correlates, suggesting that the design of BoMIs for drone teleoperation must take into account the use of Virtual Reality and the choice of the viewpoint.},
  keywords={Robot motion;Three-dimensional displays;Avatars;User interfaces;Robot sensing systems;Task analysis;Telerobotics;Virtual Reality;Presence;Human-Robot Interfaces;Human Body Motion},
  doi={10.1109/VR50410.2021.00075},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417793,
  author={Meier, Manuel and Streli, Paul and Fender, Andreas and Holz, Christian},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={TapID: Rapid Touch Interaction in Virtual Reality using Wearable Sensing}, 
  year={2021},
  volume={},
  number={},
  pages={519-528},
  abstract={Current Virtual Reality systems typically use cameras to capture user input from controllers or free-hand mid-air interaction. In this paper, we argue that this is a key impediment to productivity scenarios in VR, which require continued interaction over prolonged periods of time-a requirement that controller or free-hand input in mid-air does not satisfy. To address this challenge, we bring rapid touch interaction on surfaces to Virtual Reality-the input modality that users have grown used to on phones and tablets for continued use. We present TapID, a wrist-based inertial sensing system that complements headset-tracked hand poses to trigger input in VR. TapID embeds a pair of inertial sensors in a flexible strap, one at either side of the wrist; from the combination of registered signals, TapID reliably detects surface touch events and, more importantly, identifies the finger used for touch. We evaluated TapID in a series of user studies on event-detection accuracy (F1 = 0.997) and hand-agnostic finger-identification accuracy (within-user: F1 = 0.93; across users: F1 = 0.91 after 10 refinement taps and F1 = 0.87 without refinement) in a seated table scenario. We conclude with a series of applications that complement hand tracking with touch input and that are uniquely enabled by TapID, including UI control, rapid keyboard typing and piano playing, as well as surface gestures.},
  keywords={Wrist;Productivity;Three-dimensional displays;Wearable computers;Keyboards;Virtual reality;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Gestural input},
  doi={10.1109/VR50410.2021.00076},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417736,
  author={Lisle, Lee and Davidson, Kylie and Gitre, Edward J.K. and North, Chris and Bowman, Doug A.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sensemaking Strategies with Immersive Space to Think}, 
  year={2021},
  volume={},
  number={},
  pages={529-537},
  abstract={The process of sensemaking involves foraging through and extracting information from large sets of documents, and it can be a cognitively intensive task. A recent approach, the Immersive Space to Think (IST), allows analysts to browse, read, mark up documents, and use immersive 3D space to organize and label collections of documents. In this study, we observed seventeen novice analysts perform a historical analysis task in order to understand how users utilize the features of IST to extract meaning from large text-based datasets. We found three different layout strategies they employed to create meaning with the documents we provided. We further found patterns of interaction and organization that can inform future improvements to the IST approach.},
  keywords={Visualization;Three-dimensional displays;Layout;Virtual reality;Organizations;User interfaces;Feature extraction;Human-centered computing- Visualization- Visualization techniques-;Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality},
  doi={10.1109/VR50410.2021.00077},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417644,
  author={Beacco, Alejandro and Oliva, Ramon and Cabreira, Carlos and Gallego, Jaime and Slater, Mel},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Disturbance and Plausibility in a Virtual Rock Concert: A Pilot Study}, 
  year={2021},
  volume={},
  number={},
  pages={538-545},
  abstract={We present methods used to produce and study a first version of an attempt to reconstruct a 1983 live rock concert in virtual reality. An approximately 10 minute performance by the rock band Dire Straits was rendered in virtual reality, based on the use of computer vision techniques to extract the appearance and movements of the band, and crowd simulation for the audience. An online pilot study was conducted where participants experienced the scenario and freely wrote about their experience. The documents produced were analyzed using sentiment analysis, and groups of responses with similar sentiment scores were found and compared. The results showed that some participants were disturbed not by the band performance but by the accompanying virtual audience that surrounded them. The results point to a profound level of plausibility of the experience, though not in the way that the authors expected. The findings add to our understanding of plausibility of virtual environments.},
  keywords={Solid modeling;Sentiment analysis;Computer vision;Three-dimensional displays;Computational modeling;Virtual environments;User interfaces;concert performance;computer vision;virtual reality;historical reconstruction;plausibility;presence;1.3.7;Computer Graphics;Three-Dimensional Graphics and Realism},
  doi={10.1109/VR50410.2021.00078},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417636,
  author={Sykownik, Philipp and Graf, Linda and Zils, Christoph and Masuch, Maic},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Most Social Platform Ever? A Survey about Activities & Motives of Social VR Users}, 
  year={2021},
  volume={},
  number={},
  pages={546-554},
  abstract={We present online survey results on social virtual reality (social VR) users' activities and usage motives. Based on content analysis of users' free-text responses, we found that most users, in fact, use these applications for social activities and to satisfy diverse social needs. The second most frequently mentioned categories of activities and motives relate to experiential aspects such as entertainment activities. Another important category of motives, which has only recently been described in related work, relates to the self, such as personal growth. Our results indicate that while social VR provides a superior social experience than traditional digital social spaces, like games or social media, users still desire better and affordable tracking technology, increased sensory immersion, and further improvement concerning social features. These findings complement related work as they come from a comparatively large sample (N= 273) and summarize a general user view on social VR. Besides confirming an intuitive assumption, they help identify use cases and opportunities for further research on social VR.},
  keywords={COVID-19;Three-dimensional displays;Social networking (online);Pandemics;Entertainment industry;Virtual reality;Games;Social VR;online social worlds;user motives;virtual reality},
  doi={10.1109/VR50410.2021.00079},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417779,
  author={Khan, Meraj and Nandi, Arnab},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={DreamStore: A Data Platform for Enabling Shared Augmented Reality}, 
  year={2021},
  volume={},
  number={},
  pages={555-563},
  abstract={Unlike traditional object stores, Augmented Reality (AR) query workloads possess several unique characteristics, such as spatial and visual information. Such workloads are often keyed on a variety of attributes simultaneously, such as device orientation and position, the scene in view, and spatial anchors. The natural mode of user-interaction in these devices triggers queries implicitly based on the field in the user's view at any instant, generating data queries in excess of the device frame rate. Ensuring a smooth user experience in such a scenario requires a systemic solution exploiting the unique characteristics of the AR workloads. For exploration in such contexts, we are presented with a view-maintenance or cache-prefetching problem; how do we download the smallest subset from the server to the mixed reality device such that latency and device space constraints are met? We present a novel data platform - DreamStore, that considers AR queries as first-class queries, and view-maintenance and large-scale analytics infrastructure around this design choice. Through performance experiments on large-scale and query-intensive AR workloads on DreamStore, we show the advantages and the capabilities of our proposed platform.},
  keywords={Performance evaluation;Visualization;Three-dimensional displays;Mixed reality;User interfaces;User experience;Servers},
  doi={10.1109/VR50410.2021.00080},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417659,
  author={Arafat, Abdullah Al and Guo, Zhishan and Awad, Amro},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-Spy: A Side-Channel Attack on Virtual Key-Logging in VR Headsets}, 
  year={2021},
  volume={},
  number={},
  pages={564-572},
  abstract={In Virtual Reality (VR), users typically interact with the virtual world using virtual keyboard to insert keywords, surfing the webpages, or typing passwords to access online accounts. Hence, it becomes imperative to understand the security of virtual keystrokes. In this paper, we present VR-Spy, a virtual keystrokes recognition method using channel state information (CSI) of WiFi signals. To the best of our knowledge, this is the first work that uses WiFi signals to recognize virtual keystrokes in VR headsets. The key idea behind VR -Spy is that the side-channel information of fine-granular hand movements associated with each virtual keystroke has a unique gesture pattern in the CSI waveforms. Our novel pattern extraction algorithm leverages signal processing techniques to extract the patterns from the variations of CSI. We implement VR-Spy using two Commercially Off-The-Shelf (COTS) devices, a transmitter (WAVLINK router), and a receiver (Intel NUC with an IWL 5300 NIC). Finally, VR-Spy achieves a virtual keystrokes recognition accuracy of 69.75% in comparison to techniques that assume very advanced adversary models with vision and motion sensors near the victim.},
  keywords={Headphones;Solid modeling;Transmitters;Keyboards;Virtual reality;Receivers;Side-channel attacks;Human-centered computing-Gesture Computing-Virtual Key-logging Attack-Channel State Information},
  doi={10.1109/VR50410.2021.00081},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417792,
  author={Rebol, Manuel and Gütl, Christian and Pietroszek, Krzysztof},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech}, 
  year={2021},
  volume={},
  number={},
  pages={573-581},
  abstract={In real life, people communicate using both speech and non-verbal signals such as gestures, face expression or body pose. Non-verbal signals impact the meaning of the spoken utterance in an abundance of ways. An absence of non-verbal signals impoverishes the process of communication. Yet, when users are represented as avatars, it is difficult to translate non-verbal signals along with the speech into the virtual world without specialized motion-capture hardware. In this paper, we propose a novel, data-driven technique for generating gestures directly from speech. Our approach is based on the application of Generative Adversarial Neural Networks (GANs) to model the correlation rather than causation between speech and gestures. This approach approximates neuroscience findings on how non-verbal communication and speech are correlated. We create a large dataset which consists of speech and corresponding gestures in a 3D human pose format from which our model learns the speaker-specific correlation. We evaluate the proposed technique in a user study that is inspired by the Turing test. For the study, we animate the generated gestures on a virtual character. We find that users are not able to distinguish between the generated and the recorded gestures. Moreover, users are able to identify our synthesized gestures as related or not related to a given utterance.},
  keywords={Solid modeling;Three-dimensional displays;Correlation;Neuroscience;Pose estimation;Pipelines;Predictive models;User interfaces;Animation;Data models;Gesture Animation;GAN;3D Human Pose Estimation;Human Body Language;VR},
  doi={10.1109/VR50410.2021.00082},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417788,
  author={Mirzaei, Mohammadreza and Kán, Peter and Kaufmann, Hannes},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Head Up Visualization of Spatial Sound Sources in Virtual Reality for Deaf and Hard-of-Hearing People}, 
  year={2021},
  volume={},
  number={},
  pages={582-587},
  abstract={This paper presents a novel method for the visualization of 3D spatial sounds in Virtual Reality (VR) for Deaf and Hard-of-Hearing (DHH) people. Our method enhances traditional VR devices with additional haptic and visual feedback, which aids spatial sound localization. The proposed system automatically analyses 3D sound from VR application, and it indicates the direction of sound sources to a user by two Vibro-motors and two Light-Emitting Diodes (LEDs). The benefit of automatic sound analysis is that our method can be used in any VR application without modifying the application itself. We evaluated the proposed method for 3D spatial sound visualization in a user study. Additionally, the conducted user study investigated which condition (corresponding to different senses) leads to faster performance in 3D sound localization task. For this purpose, we compared three conditions: haptic feedback only, LED feedback only, combined haptic and LED feedback. Our study results suggest that DHH participants could complete sound-related VR tasks significantly faster using LED and haptic+LED conditions in comparison to only haptic feedback. The presented method for spatial sound visualization can be directly used to enhance VR applications for use by DHH persons, and the results of our user study can serve as guidelines for the future design of accessible VR systems.},
  keywords={Location awareness;Visualization;Three-dimensional displays;Virtual reality;Auditory system;User interfaces;Light emitting diodes;Virtual Reality-Haptic-Vision-Sound Localization-Deaf;Hard-of-Hearing},
  doi={10.1109/VR50410.2021.00083},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417786,
  author={Mirzaei, Mohammadreza and Kán, Peter and Kaufmann, Hannes},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Multi-modal Spatial Object Localization in Virtual Reality for Deaf and Hard-of-Hearing People}, 
  year={2021},
  volume={},
  number={},
  pages={588-596},
  abstract={Information visualization techniques play an important role in Virtual Reality (VR) because they improve task performance, support cognitive processes, and eventually increase the feeling of immersion. Deaf and Hard-of-Hearing (DHH) persons have special needs for information presentation because they feel and perceive VR environments differently. Therefore, it is necessary to pay attention to requirements about presenting information in VR for this group of users. Previous research showed that adding special features and using haptic methods helps DHH persons to do VR tasks better. In this paper, we propose a novel Omni-directional particle visualization method and also evaluate multi-modal presentation methods in VR for DHH persons, such as audio, visual, haptic, and a combination of them (AVH). Additionally, we compare the results with the results of persons without hearing problems. The methods for information presentation in our study focus on spatial object localization in VR. Our user studies show that both DHH persons and persons without hearing problems were able to do VR tasks significantly faster using AVH. Also, we found out that DHH persons can do visual-related VR tasks faster than persons without hearing problems by using our new proposed visualization method. Our results suggest that the benefits of using audio among persons without hearing problems and the benefits of using vision among DHH persons cause an interesting balance in the results of AVH between both groups. Finally, our qualitative and quantitative evaluation indicates that both groups of participants preferred and enjoyed AVH modality more than other modalities.},
  keywords={Location awareness;Visualization;Three-dimensional displays;Cognitive processes;Auditory system;Virtual reality;User interfaces;Virtual Reality-Information Presentation-Visualization Techniques-Spatial Object Localization-Deaf and Hard-of- Hearing},
  doi={10.1109/VR50410.2021.00084},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417698,
  author={Bozkir, Efe and Stark, Philipp and Gao, Hong and Hasenbein, Lisa and Hahn, Jens-Uwe and Kasneci, Enkelejda and Göllner, Richard},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploiting Object-of-Interest Information to Understand Attention in VR Classrooms}, 
  year={2021},
  volume={},
  number={},
  pages={597-605},
  abstract={Recent developments in computer graphics and hardware technology enable easy access to virtual reality headsets along with integrated eye trackers, leading to mass usage of such devices. The immersive experience provided by virtual reality and the possibility to control environmental factors in virtual setups may soon help to create realistic digital alternatives to conventional classrooms. The importance of such settings has become especially evident during the COVID-19 pandemic, forcing many schools and universities to provide the digital teaching. Researchers foresee that such transformations will continue in the future with virtual worlds becoming an integral part of education. Until now, however, students' behaviors in immersive virtual environments have not been investigated in depth. In this work, we study students' attention by exploiting object-of-interests using eye tracking in different classroom manipulations. More specifically, we varied sitting positions of students, visualization styles of virtual avatars, and hand-raising percentages of peer-learners. Our empirical evidence shows that such manipulations play an important role in students' attention towards virtual peer-learners, instructors, and lecture material. This research may contribute to understanding of how visual attention relates to social dynamics in the virtual classroom, including significant considerations for the design of virtual learning spaces.},
  keywords={Headphones;Visualization;Three-dimensional displays;Pandemics;Avatars;Education;Immersive experience;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Applied computing-Education-Interactive learning environments;Applied computing-Education-Computer -assisted instruction},
  doi={10.1109/VR50410.2021.00085},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417639,
  author={de Siqueira, Alexandre Gomes and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Bhargava, Ayush and Lucaites, Kathryn and Solini, Hannah and Nasiri, Moloud and Robb, Andrew and Pagano, Christopher and Ullmer, Brygg and Babu, Sabarish V.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Empirically Evaluating the Effects of Perceptual Information Channels on the Size Perception of Tangibles in Near-Field Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={Immersive Virtual Environments (IVEs) incorporating tangibles are becoming more accessible. The success of applications combining 3D printed tangibles and VR often depends on how accurately size is perceived. Research has shown that visuo-haptic perceptual information is important in the perception of size. However, it is unclear how these sensory-perceptual channels are affected by immersive virtual environments that incorporate tangible objects. Towards understanding the effects of different sensory information channels in the near field size perception of tangibles of graspable sizes in IVEs, we conducted a between-subjects study evaluating the accuracy of size perception across three experimental conditions (Vision-only, Haptics-only, Vision and Haptics). We found that overall, participants consistently over-estimated the size of the dials regardless of the type of perceptual information that was presented. Participants in the haptics only condition overestimated diameters to a larger degree as compared to other conditions. Participants were most accurate in the vision only condition and least accurate in the haptics only condition. Our results also revealed that increased efficiency in reporting size over time was most pronounced in the visuo- haptic condition.},
  keywords={Three-dimensional displays;Virtual environments;User interfaces;Haptic interfaces;Human-centered computing—Virtual reality;Human-centered computing—Haptic devices;Computing methodologies—Perception},
  doi={10.1109/VR50410.2021.00086},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417771,
  author={Cha, Young-Woon and Shaik, Husam and Zhang, Qian and Feng, Fan and State, Andrei and Ilie, Adrian and Fuchs, Henry},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mobile. Egocentric Human Body Motion Reconstruction Using Only Eyeglasses-mounted Cameras and a Few Body-worn Inertial Sensors}, 
  year={2021},
  volume={},
  number={},
  pages={616-625},
  abstract={We envision a convenient telepresence system available to users anywhere, anytime. Such a system requires displays and sensors embedded in commonly worn items such as eyeglasses, wristwatches, and shoes. To that end, we present a standalone real-time system for the dynamic 3D capture of a person, relying only on cameras embedded into a head-worn device, and on Inertial Measurement Units (IMUs) worn on the wrists and ankles. Our prototype system egocentrically reconstructs the wearer's motion via learning-based pose estimation, which fuses inputs from visual and inertial sensors that complement each other, overcoming challenges such as inconsistent limb visibility in head-worn views, as well as pose ambiguity from sparse IMUs. The estimated pose is continuously re-targeted to a prescanned surface model, resulting in a high-fidelity 3D reconstruction. We demonstrate our system by reconstructing various human body movements and show that our visual-inertial learning-based method, which runs in real time, outperforms both visual-only and inertial-only approaches. We captured an egocentric visual-inertial 3D human pose dataset publicly available at https://sites.google.com/site/youngwooncha/egovip for training and evaluating similar methods.},
  keywords={Wrist;Three-dimensional displays;Telepresence;Inertial sensors;Pose estimation;Prototypes;Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Animation-Motion capture;Computing methodologies-Artificial intelligence-Computer vision-Reconstruction;Computing methodologies-Machine learning-Machine learning approaches-Neural networks},
  doi={10.1109/VR50410.2021.00087},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417802,
  author={Dong, Tianyang and Shen, Yue and Gao, Tieqi and Fan, Jing},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dynamic Density-based Redirected Walking Towards Multi-user Virtual Environments}, 
  year={2021},
  volume={},
  number={},
  pages={626-634},
  abstract={With more attention being paid to large scale virtual environments, the demand for more users to collaborate in the same physical space is growing rapidly. Due to the complex collision problem caused by the limitation of physical space, the multi-user redirected walking methods are devoted to improving the ability of multi-user navigation in large-scale virtual environments by reducing the disturbance of resets. Because the existing multi-user redirected walking methods do not consider the density of users in the physical space, there are more boundary conflicts in the real walking for the multi-user virtual environment. In order to decrease the boundary conflicts, this paper presents a novel method of dynamic density-based redirected walking towards multi-user virtual environments. This method dynamically adjusts the user distribution to a state with high center density and low boundary density through the density force, which is generated by the density difference between standard density and actual density. In our method, the users in high-density areas are guided by a repulsive force away from the central area while the users in low-density areas are guided by the gravitational forces towards the central area. Our method can select a double-density optimal gravitational point as the turning target, so all users can move to the area of minimum density to make better use of the whole physical space. Our method also adopts the artificial potential field (APF) forces to prevent user collisions caused by usergathering. The users are guided to move in the direction of the resultant force vector of density force, gravitational force and APF force. In addition, this paper introduces a matching resetting method to further adjust the density distribution while dealing with user conflicts. The results of experiments show that our method successfully reduces the potential conflicts about 30% on average compared with the existing reactive multi-user redirection algorithms. Especially as the number of users increases, our method can avoid more boundary conflicts by using the adjustment of density.},
  keywords={Legged locomotion;Three-dimensional displays;Navigation;Heuristic algorithms;Force;Virtual environments;User interfaces;Virtual Reality (VR);Redirected Walking (RDW);Multiple users;Artificial Potential Field;Collision Avoidance},
  doi={10.1109/VR50410.2021.00088},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417651,
  author={Hsu, Chi-Hsuan and Chung, Chi-Han and Venkatakrishnan, Rohith and Venkatakrlshnan, Roshan and Wang, Yu-Shuen and Babu, Sabarish V.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparative Evaluation of Digital Writing and Art in Real and Immersive Virtual Environments}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={Virtual reality (VR) experiences currently tend to focus on full body interactions. However, fine motor control in actions such as writing and drawing are seldom studied. Challenges include the inability to perceive fine details due to the low resolution of head mounted displays, the difficulty in simulating fine motor actions in virtual environments, tracking instabilities, latency issues, etc. State of the art VR has managed to address a host of such concerns, supporting a variety of input mechanisms for activities such as writing, sketching, immersive modeling, etc. With VR increasingly being applied in education and medical contexts where writing and note taking is a crucial, it is important to study how well humans can perform these tasks in VR. In a between-subjects empirical evaluation, we studied participants' fine motor coordination with several digital input based writing and artistic tasks performed both in virtual and real world settings, further examining the effects of providing a virtual self avatar on task performance. We integrated multiple tracking systems and applied inverse kinematics to animate the virtual body and simulate hand motions. We went on to compare how different the outputs of these digital input metaphors are to a real world pen and paper approach in an effort to ascertain where we currently stand in being able to support writing and note taking in virtual world contexts. Overall, it seems to be the case that while writing and artistic activities can be successfully supported in VR applications using specialized input devices, the accuracy with which users perform such tasks is significantly higher in the real world, highlighting the need for developments that support such fine motor tasks in VR.},
  keywords={Performance evaluation;Motor drives;Solid modeling;Art;Three-dimensional displays;Tracking;Virtual environments;Writing and Art in Virtual Reality;Perception-Action Coordination;Fine Motor Control},
  doi={10.1109/VR50410.2021.00089},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417676,
  author={Bueno-Vesga, Jhon Alexander and Xu, Xinhao and He, Hao},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effects of Cognitive Load on Engagement in a Virtual Reality Learning Environment}, 
  year={2021},
  volume={},
  number={},
  pages={645-652},
  abstract={Engagement has been traditionally linked to presence in desktop-based virtual reality learning environments. Although several studies have been performed to determine other factors affecting cognitive engagement, the role of cognitive load as a factor of student's engagement in desktop-based virtual reality (VR) has received little attention in the literature. The main purpose of this study was to explain if individual dimensions of cognitive load (mental demand, effort, and frustration level) can be used in addition to factors like presence and self-efficacy to predict student's cognitive engagement. The results of the study confirmed presence and self-efficacy as significant predictors of student's engagement. Also, a three-step hierarchical regression analysis revealed that two of the three individual dimensions of cognitive load (effort and frustration level) were also significant predictors of student's engagement.},
  keywords={Three-dimensional displays;Virtual reality;User interfaces;Regression analysis;Task analysis;Virtual Reality;Cognitive Engagement;Presence;Self-efficacy;Cognitive Load;Effort;Frustration},
  doi={10.1109/VR50410.2021.00090},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417781,
  author={Kim, Dooyoung and Shin, Jae-eun and Lee, Jeongmi and Woo, Woontack},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Adjusting Relative Translation Gains According to Space Size in Redirected Walking for Mixed Reality Mutual Space Generation}, 
  year={2021},
  volume={},
  number={},
  pages={653-660},
  abstract={We propose the concept of relative translation gains, a novel Redirected Walking (RDW) method to create a mutual movable space between the Augmented Reality (AR) host's reference space and the Virtual Reality (VR) client's space. Previous RDW methods have focused on maximizing the movable space at the expense of aligning the coordinates between the AR and VR side, and could only be applied to collaborative scenarios involving sequential tasks. Our method solves these problems by adjusting the remote client's walking speed for each axis of a VR space to modify the movable area without coordinate distortion. We estimate the relative translation gain threshold, defined as the extent to which the walking speed can be altered without creating a perceived difference in distance. In order to reflect features of the reference space in generating the mutual space, we then examine how changing its size affects the threshold value. Our study showed that for remote clients connected to the larger reference space, relative translation gains can be increased to utilize a VR space bigger than their real space. Our method can be applied to create optimal mutual spaces for a wider variety of asymmetric Mixed Reality (MR) remote collaboration systems.},
  keywords={Legged locomotion;Three-dimensional displays;Mixed reality;Collaboration;User interfaces;Distortion;Task analysis;Virtual Reality;Mixed Reality;relative translation gains;redirected walking;mutual space;visual cognition;threshold},
  doi={10.1109/VR50410.2021.00091},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417652,
  author={Tatzgern, Markus and Birgmann, Christoph},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Input Approximations for Control Panels in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  abstract={Today's availability of sophisticated consumer-grade Virtual Reality (VR) hardware provides affordable access to training simulation technology. In contrast to more traditional high fidelity training simulations utilizing physical replicas of control panels that allow natural interaction, users of virtual training often rely on handheld VR controllers to manipulate simulated controls. Therefore, control manipulations of virtual buttons and sliders must be mapped to approximations of real hand manipulations. For instance, the turning of a physical knob with thumb and index finger can be mapped to a gross motor forearm rotation, or a fine motor joystick manipulation when manipulating a controller. In this paper, we present an exploration of hand input approximations of real hands to manipulate the buttons, toggles, knobs and sliders using typical handheld VR controllers. We use common Oculus Quest controllers that rely on capacitive sensing to create basic, approximate hand gestures. We demonstrate the potential of our designs by performing an experiment in which we compare approximate hand gestures against using the controller's joystick that allows fine motor control using thumb input, and a baseline ray-casting interaction that is commonly used in VR applications. We provide a detailed analysis of our interaction designs using the Framework for Interactive Fidelity Analysis (FIFA), which allows us to discuss differences between input approximations and the real-world hand manipulations.},
  keywords={Training;Solid modeling;Motor drives;Three-dimensional displays;Thumb;Virtual reality;User interfaces;Human-centered computing-Interaction paradigms-Virtual reality;Human-centered computing-Human-centered Interaction (HCI)-Empirical studies in HCI;Applied computing-Education-Interactive learning environments},
  doi={10.1109/VR50410.2021.00092},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417709,
  author={Han, Sangyoon and Yun, Gyeore and Choi, Seungmoon},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Camera Space Synthesis of Motion Effects Emphasizing a Moving Object in 4D films}, 
  year={2021},
  volume={},
  number={},
  pages={670-678},
  abstract={Four-dimensional (4D) films, which provide special physical effects to the audience with audiovisual stimuli, are gaining more popularity and acceptance. One of the most frequent 4D effects is the object-based motion effect, which refers to the vestibular stimulus generated by a motion chair to emphasize a moving object of interest, e.g., the flying iron man, displayed on the screen. In this paper, we present an algorithm for synthesizing convincing object-based motion effects automatically from a given object motion trajectory. While previous approaches use the 2D object position on the screen as input, our method takes the 3D position and orientation of the object in the camera space and computes its motion proxy that reflects both the object translation and rotation, as well as its size to the viewers' eyes. The proxy is determined based on the results of a perceptual experiment that presents an optimal additive rule of the translation and rotation information scaled by the object's visual size. The motion proxy is fed to a motion cueing algorithm (MCA) that computes the command using a washout filter or model predictive control. The most appropriate MCA for our purpose is selected from six candidates by a user study. We also consider the effects of visual perception by incorporating two types of motion field equations into the computation of the visually perceived velocity. The results of a user study indicate that our algorithm can generate compelling object-based motion effects that better enhance the 4D film viewing experience than the previous methods.},
  keywords={Visualization;Three-dimensional displays;Films;Virtual reality;User interfaces;Cameras;Prediction algorithms;Information systems-Information systems applications-Multimedia information systems-Multimedia content creation;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR50410.2021.00093},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417782,
  author={Voigt-Antons, Jan-Niklas and Spang, Robert and Kojić, Tanja and Meier, Luis and Vergari, Maurizio and Möller, Sebastian},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Don't Worry be Happy - Using virtual environments to induce emotional states measured by subjective scales and heart rate parameters}, 
  year={2021},
  volume={},
  number={},
  pages={679-686},
  abstract={Advancing technology and higher availability of Virtual Reality (VR) devices sparked its application in various research fields. For instance, health-related research showed that simulated nature environments in VR could reduce arousal and increase valence levels. This study investigates how the amount of possible interactivity influences the presence in nature environments and consequences on arousal and valence. After inducing fear (high arousal and low valence) through a VR-horror game, it was tested how participants recovered if they played a VR-nature game with either no, limited, or extensive interaction. The horror game proved to be a valid stimulus for inducing high arousal and low valence with a successful manipulation check. Igroup presence questionnaire (IPQ) scores showed that more interaction with the virtual environment increases spatial presence. A beneficial effect of experiencing nature can also be concluded. Results from the Self-Assessment Manikin questionnaire (SAM) scores for valence indicate a significant increase in the conditions with extensive and limited interaction compared to the control group. The VR Nature experience did significantly decrease arousal and increase valence compared to the post-horror game ratings. The physiological responses support this finding. These results can increase the effectiveness of health-related VR-applications to elevate mood levels by either implementing plenty of interactions and consequently increasing spatial presence or doing the opposite and leaving out any interactions at all.},
  keywords={Heart rate;Solid modeling;Three-dimensional displays;Mood;Computational modeling;Virtual environments;Games;Virtual Reality;green environment;inducing fear;VR relaxation;emotional response;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI theory;concepts and models;Human-centered computing-Interaction design},
  doi={10.1109/VR50410.2021.00094},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417669,
  author={Kelly, Jonathan W. and Cherep, Lucia A. and Lim, Alex F. and Doty, Taylor and Gilbert, Stephen B.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Who Are Virtual Reality Headset Owners? A Survey and Comparison of Headset Owners and Non-Owners}, 
  year={2021},
  volume={},
  number={},
  pages={687-694},
  abstract={The number of people who own a virtual reality (VR) head-mounted display (HMD) has reached a point where researchers can readily recruit HMD owners to participate remotely using their own equipment. However, HMD owners recruited online may differ from the university community members who typically participate in VR research. HMD owners (n=220) and non-owners (n=282) were recruited through two online work sites-Amazon's Mechanical Turk and Prolific-and an undergraduate participant pool. Participants completed a survey in which they provided demographic information and completed measures of HMD use, video game use, spatial ability, and motion sickness susceptibility. In the context of the populations sampled, the results provide 1) a characterization of HMD owners, 2) a snapshot of the most commonly owned HMDs, 3) a comparison between HMD owners and non-owners, and 4) a comparison among online workers and undergraduates. Significant gender differences were found: men reported lower motion sickness susceptibility and more video game hours than women, and men outperformed women on spatial tasks. Men comprised a greater proportion of HMD owners than non-owners, but after accounting for this imbalance, HMD owners did not differ appreciably from non-owners. Comparing across recruitment platform, male undergraduates outperformed male online workers on spatial tests, and female undergraduates played fewer video game hours than female online workers. The data removal rate was higher from Amazon compared to Prolific, possibly reflecting greater dishonesty. These results provide a description of HMD users that can inform researchers recruiting remote participants through online work sites. These results also signal a need for caution when comparing in-person VR research that primarily enrolls undergraduates to online VR research that enrolls online workers.},
  keywords={Headphones;Three-dimensional displays;Sociology;Resists;Virtual reality;Games;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR50410.2021.00095},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417768,
  author={Vergari, Maurizio and Kojić, Tanja and Vona, Francesco and Garzotto, Franca and Möller, Sebastian and Voigt-Antons, Jan-Niklas},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Influence of Interactivity and Social Environments on User Experience and Social Acceptability in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={695-704},
  abstract={Nowadays, Virtual Reality (VR) technology can be potentially used everywhere through wearable head-mounted displays. Nevertheless, it is still uncommon to see VR devices used in public settings. In these contexts, unaware bystanders in the surroundings might influence the User Experience (UX) and create concerns about the social acceptability of this technology. The user acts in a Social Environment (SE), characterized by surrounding people's number, proximity, and behavior. Simultaneously, VR applications often require a different degree of interactivity concerning body movements and controllers interaction. In this paper, the influence of Social Environments, and degree of interactivity on User Experience and social acceptability is investigated. Four Social Environments were simulated employing 360° Videos, and two VR games developed with two levels of interactivity. Results showed a statistically significant influence of Social Environments on Overall UX as well as Public VR, Interaction, Isolation, Privacy and Safety acceptability, and of the degree of interactivity on Presence, Valence, Arousal, Overall UX, UX Hedonic quality, and Safety acceptability. Findings indicate that Social Environments and degree of interactivity should be taken into account while designing VR applications.},
  keywords={Privacy;Three-dimensional displays;Virtual reality;Games;User interfaces;Particle measurements;User experience;Virtual Reality;Social Acceptability;User Experience;Social Environments;Interactivity;360° Videos},
  doi={10.1109/VR50410.2021.00096},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417658,
  author={Curtis, Vincent R. and Caira, Nicholas W. and Xu, Jiayi and Sata, Asha Gowda and Pégard, Nicolas C.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={DCGH: Dynamic Computer Generated Holography for Speckle-Free, High Fidelity 3D Displays}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  abstract={Computer Generated Holography (CGH) is a promising technique for synthesizing 3D images on-demand. CGH systems sculpt light by computing a 2D modulation pattern to shape the phase or the amplitude of a coherent light source. Yet, static wave modulation restricts feasible holograms to a very small subset of all the possible illumination patterns where speckle noise is omnipresent. Here, we introduce Dynamic Computer Generated Holography (DCGH), a novel light sculpting technique that modulates light both spatially and temporally. DCGH has many more degrees of control to eliminate unwanted wave correlations and yields high fidelity, speckle- free 3D holograms. Our technique relies on an algorithm that simultaneously computes a set of engineered coherent wavefronts by optimizing them as a whole to best render the desired 3D illumination pattern when their contributions are rapidly superimposed and averaged in time. We have implemented DCGH with a Digital Micromirror Device (DMD) and synthesized 3D holograms made of up to 50 mutually optimized waves with refresh rates of 190 3D images per second. Our experimental results indicate that DCGH yields 3D images with improved resolution and contrast, successfully addressing the shortcomings of single-frame holography. Our technique can be implemented with inexpensive, low-power hardware, as a compact, portable 3D display that is perfectly suited for virtual and augmented reality applications.},
  keywords={Three-dimensional displays;Image resolution;Shape;Phase modulation;Lighting;Holography;User interfaces;Computer Generated Holography-3D image synthesis-virtual reality-3D displays sculpted light},
  doi={10.1109/VR50410.2021.00097},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417699,
  author={Singh, Abbey and Kaur, Ramanpreet and Haltner, Peter and Peachey, Matthew and Gonzalez-Franco, Mar and Malloch, Joseph and Reilly, Derek},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Story CreatAR: a Toolkit for Spatially-Adaptive Augmented Reality Storytelling}, 
  year={2021},
  volume={},
  number={},
  pages={713-722},
  abstract={Headworn Augmented Reality (AR) and Virtual Reality (VR) displays are an exciting new medium for locative storytelling. Authors face challenges planning and testing the placement of story elements when the story is experienced in multiple locations or the environment is large or complex. We present Story CreatAR, the first locative AR/VR authoring tool that integrates spatial analysis techniques. Story CreatAR is designed to help authors think about, experiment with, and reflect upon spatial relationships between story elements, and between their story and the environment. We motivate and validate our design through developing different locative AR/VR stories with several authors.},
  keywords={Three-dimensional displays;Design methodology;Games;Tools;User interfaces;Media;Planning;augmented reality;space syntax;storytelling;prox-emics;f-formations;authoring toolkit;head-mounted display: Human-centered computing-Human computer interaction (HCI)-Interactive systems and tools-User interface toolkits;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Interaction design-Interaction design process and methods-User centered design},
  doi={10.1109/VR50410.2021.00098},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417694,
  author={Liu, Jiacheng and Drga, Vit and Yasin, Ifat},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimal Time Window for the Integration of Spatial Audio-Visual Information in Virtual Environments}, 
  year={2021},
  volume={},
  number={},
  pages={723-728},
  abstract={Sound duration and location may influence both auditory and visual perception with consequences for the judgement of both auditory-visual event location and integration. This study investigated audio-visual integration in a virtual environment using both short- and long-duration auditory stimuli with visual stimuli temporally offset from the start of the auditory stimulus, to investigate the effects of top-down neural effects on perception. Two tasks were used, an auditory localization task and a detection task (judgement of audio-visual synchrony). Eleven participants took part in the study using a HTC Vive Pro. The short-duration auditory stimuli (35-ms spatialized sound) and long-duration auditory stimuli (600-ms non-spatialized sound followed by 35 ms of spatialized sound) were presented at −60°, −30°, 0°, +30° and +60° degrees azimuth, with the visual stimulus presented synchronously or asynchronously with respect to the start of the auditory stimulus. Results showed that localization errors were larger for the longer-duration stimuli and judgements of audiovisual synchrony tended to be improved for stimuli presented at ±30°. Top-down neural processing can affect spatial localization and audio-visual processing. Auditory localization errors and audio-visual synchrony detection may reveal the effects of underlying neural feedback mechanisms that can be harnessed to optimize audio-visual experiences in virtual environments.},
  keywords={Location awareness;Visualization;Solid modeling;Three-dimensional displays;Azimuth;Virtual environments;Psychology;Auditory localization;Visual;Virtual;Temporal.: H.1.2 [Models and Principles]: User/Machine Systems-Software psychology;Human factors;H.5.1 [Information and Interfaces and presentation]: Multimedia Information Systems-Artificial;augmented;virtual realities},
  doi={10.1109/VR50410.2021.00099},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417729,
  author={Cao, Ruochen and Zou-Williams, Lena and Cunningham, Andrew and Walsh, James and Kohler, Mark and Thornas, Bruce H.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Comparing the Neuro-Physiological Effects of Cinematic Virtual Reality with 2D Monitors}, 
  year={2021},
  volume={},
  number={},
  pages={729-738},
  abstract={In this work, we explore if the immersion afforded by Virtual Reality can improve the cognitive integration of information in Cinematic Virtual Reality (CVR). We conducted a user study examining participants' cognitive activities (recall performance and cortical response) when consuming visual information of emotional and emotionally neutral scenes in a non-CVR environment (i.e. a monitor) versus a CVR environment (i.e. a head-mounted display). Cortical response was recorded using electroencephalography. The results showed that participants had greater early visual attention with neutral emotions in a CVR environment, and showed higher overall alpha power in a CVR environment. The use of CVR did not significantly affect participants' recall performance.},
  keywords={Visualization;Three-dimensional displays;Head-mounted displays;Virtual reality;User interfaces;Electroencephalography;Monitoring;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Visualization},
  doi={10.1109/VR50410.2021.00100},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417646,
  author={Saalfeld, Patrick and Böttcher, Claudia and Klink, Fabian and Preim, Bernhard},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR System for the Restoration of Broken Cultural Artifacts on the Example of a Funerary Monument}, 
  year={2021},
  volume={},
  number={},
  pages={739-748},
  abstract={We present a VR system that supports the restoration of broken cultural artifacts. As a case study, we demonstrate this approach for the restoration of a funerary monument. Among the challenges of this monument are a large number of 415 fragments, an unknown amount belongs to another artifact, missing pieces prevent a full reconstruction and the preserved fragments vary strongly in size. Our VR system supports the workflow of digital restoration by offering a configurable self-arranging fragment wall. It supports the user to organize all fragments in an overview representation and to identify relevant fragments quickly. For assembly, we implemented a jigsaw approach comprising two sets of manipulation techniques that allow the user to roughly align fragments first in sub-puzzles and precisely assemble them in a second step. The iterative development and assembling process was accompanied by a professional restorer. We report about the insights we gained from this process and how we optimized the VR system according to her requirements and feedback. Within 14 sessions that took 21 hours, the virtual reconstruction was finalized.},
  keywords={Surface reconstruction;Three-dimensional displays;Virtual reality;User interfaces;Three-dimensional printing;Surface fitting;Systems support;Human-centered computing-Human computer interaction (HCI);Human-centered computing-Interaction design Human-centered computing-Visualization Applied computing-Arts and humanities Applied computing-Digital libraries and archives},
  doi={10.1109/VR50410.2021.00101},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417706,
  author={Dalia Blaga, Andreea and Frutos-Pascual, Maite and Creed, Chris and Williams, Ian},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Freehand Grasping: An Analysis of Grasping for Docking Tasks in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={749-758},
  abstract={Natural and intuitive interaction such as freehand grasping of virtual objects is still a significant challenge in VR due to the dexterous versatility of the human grasping actions. Currently, the design considerations for creating freehand grasping interactions in VR are drawn from the body of historical knowledge presented for real object grasping. While this may be suitable for some applications, recent work has shown that users grasp virtual objects differently than they grasp real objects, presenting an absence of knowledge on how users intuitively grasp virtual objects. To begin to address this, we present an elicitation study where participants (N =39) grasped 16 virtual objects categorised by shape in a mixed docking task exploring placement and rotation. We report on a Wizard of OZ methodology, extract Grasp Type and Grasp Dimension and present grasping patterns in VR. Our results are of value to be taken forward into a framework of recommendations for grasping interactions, as well as parameterizing grasp types for developing intuitive grasp models and thus begin to bridge the gap in understanding natural grasping patters for VR object interactions.},
  keywords={Measurement;Solid modeling;Three-dimensional displays;Shape;Grasping;Virtual reality;User interfaces;Hand Interaction;Grasping Virtual Objects;Virtual Reality Interaction},
  doi={10.1109/VR50410.2021.00102},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417776,
  author={Pavanatto, Leonardo and North, Chris and Bowman, Doug A. and Badea, Carmen and Stoakley, Richard},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Do we still need physical monitors? An evaluation of the usability of AR virtual monitors for productivity work}, 
  year={2021},
  volume={},
  number={},
  pages={759-767},
  abstract={Physical monitors require space, lack flexibility, and can become expensive and less portable in large setups. Virtual monitors, on the other hand, can minimize those problems, but may be subject to technological limitations such as lower resolution and field of view. We investigate the impacts of using virtual monitors displayed on a current state-of-the-art augmented reality headset for conducting productivity work. We conducted a user study that compared physical monitors, virtual monitors, and a hybrid combination of both in terms of performance, accuracy, comfort, focus, preference, and confidence. Results show that virtual monitors are a feasible approach for performing serious productivity work, albeit currently constrained by technical limitations that lead to inferior usability and performance compared to physical monitors. We also discovered that, with current technology, the hybrid condition was a better tradeoff between the familiarity and trustworthiness of physical monitors and the extra space provided by virtual monitors. We conclude by expressing the opportunity for designing strategies for mixing virtual and physical monitors into novel hybrid interfaces.},
  keywords={Productivity;Headphones;Three-dimensional displays;User interfaces;Usability;Monitoring;Augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Empirical studies in interaction design},
  doi={10.1109/VR50410.2021.00103},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417649,
  author={Lu, Feiyu and Bowman, Doug A.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Potential of Glanceable AR Interfaces for Authentic Everyday Uses}, 
  year={2021},
  volume={},
  number={},
  pages={768-777},
  abstract={In the near future, augmented reality (AR) glasses are envisioned to become the next-generation personal computing platform. They could be always on and worn all day, delivering continuous and pervasive AR experiences for general-purpose everyday use cases. However, it remains unclear how we could enable unobtrusive and easy information access without distracting users, while being acceptable to use at the same time. To address this question, we implemented two prototypes based on the Glanceable AR paradigm, a promising way of managing and acquiring information through glancing at the periphery of AR head-worn displays (HWDs). We conducted two separate studies to evaluate our designs. In the first study, we obtained feedback from a large sample of participants of varied age and background about a video prototype that showcased some envisioned scenarios of using Glanceable AR for everyday tasks. In the second study, we asked participants to use a working prototype during authentic real-world activities for three days. We found that users appreciated the Glanceable AR approach. They found it less distracting or intrusive than existing devices in authentic everyday use cases, and would like to use the interface on a daily basis if the form factor of the AR headset was more like eyeglasses.},
  keywords={Headphones;Three-dimensional displays;Head-mounted displays;Prototypes;Glass;User interfaces;Task analysis;Human-centered computing-Mixed / augmented reality;Human-centered computing-User interface design},
  doi={10.1109/VR50410.2021.00104},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417663,
  author={Bozgeyikli, Evren and Bozgeyikli, Lal Lila},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Object Manipulation Interaction Techniques in Mixed Reality: Tangible User Interfaces and Gesture}, 
  year={2021},
  volume={},
  number={},
  pages={778-787},
  abstract={Tangible user interfaces (TUIs) have been widely studied in computer, virtual reality and augmented reality systems and are known to improve user experience in these mediums. However, there have been few evaluations of TUIs in wearable mixed reality (MR). In this study, we present the results from a comparative study evaluating three object manipulation techniques in wearable MR: (1) Space-multiplexed identical-formed TUI (i.e., a physical cube that acted as a dynamic tangible proxy with identical real and virtual forms); (2) Time-multiplexed TUI (i.e., a tangible controller that was used to manipulate virtual content); (3) Hand gesture (i.e., reaching, pinching and moving the hand to manipulate virtual content). The interaction techniques were compared with a user study with 42 participants. Results revealed that the tangible cube and the controller interaction methods were comparative to each other while both being superior to the hand gesture interaction method in terms of user experience, performance, and presence. We also present suggestions for interaction design for MR based on our findings.},
  keywords={Three-dimensional displays;Mixed reality;User interfaces;User experience;Manipulator dynamics;Augmented reality;Mixed reality;augmented reality;tangible user interfaces;object manipulation;user experience;evaluation.: Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
  doi={10.1109/VR50410.2021.00105},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417681,
  author={Choudhary, Zubin and Gottsacker, Matthew and Kim, Kangsoo and Schubert, Ryan and Stefanucci, Jeanine and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Revisiting Distance Perception with Scaled Embodied Cues in Social Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={788-797},
  abstract={Previous research on distance estimation in virtual reality (VR) has well established that even for geometrically accurate virtual objects and environments users tend to systematically mis-estimate distances. This has implications for Social VR, where it introduces variables in personal space and proxemics behavior that change social behaviors compared to the real world. One yet unexplored factor is related to the trend that avatars' embodied cues in Social VR are often scaled, e.g., by making one's head bigger or one's voice louder, to make social cues more pronounced over longer distances. In this paper we investigate how the perception of avatar distance is changed based on two means for scaling embodied social cues: visual head scale and verbal volume scale. We conducted a human-subject study employing a mixed factorial design with two Social VR avatar representations (full-body, head-only) as a between factor as well as three visual head scales and three verbal volume scales (up-scaled, accurate, down-scaled) as within factors. For three distances from social to far-public space, we found that visual head scale had a significant effect on distance judgments and should be tuned for Social VR, while conflicting verbal volume scales did not, indicating that voices can be scaled in Social VR without immediate repercussions on spatial estimates. We discuss the interactions between the factors and implications for Social VR.},
  keywords={Visualization;Three-dimensional displays;Avatars;Design methodology;Estimation;Aerospace electronics;User interfaces;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/VR50410.2021.00106},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417772,
  author={Gagnon, Holly C. and Rohovit, Taren and Finney, Hunter and Zhao, Yu and Franchak, John M. and Stefanucci, Jeanine K. and Bodenheimer, Bobby and Creem-Regehr, Sarah H.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Feedback on Estimates of Reaching Ability in Virtual Reality}, 
  year={2021},
  volume={},
  number={},
  pages={798-806},
  abstract={Immersive virtual environments (VEs) are most useful for training and education when viewers perceive and act accurately within them. Judgments of action capabilities within a VE provide a good measure of perceptual fidelity - the notion of how closely perception and action in the VE match that in the real world - and can also assess how perception for action may be calibrated with visual feedback based on one's own actions. In the current study we tested judgments of action capabilities within a VE for two different reaching behaviors: reaching out and reaching up. Our goal was to assess whether feedback from actual reaching improves judgments and if any recalibration due to feedback differed across reaching behaviors. We first measured participants' actual reaching out and reaching up capabilities so that feedback trials could be scaled to their actual abilities. Participants then completed blocks of alternating perceptual adjustment and feedback trials. In adjustment trials, they adjusted a virtual target to a distance perceived to be just reachable. In feedback trials, they viewed targets that were farther or closer than their actual reach, decided whether the target was reachable, and then reached out to the target to receive visual feedback from a hand-held controller. The first feedback block manipulated the target distance to be 30% over or under actual reach and subsequent blocks decreased the deviation to 20%,10% and 5% of actual reach. We found that for both reaching behaviors, reach was initially overestimated, and then perceptual estimations decreased to become more accurate over feedback blocks. Accuracy in the feedback trials themselves showed that targets just beyond reach were more difficult to judge correctly. This study establishes a straightforward methodology that can be used for calibration of actions in VEs and has implications for applications that depend on accurate reaching within VEs.},
  keywords={Training;Visualization;Three-dimensional displays;Current measurement;Virtual environments;Estimation;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Collaborative interaction},
  doi={10.1109/VR50410.2021.00107},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417790,
  author={Zhang, Di and Pun, Chi-Man and Yang, Yang and Gao, Hao and Xu, Feng},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Rate-based Drone Control with Adaptive Origin Update in Telexistence}, 
  year={2021},
  volume={},
  number={},
  pages={807-816},
  abstract={A new form of telexistence is achieved by recording videos with a camera on an Uncrewed aerial vehicle (UAV) and playing the videos to a user via a head-mounted display (HMD). One key problem here is how to let the user freely and naturally control the UAV and thus the viewpoint. In this paper, we develop an HMD-based telexistence technique that achieves full 6- DOF control of the viewpoint. The core of our technique is an improved rate-based control technique with our adaptive origin update (AOU), in which the origin of the coordinate system of the user changes adaptively. This makes the user naturally perceive the origin and thus easily perform the control motion to get his/her desired viewpoint changing. As a consequence, without the aid of any auxiliary equipment, the AOU scheme handles the well known self-centering problem in the rate-based control methods. A real prototype is also built to evaluate this feature of our technique. To explore the advantage of our telexistence technique, we further use it as an interactive tool to perform the task of 3D scene reconstruction. User studies demonstrate that comparing with other telexistence solutions and the widely used joystick-based solutions, our solution largely reduces the workload and saves time and moving distance for the user.},
  keywords={Solid modeling;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;Tools;Telexistence},
  doi={10.1109/VR50410.2021.00108},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417795,
  author={Todd, Russell and Zhu, Qin and Banić, Amy},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Temporal Availability of Ebbinghaus Illusions on Perceiving and Interacting with 3D Objects in a Contextual Virtual Environment}, 
  year={2021},
  volume={},
  number={},
  pages={817-825},
  abstract={Contextual illusions, such as the Ebbinghaus Illusion, can be potentially used to improve or hinder reach-to-grasp interaction in a virtual environment by affecting the perception of object size and the action. However, the illusion effect has only been evaluated using 2D objects like discs or annuluses, and limited research has been conducted in a virtual environment. Moreover, it remains unknown how the sudden, or dynamic, change of surrounding features will impact the perception and then the action towards the object. In this paper, we conducted a series of experiments to evaluate the effects of 3D Ebbinghaus illusion with dynamic surrounding features on the task of reaching to grasp a 3D object in an immersive virtual environment. An innovative 3D perceptual judgment task revealed that the static 3D illusion affected the perceived size of the 3D object. Then, we experimentally manipulated the visual gain and loss of the 3D contextual inducers, the participant's virtual hand, and the entire 3D contextual object. Results revealed that the depth error (error in depth of reaching action) was influenced by a dynamic change in the size of the inducers, the fine adjustment of grasp was dependent on the visual presence of the virtual hand and vision of 3D contextual object was required for the reaching and grasping movements. These results will benefit the understanding of reach-to-grasp interactions in immersive virtual environments and can improve interaction design.},
  keywords={Visualization;Three-dimensional displays;Virtual environments;Grasping;Kinematics;User interfaces;Apertures;3d perceptual judgement task;reach-to-grasp;Ebbing-haus;illusion;perception;3D Contextual Objects;temporal availability;dynamic inducers;static inducers;tracking;virtual reality},
  doi={10.1109/VR50410.2021.00109},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417799,
  author={Zhang, Fanxing and Liu, Zhihao and Cheng, Zhanglin and Deussen, Oliver and Chen, Baoquan and Wang, Yunhai},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mid-Air Finger Sketching for Tree Modeling}, 
  year={2021},
  volume={},
  number={},
  pages={826-834},
  abstract={2D sketch-based tree modeling cannot guarantee to generate plausible depth values and full 3D tree shapes. With the advent of virtual reality (VR) technologies, 3D sketching enables a new form for 3D tree modeling. However, it is labor-intensive and difficult to create realistically-looking 3D trees with complicated geometry and lots of detailed twigs with a reasonable amount of effort. In this paper, we explore the use of mid-air finger 3D sketching in VR for tree modeling. We present a hybrid approach that integrates freehand 3D sketches with an automatic population of branch geometries. The user only needs to draw a few 3D strokes in mid-air to define the envelope of the foliage (denoted as lobes) and main branches. Our algorithm then automatically generates a full 3D tree model based on these stroke inputs. Additionally, the shape of the 3D tree model can be modified by freely dragging, squeezing, or moving lobes in mid-air. We demonstrate the ease-of-use, efficiency, and flexibility in tree modeling and overall shape control. We perform user studies and show a variety of realistic tree models generated instantaneously from 3D finger sketching.},
  keywords={Geometry;Solid modeling;Three-dimensional displays;Shape;Computational modeling;Fingers;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Shape modeling},
  doi={10.1109/VR50410.2021.00110},
  ISSN={2642-5254},
  month={March},}@INPROCEEDINGS{9417734,
  author={Tang, Zhenyu and Meng, Hsien-Yu and Manocha, Dinesh},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Learning Acoustic Scattering Fields for Dynamic Interactive Sound Propagation}, 
  year={2021},
  volume={},
  number={},
  pages={835-844},
  abstract={We present a novel hybrid sound propagation algorithm for interactive applications. Our approach is designed for dynamic scenes and uses a neural network-based learned scattered field representation along with ray tracing to generate specular, diffuse, diffraction, and occlusion effects efficiently. We use geometric deep learning to approximate the acoustic scattering field using spherical harmonics. We use a large 3D dataset for training, and compare its accuracy with the ground truth generated using an accurate wave-based solver. The additional overhead of computing the learned scattered field at runtime is small and we demonstrate its interactive performance by generating plausible sound effects in dynamic scenes with diffraction and occlusion effects. We demonstrate the perceptual benefits of our approach based on an audio-visual user study.},
  keywords={Training;Three-dimensional displays;Runtime;Diffraction;Heuristic algorithms;Acoustic scattering;Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
  doi={10.1109/VR50410.2021.00111},
  ISSN={2642-5254},
  month={March},}

