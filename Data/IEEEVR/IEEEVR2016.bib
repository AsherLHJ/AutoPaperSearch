@INPROCEEDINGS{7504682,
  author={Argelaguet, Ferran and Hoyet, Ludovic and Trico, Michael and Lecuyer, Anatole},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The role of interaction in virtual embodiment: Effects of the virtual hand representation}, 
  year={2016},
  volume={},
  number={},
  pages={3-10},
  abstract={How do people appropriate their virtual hand representation when interacting in virtual environments? In order to answer this question, we conducted an experiment studying the sense of embodiment when interacting with three different virtual hand representations, each one providing a different degree of visual realism but keeping the same control mechanism. The main experimental task was a Pick-and-Place task in which participants had to grasp a virtual cube and place it to an indicated position while avoiding an obstacle (brick, barbed wire or fire). An additional task was considered in which participants had to perform a potentially dangerous operation towards their virtual hand: place their virtual hand close to a virtual spinning saw. Both qualitative measures and questionnaire data were gathered in order to assess the sense of agency and ownership towards each virtual hand. Results show that the sense of agency is stronger for less realistic virtual hands which also provide less mismatch between the participant's actions and the animation of the virtual hand. In contrast, the sense of ownership is increased for the human virtual hand which provides a direct mapping between the degrees of freedom of the real and virtual hand.},
  keywords={Avatars;Grasping;Virtual environments;Visualization;Synchronous motors;Rubber;Animation;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Evaluation/Methodology;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2016.7504682},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504683,
  author={Lee, Myungho and Kim, Kangsoo and Daher, Salam and Raij, Andrew and Schubert, Ryan and Bailenson, Jeremy and Welch, Greg},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The wobbly table: Increased social presence via subtle incidental movement of a real-virtual table}, 
  year={2016},
  volume={},
  number={},
  pages={11-17},
  abstract={While performing everyday interactions, we often incidentally touch and move objects in subtle ways. These objects are not necessarily directly related to the task at hand, and the movement of an object might even be entirely unintentional. If another person is touching the object at the same time, the movement can transfer through the object and be experienced — however subtly — by the other person. For example, when one person hands a drink to another, at some point both individuals will be touching the glass, and consequently exerting small (often unnoticed) forces on the other person. Despite the frequency of such subtle incidental movements of shared objects in everyday interactions, few have examined how these movements affect human-virtual human (VH) interaction. We ran an experiment to assess how presence and social presence are affected when a person experiences subtle, incidental movement through a shared real-virtual object. We constructed a real-virtual room with a table that spanned the boundary between the real and virtual environments. The participant was seated on the real side of the table, which visually extended into the virtual world via a projection screen, and the VH was seated on the virtual side of the table. The two interacted by playing a game of “Twenty Questions,” where one player asked the other a series of 20 yes/no questions to deduce what object the other player was thinking about. During the game, the “wobbly” group of subjects experienced subtle incidental movements of the real-virtual table: the entire real-virtual table tilted slightly away/toward the subject when the virtual/real human leaned on it. The control group also played the same game, except the table did not wobble. Results indicate that the wobbly group had higher presence and social presence with the virtual human in general, with statistically significant increases in presence, co-presence, and attentional allocation. We present the experiment and results, and discuss some potential implications for virtual human systems and some potential future studies.},
  keywords={Electronic mail;Measurement by laser beam;Virtual environments;Games;Training;Haptic interfaces;Force;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology},
  doi={10.1109/VR.2016.7504683},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504684,
  author={Otsuka, Kazuhiro},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={MMSpace: Kinetically-augmented telepresence for small group-to-group conversations}, 
  year={2016},
  volume={},
  number={},
  pages={19-28},
  abstract={A novel research prototype, called MMSpace, was developed for realistic social telepresence in small group-to-group conversations. MMSpace consists of kinetic display avatars, which can change pose and position by automatically mirroring the remote user's head motions. To fully explore its potential beyond previous alternatives, MMSpace has the following novel features. First, it targets symmetric group-to-group telepresence. Second, the kinetic avatars of MMSpace can produce highly accurate, low latency, and silent physical motions, by using 4-Degree-of-Freedom (DoF) direct-drive actuators, and they can express a wide range of natural human behaviors like head gestures and changing attitudes, as well as indicating the focus of attention. Third, MMSpace supports eye contact between every pair of participants, by integrating i) directional visual attention cues indicated by avatar's kinetic pose change, ii) line-of-sight alignment among the positions of persons, avatars and cameras, and iii) attention-based camera switching, which allows an avatar to always show its owner's face looking directly toward the person that the avatar's owner is looking at. The prototype targets the 2 × 2 setting, and subjective evaluations based on group discussions indicate that the kinetic display avatar is superior to static displays in various aspects including gaze-awareness, eye-contact, perception of other nonverbal behaviors, mutual understanding, and sense of telepresence.},
  keywords={Kinetic theory;Avatars;Cameras;Face;Visualization;Switches;H1.2 [Models and Princiles]: User/Machine Systems — Human factors;H4.3 [Information Systems Applications]: Communications Applications — Computer conferencing, teleconferencing, and videoconferencing},
  doi={10.1109/VR.2016.7504684},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504685,
  author={Kim, Sujeong and Bera, Aniket and Best, Andrew and Chabra, Rohan and Manocha, Dinesh},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Interactive and adaptive data-driven crowd simulation}, 
  year={2016},
  volume={},
  number={},
  pages={29-38},
  abstract={We present an adaptive data-driven algorithm for interactive crowd simulation. Our approach combines realistic trajectory behaviors extracted from videos with synthetic multi-agent algorithms to generate plausible simulations. We use statistical techniques to compute the movement patterns and motion dynamics from noisy 2D trajectories extracted from crowd videos. These learned pedestrian dynamic characteristics are used to generate collision-free trajectories of virtual pedestrians in slightly different environments or situations. The overall approach is robust and can generate perceptually realistic crowd movements at interactive rates in dynamic environments. We also present results from preliminary user studies that evaluate the trajectory behaviors generated by our algorithm.},
  keywords={Trajectory;Heuristic algorithms;Solid modeling;Adaptation models;Videos;Computational modeling;Dynamics},
  doi={10.1109/VR.2016.7504685},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504686,
  author={Hummel, Johannes and Dodiya, Janki and Center, German Aerospace and Eckardt, Laura and Wolff, Robin and Gerndt, Andreas and Kuhlen, Torsten W.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A lightweight electrotactile feedback device for grasp improvement in immersive virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={39-48},
  abstract={An immersive virtual environment is the ideal platform for the planning and training of on-orbit servicing missions, as it provides a flexible and safe environment. In such kind of virtual assembly simulation, grasping virtual objects is one of the most common and natural interactions. However, unlike grasping objects in the real world, it is a non-trivial task in virtual environments, where the primary feedback is visual only. A lot of research investigated ways to provide haptic feedback, such as force, vibrational and electrotactile feedback. Such devices, however, are usually uncomfortable and hard to integrate in projection-based immersive YR systems. In this paper, we present a novel, small and lightweight electro-tactile feedback device, specifically designed for immersive virtual environments. It consists of a small tactor with eight electrodes for each finger and a signal generator attached to the user's hand or arm. Our device can be easily integrated with an existing optical finger tracking system. The study presented in this paper assesses the feasibility and usability of the interaction device. An experiment was conducted in a repeated measures design using the electrotactile feedback modality as independent variable. As benchmark, we chose three typical assembly tasks of a YR simulation for satellite on-orbit servicing missions, including pressing a button, switching a lever switch, and pulling a module from its slot. Results show that electrotactile feedback improved the user's grasping in our virtual on-orbit servicing scenario. The task completion time was significantly lower for all three tasks and the precision of the user's interaction was higher. The workload reported by the participants was significantly lower when using electrotactile feedback. Additionally, users were more confident with their performance while completing the tasks with electrotactile feedback. We describe the device, outline the user study and report the results.},
  keywords={Electrodes;Virtual environments;Grasping;Skin;Thumb;Voltage control;B.4.2 [Input/Output Devices]: Channels and controllers;H.3.4 [Systems and Software]: Performance evaluation;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality},
  doi={10.1109/VR.2016.7504686},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504687,
  author={Hirota, Koichi and Tagawa, Kazuyoshi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Interaction with virtual object using deformable hand}, 
  year={2016},
  volume={},
  number={},
  pages={49-56},
  abstract={This study investigated the implementation of a hand model and contact simulation method for the purpose of improving the reality of object manipulation in a virtual environment. The study focused both on the hand tracking method that takes advantage of nails and also contact simulation using a deformable hand model. The manipulation of an object using a hand, is known to make more frequent use of the fingertips and palm. The proposed method seeks hand form that minimizes the position and orientation errors on those areas. Deformation of the soft tissue of the hand is considered to have an effect on both visual reality and the physical state of contact. In our implementation, the deformation was simulated by FEM and the friction of contact was introduced by the penalty method. In addition, a model that is based on metaballs (or blobs) was employed to represent the smooth surface of the object and to eliminate the problem that derives from polygon modeling. Through experimental implementation, it was proved that object manipulation such as pinching and grasping are possible and that the update rate of simulation can be approximately 50 Hz.},
  keywords={Computational modeling;Deformable models;Solid modeling;Skin;Thumb;Sensors;Deformable Hand Model;Manipulation;Hand-Tracking},
  doi={10.1109/VR.2016.7504687},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504688,
  author={Rehfeld, Stephan and Latoschik, Marc Erich and Tramberend, Henrik},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Estimating latency and concurrency of asynchronous real-time interactive systems using model checking}, 
  year={2016},
  volume={},
  number={},
  pages={57-66},
  abstract={This article introduces model checking as an alternative method to estimate the latency and parallelism of asynchronous Realtime Interactive Systems (RISs). Five typical concurrency and synchronization schemes often found in concurrent Virtual Reality (VR) and computer game systems are identified as use-cases. These use-cases guide the development a) of software primitives necessary for the use-case implementation based on asynchronous RIS architectures and b) of a graphical editor for the specification of various concurrency and synchronization schemes (including the use-cases) based on these primitives. Several model-checking tools are evaluated against typical requirements in the RIS area. As a result, the formal model checking language Rebeca and its model checker RMC are applied to the specification of the use-cases to estimate latency and parallelism for each case. The estimations are compared to measured results achieved by classical profiling from a real-world application. The estimated results of the latencies by model checking approximated the measured results adequately with a minimal difference of 3.9% in the best case and -26.8% in the worst case. It also detected a problematic execution path not covered by the stochastic nature of the measured profiling samples. The estimated results of the degree of parallelization by model checking are approximated with an minimal difference of 9.3% and a maximal difference of -28.8%. Finally, the effort of model checking is compared to the effort of implementing and profiling a RIS.},
  keywords={Model checking;Concurrent computing;Real-time systems;Computational modeling;Rendering (computer graphics);Engines;Parallel processing;D.2.4 [Software/Program Verification]: Model checking —;D.2.8 [Metrics]: Performance measures —;D.4.8 [Performance]: Modeling and prediction —},
  doi={10.1109/VR.2016.7504688},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504689,
  author={Steed, Anthony and Pan, Ye and Zisch, Fiona and Steptoe, William},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The impact of a self-avatar on cognitive load in immersive virtual reality}, 
  year={2016},
  volume={},
  number={},
  pages={67-76},
  abstract={The use of a self-avatar inside an immersive virtual reality system has been shown to have important effects on presence, interaction and perception of space. Based on studies from linguistics and cognition, in this paper we demonstrate that a self-avatar may aid the participant's cognitive processes while immersed in a virtual reality system. In our study participants were asked to memorise pairs of letters, perform a spatial rotation exercise and then recall the pairs of letters. In a between-subject factor they either had an avatar or not, and in a within-subject factor they were instructed to keep their hands still or not. We found that participants who both had an avatar and were allowed to move their hands had significantly higher letter pair recall. There was no significant difference between the other three conditions. Further analysis showed that participants who were allowed to move their hands, but could not see the self-avatar, usually didn't move their hands or stopped moving their hands after a short while. We argue that an active self-avatar may alleviate the mental load of doing the spatial rotation exercise and thus improve letter recall. The results are further evidence of the importance of an appropriate self-avatar representation in immersive virtual reality.},
  keywords={Cognition;Avatars;Solid modeling;Virtual environments;Visualization;Estimation;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2016.7504689},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504690,
  author={Ju, Uijong and Kang, June and Wallraven, Christian},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Personality differences predict decision-making in an accident situation in virtual driving}, 
  year={2016},
  volume={},
  number={},
  pages={77-82},
  abstract={Understanding how humans make decisions in challenging situations — such as trying to save peoples' lives even though this endangers one's own life — is crucial in optimizing rescue operations and dealing with natural disasters and other crises. The experimental study of these decisions, however, has often been done using text-based surveys, which is known to emphasize rational and reflective judgments. Here, we used virtual reality to investigate decision-making in a real-world context, in which a decision needs to be made intuitively under time pressure — for this we simulated an accident situation in an immersive virtual driving scenario. In the scenario, participants were told to race a course as fast as possible. After training, participants were confronted with the sudden appearance of pedestrians on the race course. We observed three different behaviors: group one ignored the pedestrians and/or hit the accelerator, group two hit the brake, and group three tried to steer the car to avoid pedestrians. We found that most Avoid-group participants had more real and virtual driving experience compared to the other two groups and they also felt more competent during the game as measured by subjective game experience questionnaires. Importantly, results from established personality questionnaires showed that participants who did not brake (therefore potentially harming the pedestrians) had significantly lower scores in perspective-taking and higher scores in psychopathy compared to participants who tried to avoid the accident situation. Our results demonstrate that personality differences to some degree are able to predict intuitive decision-making and that such processes can be studied in a controlled, immersive VR simulation.},
  keywords={Brakes;Games;Decision making;Training;Accidents;Solid modeling;Virtual reality;Virtual reality;decision-making;driving Simulation;moral judgments;personality differences},
  doi={10.1109/VR.2016.7504690},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504691,
  author={Tatzgern, Markus and Orso, Valeria and Kalkofen, Denis and Jacucci, Giulio and Gamberini, Luciano and Schmalstieg, Dieter},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Adaptive information density for augmented reality displays}, 
  year={2016},
  volume={},
  number={},
  pages={83-92},
  abstract={Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.},
  keywords={Clutter;Data visualization;Visualization;Browsers;Semantics;Electronic mail;Clustering algorithms},
  doi={10.1109/VR.2016.7504691},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504692,
  author={Ren, Donghao and Goldschwendt, Tibor and Chang, YunSuk and Höllerer, Tobias},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Evaluating wide-field-of-view augmented reality with mixed reality simulation}, 
  year={2016},
  volume={},
  number={},
  pages={93-102},
  abstract={Full-surround augmented reality, with augmentations spanning the entire human field of view and beyond, is an under-explored topic since there is currently no hardware that can support it. As current AR displays only support relatively small fields of view, most AR applications to-date employ relatively small point-based annotations of the physical world. Anticipating a change in AR capabilities, we experiment with wide-field-of-view annotations that link elements far apart in the visual field. We have built a system that uses full-surround virtual reality to simulate augmented reality with different field of views, with and without tracking artifacts. We conducted a study comparing user performance on five different task groups within an information-seeking scenario, comparing two different fields of view and presence and absence of tracking artifacts. A constrained field of view significantly increased task completion time. We found indications for task time effects of tracking artifacts to vary depending on age.},
  keywords={Augmented reality;Solid modeling;Visualization;Three-dimensional displays;Performance evaluation;Hardware;H.5.1 [Information Interfaces and Presentation (e.g., HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality},
  doi={10.1109/VR.2016.7504692},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504693,
  author={Jun, Hanseul and Kim, Gunhee},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A calibration method for optical see-through head-mounted displays with a depth camera}, 
  year={2016},
  volume={},
  number={},
  pages={103-111},
  abstract={We propose a fast and accurate calibration method for the optical see-through (OST) head-mounted displays (HMD), taking advantage of a low-cost time-of-flight depth-camera. Recently, affordable OST-HMDs and depth-cameras are widely appearing in the commercial market. In order to correctly reflect the user experience into the calibration process, our method demands a user wearing the HMD to repeatedly point at rendered virtual circles with their fingertips. From the repeated calibration data, we perform two stages of full calibration and simplified calibration, to compute key calibration parameters. The full calibration is required when the depth-camera is first installed to the HMD, and afterwards only the simplified calibration is performed whenever a user wears it again. Our experimental results show that the full and simplified calibration can be achieved with 10 and 5 user's repetitions (theoretically 3 and 2 at minimum), which are significantly less than about 20 of the stereo-SPAAM, one of the most popular existing calibration techniques. We also demonstrate that the 3D position errors of our calibration become much quickly smaller than those of the state-of-the-art method.},
  keywords={Calibration;Cameras;Three-dimensional displays;Adaptive optics;Head;Magnetic heads;Mathematical model;Calibration;optical see-through head-mounted display;depth-camera},
  doi={10.1109/VR.2016.7504693},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504694,
  author={Koulieris, George Alex and Drettakis, George and Cunningham, Douglas and Mania, Katerina},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Gaze prediction using machine learning for dynamic stereo manipulation in games}, 
  year={2016},
  volume={},
  number={},
  pages={113-120},
  abstract={Comfortable, high-quality 3D stereo viewing is becoming a requirement for interactive applications today. Previous research shows that manipulating disparity can alleviate some of the discomfort caused by 3D stereo, but it is best to do this locally, around the object the user is gazing at. The main challenge is thus to develop a gaze predictor in the demanding context of real-time, heavily task-oriented applications such as games. Our key observation is that player actions are highly correlated with the present state of a game, encoded by game variables. Based on this, we train a classifier to learn these correlations using an eye-tracker which provides the ground-truth object being looked at. The classifier is used at runtime to predict object category — and thus gaze — during game play, based on the current state of game variables. We use this prediction to propose a dynamic disparity manipulation method, which provides rich and comfortable depth. We evaluate the quality of our gaze predictor numerically and experimentally, showing that it predicts gaze more accurately than previous approaches. A subjective rating study demonstrates that our localized disparity manipulation is preferred over previous methods.},
  keywords={Games;Three-dimensional displays;Real-time systems;Stereo image processing;Gaze tracking;Manipulator dynamics;Context;Gaze Prediction;Stereo Grading;Perception},
  doi={10.1109/VR.2016.7504694},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504695,
  author={Bekele, E. and Wade, J. and Bian, D. and Fan, J. and Swanson, Amy and Warren, Z. and Sarkar, N.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Multimodal adaptive social interaction in virtual environment (MASI-VR) for children with Autism spectrum disorders (ASD)}, 
  year={2016},
  volume={},
  number={},
  pages={121-130},
  abstract={Difficulties in social interaction, verbal and non-verbal communications as well as repetitive and atypical patterns of behavior, characterizes Autism spectrum disorders (ASD). A number of studies indicated that many children with ASD prefer technology and this preference can be explored to develop systems that may alleviate several challenges of traditional treatment and intervention. As a result, recent advances in computer and robotic technology are ushering in innovative assistive technologies for ASD intervention. The current work presents design, development and a usability study of an adaptive multimodal virtual reality-based social interaction platform for children with ASD. It is hypothesized that endowing a technological system that can detect the processing pattern and mental state of the child using implicit cues from eye tracking and electrophysiological, including peripheral physiological and electroencephalography (EEG), signals and adapt its interaction accordingly is of great importance in assisting and individualizing traditional intervention approaches. The presented VR system is based on a virtual reality based social environment, a school cafeteria, where an individual with ASD interacts with virtual characters. An eye tracker, an EEG monitor and biosensors to measure peripheral electrophysiological signals are integrated with the VR task environment to obtain gaze, EEG signals and several peripheral physiological signals in real-time. In the current work, we show how eye gaze and task performance can be used in real-time to adapt intervention in VR. The other signals are collected for offline analysis. The results from a usability study with 12 subjects with ASD are presented to demonstrate the viability of the proposed concepts within the VR system.},
  keywords={Biomedical monitoring;Face;Electroencephalography;Engines;Emotion recognition;Context;Autism;3D social Interaction;multimodal interaction;psychology;usability;VR-based adaptive systems},
  doi={10.1109/VR.2016.7504695},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504696,
  author={Parmar, Dhaval and Isaac, Joseph and Babu, Sabarish V. and D'Souza, Nikeetha and Leonard, Alison E. and Jörg, Sophie and Gundersen, Kara and Daily, Shaundra B.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Programming moves: Design and evaluation of applying embodied interaction in virtual environments to enhance computational thinking in middle school students}, 
  year={2016},
  volume={},
  number={},
  pages={131-140},
  abstract={We detail the design, implementation, and an initial evaluation of a virtual reality education and entertainment (edutainment) application called Virtual Environment Interactions (VEnvI). VEnvI is an application in which students learn computer science concepts through the process of choreographing movement for a virtual character using a fun and intuitive interface. In this exploratory study, 54 participants as part of a summer camp geared towards promoting participation of women in science and engineering programmatically crafted a dance performance for a virtual human. A subset of those students (16) participated in an immersive embodied interaction metaphor in VEnvI. In creating this metaphor that provides first-person, embodied experiences using self-avatars, we seek to facilitate engagement, excitement and interest in computational thinking. We qualitatively and quantitatively evaluated the extent to which the activities of the summer camp, programming the dance moves, and the embodied interaction within VEnvI facilitated students' edutainment, presence, interest, excitement, and engagement in computing, and the potential to alter their perceptions of computing and computer scientists. Results indicate that students enjoyed the experience and successfully engaged the virtual character in the immersive embodied interaction, thus exhibiting high telepresence and social presence. Students also showed increased interest and excitement regarding the computing field at the end of their summer camp experience using VEnvI.},
  keywords={Education;Programming profession;Virtual reality;Computers;Cognition;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality;K.3.2 [Computers and Education]: Computer and Information Science Education — Computer Science Education},
  doi={10.1109/VR.2016.7504696},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504697,
  author={Rahimian, Pooya and O'Neal, Elizabeth E. and Yon, Junghum Paul and Franzen, Luke and Jiang, Yuanyuan and Plumert, Jodie M. and Kearney, Joseph K.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Using a virtual environment to study the impact of sending traffic alerts to texting pedestrians}, 
  year={2016},
  volume={},
  number={},
  pages={141-149},
  abstract={This paper presents an experiment conducted in a large-screen immersive virtual environment to evaluate how texting pedestrians respond to permissive traffic alerts delivered via their cell phone. We developed a cell phone app that delivered information to texting pedestrians about when traffic conditions permit safe crossing. We compared gap selection and movement timing in three groups of pedestrians: texting, texting with alerts, and no texting (control). Participants in the control and alert groups chose larger gaps and were more discriminating in their gap choices than participants in the texting group. Both the control and alert groups had more time to spare than the texting group when they exited the roadway even though the alert group timed their entry relative to the lead car less tightly than the control and texting groups. By choosing larger gaps, participants in the alert group were able to compensate for their poorer timing of entry, resulting in a margin of safety that did not differ from those who were not texting. However, they also relied heavily on the alert system and paid less attention to the roadway. The discussion focuses on the potential of assistive technologies based on Vehicle-to-Pedestrian (V2P) communications technology for mitigating pedestrian-motor vehicle crashes.},
  keywords={Roads;Vehicles;Cellular phones;Mobile handsets;Virtual environments;Safety;Electronic mail;Pedestrian simulation;Texting;Mobile device use;Pedestrian safety;Connected vehicles technology;Vehicle-to-pedestrian (V2P) communication},
  doi={10.1109/VR.2016.7504697},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504698,
  author={},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Posters}, 
  year={2016},
  volume={},
  number={},
  pages={1-1},
  abstract={Start of the above-titled section of the conference proceedings record.},
  keywords={},
  doi={10.1109/VR.2016.7504698},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504699,
  author={Bodenheimer, Bobby and Wang, Yiming and Maloney, Divine and Rieser, John},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Induction of linear and circular vection in real and virtual worlds}, 
  year={2016},
  volume={},
  number={},
  pages={153-154},
  abstract={Vection is the illusion of self-motion, usually induced by a visual stimulus. It is important in virtual reality because inducing it in motion simulations can lead to improved experiences. We examine linear and circular vection in commodity level head-mounted displays. We compare the experience of circular vection induced through a real world stimulus, an optokinetic drum, with that experienced through a virtual stimulus. With virtual stimuli, we also compare circular vection with linear horizontal and linear vertical vection. Finally, we examine circular and linear vection in more naturalistic virtual environments. Linear vection was induced more rapidly than any other type, but circular vection occurs more rapidly with a real world stimulus than a virtual one. Our results have practical application and can inform virtual reality design that uses head-mounted display technology and wishes to establish vection. Circular vection can be induced easily and rapidly through visual mechanisms that have been known since Mach [1]. We have an optokinetic drum that is capable of inducing compelling circular vection that works in almost all people we have tested in it. The goal of this project is thus twofold: (1) to see how well and how reliably vection can be induced in virtual reality with commodity-level HMDs as compared to real-world stimuli, and (2) to see if compelling vection could be comparably induced when the stimuli are complex naturalistic scenes, not abstract optic flow patterns.},
  keywords={Virtual environments;Analysis of variance;Visualization;Solid modeling;Software;Wireless communication},
  doi={10.1109/VR.2016.7504699},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504700,
  author={Bonsch, Andrea and Freitag, Sebastian and Kuhlen, Torsten W.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Automatic generation of world in miniatures for realistic architectural immersive virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={155-156},
  abstract={Orientation and wayfinding in architectural Immersive Virtual Environments (IVEs) are non-trivial, accompanying tasks which generally support the users' main task. World in Miniatures (WIMs) - essentially 3D maps containing a scene replica - are an established approach to gain survey knowledge about the virtual world, as well as information about the user's relation to it. However, for large-scale, information-rich scenes, scaling and occlusion issues result in diminishing returns. Since there typically is a lack of standardized information regarding scene decompositions, presenting the inside of self-contained scene extracts is challenging. Therefore, we present an automatic WIM generation workflow for arbitrary, realistic in- and outdoor IVEs in order to support users with meaningfully selected and scaled extracts of the IVE as well as corresponding context information. Additionally, a 3D user interface is provided to manually manipulate the represented extract.},
  keywords={Three-dimensional displays;Context;Data mining;Solid modeling;Buildings;Runtime;Geometry;I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction Techniques},
  doi={10.1109/VR.2016.7504700},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504701,
  author={Borst, Christoph W. and Ritter, Kenneth A. and Chambers, Terrence L.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Virtual energy center for teaching alternative energy technologies}, 
  year={2016},
  volume={},
  number={},
  pages={157-158},
  abstract={We overview the Virtual Energy Center, a VR environment that models a real energy facility to enable virtual field trips and self-guided exploration. Our goal is to take advantage of emerging low-cost hardware and improved networks to provide students who cannot travel to the real facility with alternatives that provide comparable educational benefit. The virtual facility is augmented by visual guides and educational content to teach students about concentrating solar power technology. A teacher physically near the student can appear in the scene via depth camera imagery, allowing the teacher to walk around in a classroom setting and assist students. Additionally, work-in-progress is streaming the depth images over a network to allow students to virtually meet expert guides from the real facility. We summarize these features, some interaction-related challenges, and ongoing testing.},
  keywords={Cameras;Virtual reality;Visualization;Games;Education;Solid modeling;Tracking;Virtual Reality;Education;Alternative Energy},
  doi={10.1109/VR.2016.7504701},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504702,
  author={Boustila, Sabah and Capobianco, Antonio and Génevaux, Olivier and Bechmann, Dominique},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={New hybrid projection to widen the vertical field of view with large screen to improve the perception of personal space in architectural project review}, 
  year={2016},
  volume={},
  number={},
  pages={159-160},
  abstract={In architectural project review, the perception of the near surrounding ground is important for the evaluation of the virtual environment (VE). This near surrounding ground is missing when using wall screens. To address this problem we suggest increasing the Vertical Field of View (VFoV). One solution is the use of the rendering approach such as non-planar projection. However, simply increasing the FoV of the rendering leads to much distortion in the VE which is not suitable for architectural project review. We propose a new hybrid projection combining a perspective projection in the center of the screen and a cylindrical-like projection on the top and bottom boarders. By this, we increase VFoV without incurring large deformations to preserve the perception of distances and to allow seeing the surrounding ground. We also report the results of an experiment we conducted to evaluate distance perception with our projection.},
  keywords={Rendering (computer graphics);Electronic mail;Navigation;Hafnium compounds;Estimation;Distortion;Virtual reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality},
  doi={10.1109/VR.2016.7504702},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504703,
  author={Bui, Giang and Morago, Brittany and Le, Truc and Karsch, Kevin and Lu, Zheyu and Duan, Ye},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Integrating videos with LIDAR scans for virtual reality}, 
  year={2016},
  volume={},
  number={},
  pages={161-162},
  abstract={LIDAR range scans can be used to quickly create accurate 3D models for virtual reality and as a basis to visualize sets of photographs, videos, and virtual objects in a cohesive environment. The number of existing virtual reality programs that use LIDAR data as input has motivated our group to develop methods for fusing images with 3D scans and for augmenting the scans with both dynamic objects present in videos and virtual models. Bringing together as many data sources as possible increases users' abilities to present related information in one, intuitive venue. We demonstrate how to register a variety of 2D imagery with a range scan to construct photo-realistic models and to extract walking people captured in videos and model them in a 3D space. We also present a method for determining the sun position from a set of stitched photographs in order to apply correct lighting to virtual objects placed amongst real world data. Naturally lit objects can be inserted into original photographs using our 2D-3D registration information. These methods are all combined to display and study photos, videos, and virtual objects in a complete 3D environment.},
  keywords={Three-dimensional displays;Videos;Solid modeling;Laser radar;Cameras;Virtual reality;Image segmentation},
  doi={10.1109/VR.2016.7504703},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504704,
  author={Carvalho, Alexandre and Cardoso, Alexandre and Barreto, Camilo and Lamounier, Edgard and Lima, Gerson F M and Mattioli, Leandro R and Miranda, Milton and Prado, Paulo R M},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A methodology for reducing the time necessary to generate virtual electric substations}, 
  year={2016},
  volume={},
  number={},
  pages={163-164},
  abstract={In this work, an electric power energy utility company, called CEMIG, is a research partner with more than 50 electric substations, whose operation can be improved by using virtual environments. Therefore, time to model all these substations, with a high-level of required photorealism, is a critical issue. To achieve this goal, this paper presents a pertinent and appropriate methodology. First, data is retrieved from field components (CADs, satellite images, manufacturer sheets etc.) to model suitable electric components by means of dimensioning and angles. Next, rules such as cable connectors positioning and monitoring of the quantity of polygons (low-poly) are established. In addition, since each electric substation has circuit arrangements composed of different electric components, a pattern recognition tool is applied to extract information from 2D basic plants in order to generate automatic positioning of components within a virtual substation. Also, considering the need for control and monitoring of the electric system, in real time, a set of interface templates are provided to support direct access to data from supervisory system (SCADA), without the loss of immersion and navigation which are imperative for Virtual Reality applications. In the very first trials used for generating a virtual electric substation a lot of work and time was spent by our research team. After the establishment of the proposed methodology, results show that the time to generate new substations was reduced by the order of 83%.},
  keywords={Substations;Three-dimensional displays;Solid modeling;Virtual environments;Design automation;Monitoring;Virtual reality;electric substations;time-to-market;CAD-to-VR},
  doi={10.1109/VR.2016.7504704},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504705,
  author={Daher, Salam and Kim, Kangsoo and Lee, Myungho and Raij, Andrew and Schubert, Ryan and Bailenson, Jeremy and Welch, Greg},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Exploring social presence transfer in real-virtual human interaction}, 
  year={2016},
  volume={},
  number={},
  pages={165-166},
  abstract={We explore whether a peripheral observation of apparent mutual social presence between a real human (RH) and a virtual human (VH) can in turn increase a subject's sense of social presence with the VH. In other words, we explore whether social presence can “transfer” from one RH-VH interaction to another. Specifically, we carried out an experiment where human subjects were asked to play a game with a VH. As they entered the game room, approximately half of the subjects were exposed to a brief but apparently engaging conversation between an RH and the VH. The subjects who were exposed to the brief RH-VH interaction had significantly higher measures of both emotional connection and the attentional allocation dimension of social presence for the VH, compared to those who were not. We describe the motivation, the experiment, and the results.},
  keywords={Games;Electronic mail;Psychology;Resource management;Medical services;Virtual environments;Training;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented, and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology},
  doi={10.1109/VR.2016.7504705},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504706,
  author={Dallaire-Côté, Mikael and Charbonneau, Philippe and Côté, Sara St-Pierre and Aissaoui, Rachid and Labbe, David R.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Animated self-avatars for motor rehabilitation applications that are biomechanically accurate, low-latency and easy to use}, 
  year={2016},
  volume={},
  number={},
  pages={167-168},
  abstract={The emerging use of self-avatars for physical and motor rehabilitation leads to specific requirements for their real-time animation that combine properties from the fields of computer graphics and of biomechanics. We present a method for animating a self-avatar in real-time that allows for high-fidelity representation of whole-body kinematics using anatomical and reproducible bone-segment definition. The method requires little setup time and has low motion-to-photon latency.},
  keywords={Avatars;Animation;Real-time systems;Kinematics;Knee;Biomechanics;Calibration;Animation;forward kinematics;rehabilitation;self-avatar},
  doi={10.1109/VR.2016.7504706},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504707,
  author={Davis, Matthew M. and Gabbard, Joseph L. and Bowman, Doug A. and Gracanin, Dennis},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Depth-based 3D gesture multi-level radial menu for virtual object manipulation}, 
  year={2016},
  volume={},
  number={},
  pages={169-170},
  abstract={In this work, we present a depth-based solution to multi-level menus for selection and manipulation of virtual objects using freehand gestures. Navigation between and through menus is performed using three gesture states that utilize X, Y translations of the finger with boundary crossing. Although presented in a single context, this menu structure can be applied to a myriad of domains requiring several levels of menu data, and serves to supplement existing and emerging menu design for augmented, virtual, and mixed-reality applications.},
  keywords={Thumb;Navigation;Virtual reality;User interfaces;Indexes;Visualization;Natural user interface;translation;boundary crossing},
  doi={10.1109/VR.2016.7504707},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504708,
  author={Eck, Ulrich and Hoang, Liem and Sandor, Christian and Yamamoto, Goshiro and Taketom, Takafumi and Kato, Hirokazu and Laga, Hamid},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Exploring the perception of co-location errors during tool interaction in visuo-haptic augmented reality}, 
  year={2016},
  volume={},
  number={},
  pages={171-172},
  abstract={Co-located haptic feedback in mixed and augmented reality environments can improve realism and user performance, but it also requires careful system design and calibration. In this poster, we determine the thresholds for perceiving co-location errors through two psychophysics experiments in a typical fine-motor manipulation task. In these experiments we simulate the two fundamental ways of implementing VHAR systems: first, attaching a real tool; second, augmenting a virtual tool. We determined the just-noticeable co-location errors for position and orientation in both experiments and found that users are significantly more sensitive to co-location errors with virtual tools. Our overall findings are useful for designing visuo-haptic augmented reality workspaces and calibration procedures.},
  keywords={Haptic interfaces;Augmented reality;Visualization;Calibration;Performance evaluation;Rendering (computer graphics);Australia;H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems — Artificial, augmented and virtual realities},
  doi={10.1109/VR.2016.7504708},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504709,
  author={Feng, Mi and Dey, Arindam and Lindeman, Robert W.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The effect of multi-sensory cues on performance and experience during walking in immersive virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={173-174},
  abstract={To examine the effects of multi-sensory cues during non-fatiguing walking in immersive virtual environments, we selected sensory cues including movement wind, directional wind, footstep vibration, and footstep sounds, and investigated their influence and interaction with each other. We developed a virtual reality system with non-fatiguing walking interaction and low-latency, multi-sensory feedback, and used it to conduct two successive experiments measuring user experience and performance through a triangle-completion task. We noticed some positive effects due to the addition of footstep vibration on task performance, and saw significant improvement in reported user experience due to the added wind and vibration cues.},
  keywords={Legged locomotion;Virtual environments;Australia;Navigation;Conferences;Vibrations;Immersive Virtual Environments;Multi-sensory Cues},
  doi={10.1109/VR.2016.7504709},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504710,
  author={Fountain, Jake and Smith, Shamus P.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Automatic identification of rigidly linked 6DoF sensors}, 
  year={2016},
  volume={},
  number={},
  pages={175-176},
  abstract={We present techniques for automatically identifying relationships between rigidly linked 6DoF and 3DoF sensors belonging to different sensor systems. The techniques allow for subsequent automatic alignment of the sensor systems, increasing the usability of modular sensor systems. Two techniques are presented and analysed in simulation and a case study for performance under varying noise and latency conditions. Good results were achieved, with each sensor identified correctly in at least 60% of estimates, or 4 times greater than random selection. After a sample collection period of around 5 seconds, the matching is performed in less than 5ms and is scalable to noisier systems by using more samples. Our methods represent a key step in creating highly accessible modular multi-device 3D systems.},
  keywords={Sensor systems;Calibration;Knee;Transforms;Robot sensing systems;tracking;input devices;calibration;usability;sensors},
  doi={10.1109/VR.2016.7504710},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504711,
  author={Hashiguchi, Satoshi and Shibata, Fumihisa and Kimura, Asako},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Psychophysical influence on temperature perception by mixed-reality visual stimulation}, 
  year={2016},
  volume={},
  number={},
  pages={177-178},
  abstract={In Mixed-Realty (MR) space, the visual appearance (shape, texture, etc.) of a real object can be changed by superimposing a virtual object on it. Therefore, by creating systematic differences between visual and haptic perceptions using MR technology, we can analyze their influence on temperature perception. In our research group, we defined the changes in the visual information of a real object in MR space as “MR visual stimulation” and examine the influence of the haptic sense using MR visual stimulation [1]. For our research, we focused on the temperature perception of the skin. In the first step, we verified the influence presenting MR visual stimulation has on the perceived position of the temperature perception. In the experiment, we presented MR visual stimulation and temperature stimulation in different positions. We confirmed the influence this difference has on the temperature perceived position. Our results demonstrate that temperature perception is strongly affected by visual stimulation.},
  keywords={Visualization;Virtual reality;Information science;Decision support systems;Conferences;Green products;Control systems;Temperature Perception;Mixed-Reality},
  doi={10.1109/VR.2016.7504711},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504712,
  author={Hashimoto, Kazuki and Nakamoto, Takamichi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Olfactory display using surface acoustic wave device and micropumps for wearable applications}, 
  year={2016},
  volume={},
  number={},
  pages={179-180},
  abstract={Olfaction is expected to provide reality and a sense of immersion in multimedia contents. Therefore, an olfactory display, a gadget to present scents to one or more user(s), has been developed. A wearable olfactory display has advantage from the viewpoint of reducing the odorant diffusion into the atmosphere and is suitable for virtual reality applications. In this study, we developed a portable olfactory display using surface acoustic wave (SAW) device and micropumps. In the experiment using quartz crystal microbalance (QCM) gas sensor, we confirmed that the olfactory display can present the odorant with intended intensity.},
  keywords={Olfactory;Micropumps;Surface acoustic wave devices;Liquids;Virtual reality;Surface acoustic waves;Compounds;Olfactory display SAW;micropump;QCM gas sensor},
  doi={10.1109/VR.2016.7504712},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504713,
  author={Hughes, Charles E. and Ingraham, Kathleen M.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={De-escalation training in an augmented virtuality space}, 
  year={2016},
  volume={},
  number={},
  pages={181-182},
  abstract={This poster describes the TeachLivE paradigm, its application to de-escalation training, especially for law enforcement personnel, and the realization of this in a four-walled augmented virtuality space. Emphasis is placed on how the resulting physical presence increases co-presence and social presence, leading to an immersive and effective learning environment.},
  keywords={Law enforcement;Avatars;Training;Augmented virtuality;Virtual environments;Personnel;Augmented virtuality;de-escalation;digital puppetry;law enforcement;teacher preparation;virtual environments},
  doi={10.1109/VR.2016.7504713},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504714,
  author={Hutton, Courtney and Suma, Evan},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A realistic walking model for enhancing redirection in virtual reality}, 
  year={2016},
  volume={},
  number={},
  pages={183-184},
  abstract={Redirected walking algorithms require the prediction of human motion in order to effectively steer users away from the boundaries of the physical space. While a virtual walking trajectory may be represented using straight lines connecting waypoints of interest, this simple model does not accurately represent typical user behavior. In this poster we present a more realistic walking model for use in real-time virtual environments that employ redirection techniques. We implemented the model within a framework that can be used for simulation of redirected walking within different virtual and physical environments. Such simulations are useful for the evaluation of redirected walking algorithms and the tuning of parameters under varying conditions. Additionally, the model can also be used to animate an artificial humanoid “ghost walker” to provide a visual demonstration of redirected walking in virtual reality.},
  keywords={Legged locomotion;Solid modeling;Biological system modeling;Trajectory;Computational modeling;Virtual reality;Prediction algorithms;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2016.7504714},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504715,
  author={Ikei, Yasushi and Kato, Shunki and Komase, Kohei and Imao, Shogo and Sakurai, Sho and Amemiya, Tomohiro and Kitazaki, Michiteru and Hirota, Koichi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Vestibulohaptic passive stimulation for a walking sensation}, 
  year={2016},
  volume={},
  number={},
  pages={185-186},
  abstract={This paper describes a passive stimulation of a body to evoke a walking sensation using a vestibular and haptic device while the real body of the user is sitting. It imparts a pseudo body image to the user through the real (physical) body of the user as a part of the virtual reality (VR) display system. The created walking sensation was evaluated by nine factors to analyze the complex nature of the walking sensation.},
  keywords={Legged locomotion;Avatars;Haptic interfaces;Electronic mail;Muscles;Solid modeling;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — artificial realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces},
  doi={10.1109/VR.2016.7504715},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504716,
  author={Ishiyama, Hidetoshi and Kurabayashi, Shuichi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Monochrome glove: A robust real-time hand gesture recognition method by using a fabric glove with design of structured markers}, 
  year={2016},
  volume={},
  number={},
  pages={187-188},
  abstract={This paper presents a method for recognizing human-hand postures in real time, even if environment light cannot be configured appropriately. The key technology is a monochrome glove that is patterned with augmented reality (AR) markers on its palm and is also designed with a structured marker on each finger. As the glove only uses white color for the design of the patterns, it can achieve robust gesture recognition in a natural lighting environment by using a single camera to track a hand wearing the glove. The structured marker facilitates the recognition of a fingertip. Our system can recognize at least 96 hand postures by using the features of the fingers. The AR pattern can be of various configurations. Thus, by utilizing different patterned gloves, we can represent different interactions from the same hand posture. The extensive experiments we conducted demonstrate the accuracy, efficiency, and robustness of our gesture recognition method.},
  keywords={Thumb;Image color analysis;Real-time systems;Gesture recognition;Robustness;Augmented reality;human-computer interaction (HCI);hand gesture recognition;augmented reality;fingertips detection;user interface},
  doi={10.1109/VR.2016.7504716},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504717,
  author={Itoh, Yuta and Orlosky, Jason and Huber, Manuel and Kiyokawa, Kiyoshi and Klinker, Gudrun},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={OST Rift: Temporally consistent augmented reality with a consumer optical see-through head-mounted display}, 
  year={2016},
  volume={},
  number={},
  pages={189-190},
  abstract={We present an off-the-shelf, low-latency Optical See-through Head-Mounted Displays (OST-HMD) for Augmented Reality (AR). Temporally consistent visualization is crucial for realizing immersive AR experiences. This is challenging since it requires both accurate head-tracking and low-latency rendering of AR content. Building a system which meets both constraints usually requires experts on computer vision/graphics and expensive display hardware. This work demonstrates that such high spatio-temporal fidelity is achievable with commodity hardware available today. We build a custom OST-HMD system that consists of a virtual reality HMD, i.e., the Oculus Rift DK2, and half-mirror optics, and adapt the rendering pipeline in order to integrate the OST-HMD calibration framework. An evaluation with a user-perspective camera shows that the system achieves mean temporal error of <1 ms (95% reduction of the latency from naive, no-predictive rendering), and median spatial error <0.3° in the viewing angle with maximum error at most 1.0°.},
  keywords={Rendering (computer graphics);Cameras;Three-dimensional displays;Hardware;Pipelines;Calibration;Mirrors;low-cost HMD;post-rendering;optical see-through},
  doi={10.1109/VR.2016.7504717},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504718,
  author={Jiang, Hao and Gao, Chang and Mao, Tianlu and Li, Hui and Wang, Zhaoqi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Groupnect: Integrating group interaction into large display system}, 
  year={2016},
  volume={},
  number={},
  pages={191-192},
  abstract={Large display systems have been successfully applied in virtual reality domains because they can provide full sense of immersion through large visual space and high display resolution. However, only a few users can interact with these systems by using pen-like or marker-based devices. In addition, user experience and application mode are constrained in many areas. In this paper, we propose a novel application framework called “Groupnect”, which gives users unique experience of group interaction in a large display system. By using optical tracking and 3D gesture recognition technologies, our approach can automatically recognize gesture-based control signals for 12 users simultaneously, and the backend system can trigger corresponding actions in real time. We conduct a user study and compare the results with a standard interaction mode. The results demonstrate that our approach greatly increases recorded objective activities and subjective efforts. Moreover, the physical and mental participation of users can be promoted by Groupnect. It indicates great potential to design novel applications in entertainment, education and training areas.},
  keywords={Games;Heart rate;Virtual reality;Real-time systems;Rendering (computer graphics);Visualization;Standards;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation (e.g. HCI)]: User Interfaces — Interaction styles},
  doi={10.1109/VR.2016.7504718},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504719,
  author={Jiang, Yuanyuan and Rahimian, Pooya and O'Neal, Elizabeth E. and Plumert, Jodie M. and Yon, Junghum Paul and Kearney, Joseph K. and Franzen, Luke},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Acting together: Joint pedestrian road crossing in an immersive virtual environment}, 
  year={2016},
  volume={},
  number={},
  pages={193-194},
  abstract={We investigated how two people jointly coordinate their decisions and actions in a co-occupied, large-screen virtual environment. The task for participants was to physically cross a virtual road with continuous traffic without getting hit by a car. Participants performed this task either alone or with another person (see Fig.1). We found that pairs often crossed the same gap together and closely synchronized their movements when crossing. Pairs also chose larger gaps than individuals to accommodate the extra time needed to cross through gaps together. These results reveal how two people interact and coordinate their behaviors in performing whole-body, joint motions. This study also provides a foundation for future studies examining joint actions in shared VEs where participants are represented by graphic avatars.},
  keywords={Roads;Virtual environments;Electronic mail;Synchronization;Avatars;Glass;Immersive virtual environments;joint action;pedestrian road crossing;co-occupied virtual environments},
  doi={10.1109/VR.2016.7504719},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504720,
  author={Kane, Valerie and Smith, Missie and Burnett, Gary and Gabbard, Joseph L. and Large, David},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Depth perception in mirrors: The effects of video-based augmented reality in driver's side view mirrors}, 
  year={2016},
  volume={},
  number={},
  pages={195-196},
  abstract={This study explored the effects that augmented reality graphics have on a drivers' distance estimation/depth perception in an augmented side view mirror. The study was conducted both inside a simulator and outside in a parking lot. Sixteen participants partook in the study, 8 in the simulator and 8 in the test vehicle outside. Distance judgments were compared across four side view mirror conditions both simulator and outdoor scenarios. Results show some opportunities for AR to improve depth judgments, but further analysis is necessary.},
  keywords={Mirrors;Augmented reality;Vehicle dynamics;Accidents;Automobiles;Augmented reality;depth perception;near side mirror},
  doi={10.1109/VR.2016.7504720},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504721,
  author={Kaneto, Yuki and Komuro, Takashi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Space-sharing AR interaction on multiple mobile devices with a depth camera}, 
  year={2016},
  volume={},
  number={},
  pages={197-198},
  abstract={In this paper, we propose a markerless augmented reality (AR) system that works on multiple mobile devices. The relative positions and orientations of the devices and their individual motions are estimated from 3D information in real space obtained by depth cameras attached to the devices. The system allows multiple users to share the AR space and to interact with the same virtual object. To estimate the relative positions and orientations of the devices, the system generates 2D images by looking down from above at the 3D scene obtained by the depth cameras, performs 2D registration using template matching, and obtains a transformation matrix that transforms the coordinate system of one camera to that of another camera. The motion of a camera is estimated using the ICP algorithm to realize markerless AR. Using the proposed system, we created an application that enables multiple users to interact with the same virtual object.},
  keywords={Cameras;Three-dimensional displays;Augmented reality;Mobile handsets;Performance evaluation;Iterative closest point algorithm;Feature extraction;Mobile AR;markerless AR;depth camera;registration;3D interaction},
  doi={10.1109/VR.2016.7504721},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504722,
  author={Kawamura, Soma and Kijima, Ryugo},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Effect of head mounted display latency on human stability during quiescent standing on one foot}, 
  year={2016},
  volume={},
  number={},
  pages={199-200},
  abstract={It has been shown that latency from a virtual reality (VR) user's motion to the display output leads to VR sickness, and in turn decreases the task performance. However, the effects of small delays under 13 ms have not been investigated as the VR systems have not been able to achieve these small latency values. The purpose of this study is to reveal the effects of small latencies on head mounted display users. The subjects in this study were asked to simply and stably stand on a force plate with one foot. The speed of body sway was measured with several lags, from 1 ms to 66 ms, using an Oculus Rift DK2. The results showed that the sway speed increased monotonically with increase in the latency values. The sense of balance is regarded as a relatively direct index of quality of a VR system, including the effects of lag.},
  keywords={Head;Visualization;Foot;Force;Virtual environments;Delays;Head Mounted Display;Latency;Cybersickness},
  doi={10.1109/VR.2016.7504722},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504723,
  author={Kijima, Ryugo and Yamaguchi, Katsuya},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={VR device time — Hi-precision time management by synchronizing times between devices and host PC through USB}, 
  year={2016},
  volume={},
  number={},
  pages={201-202},
  abstract={In the virtual reality system, accurate time management is necessary to maintain the correct relation between the displayed image and the users' motion by the latency compensation. The timestamp concept is effective in managing the timing of the display and sensing device. For time stamping, a unified time axis is necessary among the devices and the PC. In this paper, a time synchronization system via USB was developed. This prototype was proved to have accuracy below 20 μs among multiple sensors.},
  keywords={Synchronization;Universal Serial Bus;Virtual reality;Sensor systems;Hardware;Head Mounted Display;Latency;Cybersickness},
  doi={10.1109/VR.2016.7504723},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504724,
  author={Kijima, Ryugo and Miyajima, Kento},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Measurement of Head Mounted Display's latency in rotation and side effect caused by lag compensation by simultaneous observation — An example result using Oculus Rift DK2}, 
  year={2016},
  volume={},
  number={},
  pages={203-204},
  abstract={Latency is an important specification of the Head Mounted Display (HMD). In this paper, the authors proposed the measurement methods to evaluate the average latency as well as the effects/ side effects of the lag compensation. The Oculus Rift DK2 was selected for measurement. The results showed that the average latency of an DK2 without compensation was about 26.3 ms and that with full compensation was 1 ms. The rest part of the dynamic response was observed and evaluated by subtracting the measured latency from the observed trajectory. The side effect of Timewarp was observed as the spike like angular error.},
  keywords={Trajectory;Cameras;Visualization;Virtual reality;Rotation measurement;Timing;Electronic mail;Head Mounted Display;Latency;Cybersickness},
  doi={10.1109/VR.2016.7504724},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504725,
  author={Kim, Hyungil and Isleib, Jessica D. and Gabbard, Joseph L.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Casting shadows: Ecological interface design for augmented reality pedestrian collision warning}, 
  year={2016},
  volume={},
  number={},
  pages={205-206},
  abstract={Ecological interface design (EID) has the opportunity to complement current approaches for augmented reality (AR) interface design by considering human-environment interaction and leveraging the inherent benefit of AR interfaces: conformal graphics. This work applies EID to design a novel interface for pedestrian collision warning for an automotive AR head-up display (HUD). Our initial usability evaluation shows potential benefits of incorporating EID into AR interface design.},
  keywords={Vehicles;Usability;Roads;Augmented reality;Human computer interaction;Information processing;Augmented reality;ecological interface design;driving},
  doi={10.1109/VR.2016.7504725},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504726,
  author={Kim, Taeheon and Kachhara, Ashwin and MacIntyre, Blair},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Redirected head gaze to support AR meetings distributed over heterogeneous environments}, 
  year={2016},
  volume={},
  number={},
  pages={207-208},
  abstract={We demonstrate a method for redirecting gaze of virtual avatars in distributed augmented reality(AR) meetings. As social cues are a necessity for effective communication, our method tries to preserve gaze awareness, one of the key elements of a face-to-face meeting. When using AR to bring multiple sites together in a distributed meeting, with different numbers of participants and physical arrangements across sites, gaze awareness is maintained regardless of the seating topology. By maintaining gaze, we hope to enhance the presence of remote attendees and improve communication among the users, making meetings in AR a practical option for teleconferencing.},
  keywords={Avatars;Teleconferencing;Head;Topology;Prototypes;Human factors;Augmented Reality Teleconferencing;Computer Supported Collaborative Work},
  doi={10.1109/VR.2016.7504726},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504727,
  author={Krum, David M. and Kang, Sin-Hwa and Phan, Thai and Dukes, Lauren Cairco and Bolas, Mark},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Head mounted projection for enhanced gaze in social interactions}, 
  year={2016},
  volume={},
  number={},
  pages={209-210},
  abstract={Projected displays can present life-sized imagery of a virtual human character that can be seen by multiple observers. However, typical projected displays can only render that virtual human from a single viewpoint, regardless of whether head tracking is employed. This results in the virtual human being rendered from an incorrect perspective for most individuals. This could cause perceptual miscues, such as the “Mona Lisa” effect, causing the virtual human to appear as if it is simultaneously gazing and pointing at all observers regardless of their location. This may be detrimental to training scenarios in which all trainees must accurately assess where the virtual human is looking or pointing. We discuss our investigations into the presentation of eye gaze using REFLCT, a previously introduced head mounted projective display. REFLCT uses head tracked, head mounted projectors and retroreflective screens to present personalized, perspective correct imagery to multiple users without the occlusion of a traditional head mounted display. We examined how head mounted projection for enhanced presentation of eye gaze might facilitate or otherwise affect social interactions during a multi-person guessing game of “Twenty Questions”.},
  keywords={Head;Optical imaging;Observers;Games;Adaptive optics;Virtual reality;Training;H.5.1 [Information Interfaces and Presentation (I.7)]: Multimedia Information Systems — Artificial, augmented, and virtual realities;B.4.2 [Input/Output and Data Communications]: Input/Output Devices — Image display},
  doi={10.1109/VR.2016.7504727},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504728,
  author={Kumazawa, Itsuo and Yano, Shu and Suzuki, Souma and Ono, Shunsuke},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A multi-modal interactive tablet with tactile feedback, rear and lateral operation for maximum front screen visibility}, 
  year={2016},
  volume={},
  number={},
  pages={211-212},
  abstract={When we use a tablet style handheld device such as a smart phone as a part of a virtual really system, its most outstanding future: the touch screen dominating the most area of the front face should be incorporated into the system effectively and beneficially. For example, the visual information displayed on the screen can be merged with the surrounding or background scenes and the intuitive touch operation can be performed in a suitable scenario. However, if the means of the interaction is limited to the touch operation, finger operation on the front screen must be performed even for unsuitable scenarios, and the fingers or hands occluding the visual information disturb our immersive experience. To deal with this situation, we propose a multi-modal interactive tablet that uses its cameras, accelerometer, track ball and pressure sensors implemented on its rear and side for operations ensuring visibility. The pressing and ball-rotating operation on the rear and the side and the tactile feedback generated by voice-coil-based actuators assist and guide the multi-modal interaction. The effectiveness of the multi-modality with the rear and the side operation and the tactile feedback is evaluated by an experiment.},
  keywords={Tactile sensors;Thumb;Face;Visualization;Accelerometers;Multi modal user interface;tactile feedback;tablet computer;virtual reality;tactile display;voice coil},
  doi={10.1109/VR.2016.7504728},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504729,
  author={Kyrlitsias, Christos and Michael, Despina},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Influence by others' opinions: Social pressure from agents in immersive virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={213-214},
  abstract={Virtual Reality is used in fields of cognitive sciences to study participants' reactions. In such cases, existence of other avatars in the virtual environment is a crucial factor. In this study we investigate whether agents have social influence on the participants by performing the Asch conformity experiment (1951) in an immersive virtual environment. Findings are demonstrating that participants' response times were affected by the judgments of agents existing in the virtual environment.},
  keywords={Avatars;Virtual environments;Time factors;Psychology;Visualization;Computers;Virtual Reality;Computer Agents;Social Influence},
  doi={10.1109/VR.2016.7504729},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504730,
  author={Lages, Wallace S. and Laha, Bireswar and Miller, Wesley and Novotny, Johannes and Laidlaw, David H. and Socha, John J. and Bowman, Doug A.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Effects of field of regard and stereoscopy and the validity of MR simulation for visual analysis of scientific data}, 
  year={2016},
  volume={},
  number={},
  pages={215-216},
  abstract={We report the findings of a study designed to evaluate the effect of stereopsis and field of regard (FOR) in two different mixed reality (MR) simulation platforms: a head-mounted display (HMD) and a CAVE. We compared the performance of participants on two levels of stereopsis (mono and stereo) and two levels of FOR (90 degrees and 270 degrees) using a variety of scientific visualization tasks. Among the findings, we observed that not all the effects were consistent between the platforms. Stereo alone or in combination with higher FOR improved completion time on both platforms. However, adding stereo solely reduced the accuracy of the participants on the CAVE and improved on the HMD. Our findings extend prior knowledge on the contribution of visual fidelity components and suggests potential limits on MR simulation between platforms.},
  keywords={Virtual reality;Visualization;Solid modeling;Biology;Analytical models;Data models;I.2.10 [Vision and Scene Understanding]: 3D/stereo scene analysis — Perceptual reasoning;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities — Life Cycle;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual reality —},
  doi={10.1109/VR.2016.7504730},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504731,
  author={Lin, Lorraine and Jörg, Sophie},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The effect of realism on the virtual hand illusion}, 
  year={2016},
  volume={},
  number={},
  pages={217-218},
  abstract={The virtual hand illusion is a body ownership illusion that occurs in a virtual environment. Previous studies reached different conclusions on the effect of realism of the controllable virtual hand model on the intensity of the perceived illusion. In our experiments, we compare participants' responses to virtual impacts and threats when using hand models with different levels of realism. Our findings indicate that an illusion can be created for any model but that the effect is perceived weakest for a non-anthropomorphic block model and strongest for a realistic human hand model in direct comparison. We furthermore find that the reactions to our experiments highly vary between participants.},
  keywords={Solid modeling;Rubber;Skin;Robots;Virtual environments;Shape},
  doi={10.1109/VR.2016.7504731},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504732,
  author={Lind, Stine and Thomsen, Lui and Egeberg, Mie and Nilsson, Niels and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Effects of vibrotactile stimulation during virtual sandboarding}, 
  year={2016},
  volume={},
  number={},
  pages={219-220},
  abstract={This poster details a within-subjects study (n=17) investigating the effects of vibrotactile stimulation on illusory self-motion, presence and perceived realism during an interactive sandboarding simulation. The study compared three conditions: no vibration, constant vibration and dynamic vibration. The results suggest that constant vibrotactile feedback led to significantly more compelling self-motion illusions and a higher degree of perceived realism.},
  keywords={Vibrations;Virtual reality;Visualization;Atmospheric measurements;Particle measurements;Analysis of variance;Tracking;H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimenshional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2016.7504732},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504733,
  author={Lindeman, Robert W.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A low-cost, low-latency approach to dynamic immersion in occlusive head-mounted displays}, 
  year={2016},
  volume={},
  number={},
  pages={221-222},
  abstract={In this poster, we introduce the notion of using LCD panels for controlling the level of immersion provided by head-mounted displays (HMDs). We propose replacing the cowling around typical “ski-goggle” type HMDs with LCD panels of the type used in commodity active-stereo glasses used in movie theaters. The panels can be controlled using very simple stand-alone circuitry and/or a micro-controller to vary the amount of the real world that is visible in the periphery of the user. This can drastically increase the usability of consumer HMDs, because it allows users to see objects in their immediate surroundings (e.g., the keyboard and mouse), can be used to counter cybersickness by providing natural cues when needed, and introduces no added latency into the system. We show several prototypes of our approach using Google Cardboard, and provide some initial thoughts on the systems.},
  keywords={Visualization;Prototypes;Virtual reality;Cameras;Glass;Google;Human computer interaction;Immersion;head-mounted display},
  doi={10.1109/VR.2016.7504733},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504734,
  author={Link, Sascha and Barkschat, Berit and Zimmerer, Chris and Fischbach, Martin and Wiebusch, Dennis and Lugrin, Jean-Luc and Latoschik, Marc Erich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={An intelligent multimodal mixed reality real-time strategy game}, 
  year={2016},
  volume={},
  number={},
  pages={223-224},
  abstract={This paper presents a mixed reality tabletop role-playing game with a novel combination of interaction styles and gameplay mechanics. Our contribution extends previous approaches by abandoning the traditional turn-based gameplay in favor of simultaneous real-time interaction. The increased cognitive and physical load during the simultaneous control of multiple game characters is counteracted by two features: First, certain game characters are equipped with AI-driven capabilities to become semi-autonomous virtual agents. Second, (groups of) these agents can be instructed by high-level commands via a multimodal - speech and gesture - interface.},
  keywords={Games;Virtual reality;Real-time systems;Speech;Media;Human computer interaction;H.5.2 [Information Interfaces and Presentation]: User Interfaces —},
  doi={10.1109/VR.2016.7504734},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504735,
  author={Lugrin, Jean-Luc and Zilch, David and Roth, Daniel and Bente, Gary and Latoschik, Marc Erich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={FaceBo: Real-time face and body tracking for faithful avatar synthesis}, 
  year={2016},
  volume={},
  number={},
  pages={225-226},
  abstract={This paper introduces a low-cost framework capable of combining both real-time markerless face and body tracking for faithful avatar embodiment in Virtual Reality (VR). We discuss suitable hardware and software solutions and present a first prototype. This work lays the technological basis for further research on the importance of the appearance and behavioral realism of avatars, e.g., for the illusion of virtual body ownership, for social interactions in VR, as well as for VR entertainment applications (immersive games or movies).},
  keywords={Avatars;Software;Face;Tracking;Real-time systems;Prototypes;H.5.1 [Information Systems]: Artificial — Augmented and Virtual Realities},
  doi={10.1109/VR.2016.7504735},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504736,
  author={MacIntyre, Blair and Zhang, Dingtian and Jones, Ryan and Solomon, Amber and Disalvo, Elizabeth and Guzdial, Mark},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Using projection AR to add design studio pedagogy to a CS classroom}, 
  year={2016},
  volume={},
  number={},
  pages={227-228},
  abstract={The goal of this project is to use projection augmented reality to add design studio learning models to a classroom for an introductory Media Computation computer science class. Students do classwork using an enhanced version of Pythy, a web IDE for Python and Jython, that captures students' work and displays it around the room. We leverage the Microsoft RoomAlive Toolkit to construct a room-scale augmented reality using pairs of projectors and depth cameras. The system “pins” students' work to the walls, where teachers and students can see and discuss the work. We hope that by seeing each other's work, the system will foster collaboration and support the creation of STEM learning experiences that encourage creativity, innovation, and help build strong peer learning environments.},
  keywords={Servers;Three-dimensional displays;Augmented reality;Collaboration;Media;Creativity;Education technology;spatial augmented reality;projector-camera system},
  doi={10.1109/VR.2016.7504736},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504737,
  author={McEvoy, Kelly A. and Oyekoya, Oyewole and Ivory, Adrienne Holz and Ivory, James D.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Through the eyes of a bystander: The promise and challenges of VR as a bullying prevention tool}, 
  year={2016},
  volume={},
  number={},
  pages={229-230},
  abstract={Two studies explored the potential of virtual reality (VR) in bystander-focused bullying prevention campaigns. An experiment compared responses to three versions of a bullying scenario in which users (N = 78) were placed in the perspective of a bystander: customized VR, non-customized VR, and video. Measures included empathy, attitudes toward bullying victims and bullying, anticipated future behavior, presence, and other perceptions of bullying. The only significant effects observed were on feelings of empathy, with scores in the video condition higher than in the other two conditions, and on perceptions of bullying as a problem in participants' schools, again with scores highest in the video condition. These results were further explored in a follow-up qualitative focus group study (N = 10). Findings from both studies suggest that to elicit empathy-related responses, VR simulations should use photorealistic graphics, employ interactive features, and make customization prominent and carefully tailored. Lessons learned could inform the use of virtual reality in future campaigns.},
  keywords={Virtual reality;Solid modeling;Atmospheric measurements;Particle measurements;Media;Psychology;Indexes;Virtual/digital characters;simulation and behavior;psychology;education},
  doi={10.1109/VR.2016.7504737},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504738,
  author={Morioka, Hirofumi and Okubo, Hidehiko and Mitsumine, Hideki},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A handy system for natural composition of CG and real scene with real-time reflection of lighting changes}, 
  year={2016},
  volume={},
  number={},
  pages={231-232},
  abstract={A handy system is presented for creating in real time a natural television image composed of a real scene and a computer graphics object. The light positions and colors (RGB values) in the real scene are estimated and then applied to the object by using simple equipment. Objective and subjective experiments demonstrated the effectiveness of this system. An enhanced algorithm is also presented that improves the accuracy of light estimation.},
  keywords={Image color analysis;Lighting;Real-time systems;Estimation;Cameras;TV;Production;light estimation;real-time;image composition},
  doi={10.1109/VR.2016.7504738},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504739,
  author={Moser, Kenneth R. and Swan, J. Edward},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Evaluation of hand and stylus based calibration for optical see-through head-mounted displays using leap motion}, 
  year={2016},
  volume={},
  number={},
  pages={233-234},
  abstract={Next generation OST HMDs promise the inclusion of a variety of integrated and on-board sensors. In particular, hand tracking cameras, such as the Leap Motion, show potential for facilitating intuitive OST calibration procedures accessible to researchers, developers, and novice users alike. In this work, we evaluate hand and stylus based OST calibration utilizing tracking data from a Leap Motion. Our findings show that performance of both methods is comparable to results from prior studies using standard environment-centric methods. Also, while our hand based calibration improved through the use of more contextual reticle designs, calibrations performed with a stylus yielded the most accurate and precise results over all.},
  keywords={Calibration;Optical sensors;Tracking;Three-dimensional displays;Thumb;Adaptive optics;H.5.1 [[Information Interfaces and Presentation]: Multimedia Information Systems]: Artificial, augmented, and virtual realities —},
  doi={10.1109/VR.2016.7504739},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504740,
  author={Moteki, Atsunori and Yamaguchi, Nobuyasu and Karasudani, Ayu and Yoshitake, Toshiyuki},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Fast and accurate relocalization for keyframe-based SLAM using geometric model selection}, 
  year={2016},
  volume={},
  number={},
  pages={235-236},
  abstract={In this paper, we propose a relocalization method for keyframe-based SLAM that enables real-time and accurate recovery from tracking failures. To realize an AR-based application in a real world situation, not only accurate camera tracking but also fast and accurate relocalization from tracking failure is required. The previous keyframe-based relocalization methods have some drawbacks with regard to speed and accuracy. The proposed relocalization method selects two algorithms adaptively depending on the relative camera pose between a current frame and a target keyframe. In addition, it estimates a degree of false matches to speed up RANSAC-based model estimation. We present effectiveness of our method by an evaluation using public tracking dataset.},
  keywords={Cameras;Simultaneous localization and mapping;Estimation;Target tracking;Solid modeling;Three-dimensional displays;Feature extraction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking},
  doi={10.1109/VR.2016.7504740},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504741,
  author={Nakano, Takuya and Yanagida, Yasuyuki},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Measurement of wind direction perception by the entire head}, 
  year={2016},
  volume={},
  number={},
  pages={237-238},
  abstract={In many cases, when we present a video to users, we present sound too. Hence, the senses of sight and hearing are stimulated, improving the users' sensation of presence. In addition, recently, several virtual reality systems using wind have been built to enhance this sense of presence. If we use wind, users do not need to use additional devices for sensing presence because in such cases, we can present them with non-contact sensations. Therefore, wind-based systems do not hamper an improvement in the sense of presence. In fact, some studies have concluded that presenting wind and video simultaneously improves the sense of presence. However, in these studies, wind sources were arranged rather sparsely. In this sparse arrangement, we are not sure whether a precise environment can be reproduced. Therefore, we have examined the perception of wind direction by the entire head.},
  keywords={Virtual reality;Standards;Auditory system;Moon;Motion pictures;Fans;Electronic mail;Wind sensation;sensory property;wind display;just noticeable difference},
  doi={10.1109/VR.2016.7504741},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504742,
  author={Nescher, Thomas and Zank, Markus and Kunz, Andreas},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Simultaneous mapping and redirected walking for ad hoc free walking in virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={239-240},
  abstract={Providing real walking in virtual environments remains a challenge because of space, setup, and tracking system requirements. With the help of redirected walking (RDW), natural and unconstrained walking in virtual environments has become possible - without using mechanical locomotion devices - by manipulating the user's real world trajectory such that he remains within the boundaries of the walkable space. Nevertheless, typical home or office environments do not provide sufficient space or a suitable room which is free of obstacles. This paper presents an approach that combines RDW with a low-cost and user-worn tracking approach based on simultaneous localization and mapping (SLAM). I.e. learning the environment with the walkable area, tracking the user's viewpoint, and RDW is done on the fly without any prior setup, without preparing a room, and without setting up a tracking system. This allows ad hoc free walking in virtual environments even within dynamic and cluttered physical rooms, where the walkable area is of arbitrary shape. Furthermore, by combining SLAM and a planning RDW controller, this approach has the potential to provide the best free walking experience for any given physical environment.},
  keywords={Legged locomotion;Simultaneous localization and mapping;Virtual environments;Cameras;Three-dimensional displays;SLAM;redirected walking;tracking;head tracking;virtual reality;locomotion;walkable area;obstacle avoidance},
  doi={10.1109/VR.2016.7504742},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504743,
  author={Nilsson, Niels Christian and Suma, Evan and Nordahl, Rolf and Bolas, Mark and Serafin, Stefania},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Estimation of detection thresholds for audiovisual rotation gains}, 
  year={2016},
  volume={},
  number={},
  pages={241-242},
  abstract={Redirection techniques allow users to explore large virtual environments on foot while remaining within a limited physical space. However, research has primarily focused on redirection through manipulation of the visuals used to represent the virtual environment. We describe a within-subjects study (n=31) exploring if participants' ability to detect differences between real and virtual rotations is influenced by the addition of sound that is spatially aligned with its virtual source. The results revealed similar detection thresholds for conditions involving moving audio, static audio, and no audio. This may be viewed as an indication of visual dominance during scenarios such as the one used for the current study.},
  keywords={Visualization;Virtual environments;Electronic mail;Estimation;Legged locomotion;H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2016.7504743},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504744,
  author={Nishida, Jun and Suzuki, Kenji},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={bioSync: Wearable haptic I/O device for synchronous kinesthetic interaction}, 
  year={2016},
  volume={},
  number={},
  pages={243-244},
  abstract={This paper presents a synchronous kinesthetic interaction among people through haptic input/output based on biosignal measurement and stimulation. Users are able to bi-directionally transmit kinesthetic experiences such as rigidity of joints or exertion of muscles in addition to physical bodily motion. Such interaction would be very important in the fields of rehabilitation and sports training. In this study, we introduce a set of wearable devices that is capable of both electromyogram (EMG) measurement and electrical muscle stimulation (EMS) simultaneously on the same muscle by using common electrodes. To achieve smooth kinesthetic bi-directional interaction, we propose a new method for discharging the residual potential of the stimulation in order to enable fastest simultaneous operation (40Hz). Through a performance evaluation, participants could recognize the teacher's exertion strength on a 5-point scale without visual information.},
  keywords={Electrodes;Muscles;Training;Electromyography;Switches;Haptic interfaces;Performance evaluation;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Haptic I/O, Input devices and strategies, Interaction Styles, Theory and methods},
  doi={10.1109/VR.2016.7504744},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504745,
  author={Nogalski, Malte and Fohl, Wolfgang},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Acoustic redirected walking with auditory cues by means of wave field synthesis}, 
  year={2016},
  volume={},
  number={},
  pages={245-246},
  abstract={We present an experiment to identify detection thresholds for acoustic redirected walking by means of wave field synthesis. The most natural way to navigate an avatar through an immersive virtual environment (IVE) is by copying the tracked physical movements of a user. Redirected walking offers an approach to tackle the discrepancy between the potentially infinite IVE and the generally limited available physical space or tracking area, by applying manipulations, such as rotations or translations, to the IVE in form of gains to the users movements. 39 blindfolded test subjects performed a total of 2777 constant stimulus trials with various amounts of rotation and curvature gains. The test subjects were divided into four groups with different knowledge of the experiment, and one group performed two-alternative-forced-choice tasks, while the others could give feedback freely. The detection thresholds were greatly dependent on the groups i.e., the knowledge of the experiment. The 25% detection threshold was reached by the most relevant test group at gains that up-scaled rotations by 5%, down-scaled them by 37.5%, and bend a straight path into a circle with a radius of 5.71 meters. Almost no signs of simulator sickness could be observed.},
  keywords={Legged locomotion;Acoustics;Tracking;Virtual environments;Navigation;Software;H.1.2 [Models and Principles]: User/Machine Systems — Software psychology, Human factors;J.7 [Computers in other systems]: Realtime —;H.5.1 [Information and Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2016.7504745},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504746,
  author={Nuernberger, Benjamin and Lien, Kuo-Chin and Höllerer, Tobias and Turk, Matthew},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Anchoring 2D gesture annotations in augmented reality}, 
  year={2016},
  volume={},
  number={},
  pages={247-248},
  abstract={Augmented reality enhanced collaboration systems often allow users to draw 2D gesture annotations onto video feeds to help collaborators to complete physical tasks. This works well for static cameras, but for movable cameras, perspective effects cause problems when trying to render 2D annotations from a new viewpoint in 3D. In this paper, we present a new approach towards solving this problem by using gesture enhanced annotations. By first classifying which type of gesture the user drew, we show that it is possible to render annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for remote collaboration by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel system to automatically handle the top two 2D gesture annotations - arrows and circles. Arrows are handled by identifying their anchor points and using surface normals for better perspective rendering. For circles, we designed a novel energy function to help infer the object of interest using both 2D image cues and 3D geometric cues. Results indicate that our approach outperforms previous methods in terms of better conveying the original drawing's meaning from different viewpoints.},
  keywords={Three-dimensional displays;Augmented reality;Collaboration;Cameras;Head;Printers;Rendering (computer graphics);H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Interaction styles},
  doi={10.1109/VR.2016.7504746},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504747,
  author={Oh, Soo Youn and Shriram, Ketaki and Laha, Bireswar and Baughman, Shawnee and Ogle, Elise and Bailenson, Jeremy},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Immersion at scale: Researcher's guide to ecologically valid mobile experiments}, 
  year={2016},
  volume={},
  number={},
  pages={249-250},
  abstract={While there have been hundreds of psychological studies using virtual reality (VR) over the past few decades, those studies have almost exclusively been conducted in laboratory settings using small samples of college students with little demographic variance. Hence, the generalizability of the results is limited, as not all findings will apply outside the college demographic. In this paper, we present our mobile VR project (Immersion at Scale) where we conduct VR experiment sessions in naturalistic settings (e.g., local events, museums, etc.). On average, we were able to collect data from 20-25 people for each 4-hour data collection session of Immersion at Scale. We discovered a number of obstacles and opportunities based on bringing VR out into the field. Thus, we do not focus on experimental stimuli and results, but methodological guidelines based on our iterative design improvements from pilot testing.},
  keywords={Headphones;Psychology;Data collection;Mobile communication;Virtual reality;Guidelines;Sociology;Experimental methodology;Field study;Psychology},
  doi={10.1109/VR.2016.7504747},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504748,
  author={de Jesus Oliveira, Victor Adriel and Nedel, Luciana and Maciel, Anderson},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Speaking Haptics: Proactive haptic articulation for intercommunication in virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={251-252},
  abstract={Communication is crucial in collaborative tasks. Multimodal strategies are commonly applied to complement, reinforce and disam-biguate information exchange. However, although multimodal communication is commonplace in Collaborative Virtual Environments, the proactive use of touch for intercommunication is surprisingly neglected regardless its importance for communication. In this paper, we look up to elements present in speech articulation to introduce the proactive haptic articulation as a novel approach for communication in CVEs. We defend the hypothesis that elements present in natural language, when added to the design of the vibro-tactile vocabulary, should provide an expressive medium for intercommunication. Moreover, we hypothesize that the ability to render tactile cues to a teammate will encourage users to adapt a given vocabulary spontaneously during its use. We implemented a case study around a collaborative puzzle task to demonstrate the use of such vocabulary. Results show that the proactive haptic articulation provided a way for participants to autonomously and dynamically adapt the provided tactile vocabulary to attend their communication needs during the task.},
  keywords={Vocabulary;Haptic interfaces;Collaboration;Virtual environments;Electronic mail;Actuators;H.5.1 [Information Interface and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces & Presentation]: User Interfaces — Haptic I/O},
  doi={10.1109/VR.2016.7504748},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504749,
  author={Oshima, Kohei and Moser, Kenneth R and Rompapas, Damien Constantine and Swan, J Edward and Ikeda, Sei and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={SharpView: Improved clarity of defocussed content on optical see-through head-mounted displays}, 
  year={2016},
  volume={},
  number={},
  pages={253-254},
  abstract={A common factor among current generation optical see-through augmented reality systems is fixed focal distance to virtual content. In this work, we investigate the issue of focus blur, in particular, the blurring caused by simultaneously viewing virtual content and physical objects in the environment at differing focal distances. We examine the application of dynamic sharpening filters as a straight forward, system independent, means for mitigating this effect improving the clarity of defocused AR content. We assess the utility of this method, termed SharpView, by employing an adjustment experiment in which users actively apply varying amounts of sharpening to reduce the perception of blur in AR content. Our experimental results validate the ability of our SharpView model to improve the visual clarity of focus blurred content, with optimal performance at focal differences well suited for near field AR applications.},
  keywords={Optical imaging;Adaptive optics;Lenses;Augmented reality;Optical distortion;Retina;Deconvolution;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.4.4 [Image Processing and Computer Vision]: Restoration — Wiener filtering},
  doi={10.1109/VR.2016.7504749},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504750,
  author={Outram, Benjamin I. and Nakatani, Masashi and Minamizawa, Kouta},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Extra-normal interactions in mediated virtual environments: An investigation of an audio-visual crossed-sense modality}, 
  year={2016},
  volume={},
  number={},
  pages={255-256},
  abstract={Here, we propose that the use of unusual crossed-sense-modality couplings can be exploited to engineer non-realistic but potentially beneficial VEs providing high degrees of interaction and presence, and provide new avenues for enhancing the experience and utility of single- and multi-user VE technologies. We report user's reactions to mediated environments in which the sound of their voice is represented via a frequency-to-color-mapped visualization. Users reported a sense of interaction and enjoyment, but results from a preliminary user study have yet to show a clear link with presence. We discuss the next stages of the research which will investigate how mixed-sense modalities contribute to co-presence and collaboration in multi-user environments.},
  keywords={Visualization;Color;Virtual environments;Standards;Media;Couplings},
  doi={10.1109/VR.2016.7504750},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504751,
  author={Palmer, Eric and Michaux, Aaron and Pizlo, Zygmunt},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Using virtual environments to evaluate assumptions of the human visual system}, 
  year={2016},
  volume={},
  number={},
  pages={257-258},
  abstract={Virtual reality applications provide an opportunity to test human vision in well-controlled scenarios that would be difficult or impossible to generate in real physical spaces. This paper presents a study intended to evaluate the importance of possible assumptions made by the human visual system. Using a CAVE simulation, participants viewed and counted virtual furniture objects in a variety of experimental manipulations. The assumption of uprightness against inversion, or the `gravity constraint,' was identified as a significant assumption of the visual system (p <; 0.001). Monocular vs. binocular vision was also demonstrated as an important factor in this study (p = 0.01), while color vs. grayscale did not have a significant impact on task performance (p = 0.16). By including the binocular cue, and the assumption about the direction of gravity, the scene reconstruction produced by our computer vision model is reliable. The model can detect and count symmetrical objects in a 3D real scene and then recover their 3D shapes.},
  keywords={Three-dimensional displays;Computational modeling;Solid modeling;Visual systems;Gravity;Image color analysis;Gray-scale;Psychology;perception;computer vision},
  doi={10.1109/VR.2016.7504751},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504752,
  author={Paludan, Anders and Elbaek, Jacob and Mortensen, Mathias and Zobbe, Morten and Nilsson, Niels Christian and Nordahl, Rolf and Reng, Lars and Serafin, Stefania},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Disguising rotational gain for redirected walking in virtual reality: Effect of visual density}, 
  year={2016},
  volume={},
  number={},
  pages={259-260},
  abstract={In virtual reality environments that allow users to walk freely, the area of the virtual environment (VE) is constrained to the size of the tracking area. By using redirection techniques, this problem can be partially circumvented; one of the techniques involves rotating the user more or less in the virtual world than in the physical world; this technique is referred to as rotational gain. Experiments conducted by Frank Steinicke et al. [7] sought to uncover users' ability to detect the usage of rotational gain. This paper seeks to further investigate this area, examining the effect of visual density in the VE. Testing visual density using 0, 4 and 16 objects did not seem to reveal the point at which the test participants become noticeably affected.},
  keywords={Visualization;Legged locomotion;Virtual environments;Bridges;Tracking;Perception;virtual reality;redirected walking},
  doi={10.1109/VR.2016.7504752},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504753,
  author={Peer, Alex and Ponto, Kevin},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Perceptual space warping: Preliminary exploration}, 
  year={2016},
  volume={},
  number={},
  pages={261-262},
  abstract={Distance has been shown to be incorrectly estimated in virtual environments relative to the same estimation tasks in a real environment. This work describes a preliminary exploration of Perceptual Space Warping, which influences perceived distance in virtual environments by using a vertex shader to warp geometry. Empirical tests demonstrate significant effects, but of smaller magnitude than expected.},
  keywords={Calibration;Distortion;Space exploration;Virtual environments;Atmospheric measurements;Particle measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.3.7 [Computing Methodologies]: Graphics Utilities — Virtual reality},
  doi={10.1109/VR.2016.7504753},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504754,
  author={Pfeiffer, Thies and Schmidt, Aljoscha and Renner, Patrick},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Detecting movement patterns from inertial data of a mobile head-mounted-display for navigation via walking-in-place}, 
  year={2016},
  volume={},
  number={},
  pages={263-264},
  abstract={While display quality and rendering for Head-Mounted-Displays (HMDs) has increased in quality and performance, the interaction capabilities with these devices are still very limited or relying on expensive technology. Current experiences offered for mobile HMDs often stick to dome-like looking around, automatic or gaze-triggered movement, or flying techniques. We developed an easy to use walking-in-place technique that does not require additional hardware to enable basic navigation, such as walking, running, or jumping, in virtual environments. Our approach is based on the analysis of data from the inertial unit embedded in mobile HMDs. In a first prototype realized for the Samsung Galaxy Gear VR we detect steps and jumps. A user study shows that users novice to virtual reality easily pick up the method. In comparison to a classic input device, using our walking-in-place technique study participants felt more present in the virtual environment and preferred our method for exploration of the virtual world.},
  keywords={Navigation;Legged locomotion;Virtual environments;Sensors;Gravity;Accelerometers;Mobile communication;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Interaction Styles;I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction Techniques},
  doi={10.1109/VR.2016.7504754},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504755,
  author={Plopski, Alexander and Moser, Kenneth R. and Kiyokawa, Kiyoshi and Swan, J. Edward and Takemura, Haruo},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Spatial consistency perception in optical and video see-through head-mounted augmentations}, 
  year={2016},
  volume={},
  number={},
  pages={265-266},
  abstract={Correct spatial alignment is an essential requirement for convincing augmented reality experiences. Registration error, caused by a variety of systematic, environmental, and user influences decreases the realism and utility of head mounted display AR applications. Focus is often given to rigorous calibration and prediction methods seeking to entirely remove misalignment error between virtual and real content. Unfortunately, producing perfect registration is often simply not possible. Our goal is to quantify the sensitivity of users to registration error in these systems, and identify acceptability thresholds at which users can no longer distinguish between the spatial positioning of virtual and real objects. We simulate both video see-through and optical see-through environments using a projector system and experimentally measure user perception of virtual content misalignment. Our results indicate that users are less perceptive to rotational errors over all and that translational accuracy is less important in optical see-through systems than in video see-through.},
  keywords={Calibration;Adaptive optics;Solid modeling;Cameras;Augmented reality;Optical sensors;H.5.1 [[Information Interfaces and Presentation]: Multimedia Information Systems]: Artificial, augmented, and virtual realities —;H.5.2 [[Information Interfaces and Presentation]: User Interfaces]: Ergonomics, Evaluation/methodology, Screen design —},
  doi={10.1109/VR.2016.7504755},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504756,
  author={Pohl, Daniel and de Tejada Quemada, Carlos Fernandez},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={See what I see: Concepts to improve the social acceptance of HMDs}, 
  year={2016},
  volume={},
  number={},
  pages={267-268},
  abstract={Virtual Reality is reaching into the consumer space. Mobile virtual reality solutions are nowadays widely available and affordable for many smartphones, by adding a case with attached lenses around the phone to create a head-mounted display. While using these in public places or at social gatherings where the head-mounted display is given around to others, it can lead to problems regarding social acceptance, as the surrounding people are not aware of what the virtual reality user is seeing and doing. We address this problem by adding a second, front-facing screen to the head-mounted display. We build and evaluate two prototypes for this usage.},
  keywords={Prototypes;Virtual reality;Mobile communication;Google;Smart phones;Wireless communication;Cameras;I.3.3 [Computer Graphics]: Picture / Image Generation —},
  doi={10.1109/VR.2016.7504756},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504757,
  author={Pohl, Daniel and Zhang, Xucong and Bulling, Andreas},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Combining eye tracking with optimizations for lens astigmatism in modern wide-angle HMDs}, 
  year={2016},
  volume={},
  number={},
  pages={269-270},
  abstract={Virtual Reality has hit the consumer market with affordable head-mounted displays. When using these, it quickly becomes apparent that the resolution of the built-in display panels still needs to be highly increased. To overcome the resulting higher performance demands, eye tracking can be used for foveated rendering. However, as there are lens distortions in HMDs, there are more possibilities to increase the performance with smarter rendering approaches. We present a new system using optimizations for rendering considering lens astigmatism and combining this with foveated rendering through eye tracking. Depending on the current eye gaze, this delivers a rendering speed-up of up to 20%.},
  keywords={Rendering (computer graphics);Gaze tracking;Lenses;Optimization;Optical distortion;Cameras;Distortion;virtual reality;astigmatism;eye tracking;ray tracing;optics},
  doi={10.1109/VR.2016.7504757},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504758,
  author={Radu, Iulian and McCarthy, Betsy and Kao, Yvonne},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Discovering educational augmented reality math applications by prototyping with elementary-school teachers}, 
  year={2016},
  volume={},
  number={},
  pages={271-272},
  abstract={In recent years, augmented reality (AR) applications for children's entertainment have been gaining popularity, and educational organizations are increasingly interested in applying this technology to children's educational games. In this paper we describe our collaboration with teachers and game designers, in order to explore educational potential for AR technology. This paper specifically investigates the topics of: What mathematics curriculum topics should technological innovations address in the Grade 1-3 classrooms? Which of the topics are suitable for AR games? And, how can we facilitate an efficient dialogue between educators and game designers?},
  keywords={Prototypes;Augmented reality;Games;Mathematics;Education;Standards;Organizations;Augmented Reality;Prototyping;Mathematics;Elementary Classroom;Pedagogy;Teachers},
  doi={10.1109/VR.2016.7504758},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504759,
  author={Ren, Yi and Fuchs, Henry},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Faster feedback for remote scene viewing with pan-tilt stereo camera}, 
  year={2016},
  volume={},
  number={},
  pages={273-274},
  abstract={We demonstrate a remote scene viewing system for telepresence purposes. The system is based on a pan-tilt stereo camera that captures stereo video and transfers it to a remote user over a network. On the user end, the live stereo video is processed and displayed in a Head-Mounted Display. Faster feedback can be achieved through latency compensation. Using a wider field-of-view, higher resolution camera, the appropriate subset of the image is selected and displayed. We introduce the hardware configuration and software framework for the system and a method to calculate the homography between the camera image space and user head image space. Our perceived latency of the system is estimated to be 50–100ms.},
  keywords={Head;Streaming media;Transforms;Tracking;Webcams;Virtual reality;I.3.7 [Computer Graphics]: Tree-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2016.7504759},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504760,
  author={Roth, Daniel and Lugrin, Jean-Luc and Büser, Julia and Bente, Gary and Fuhrmann, Arnulph and Latoschik, Marc Erich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A simplified inverse kinematic approach for embodied VR applications}, 
  year={2016},
  volume={},
  number={},
  pages={275-276},
  abstract={In this paper, we compare a full body marker set with a reduced rigid body marker set supported by inverse kinematics. We measured system latency, illusion of virtual body ownership, and task load in an applied scenario for inducing acrophobia. While not showing a significant change in body ownership or task performance, results do show that latency and task load are reduced when using the rigid body inverse kinematics solution. The approach therefore has the potential to improve virtual reality experiences.},
  keywords={Kinematics;Tracking;Virtual environments;Electronic mail;Frequency modulation;Real-time systems;H.5.1 [Information Systems]: Artificial — Augmented and Virtual Realities},
  doi={10.1109/VR.2016.7504760},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504761,
  author={Roth, Daniel and Lugrin, Jean-Luc and Galakhov, Dmitri and Hofmann, Arvid and Bente, Gary and Latoschik, Marc Erich and Fuhrmann, Arnulph},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Avatar realism and social interaction quality in virtual reality}, 
  year={2016},
  volume={},
  number={},
  pages={277-278},
  abstract={In this paper, we describe an experimental method to investigate the effects of reduced social information and behavioral channels in immersive virtual environments with full-body avatar embodiment. We compared physical-based and verbal-based social interactions in real world (RW) and virtual reality (VR). Participants were represented by abstract avatars that did not display gaze, facial expressions or social cues from appearance. Our results show significant differences in terms of presence and physical performance. However, differences in effectiveness in the verbal task were not present. Participants appear to efficiently compensate for missing social and behavioral cues by shifting their attentions to other behavioral channels.},
  keywords={Avatars;Virtual environments;Electronic mail;Collaboration;Games;Atmospheric measurements;H.5.1 [Information Systems]: Artificial — Augmented and Virtual Realities},
  doi={10.1109/VR.2016.7504761},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504762,
  author={Sakurai, Satoru and Aoyama, Kazuma and Miyamoto, Nobuhisa and Mizukami, Makoto and Furukawa, Masahiro and Maeda, Taro and Ando, Hideyuki},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Mechanism of inhibitory effect of cathodal current tongue stimulation on five basic tastes}, 
  year={2016},
  volume={},
  number={},
  pages={279-280},
  abstract={The mechanism by which cathodal current stimulation exerts inhibitory effects on the five basic tastes is revealed herein. The objective of this paper is to successfully achieve inhibition of sweetness by cathodal current electrical stimulation to the tongue, which has not been reported to date, though inhibition of salty and umami perception has been documented. By focusing on the electrophoresis of ions generated by dissolution of taste-inducing substances in water, this paper indicates how human gustation is inhibited by electrical stimulation, which is a key addition to the knowledgebase for achieving control of all five basic tastes.},
  keywords={Sugar;Tongue;Electrical stimulation;Ions;Virtual reality;Electrodes;Conferences;taste display;electrical stimulation;inhibitory effect},
  doi={10.1109/VR.2016.7504762},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504763,
  author={Schillebeeckx, Ian and Pless, Robert},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Using Chromo-coded light fields for augmented reality}, 
  year={2016},
  volume={},
  number={},
  pages={281-282},
  abstract={The light field, or plenoptic function, is a mathematical construct that models the location and direction of light to describe the complete visual appearance of a scene. The light field provides a unifying construct to describe cameras and displays. Explicitly manipulating the light field is an approach to changing the world to make it easier to understand. One way to create light fields is with materials like lenticular arrays, whose color appearance can be made to vary by viewing angle. These Chromo-coded light fields use color to create additional geometric cues, making it cheaper, faster and more accurate to measure object pose. For high-end applications like augmented reality, the color-cues make it possible to accurately measure pose of small objects to project digital content. This poster offers demonstrations of a few augmented reality applications made possible by chromo-coded light fields.},
  keywords={Cameras;Augmented reality;Image color analysis;Three-dimensional displays;Lenses;Solid modeling;Visualization;Light Field;Lenticular Array;Augmented Reality},
  doi={10.1109/VR.2016.7504763},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504764,
  author={Ferdous, Sharif Mohammad Shahnewaz and Arafat, Imtiaz Muhammad and Quarles, John},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Visual feedback to improve the accessibility of head-mounted displays for persons with balance impairments}, 
  year={2016},
  volume={},
  number={},
  pages={283-284},
  abstract={The objective of this research is to improve the accessibility of Head-Mounted Displays (HMDs) for users with balance impairments while they are in immersive Virtual Environments (VEs). Previous research has shown that most users experience some imbalance in a fully immersive VE. However, this imbalance is significantly worse in users with balance deficits. Thus, this research aims to determine an effective visual feedback technique to improve balance of persons while using VEs to improve the accessibility of HMDs. In order to do that, we conducted a study with seven users without impairment and seven users with balance impairments due to Multiple Sclerosis (MS). We investigated how a static reference frame (SRF) (e.g., a cross-hair always rendered in the same position on the user's display screen) impacts the participants' balances in VR. Results indicate that a SRF significantly improves balance in VR for users with MS. Based on these results, we propose guidelines for designing more accessible VEs for persons with balance impairments.},
  keywords={Visualization;Games;Atmospheric measurements;Particle measurements;Virtual environments;Multiple sclerosis;Virtual Reality;balance;accessibility;head-mounted display},
  doi={10.1109/VR.2016.7504764},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504765,
  author={Sikström, Erik and Nilsson, Niels Christian and de Götzen, Amalia and Serafin, Stefania},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Is this bridge safe? Evaluation of audiovisual cues for a walk on a small bridge over a canyon}, 
  year={2016},
  volume={},
  number={},
  pages={285-286},
  abstract={This paper presents two within-subjects studies (n=23) exploring how different combinations of visual and auditory feedback influence perceived realism, virtual self-perception and the experience of safety during walks on a virtual platform suspended over a canyon. In the first study, the frequency factor of the footstep sounds was altered and the visual appearance was changed between a newly built wooden bridge and an old bridge with a weaker structure and broken planks. In the second study, the sounds of creaking wood were added to the footstep sounds in half of the trails and compared against footsteps without creaking sounds. Moreover, the frequency factor of the frequency controls for footsteps was also manipulated between trails, but the visual appearance of the bridge was limited to the model of the old broken bridge.},
  keywords={Bridges;Visualization;Legged locomotion;Virtual environments;Electronic mail;Avatars;Frequency control;H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2016.7504765},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504766,
  author={Stauffert, Jan-Philipp and Niebling, Florian and Latoschik, Marc Erich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Reducing application-stage latencies of interprocess communication techniques for real-time interactive systems}, 
  year={2016},
  volume={},
  number={},
  pages={287-288},
  abstract={Latency jitter is a pressing problem in Virtual Reality (VR) applications. This paper analyzes latency jitter caused by typical interprocess communication (IPC) techniques commonly found in today's computer systems used for VR. Test programs measure the scalability and latencies for various IPC techniques, where increasing number of threads are performing the same task concurrently. We use four different implementations on a vanilla Linux kernel as well as on a real-time (RT) Linux kernel to further assess if a RT variant of a multiuser multiprocess operating system can prevent latency spikes and how this behavior would apply to different programming languages and IPC techniques. We found that Linux RT can limit the latency jitter at the cost of throughput for certain implementations. Further, coarse grained concurrency should be employed to avoid adding up of scheduler latencies, especially for native system space IPC, while actor systems are found to support a higher degree of concurrency granularity and a higher level of abstraction.},
  keywords={Message systems;Linux;Concurrent computing;Real-time systems;Jitter;Virtual reality;Kernel;D.1.3 [Programming Techniques]: Concurrent Programming — Parallel programming;D.4.8 [Operating Systems]: Performance — Measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2016.7504766},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504767,
  author={Steed, Anthony},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Supporting multiple immersive configurations using a shape-changing display}, 
  year={2016},
  volume={},
  number={},
  pages={289-290},
  abstract={Immersive displays for virtual reality systems can be roughly classified into spatially immersive displays (similar to CAVE-like displays or large-screen simulators) or head-mounted displays. The former type is usually static in spatial configuration and configured to support a small group of users. The latter supports only a single user. We propose a new class of actuated, reconfigurable display that can support both small groups and individual users: in particular we suggest a robotic display that can change shape. The display can change shape to support different usage conditions, and can also move rapidly to give a larger apparent field of view for an individual user. We explore the potential advantages of a display that can move independently from its user(s), and we present a prototype that demonstrates some of the potential use scenarios.},
  keywords={Robots;Virtual reality;Prototypes;Shape;Head;Human factors;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality},
  doi={10.1109/VR.2016.7504767},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504768,
  author={Tanaka, Ryohei and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Navigation interface for virtual environments constructed with spherical images}, 
  year={2016},
  volume={},
  number={},
  pages={291-292},
  abstract={We propose a navigation interface, which enables users to control locomotion and the speed intuitively in virtual environments constructed with spherical images. In these virtual environments, users can only pass through lines where the camera has passed through during recording. Therefore, it is necessary to inform users of the accessible directions. Furthermore, velocity control is needed for close exploration in virtual environments. Therefore, we propose the Motive Compass input interface, which intuitively shows users the accessible directions and enables them to control the velocity of locomotion. We conducted a large-scale demonstration experiment in a real exhibition and showed that our interface more effectively presents accessible directions and enables users to control their velocities.},
  keywords={Compass;Virtual environments;Velocity control;Cameras;Three-dimensional displays;Navigation;Aerospace electronics;User interface;3D navigation;3D interaction;spherical image;user studies},
  doi={10.1109/VR.2016.7504768},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504769,
  author={Teófilo, Mauro and Nascimento, Josiane and Santos, Jonathan and Albuquerque, Yves and Souza, Andre L. and Nogueira, Daniel},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Bringing basic accessibility features to virtual reality context}, 
  year={2016},
  volume={},
  number={},
  pages={293-294},
  abstract={Virtual reality is an experience, often generated by computer, which brings immersive environments that can be interacted with. Since 2014, the spread of VR technology created a content demand as well as new paradigms of interactions. One of the key aspects of these new paradigms is that they consider the use of virtual reality by visually impaired people. In HCI, accessibility features are special computer functions that help people with disabilities to use technology more easily. This paper introduces Virtual Reality (VR) basic scenarios of accessibility tools like zooming, negative colors, auto reading, text-to-speech, subtitles, cursor based on context, and so on. The proposed solutions were designed based on accessibility features already in use by other platforms.},
  keywords={Virtual reality;Context;Visualization;Gears;Color;Assistive technology;Image color analysis;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities —;K.4.2 [Social Issues]: Assistive technologies for persons with disabilities —},
  doi={10.1109/VR.2016.7504769},
  ISSN={2375-5334},
  month={March},}
@INPROCEEDINGS{7504770,
  author={Tiefenbacher, Philipp and Techmer, Clemens and Rigoll, Gerhard},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={RayOnPlane: A translation technique minimizing gesture size}, 
  year={2016},
  volume={},
  number={},
  pages={295-296},
  abstract={In this work, we propose a device-based manipulation technique named RayOnPlane, which maintains the ease of use also in case of increasing work space size. We compare this technique to the state-of-the-art device-based manipulation technique HOMER-S. An experiment incorporating different work space sizes indicates comparable performance in completion time, while minimizing gesture size as well as user frustration and physical strain.},
  keywords={Three-dimensional displays;Mobile handsets;Electronic mail;Euclidean distance;NASA;Performance evaluation;Mobile communication;Manipulation;mobile device;interaction styles},
  doi={10.1109/VR.2016.7504770},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504771,
  author={Tiefenbacher, Philipp and Gillich, Jan and Schott, Paul and Rigoll, Gerhard},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Comparison of mobile touch interfaces for object identification and troubleshooting tasks in augmented reality}, 
  year={2016},
  volume={},
  number={},
  pages={297-298},
  abstract={This work adapts common HMD interfaces for the use on a handheld device. The proposed interfaces focus on: Easy interaction on the mobile device and independence of the provided content to the user's view. We compare two AR interface techniques for object identification and three AR interfaces for troubleshooting. The results show that exocentric-based AR interfaces outperform egocentric ones in respect to completion time, walking distance and number of interactions.},
  keywords={Three-dimensional displays;Manuals;Cameras;Legged locomotion;IP networks;Switches;Maintenance engineering;Assembly;maintenance;viewpoint;exocentric},
  doi={10.1109/VR.2016.7504771},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504772,
  author={Nielsen, Mikkel and Toft, Christian and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Evaluating two alternative walking in place interfaces for virtual reality gaming}, 
  year={2016},
  volume={},
  number={},
  pages={299-300},
  abstract={This study investigates sliding as a walking-in-place (WIP) method for virtual reality navigation using the Wizdish. The Wizdish is a novel WIP device built for home usage. Two WIP methods, sliding and marching, were compared for naturalness, presence, and surface difference. The sliding technique used on the Wizdish was found to be significantly more disruptive during the experience compared to marching. This could be due to the size of the Wizdish, restricting the users stride, or due to a longer acclimatization time.},
  keywords={Legged locomotion;Friction;Tracking;Training;Virtual reality;Footwear;Navigation;3D Navigation;walking;virtual environments},
  doi={10.1109/VR.2016.7504772},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504773,
  author={Tredinnick, Ross and Broecker, Markus and Ponto, Kevin},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Progressive feedback point cloud rendering for virtual reality display}, 
  year={2016},
  volume={},
  number={},
  pages={301-302},
  abstract={Previous approaches to rendering large point clouds on immersive displays have generally created a trade-off between interactivity and quality. While these approaches have been quite successful for desktop environments when interaction is limited, virtual reality systems are continuously interactive, which forces users to suffer through either low frame rates or low image quality. This paper presents a novel approach to this problem through a progressive feedback-driven rendering algorithm. This algorithm uses reprojections of past views to accelerate the reconstruction of the current view. The presented method is tested against previous methods, showing improvements in both rendering quality and interactivity.},
  keywords={Rendering (computer graphics);Three-dimensional displays;Graphics processing units;Measurement;Virtual reality;Feedback loop;Geometry;Point-based graphics;virtual reality;3D scanning},
  doi={10.1109/VR.2016.7504773},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504774,
  author={Uranishi, Yuki and Imura, Masataka and Kuroda, Tomohiro},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The Rainbow Marker: An AR marker with planar light probe based on structural color pattern matching}, 
  year={2016},
  volume={},
  number={},
  pages={303-304},
  abstract={This paper proposes The Rainbow Marker, a planar marker for estimating the direction of a light source using a structural color. A structural color is a color produced by microscopically structured surfaces that vary in appearance according to the viewpoint, the direction and the spectrum of the light source. The proposed marker contains a planar material which causes structural coloration. The direction of the light source is estimated by structural color pattern matching between an input pattern and referential color patterns. In this paper, two types of the marker were implemented, with a grating sheet and with a holographic sheet, to demonstrate that the proposed method is applicable in the field of augmented reality.},
  keywords={Light sources;Image color analysis;Probes;Estimation;Pattern matching;Rendering (computer graphics);Microscopy;K.6.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented, and Virtual Realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Photometry},
  doi={10.1109/VR.2016.7504774},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504775,
  author={Valkov, Dimitar and Martens, John and Hinrichs, Klaus},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Evaluation of the effect of a virtual avatar's representation on distance perception in immersive virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={305-306},
  abstract={It is well known that distance estimation in immersive virtual reality environments suffers from compression when viewed from an egocentric perspective with a head mounted display. While previous research indicates that providing the user with a virtual avatar with high geometric and motion fidelity may alleviate this problem, little work has been done to investigate which properties of the avatar's representation influence the distance estimation and how. In this poster we report the results of an evaluation of the user's distance perception with different avatar representations. Our results indicate that anthropometric fidelity of the avatar has stronger effect on the distance perception than its visual fidelity.},
  keywords={Avatars;Estimation;Visualization;Virtual environments;Electronic mail;Solid modeling;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality},
  doi={10.1109/VR.2016.7504775},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504776,
  author={Wei, Xiaodong and Weng, Dongdong and Liu, Yue and Wang, Yongtian},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A tour guiding system of historical relics based on augmented reality}, 
  year={2016},
  volume={},
  number={},
  pages={307-308},
  abstract={Yuanmingyuan is a relic park and only few cultural relics are left due to the looting and burning down in history, which makes that most of the scenic spots of the park look boring. To address such issue, a game-based guidance system for Yuanmingyuan and a time travel game called MAGIC-EYES has been proposed with Augmented Reality technology. Six interactive modes are designed in the proposed system to guide tourists to visit the specified place. The evaluation results of a pilot study shows that the proposed guidance system has significantly improved the tourist experiences.},
  keywords={Augmented reality;Games;Buildings;History;Cultural differences;Multimedia communication;Guidance system;Augmented reality;Game-based learning;Location-based},
  doi={10.1109/VR.2016.7504776},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504777,
  author={Wheatland, Nkenge and Abdullah, Ahsan and Neff, Michael and Jörg, Sophie and Zordan, Victor},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Analysis in support of realistic timing in animated fingerspelling}, 
  year={2016},
  volume={},
  number={},
  pages={309-310},
  abstract={American Sign Language (ASL) fingerspelling is the act of spelling a word letter-by-letter when a specific sign does not exist to represent it. Synthesizing intelligible ASL, which includes fingerspelling as an integral part, is important to create signing virtual characters for training and communicating in virtual environments or further applications. The rhythm and speed of fingerspelling play a large role in how well fingerspelling is understood. Using motion capture technologies, we record fingerspelling and analyze timing information about letters in the words. Our goal is to identify fingerspelling timing information and use it to create fingerspelling animations that are natural and understandable.},
  keywords={Timing;Animation;Assistive technology;Gesture recognition;Rhythm;Data mining;Databases;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Animation;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2016.7504777},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504778,
  author={Wiebusch, Dennis and Fischbach, Martin and Niebling, Florian and Latoschik, Marc Erich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Low-cost raycast-based coordinate system registration for consumer depth cameras}, 
  year={2016},
  volume={},
  number={},
  pages={311-312},
  abstract={We present four raycast-based techniques that determine the transformation between a depth camera's coordinate system and the coordinate system defined by a rectangular surface. In addition, the surface's dimensions are measured. In contrast to other approaches, these techniques limit additional hardware requirements to commonly available, low-cost artifacts and focus on simple non-laborious procedures. A preliminary study examining our Kinect v2-based proof of concept revealed promising first results. The utilized software is available as an open-source project.},
  keywords={Cameras;Thumb;Hardware;Three-dimensional displays;Software;Registers;Conferences;H.5.1 [Information Systems]: INFORMATION INTERFACES AND PRESENTATION — Multimedia Information Systems;I.4.8 [Computing Methodologies]: IMAGE PROCESSING AND COMPUTER VISION — Scene Analysis},
  doi={10.1109/VR.2016.7504778},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504779,
  author={Wilson, Brandon and Bounds, Matthew and Tavakkoli, Alireza},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Hand motion calibration and retargeting for intuitive object manipulation in immersive virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={313-314},
  abstract={In this paper a system is proposed to combine small finger movements with the large scale body movements captured from a motion capture system. The strength of the proposed work over previous research is in the real-time and natural interactions that the virtual hands have with their environment. By being able to conform to physics, the virtual hands feel like virtual extensions of one's own hands. This provides a higher degree of immersion and interactivity when compared to more traditional virtual reality systems.},
  keywords={Skeleton;Calibration;Virtual environments;Silicon;Real-time systems;Tracking;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Animation;I.3.8 [Computer Graphics]: Applications},
  doi={10.1109/VR.2016.7504779},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504780,
  author={Xi, Mingze and Smith, Shamus P.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Supporting path switching for non-player characters in a virtual environment}, 
  year={2016},
  volume={},
  number={},
  pages={315-316},
  abstract={Realistic non-player characters (NPC) are an important component of virtual environments. However, generating NPCs with humanlike behaviour can be difficult. A previously proposed pipeline used evacuation simulators to generate domain relevant NPC behaviour for virtual environments. However, the NPCs generated through this process had static behaviours and could not dynamically change pre-computed evacuation paths. In this paper, we extend this pipeline with an approach that supports path switching for NPCs in four situations and demonstrates the pipeline in a virtual environment modelled on a large real building. A scalability test on the large building showed overall evacuation time in both source evacuation simulator and target virtual environment was consistent regardless of building size and complexity. Three test-cases demonstrate path switching for NPCs with increasing evacuation success rates through different situations. The reuse of static paths to enable dynamic NPC behaviour supports ongoing work to develop low cost and realistic training systems using game engine technology.},
  keywords={Switches;Pipelines;Solid modeling;Buildings;Training;Virtual environments;Games;virtual environment;non-player character;NPC behaviour;domain knowledge reuse},
  doi={10.1109/VR.2016.7504780},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504781,
  author={Zielasko, Daniel and Horn, Sven and Freitag, Sebastian and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Evaluation of hands-free HMD-based navigation techniques for immersive data analysis}, 
  year={2016},
  volume={},
  number={},
  pages={317-318},
  abstract={To use the full potential of immersive data analysis when wearing a head-mounted display, the user has to be able to navigate through the spatial data. We collected, developed and evaluated 5 different handsfree navigation methods that are usable while seated in the analyst's usual workplace. All methods meet the requirements of being easy to learn and inexpensive to integrate into existing workplaces. We conducted a user study with 23 participants which showed that a body leaning metaphor and an accelerometer pedal metaphor performed best within the given task.},
  keywords={Navigation;Virtual reality;Three-dimensional displays;Foot;Employment;Data analysis;Accelerometers;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Evaluation/methodology},
  doi={10.1109/VR.2016.7504781},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504782,
  author={Zielinski, David J. and Rao, Hrishikesh and Potter, Nick and Appelbaum, Lawrence G. and Kopper, Regis},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Evaluating the effects of image persistence on dynamic target acquisition in low frame rate virtual environments}, 
  year={2016},
  volume={},
  number={},
  pages={319-320},
  abstract={Here we explore a visual display technique for low frame rate virtual environments called low persistence (LP). This involves displaying the rendered frame for a single display frame and blanking the screen while waiting for the next frame to be generated. To gain greater knowledge about the LP technique, we have conducted a user study to evaluate user performance and learning during a dynamic target acquisition task. The task involved the acquisition of targets moving along several different trajectories, modeled after a shotgun trap shooting task. The results of our study indicate the LP condition approaches high frame rate performance within certain classes of target trajectories. Interestingly we also see that learning is consistent across conditions, indicating that it may not always be necessary to train under a visually high frame rate system.},
  keywords={Visualization;Training;Trajectory;Virtual environments;Phantoms;Solid modeling;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2016.7504782},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504783,
  author={},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Videos}, 
  year={2016},
  volume={},
  number={},
  pages={1-1},
  abstract={Start of the above-titled section of the conference proceedings record.},
  keywords={},
  doi={10.1109/VR.2016.7504783},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504784,
  author={Bera, Aniket and Kim, Sujeong and Manocha, Dinesh},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Interactive and adaptive data-driven crowd simulation: User study}, 
  year={2016},
  volume={},
  number={},
  pages={325-325},
  abstract={We present an adaptive data-driven algorithm for interactive crowd simulation. Our approach combines realistic trajectory behaviors extracted from videos with synthetic multi-agent algorithms to generate plausible simulations. We use statistical techniques to compute the movement patterns and motion dynamics from noisy 2D trajectories extracted from crowd videos. These learned pedestrian dynamic characteristics are used to generate collision-free trajectories of virtual pedestrians in slightly different environments or situations. The overall approach is robust and can generate perceptually realistic crowd movements at interactive rates in dynamic environments. We also present results from preliminary user studies that evaluate the trajectory behaviors generated by our algorithm.},
  keywords={Solid modeling;Trajectory;Adaptation models;Videos;Dynamics;Computational modeling;Behavioral sciences},
  doi={10.1109/VR.2016.7504784},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504785,
  author={Borba, Eduardo Zilles and Cabral, Marcio and de Deus Lopes, Roseli and Zuffo, Marcelo Knorich and Kopper, Regis},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={A fully immersive virtual model to explore archaeological sites}, 
  year={2016},
  volume={},
  number={},
  pages={326-326},
  abstract={In this work we present the methodological approach applied to develop a fully immersive and interactive virtual environment that simulates an archaeological site located in Sao Paulo (Brazil). To create a realistic 3D space, which would be relevant for research through a cyber-archeology exploration, laser scanners and photometry were used for collecting 3D point clouds data from the physical. In consequence, the digital data acquired from these apparatus generated a huge density of point clouds, requiring a many gigabytes computer storage and a research work on design to compact all the information in an user friendly interactive virtual model, but realistic to archaeologists. Also to provide an immersive feeling when exploring the virtual reality we decided to allow the user to navigate through the scene using control devices (keyboard, mouse and joysticks) and a head-mounted display (Oculus Rift) to visualize the aesthetical and spatial elements of the archaeological site as if she/he was really in that place (forms, scales, proportions, perspective, textures, illumination, shadows). In resume, through a sophisticated digital simulation environment, which regards the playful of an electronic game in first-person field of vision, we created a telepresence sense to the user, as well providing archeologists a landscape (and objects) exploration through a non-destructive way.},
  keywords={Three-dimensional displays;Solid modeling;Computational modeling;Virtual environments;Telepresence;Point cloud compression;Excavation;Cyber-archeology;3D big-data;telepresence;virtual reality;immersion;e-science},
  doi={10.1109/VR.2016.7504785},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504786,
  author={Cabral, Marcio and Belloc, Olavo and Montes, Andre and Borba, Eduardo Zilles and Zuffo, Marcelo Knorich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={VR THOR — Virtual reality training with hotstick on operations risks}, 
  year={2016},
  volume={},
  number={},
  pages={327-327},
  abstract={In this work we present a simulator system for training operation and maintenance of power grids distribution lines with focus on workplace safety and risk control of fuse cutout activities. The student uses a VR goggles to visualize the virtual environment (Oculus Rift) and maneuver a real bat to interact with the 3D environment, both tracked by a high precision infrared camera system (OptiTrack). It all provides a high degree of immersion and realism to the user experience. The student arms, back and head are also tracked, and the movements are replicated in a virtual avatar, allowing the instructor to evaluate ergonomic aspects. The system consists of two modules: a) Instructor Interface, which helps her/him to create and to control different challenges in the scenario and, also, to follow the student reactions and behavior; and b) Simulation Interface, which is presented to the student through VR goggles. It is important to underline that the training session can also be viewed on a projected screen by other students, extending the learning process to the observation of mistakes and successes of their peers. The simulator features various risk scenarios such as: climate (sun, rain and wind), lighting (day and night), types of structures, transformer on fire and explosions, short-circuit and electric arc, defective equipment and many other obstacles (trees, cars, windows, swarm of bees, etc)},
  keywords={Virtual reality;Training;Solid modeling;Maintenance engineering;Computational modeling;Tracking;Power grids;Immersion;virtual reality;simulation;power grid maintenance;computer graphic;natural to the user interactions},
  doi={10.1109/VR.2016.7504786},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504787,
  author={Cabral, Marcio and Roque, Gabriel and Nagamura, Mario and Montes, Andre and Borba, Eduardo Zilles and Kurashima, Celso and Zuffo, Marcelo Knorich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Batmen — Hybrid collaborative object manipulation using mobile devices}, 
  year={2016},
  volume={},
  number={},
  pages={328-328},
  abstract={In this work we present an interactive and collaborative 3D object manipulation system using the shelf mobile devices coupled with Augmented Reality (AR) technology that allows multiple users to collaborate concurrently on a scene. Each user interested in participating in this collaboration uses both a mobile device running android and a desktop (or laptop) working in tandem. The 3D scene was visualized by the user in the desktop system. The changes in the scene viewpoint changes and the object manipulations were performed using a mobile device through the AR. The system leverages user's knowledge of common tasks performed on current mobile devices such as pinching for zooming in and out; swiping with one or two fingers for object rotation and press-and-hold for 2 seconds for object translation. As you will see in this video, we built a prototype system (in a maze style) and applied an informal user study with three experienced VR researchers. Users had to carry a 3D cube through three square rings along the maze. In resume, we diagnosed that working in a collaborative way (users A and B) was better and easier than individual one (user C). We registered more than 2 minutes late for the individual experience comparing to the teamwork. It may happen because the two player team shared information, functions and had a multi-perspective view during the task.},
  keywords={Mobile handsets;Three-dimensional displays;Task analysis;Performance evaluation;Visualization;Urban areas;Teamwork;Interaction techniques;collaborative environment;hybrid reality;object manipulation;computer graphics;user experience},
  doi={10.1109/VR.2016.7504787},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504788,
  author={Krupke, Dennis and Lubos, Paul and Demski, Lena and Brinkhoff, Jonas and Weber, Gregor and Willke, Fabian and Steinicke, Frank},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Control methods in a supernatural flight simulator}, 
  year={2016},
  volume={},
  number={},
  pages={329-329},
  abstract={This video presents the experimental setup of an immersive flight simulation system, which combines body tracking with a head-mounted display. Users are hanging freely in a climbing harness in order to improve the impression to fly. The base system was created by a group of students during one semester as a bachelor project. A study revealed that a high degree of presence is achievable by the combination of body-posture based control and 3D visualization via a head-mounted display while hanging freely in a climbing gear. Furthermore the results of a study that compares two different steering methods are presented. Both control methods seem to be quite interesting and intuitively controllable but show individual preferences among the participants of the study. In our experiments very frequently strong vaction effects were reported when flying turns to one side. The setup very likely causes simulator sickness, especially if a participant was bad in controlling his flight.},
  keywords={Resists;Tracking;Three-dimensional displays;Head-mounted displays;Workstations;Visualization;Virtual reality},
  doi={10.1109/VR.2016.7504788},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504789,
  author={Kuchelmeister, Volker and Bennett, Jill},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The Amnesia Atlas VR. A photographic media interface as memory-prosthesis}, 
  year={2016},
  volume={},
  number={},
  pages={330-330},
  abstract={The Amnesia Atlas constitutes a pilot study for a photographic media interface as memory prosthesis. It utilises the wearable life-logging camera SenseCam to shed light on the importance of photographic imagery and place for memory retrieval. It investigates the relationships of spatiotemporal context on memory encoding and retrieval, utilising immersive 3D visualisation. It is doing so by locating geo-tagged SenseCam image sequences within a virtual representation of the place they were captured at. Using an interactive map as a navigational interface, visitors are able to engage with the imagery in their spatial context, and retrace individual journeys by freely explore this virtual representation. The capacity of a viewer to experience ‘presence’ in relation to a place is known to be beneficial to memory encoding and retrieval, and so is the stimulation with multiple cues. The Amnesia Atlas provides users with a rich audio-visual context to view and interact with SenseCam recordings, and has the potential to be a valuable tool in memory research.},
  keywords={Media;Encoding;Cameras;Visualization;Virtual reality;Three-dimensional displays;Spatiotemporal phenomena;SenseCam;Memory;Immersion;Interface;VR},
  doi={10.1109/VR.2016.7504789},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504790,
  author={Marinho, Thiago and Lakshmanan, Arun and Cichella, Venanzio and Widdowson, Christopher and Cui, Hang and Jones, Robert Mitchell and Sebastian, Bentic and Goudeseune, Camille},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={VR study of human-multicopter interaction in a residential setting}, 
  year={2016},
  volume={},
  number={},
  pages={331-331},
  abstract={We have built a virtual-reality simulator to design quadrotor robots that increase the independence of elderly persons living alone. The simulator lets us measure under what conditions a robot makes someone feel intrigued, or alarmed, or unconcerned, extending to nonhumanoids the studies of emotion that have been done for robots with facial expressions. This better model of how the virtual robot is perceived lets us agilely improve its appearance and behavior, containing the combinatorial explosion of parameters to study in costlier real-world experiments. The residence and the robot are rendered with Unity, and viewed with an Oculus Rift. Audio corresponding to flight maneuvers is resynthesized from recordings of an actual quadrotor. Flight dynamics using L1 adaptive control run in MATLAB and Simulink. Through menus operated by a Leap Motion hand tracker, the human subject commands the quadrotor to fly to various rooms (Unity waypoints), along precomputed paths that avoid collisions. The flight path is varied through a mathematical mapping to Laban Motion Factors (e.g., directness, sustainedness, lightness, and boundness) to provoke a range of responses. The subject's emotion is estimated from heart rate, skin conductance, and head tilt from the Rift's IMU (which is a quick metric for discomfort level).},
  keywords={Robots;Older adults;Conferences;Behavioral sciences;Adaptive control;Virtual reality;Transient analysis},
  doi={10.1109/VR.2016.7504790},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504791,
  author={Moser, Kenneth R and Anreddy, Sujan and Swan, J. Edward},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Calibration and interaction in optical see-through augmented reality using leap motion}, 
  year={2016},
  volume={},
  number={},
  pages={332-332},
  abstract={The growing prevalence of hand and gesture tracking technology has led to an increased availability of consumer level devices, such as the Leap Motion controller, and also facilitated the inclusion of similar hardware into forth coming head mounted display offerings, including the Microsoft HoloLens and Moverio Pro BT-2000. In this video, we demonstrate the utility of the Leap Motion for calibrating optical see-through augmented reality systems by employing a variation on Tuceryan and Navab's Single Point Active Alignment Method [3]. We also showcase a straightforward method for calibrating the coordinate frame of the Leap Motion to a secondary tracking system by employing absolute orientation algorithms [2, 1, 4], allowing us to properly transform and visualize hand and finger tracking data from the user's viewpoint. Our combined display and coordinate frame calibration techniques produce a viable mechanism for not only intuitive interaction with virtual objects but also the creation of natural occlusion between computer generated content and the user themselves. We believe that these techniques will be pivotal in the development of novel consumer applications for next generation head mounted display hardware.},
  keywords={Tracking;Calibration;Pattern analysis;Magnetic heads;Hardware;Augmented reality;Transforms;H.5.1 [[Information Interfaces and Presentation]: Multimedia Information Systems]: Artificial, augmented and virtual realities —},
  doi={10.1109/VR.2016.7504791},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7504792,
  author={Nooren, Marieke and Hallema, Steye},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={What do we care 4? A virtual reality music video}, 
  year={2016},
  volume={},
  number={},
  pages={333-333},
  abstract={How to make a filmic language for VR is the issue filmmakers, artists and scientists are breaking their heads over. With the team of WildVreemd, Steye Hallema invented new ways of montage for this VR. Because how to edit in a movie where all the old film rules no longer apply? This resulted in ‘What do we care 4’, a fantastic journey through the vast and open plains of filmic language in VR-video. It's the first serious attempt to explore questions like: Where do you look when you can choose yourself? How do you manipulate the viewer in such a way that he/she focuses on what the storyteller wants? How do you go from one shot to another? Be prepared to be amazed by some creative solutions. The VR-video was nominated for the UK Music Video Awards 2015 in the category Best Interactive and innovative Video, next to international big names like Coldplay, Cee Lo Green and Years & Years. Director Steye Hallema already saw in 2009 the great potential of VR, especially for music. Especially for musicians it literally opens worlds to let fans ‘disappear’ into their musical world. ‘VR is the headphones for the eyes’.},
  keywords={Virtual reality;Music;Video on demand;Production;Motion pictures;Headphones;Fans},
  doi={10.1109/VR.2016.7504792},
  ISSN={2375-5334},
  month={March},}
