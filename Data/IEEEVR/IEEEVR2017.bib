@INPROCEEDINGS{7892225,
  author={Naz, Asma and Kopper, Regis and McMahan, Ryan P. and Nadin, Mihai},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Emotional qualities of VR space}, 
  year={2017},
  volume={},
  number={},
  pages={3-11},
  abstract={The emotional response a person has to a living space is predominantly affected by light, color and texture as space-making elements. In order to verify whether this phenomenon could be replicated in a simulated environment, we conducted a user study in a six-sided projected immersive display that utilized equivalent design attributes of brightness, color and texture in order to assess to which extent the emotional response in a simulated environment is affected by the same parameters affecting real environments. Since emotional response depends upon the context, we evaluated the emotional responses of two groups of users: inactive (passive) and active (performing a typical daily activity). The results from the perceptual study generated data from which design principles for a virtual living space are articulated. Such a space, as an alternative to expensive built dwellings, could potentially support new, minimalist lifestyles of occupants, defined as the neo-nomads, aligned with their work experience in the digital domain through the generation of emotional experiences of spaces. Data from the experiments confirmed the hypothesis that perceivable emotional aspects of real-world spaces could be successfully generated through simulation of design attributes in the virtual space. The subjective response to the virtual space was consistent with corresponding responses from real-world color and brightness emotional perception. Our data could serve the virtual reality (VR) community in its attempt to conceive of further applications of virtual spaces for well-defined activities.},
  keywords={Image color analysis;Color;Brightness;Virtual reality;Context;Psychology;Extraterrestrial measurements;Architectural design;affective space;neo-nomads;aesthetics},
  doi={10.1109/VR.2017.7892225},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892226,
  author={Southgate, Erica and Smith, Shamus P. and Scevak, Jill},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Asking ethical questions in research using immersive virtual and augmented reality technologies with children and youth}, 
  year={2017},
  volume={},
  number={},
  pages={12-18},
  abstract={The increasing availability of intensely immersive virtual, augmented and mixed reality experiences using head-mounted displays (HMD) has prompted deliberations about the ethical implications of using such technology to resolve technical issues and explore the complex cognitive, behavioral and social dynamics of human `virtuality'. However, little is known about the impact such immersive experiences will have on children (aged 0-18 years). This paper outlines perspectives on child development to present conceptual and practical frameworks for conducting ethical research with children using immersive HMD technologies. The paper addresses not only procedural ethics (gaining institutional approval) but also ethics-in-practice (on-going ethical decision-making).},
  keywords={Ethics;Australia;Decision making;Protocols;Augmented reality;Resists;Ethics;virtual reality;augmented reality;mixed reality;children;adolescents;evaluation;child development},
  doi={10.1109/VR.2017.7892226},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892227,
  author={Sargunam, Shyam Prathish and Moghadam, Kasra Rahimi and Suhail, Mohamed and Ragan, Eric D.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Guided head rotation and amplified head rotation: Evaluating semi-natural travel and viewing techniques in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},
  abstract={Traditionally in virtual reality systems, head tracking is used in head-mounted displays (HMDs) to allow users to control viewing using 360-degree head and body rotations. Our research explores interaction considerations that enable semi-natural methods of view control that will work for seated use of virtual reality with HMDs when physically turning all the way around is not ideal, such as when sitting on a couch or at a desk. We investigate the use of amplified head rotations so physically turning in a comfortable range can allow viewing of a 360-degree virtual range. Additionally, to avoid situations where the user's neck is turned in an uncomfortable position for an extended period, we also use redirection during virtual movement to gradually realign the user's head position back to the neutral, straight-ahead position. We ran a controlled experiment to evaluate guided head rotation and amplified head rotation without realignment during movement, and we compared both to traditional one-to-one head-tracked viewing as a baseline for reference. After a navigation task, overall errors on spatial orientation tasks were relatively low with all techniques, but orientation effects, sickness, and preferences varied depending on participants' 3D gaming habits. Using the guided rotation technique, participants who played 3D games performed better, reported higher preference scores, and demonstrated significantly lower sickness results compared to non-gamers.},
  keywords={Legged locomotion;Navigation;Resists;Turning;Three-dimensional displays;Virtual reality;Tracking;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892227},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892228,
  author={Mirhosseini, Seyedkoosha and Gutenko, Ievgeniia and Ojal, Sushant and Marino, Joseph and Kaufman, Arie E.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Automatic speed and direction control along constrained navigation paths}, 
  year={2017},
  volume={},
  number={},
  pages={29-36},
  abstract={For many virtual reality applications, a pre-calculated fly-through path is the de facto standard navigation method. Such a path is convenient for users and ensures coverage of critical areas throughout the scene. Traditional applications use constant camera speed, allow for fully user-controlled manual speed adjustment, or use automatic speed adjustment based on heuristics from the scene. We introduce two novel methods for constrained path navigation and exploration in virtual environments which rely on the natural orientation of the user's head during scene exploration. Utilizing head tracking to obtain the user's area of focus, we perform automatic camera speed adjustment to allow for natural off-axis scene examination. We expand this to include automatic camera navigation along the pre-computed path, abrogating the need for any navigational inputs from the user. Our techniques are applicable for any scene with a pre-computed navigation path, including medical applications such as virtual colonoscopy, coronary fly-through, or virtual angioscopy, and graph navigation. We compare the traditional methods (constant speed and manual speed adjustment) and our two methods (automatic speed adjustment and automatic speed/direction control) to determine the effect of speed adjustment on system usability, mental load, performance, and user accuracy. Through this evaluation we observe the effect of automatic speed adjustment compared to traditional methods. We observed no negative impact from automatic navigation, and the users performed as well as with the manual navigation.},
  keywords={Navigation;Cameras;Manuals;Virtual reality;Head;Resists;Acceleration;3D Navigation;tracking;visualization;medicine;immersion;usability;cybersickness;user studies;HMD},
  doi={10.1109/VR.2017.7892228},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892229,
  author={Huang, Jingwei and Chen, Zhili and Ceylan, Duygu and Jin, Hailin},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={6-DOF VR videos with a single 360-camera}, 
  year={2017},
  volume={},
  number={},
  pages={37-44},
  abstract={Recent breakthroughs in consumer level virtual reality (VR) headsets are creating a growing user-base in demand for immersive, full 3D VR experiences. While monoscopic 360-videos are perhaps the most prevalent type of content for VR headsets, they lack 3D information and thus cannot be viewed with full 6 degree-of-freedom (DOF). We present an approach that addresses this limitation via a novel warping algorithm that can synthesize new views both with rotational and translational motion of the viewpoint. This enables the ability to perform VR playback of input monoscopic 360-videos files in full stereo with full 6-DOF of head motion. Our method synthesizes novel views for each eye in accordance with the 6-DOF motion of the headset. Our solution tailors standard structure-from-motion and dense reconstruction algorithms to work accurately for 360-videos and is optimized for GPUs to achieve VR frame rates (>120 fps). We demonstrate the effectiveness our approach on a variety of videos with interesting content.},
  keywords={Cameras;Three-dimensional displays;Videos;Image reconstruction;Headphones;Geometry;Tracking;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding — Video analysis;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Motion},
  doi={10.1109/VR.2017.7892229},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892230,
  author={MacQuarrie, Andrew and Steed, Anthony},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Cinematic virtual reality: Evaluating the effect of display type on the viewing experience for panoramic video}, 
  year={2017},
  volume={},
  number={},
  pages={45-54},
  abstract={The proliferation of head-mounted displays (HMD) in the market means that cinematic virtual reality (CVR) is an increasingly popular format. We explore several metrics that may indicate advantages and disadvantages of CVR compared to traditional viewing formats such as TV. We explored the consumption of panoramic videos in three different display systems: a HMD, a SurroundVideo+ (SV+), and a standard 16:9 TV. The SV+ display features a TV with projected peripheral content. A between-groups experiment of 63 participants was conducted, in which participants watched panoramic videos in one of these three display conditions. Aspects examined in the experiment were spatial awareness, narrative engagement, enjoyment, memory, fear, attention, and a viewer's concern about missing something. Our results indicated that the HMD offered a significant benefit in terms of enjoyment and spatial awareness, and our SV+ display offered a significant improvement in enjoyment over traditional TV. We were unable to confirm the work of a previous study that showed incidental memory may be lower in a HMD over a TV. Drawing attention and a viewer's concern about missing something were also not significantly different between display conditions. It is clear that passive media viewing consists of a complex interplay of factors, such as the media itself, the characteristics of the display, as well as human aspects including perception and attention. While passive media viewing presents many challenges for evaluation, identifying a number of broadly applicable metrics will aid our understanding of these experiences, and allow the creation of better, more engaging CVR content and displays.},
  keywords={TV;Streaming media;Virtual reality;Resists;Atmospheric measurements;Particle measurements;Extraterrestrial measurements;Cinematic virtual reality;panoramic video;user study},
  doi={10.1109/VR.2017.7892230},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892231,
  author={Verhulst, Adrien and Normand, Jean-Marie and Lombart, Cindy and Moreau, Guillaume},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A study on the use of an immersive virtual reality store to investigate consumer perceptions and purchase behavior toward non-standard fruits and vegetables}, 
  year={2017},
  volume={},
  number={},
  pages={55-63},
  abstract={In this paper we present an immersive virtual reality user study aimed at investigating how customers perceive and if they would purchase non-standard (i.e. misshaped) fruits and vegetables (FaVs) in supermarkets and hypermarkets. Indeed, food waste is a major issue for the retail sector and a recent trend is to reduce it by selling non-standard goods. An important question for retailers relates to the FaVs' “level of abnormality” that consumers would agree to buy. However, this question cannot be tackled using “classical” marketing techniques that perform user studies within real shops since fresh produce such as FaVs tend to rot rapidly preventing studies to be repeatable or to be run for a long time. In order to overcome those limitations, we created a virtual grocery store with a fresh FaVs section where 142 participants were immersed using an Oculus Rift DK2 HMD. Participants were presented either “normal”, “slightly misshaped”, “misshaped” or “severely misshaped” FaVs. Results show that participants tend to purchase a similar number of FaVs whatever their deformity. Nevertheless participants' perceptions of the quality of the FaV depend on the level of abnormality.},
  keywords={Three-dimensional displays;Virtual reality;Color;Clothing;Solid modeling;Visualization;Market research;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Evaluation/Methodology;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892231},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892232,
  author={Sagardia, Mikel and Hulin, Thomas},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Evaluation of a penalty and a constraint-based haptic rendering algorithm with different haptic interfaces and stiffness values}, 
  year={2017},
  volume={},
  number={},
  pages={64-73},
  abstract={This work presents an evaluation study in which the effects of a penalty-based and a constraint-based haptic rendering algorithm on the user performance and perception are analyzed. A total of N = 24 participants performed in a within-design study three variations of peg-in-hole tasks in a virtual environment after trials in an identically replicated real scenario as a reference. In addition to the two mentioned haptic rendering paradigms, two haptic devices were used, the HUG and a Sigma.7, and the force stiffness was also varied with maximum and half values possible for each device. Both objective (time and trajectory, collision performance, and muscular effort) and subjective ratings (contact perception, ergonomy, and workload) were recorded and statistically analyzed. The results show that the constraint-based haptic rendering algorithm with a lower stiffness than the maximum possible yields the most realistic contact perception, while keeping the visual inter-penetration between the objects roughly at around 15% of that caused by penalty-based algorithm (i.e., non perceptible in many cases). This result is even more evident with the HUG, the haptic device with the highest force display capabilities, although user ratings point to the Sigma.7 as the device with highest usability and lowest workload indicators. Altogether, the paper provides qualitative and quantitative guidelines for mapping properties of haptic algorithms and devices to user performance and perception.},
  keywords={Rendering (computer graphics);Force;Performance evaluation;Visualization;Trajectory;Force feedback;H.1.2 [Models and Principles]: User/Machine Systems — Human factors;Human information processing;H.5.2 [Information Systems and Presentation]: User Interfaces — Haptic I/O;I.3.4. [Computer Graphics]: Graphics Utilities — Virtual device interfaces},
  doi={10.1109/VR.2017.7892232},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892233,
  author={Vonach, Emanuel and Gatterer, Clemens and Kaufmann, Hannes},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={VRRobot: Robot actuated props in an infinite virtual environment}, 
  year={2017},
  volume={},
  number={},
  pages={74-83},
  abstract={We present the design and development of a fully immersive virtual reality (VR) system that can provide prop-based haptic feedback in an infinite virtual environment. It is conceived as a research tool for studying topics related to haptics in VR and based on off-the-shelf components. A robotic arm moves physical props, dynamically matching pose and location of an object in the virtual world. When the user reaches for the virtual object, his or her hands also encounter it in the real physical space. The interaction is not limited to specific body parts and does not rely on an external structure like an exoskeleton. In combination with a locomotion platform for close-to-natural walking, this allows unrestricted haptic interaction in a natural way in virtual environments of unlimited size. We describe the concept, the hardware and software architecture in detail. We establish safety design guidelines for human-robot interaction in VR. Our technical evaluation shows good response times and accuracy. We report on a user study conducted with 34 participants indicating promising results, and discuss the potential of our system.},
  keywords={Haptic interfaces;Thumb;Manipulators;Virtual environments;Prop-based virtual reality;encounter-type haptics;passive haptic feedback;fully immersive virtual reality},
  doi={10.1109/VR.2017.7892233},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892234,
  author={Bouyer, Guillaume and Chellali, Amine and Lécuyer, Anatole},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Inducing self-motion sensations in driving simulators using force-feedback and haptic motion}, 
  year={2017},
  volume={},
  number={},
  pages={84-90},
  abstract={Producing sensations of motion in driving simulators often requires using cumbersome and expensive motion platforms. In this article we present a novel and alternative approach for producing self-motion sensations in driving simulations by relying on haptic-feedback. The method consists in applying a force-feedback proportional to the acceleration of the virtual vehicle directly to the hands of the driver, by means of a haptic device attached to the manipulated controller (or a steering wheel). We designed a proof-of-concept based on a standard gamepad physically attached at the extremity of a standard 3DOF haptic display. Haptic effects were designed to match notably the acceleration/braking (longitudinal forces) and left/right turns (lateral forces) of the virtual vehicle. A preliminary study conducted with 23 participants, engaged in gamepad-based active VR navigations in a straight line, showed that haptic motion effects globally improved the involvement and realism of motion sensation for participants with prior experience with haptic devices. Taken together, our results suggest that our approach could be further tested and used in driving simulators in entertainment and/or professional contexts.},
  keywords={Haptic interfaces;Acceleration;Three-dimensional displays;Solid modeling;Context;Wheels;Navigation;Driving Simulation;Self-motion;Haptic;Force-feedback},
  doi={10.1109/VR.2017.7892234},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892235,
  author={Azmandian, Mahdi and Grechkin, Timofey and Rosenberg, Evan Suma},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={An evaluation of strategies for two-user redirected walking in shared physical spaces}, 
  year={2017},
  volume={},
  number={},
  pages={91-98},
  abstract={As the focus of virtual reality technology is shifting from singleperson experiences to multi-user interactions, it becomes increasingly important to accommodate multiple co-located users within a shared real-world space. For locomotion and navigation, the introduction of multiple users moving both virtually and physically creates additional challenges related to potential user-on-user collisions. In this work, we focus on defining the extent of these challenges, in order to apply redirected walking to two users immersed in virtual reality experiences within a shared physical tracked space. Using a computer simulation framework, we explore the costs and benefits of splitting available physical space between users versus attempting to algorithmically prevent user-to-user collisions. We also explore fundamental components of collision prevention such as steering the users away from each other, forced stopping, and user re-orientation. Each component was analyzed for the number of potential disruptions to the flow of the virtual experience. We also develop a novel collision prevention algorithm that reduces overall interruptions by 17.6% and collision prevention events by 58.3%. Our results show that sharing space using our collision prevention method is superior to subdividing the tracked space.},
  keywords={Legged locomotion;Tracking;Collision avoidance;Virtual environments;Trajectory;Navigation;Virtual Reality;Locomotion;Redirected Walking},
  doi={10.1109/VR.2017.7892235},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892236,
  author={Yem, Vibol and Kajimoto, Hiroyuki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Wearable tactile device using mechanical and electrical stimulation for fingertip interaction with virtual world}, 
  year={2017},
  volume={},
  number={},
  pages={99-104},
  abstract={We developed “Finger Glove for Augmented Reality” (FinGAR), which combines electrical and mechanical stimulation to selectively stimulate skin sensory mechanoreceptors and provide tactile feedback of virtual objects. A DC motor provides high-frequency vibration and shear deformation to the whole finger, and an array of electrodes provide pressure and low-frequency vibration with high spatial resolution. FinGAR devices are attached to the thumb, index finger and middle finger. It is lightweight, simple in mechanism, easy to wear, and does not disturb the natural movements of the hand. All of these attributes are necessary for a general-purpose virtual reality system. User study was conducted to evaluate its ability to reproduce sensations of four tactile dimensions: macro roughness, friction, fine roughness and hardness. Result indicated that skin deformation and cathodic stimulation affect macro roughness and hardness, whereas high-frequency vibration and anodic stimulation affect friction and fine roughness.},
  keywords={Vibrations;Thumb;Skin;Electrodes;DC motors;Electrical stimulation;FinGAR;mechanical stimulation;electrical stimulation;virtual touch},
  doi={10.1109/VR.2017.7892236},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892237,
  author={Lee, Myungho and Bruder, Gerd and Welch, Gregory F.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Exploring the effect of vibrotactile feedback through the floor on social presence in an immersive virtual environment}, 
  year={2017},
  volume={},
  number={},
  pages={105-111},
  abstract={We investigate the effect of vibrotactile feedback delivered to one's feet in an immersive virtual environment (IVE). In our study, participants observed a virtual environment where a virtual human (VH) walked toward the participants and paced back and forth within their social space. We compared three conditions as follows: participants in the “Sound” condition heard the footsteps of the VH; participants in the “Vibration” condition experienced the vibration of the footsteps along with the sounds; while participants in the “Mute” condition were not exposed to sound nor vibrotactile feedback. We found that the participants in the “Vibration” condition felt a higher social presence with the VH compared to those who did not feel the vibration. The participants in the “Vibration” condition also exhibited greater avoidance behavior while facing the VH and when the VH invaded their personal space.},
  keywords={Vibrations;Virtual environments;Rubber;Foot;Transducers;Legged locomotion;Back;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented, and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology},
  doi={10.1109/VR.2017.7892237},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892238,
  author={Luo, Ran and Fang, Qiang and Wei, Jianguo and Lu, Wenhuan and Xu, Weiwei and Yang, Yin},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Acoustic VR in the mouth: A real-time speech-driven visual tongue system}, 
  year={2017},
  volume={},
  number={},
  pages={112-121},
  abstract={We propose an acoustic-VR system that converts acoustic signals of human language (Chinese) to realistic 3D tongue animation sequences in real time. It is known that directly capturing the 3D geometry of the tongue at a frame rate that matches the tongue's swift movement during the language production is challenging. This difficulty is handled by utilizing the electromagnetic articulography (EMA) sensor as the intermediate medium linking the acoustic data to the simulated virtual reality. We leverage Deep Neural Networks to train a model that maps the input acoustic signals to the positional information of pre-defined EMA sensors based on 1,108 utterances. Afterwards, we develop a novel reduced physics-based dynamics model for simulating the tongue's motion. Unlike the existing methods, our deformable model is nonlinear, volume-preserving, and accommodates collision between the tongue and the oral cavity (mostly with the jaw). The tongue's deformation could be highly localized which imposes extra difficulties for existing spectral model reduction methods. Alternatively, we adopt a spatial reduction method that allows an expressive subspace representation of the tongue's deformation. We systematically evaluate the simulated tongue shapes with real-world shapes acquired by MRI/CT. Our experiment demonstrates that the proposed system is able to deliver a realistic visual tongue animation corresponding to a user's speech signal.},
  keywords={Tongue;Speech;Magnetic resonance imaging;Hidden Markov models;Solid modeling;Real-time systems;Three-dimensional displays;H.5.1 [Information Systems]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892238},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892239,
  author={Schissler, Carl and Stirling, Peter and Mehra, Ravish},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Efficient construction of the spatial room impulse response}, 
  year={2017},
  volume={},
  number={},
  pages={122-130},
  abstract={An important component of the modeling of sound propagation for virtual reality (VR) is the spatialization of the room impulse response (RIR) for directional listeners. This involves convolution of the listener's head-related transfer function (HRTF) with the RIR to generate a spatial room impulse response (SRIR) which can be used to auralize the sound entering the listener's ear canals. Previous approaches tend to evaluate the HRTF for each sound propagation path, though this is too slow for interactive VR latency requirements. We present a new technique for computation of the SRIR that performs the convolution with the HRTF in the spherical harmonic (SH) domain for RIR partitions of a fixed length. The main contribution is a novel perceptually-driven metric that adaptively determines the lowest SH order required for each partition to result in no perceptible error in the SRIR. By using lower SH order for some partitions, our technique saves a significant amount of computation and is almost an order of magnitude faster than the previous approach. We compared the subjective impact of this new method to the previous one and observe a strong scene-dependent preference for our technique. As a result, our method is the first that can compute high-quality spatial sound for the entire impulse response fast enough to meet the audio latency requirements of interactive virtual reality applications.},
  keywords={Harmonic analysis;Ear;Rendering (computer graphics);Computational modeling;Virtual reality;Measurement;Solid modeling;Spatial audio;HRTF;sound propagation;spherical harmonics},
  doi={10.1109/VR.2017.7892239},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892240,
  author={Malleson, Charles and Kosek, Maggie and Klaudiny, Martin and Huerta, Ivan and Bazin, Jean-Charles and Sorkine-Hornung, Alexander and Mine, Mark and Mitchell, Kenny},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Rapid one-shot acquisition of dynamic VR avatars}, 
  year={2017},
  volume={},
  number={},
  pages={131-140},
  abstract={We present a system for rapid acquisition of bespoke, animatable, full-body avatars including face texture and shape. A blendshape rig with a skeleton is used as a template for customization. Identity blendshapes are used to customize the body and face shape at the fitting stage, while animation blendshapes allow the face to be animated. The subject assumes a T-pose and a single snapshot is captured using a stereo RGB plus depth sensor rig. Our system automatically aligns a photo texture and fits the 3D shape of the face. The body shape is stylized according to body dimensions estimated from segmented depth. The face identity blendweights are optimised according to image-based facial landmarks, while a custom texture map for the face is generated by warping the input images to a reference texture according to the facial landmarks. The total capture and processing time is under 10 seconds and the output is a light-weight, game-engine-ready avatar which is recognizable as the subject. We demonstrate our system in a VR environment in which each user sees the other users' animated avatars through a VR headset with real-time audio-based facial animation and live body motion tracking, affording an enhanced level of presence and social engagement compared to generic avatars.},
  keywords={Face;Avatars;Cameras;Three-dimensional displays;Shape;Solid modeling;Animation;Avatars;Capture;Virtual Reality},
  doi={10.1109/VR.2017.7892240},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892241,
  author={Bodenheimer, Bobby and Creem-Regehr, Sarah and Stefanucci, Jeanine and Shemetova, Elena and Thompson, William B.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Prism aftereffects for throwing with a self-avatar in an immersive virtual environment}, 
  year={2017},
  volume={},
  number={},
  pages={141-147},
  abstract={The use of first-person self-avatars in immersive virtual environments (VEs) has grown over recent years. It is unknown, however, how visual feedback from a self-avatar influences a user's online actions and subsequent calibration of actions within an immersive VE. The current paper uses a prism throwing adaptation paradigm to test the role of a self-avatar arm or full body on action calibration in a VE. Participants' throwing accuracy to a target on the ground was measured first in a normal viewing environment, then with the visual field rotated clockwise about their vertical axis by 17° (prism simulation), and then again in the normal viewing environment with the prism distortion removed. Participants experienced either no-avatar, a first-person avatar arm and hand, or a first-person full body avatar during the entire experimental session, in a between-subjects manipulation. Results showed similar throwing error and adaptation during the prism exposure for all conditions, but a reduced aftereffect (displacement with respect to the target in the opposite direction of the prism-exposure) when the avatar arm or full body was present. The results are discussed in the context of how an avatar can provide a visual frame of reference to aid in action calibration.},
  keywords={Avatars;Visualization;Calibration;Legged locomotion;Tracking;Virtual environments;Head;Virtual reality;prism adaptation;self-avatar},
  doi={10.1109/VR.2017.7892241},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892242,
  author={Cordar, Andrew and Wendling, Adam and White, Casey and Lampotang, Samsun and Lok, Benjamin},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Repeat after me: Using mixed reality humans to influence best communication practices}, 
  year={2017},
  volume={},
  number={},
  pages={148-156},
  abstract={In the past few years, advances have been made on how mixed reality humans (MRHs) can be used for interpersonal communication skills training for medical teams; however, little research has looked at how MRHs can influence communication skills during training. One way to influence communication skills is to leverage MRHs as models of communication behavior. We created a mixed reality medical team training exercise designed to impact communication behaviors that are critical for patient safety. We recruited anesthesia residents to go through an operating room training exercise with MRHs to assess and influence residents' closed loop communication behaviors during medication administration. We manipulated the behavior of the MRHs to determine if the MRHs could influence the residents' closed loop communication behavior. Our results showed that residents' closed loop communications behaviors were influenced by MRHs. Additionally, we found there was a statistically significant difference between groups based on which MRH behavior residents observed. Because the MRHs significantly impacted how residents communicated in simulation, this work expands the boundaries for how VR can be used and demonstrates that MRHs could be used as tools to address complex communication dynamics in a team setting.},
  keywords={Surgery;Training;Virtual reality;Solid modeling;Safety;Information exchange;mixed reality;virtual humans;social influence;training},
  doi={10.1109/VR.2017.7892242},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892243,
  author={Chabra, Rohan and Ilie, Adrian and Rewkowski, Nicholas and Cha, Young-Woon and Fuchs, Henry},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Optimizing placement of commodity depth cameras for known 3D dynamic scene capture}, 
  year={2017},
  volume={},
  number={},
  pages={157-166},
  abstract={Commodity depth cameras, such as the Microsoft Kinect®, have been widely used for the capture and reconstruction of the 3D structure of room-sized dynamic scenes. Camera placement and coverage during capture significantly impact the quality of the resulting reconstruction. In particular, dynamic occlusions and sensor interference have been shown to result in poor resolution and holes in the reconstruction results. This paper presents a novel algorithmic framework and a method for off-line optimization of depth cameras placements for a given 3D dynamic scene, simulated using virtual 3D models. We derive a fitness metric for a particular configuration of sensors by combining factors such as visibility and resolution of the entire dynamic scene with probabilities of interference between sensors. We employ this fitness metric both in a greedy algorithm that determines the number of depth cameras needed to cover the scene, and in a simulated annealing algorithm that optimizes the placements of those sensors. We compare our algorithm's optimized placements with manual sensor placements for a real dynamic scene. We present quantitative assessments using our fitness metric, as well as qualitative assessments to demonstrate that our algorithm not only enhances the resolution and total coverage of the reconstruction, but also fills in voids by avoiding occlusions and sensor interference when compared with the reconstruction of the same scene using mual sensor placement.},
  keywords={Cameras;Three-dimensional displays;Surface reconstruction;Measurement;Heuristic algorithms;Solid modeling;Computational modeling;G.1.6 [Numerical Analysis]: Optimization — Global optimization;Simulated annealing;I.4.8 [Computing Methodologies]: Image Processing and Computer Vision — Reconstruction;Scene Analysis;I.6.3 [Computing Methodologies]: Simulation and Modeling — Applications;Model Development},
  doi={10.1109/VR.2017.7892243},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892244,
  author={Beck, Stephan and Froehlich, Bernd},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Sweeping-based volumetric calibration and registration of multiple RGBD-sensors for 3D capturing systems}, 
  year={2017},
  volume={},
  number={},
  pages={167-176},
  abstract={The accurate calibration and registration of a set of color and depth (RGBD) sensors into a shared coordinate system is an essential requirement for 3D surround capturing systems. We present a method to calibrate multiple unsynchronized RGBD-sensors with high accuracy in a matter of minutes by sweeping a tracked checkerboard through the desired capturing space in front of each sensor. Through the sweeping process, a large number of robust correspondences between the depth and the color image as well as the 3D world positions can be automatically established. In order to obtain temporally synchronized correspondences between an RGBD-sensor's data streams and the tracked target's positions we apply an off-line optimization process based on error minimization and a coplanarity constraint. The correspondences are entered into a 3D look-up table which is used during runtime to transform depth and color information into the application's world coordinate system. Our proposed method requires a manual effort of less than one minute per RGBD-sensor and achieves a high calibration accuracy with an average 3D error below 3.5 mm and an average texture reprojection error smaller than 1 pixel.},
  keywords={Calibration;Three-dimensional displays;Adaptive optics;Optical sensors;Optical imaging;Image color analysis;I.4 [Image Processing and computer vision]: Digitization and Image Capture — Camera calibration},
  doi={10.1109/VR.2017.7892244},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892245,
  author={Suzuki, Katsuhiro and Nakamura, Fumihiko and Otsuka, Jiu and Masai, Katsutoshi and Itoh, Yuta and Sugiura, Yuta and Sugimoto, Maki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Recognition and mapping of facial expressions to avatar by embedded photo reflective sensors in head mounted display}, 
  year={2017},
  volume={},
  number={},
  pages={177-185},
  abstract={We propose a facial expression mapping technology between virtual avatars and Head-Mounted Display (HMD) users. HMD allow people to enjoy an immersive Virtual Reality (VR) experience. A virtual avatar can be a representative of the user in the virtual environment. However, the synchronization of the the virtual avatar's expressions with those of the HMD user is limited. The major problem of wearing an HMD is that a large portion of the user's face is occluded, making facial recognition difficult in an HMD-based virtual environment. To overcome this problem, we propose a facial expression mapping technology using retro-reflective photoelectric sensors. The sensors attached inside the HMD measures the distance between the sensors and the user's face. The distance values of five basic facial expressions (Neutral, Happy, Angry, Surprised, and Sad) are used for training the neural network to estimate the facial expression of a user. We achieved an overall accuracy of 88% in recognizing the facial expressions. Our system can also reproduce facial expression change in real-time through an existing avatar using regression. Consequently, our system enables estimation and reconstruction of facial expressions that correspond to the user's emotional changes.},
  keywords={Avatars;Resists;Face recognition;Face;Neural networks;Optical sensors;H.5.m. [Information Interfaces and Presentation (e.g. HCI)]: Miscellaneous},
  doi={10.1109/VR.2017.7892245},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892246,
  author={O'Leary, Patrick and Jhaveri, Sankhesh and Chaudhary, Aashish and Sherman, William and Martin, Ken and Lonie, David and Whiting, Eric and Money, James and McKenzie, Sandy},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Enhancements to VTK enabling scientific visualization in immersive environments}, 
  year={2017},
  volume={},
  number={},
  pages={186-194},
  abstract={Modern scientific, engineering and medical computational simulations, as well as experimental and observational data sensing/measuring devices, produce enormous amounts of data. While statistical analysis provides insight into this data, scientific visualization is tactically important for scientific discovery, product design and data analysis. These benefits are impeded, however, when scientific visualization algorithms are implemented from scratch — a time-consuming and redundant process in immersive application development. This process can greatly benefit from leveraging the state-of-the-art open-source Visualization Toolkit (VTK) and its community. Over the past two (almost three) decades, integrating VTK with a virtual reality (VR) environment has only been attempted to varying degrees of success. In this paper, we demonstrate two new approaches to simplify this amalgamation of an immersive interface with visualization rendering from VTK. In addition, we cover several enhancements to VTK that provide near real-time updates and efficient interaction. Finally, we demonstrate the combination of VTK with both Vrui and OpenVR immersive environments in example applications.},
  keywords={Rendering (computer graphics);Data visualization;Context;Software;Geometry;Pipelines;Laboratories;Scientific visualization;immersive environments;virtual reality},
  doi={10.1109/VR.2017.7892246},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892247,
  author={Feng, Lele and Yang, Xubo and Xiao, Shuangjiu},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={MagicToon: A 2D-to-3D creative cartoon modeling system with mobile AR}, 
  year={2017},
  volume={},
  number={},
  pages={195-204},
  abstract={We present MagicToon, an interactive modeling system with mobile augmented reality (AR) that allows children to build 3D cartoon scenes creatively from their own 2D cartoon drawings on paper. Our system consists of two major components: an automatic 2D-to-3D cartoon model creator and an interactive model editor to construct more complicated AR scenes. The model creator can generate textured 3D cartoon models according to 2D drawings automatically and overlay them on the real world, bringing life to flat cartoon drawings. With our interactive model editor, the user can perform several optional operations on 3D models such as copying and animating in AR context through a touchscreen of a handheld device. The user can also author more complicated AR scenes by placing multiple registered drawings simultaneously. The results of our user study have shown that our system is easier to use compared with traditional sketch-based modeling systems and can give more play to children's innovations compared with AR coloring books.},
  keywords={Solid modeling;Three-dimensional displays;Two dimensional displays;Color;Augmented reality;Mobile handsets;Atmospheric modeling;2D-to-3D;modeling;augmented reality;mobile devices;user interface;coloring},
  doi={10.1109/VR.2017.7892247},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892248,
  author={Danieau, Fabien and Guillo, Antoine and Doré, Renaud},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Attention guidance for immersive video content in head-mounted displays}, 
  year={2017},
  volume={},
  number={},
  pages={205-206},
  abstract={Immersive videos allow users to freely explore 4 π steradian scenes within head-mounted displays (HMD), leading to a strong feeling of immersion. However users may miss important elements of the narrative if not facing them. Hence, we propose four visual effects to guide the user's attention. After an informal pilot study, two of the most efficient effects were evaluated through a user study. Results show that our approach has potential but it remains challenging to implicitly drive the user's attention outside of the field of view.},
  keywords={Cameras;Visual effects;Virtual reality;Head;Resists;Motion pictures;Image color analysis;HMD;immersive movies;visual attention},
  doi={10.1109/VR.2017.7892248},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892249,
  author={Barbulescu, Adela and Garcia, Maxime and Begault, Antoine and Cani, Marie-Paule and Portaz, Maxime and Viand, Alexis and Dulery, Romain and Boissieux, Laurence and Heinish, Pierre and Ronfard, Remi and Vaufreydaz, Dominique},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A system for creating virtual reality content from make-believe games}, 
  year={2017},
  volume={},
  number={},
  pages={207-208},
  abstract={Pretend play is a storytelling technique, naturally used from very young ages, which relies on object substitution to represent the characters of the imagined story. We propose a system which assists the storyteller by generating a virtualized story from a recorded dialogue performed with 3D printed figurines. We capture the gestures and facial expressions of the storyteller using Kinect cameras and IMU sensors and transfer them to their virtual counterparts in the story-world. As a proof-of-concept, we demonstrate our system with an improvised story involving a prince and a witch, which was successfully recorded and transferred into 3D animation.},
  keywords={Head;Speech;Virtual reality;Cameras;Magnetic heads;Three-dimensional displays;Sensors;I.3.7 [Computer graphics]: Three-Dimensional Graphics and Realism — Virtual reality;I.3.7 [Computer graphics]: Three-Dimensional Graphics and Realism — Animation},
  doi={10.1109/VR.2017.7892249},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892250,
  author={Singh, Mohit and Jung, Byunghoo},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={High-definition wireless personal area tracking using AC magnetic field for virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={209-210},
  abstract={This paper presents an AC magnetic field based High-Definition Personal Area Tracking (PAT) system. A low-power transmitter antenna acts as a reference for three tracker modules. One module, attached to the Head Mount Display (HMD), tracks the position and orientation of user's head and the other two hand-held modules act as an interface device (like virtual hands) in Virtual Reality. This precise, low power, low latency, non-line-of-sight system provides an easy-to-use human-computer interface. The system achieves a precision of 1 mm in position with 0.1 degree in orientation and an accuracy of 20 cm in position at a distance of 2 m from the antenna. The transmitter and the receiver consume 5 W and 0.4 W of power, respectively, providing 140 updates/sec with 11 ms of latency.},
  keywords={Transmitters;Receiving antennas;Resists;Signal to noise ratio;Biological system modeling;Virtual reality;K.5.2 [Information Interfaces and Presentation]: User Interfaces — Input devices and strategies},
  doi={10.1109/VR.2017.7892250},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892251,
  author={Hiroi, Yuichi and Itoh, Yuta and Hamasaki, Takumi and Iwai, Daisuke and Sugimoto, Maki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={HySAR: Hybrid material rendering by an optical see-through head-mounted display with spatial augmented reality projection}, 
  year={2017},
  volume={},
  number={},
  pages={211-212},
  abstract={We propose a hybrid SAR concept combining a projector and Optical See-Through Head-Mounted Displays (OST-HMD). Our proposed hybrid SAR system utilizes OST-HMD as an extra rendering layer to render a view-dependent property in OST-HMDs according to the viewer's viewpoint. Combined with view-independent components created by a static projector, the viewer can see richer material contents. Unlike conventional SAR systems, our system theoretically allows unlimited number of viewers seeing enhanced contents in the same space while keeping the existing SAR experiences. Furthermore, the system enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With a proof-of-concept system that consists of a projector and an OST-HMD, we qualitatively demonstrate that our system successfully creates hybrid rendering on a hemisphere object from five horizontal viewpoints. Our quantitative evaluation also shows that our system increases the dynamic range by 2.1 times and the maximum intensity by 1.9 times compared to an ordinary SAR system.},
  keywords={Rendering (computer graphics);Dynamic range;Spatial resolution;Cameras;Optical imaging;Electronic mail;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892251},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892252,
  author={Itoh, Yuta and Orlosky, Jason and Kiyokawa, Kiyoshi and Amano, Toshiyuki and Sugimoto, Maki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Monocular focus estimation method for a freely-orienting eye using Purkinje-Sanson images}, 
  year={2017},
  volume={},
  number={},
  pages={213-214},
  abstract={We present a method for focal distance estimation of a freely-orienting eye using Purkinje-Sanson (PS) images, which are reflections of light on the inner structures of the eye. Using an infrared camera with a rigidly-fixed LED, our method creates an estimation model based on 3D gaze and the distance between reflections in the PS images that occur on the corneal surface and anterior surface of the eye lens. The distance between these two reflections changes with focus, so we associate that information to the focal distance on a user. Unlike conventional methods that mainly relies on 2D pupil size which is sensitive to scene lighting and the fourth PS image, our method detects the third PS image which is more representative of accommodation. Our feasibility study on a single user with a focal range from 15–45 cm shows that our method achieves mean and median absolute errors of 3.15 and 1.93 cm for a 10-degree viewing angle. The study shows that our method is also tolerant against environment lighting changes.},
  keywords={Cameras;Three-dimensional displays;Lighting;Estimation;Measurement by laser beam;Solid modeling;Two dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892252},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892253,
  author={Kitson, Alexandra and Hashemian, Abraham M. and Stepanova, Ekaterina R. and Kruijff, Ernst and Riecke, Bernhard E.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Lean into it: Exploring leaning-based motion cueing interfaces for virtual reality movement}, 
  year={2017},
  volume={},
  number={},
  pages={215-216},
  abstract={We describe here a pilot user study comparing five different locomotion interfaces for virtual reality (VR) locomotion. We compared a standard non-motion cueing interface, Joystick, with four leaning-based seated motion-cueing interfaces: NaviChair, MuvMan, Head-Directed and Swivel Chair. The aim of this mixed methods study was to investigate the usability and user experience of each interface, in order to better understand relevant factors and guide the design of future ground-based VR locomotion interfaces. We asked participants to give talk-aloud feedback and simultaneously recorded their responses while they were performing a search task in VR. Afterwards, participants completed an online questionnaire. Although the Joystick was rated as more comfortable and precise than the other interfaces, the leaning-based interfaces showed a trend to provide more enjoyment and a greater sense of self-motion. There were also potential issues of using velocity-control for rotations in leaning-based interfaces when using HMDs instead of stationary displays. Developers need to focus on improving the controllability and perceived safety of these seated motion cueing interfaces.},
  keywords={Tracking;Controllability;Navigation;Usability;Electronic mail;Virtual environments;Active locomotion;motion cueing;natural user interface;virtual reality;virtual locomotion},
  doi={10.1109/VR.2017.7892253},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892254,
  author={Janeh, Omar and Langbehn, Eike and Steinicke, Frank and Bruder, Gerd and Gulberti, Alessandro and Poetter-Nerger, Monika},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Biomechanical analysis of (non-)isometric virtual walking of older adults}, 
  year={2017},
  volume={},
  number={},
  pages={217-218},
  abstract={Our study investigates the effects of (non-)isometric mappings between physical movements and virtual motions in the virtual environment (VE) on walking biomechanics of older adults. Three primary domains (pace, base of support and phase) of spatio-temporal and temporo-phasic parameters were used to evaluate gait performance. Our results show similar results in pace and phasic domains when older adults walk in the VE in the isometric mapping condition compared to the corresponding parameters in the real world. We found significant differences in base of support for our user group between walking in the VE and real world. For non-isometric mappings we found an increased divergence of gait parameters in all domains correlating with the up-or down-scaled velocity of visual self-motion feedback.},
  keywords={Legged locomotion;Resists;Biomechanics;Electronic mail;Virtual environments;Visualization;Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892254},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892255,
  author={Azimi, Ehsan and Qian, Long and Kazanzides, Peter and Navab, Nassir},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Robust optical see-through head-mounted display calibration: Taking anisotropic nature of user interaction errors into account}, 
  year={2017},
  volume={},
  number={},
  pages={219-220},
  abstract={Uncertainty in measurement of point correspondences negatively affects the accuracy and precision in the calibration of head-mounted displays (HMD). In general, the distribution of alignment errors for optical see-through calibration are not isotropic, and one can estimate its distribution based on interaction requirements of a given calibration process and the user's measurable head motion and hand-eye coordination characteristics. Current calibration methods, however, mostly utilize the Direct Linear Transformation (DLT) method which minimizes Euclidean distances for HMD projection matrix estimation, disregarding the anisotropicity in the alignment errors. We utilize the error covariance in order to take the anisotropic nature of error distribution into account. The main hypothesis of this study is that using Mahalonobis distance within the nonlinear optimization can improve the accuracy of the HMD calibration. The simulation results indicate that our new method outperforms the standard DLT method both in accuracy and precision, and is more robust against user alignment errors. To the best of our knowledge, this is the first time that anisotropic noise has been accommodated in the optical see-through HMD calibration.},
  keywords={Calibration;Resists;Robustness;Adaptive optics;Nonlinear optics;Standards;Uncertainty;augmented reality;HMD;calibration;error propagation;Mahalonobis distance},
  doi={10.1109/VR.2017.7892255},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892256,
  author={Zielinski, David J. and Nankivil, Derek and Kopper, Regis},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={6 Degrees-of-freedom manipulation with a transparent, tangible object in world-fixed virtual reality displays}, 
  year={2017},
  volume={},
  number={},
  pages={221-222},
  abstract={We propose Specimen Box, an interaction technique that allows world-fixed display (such as CAVEs) users to naturally hold a plausible physical object while manipulating virtual content inside it. This virtual content is rendered based on the tracked position of the box. Specimen Box provides the weight and tactile feel of an actual object and does not occlude rendered objects in the scene. The end result is that the user sees the virtual content as if it exists inside the clear physical box. We conducted a user study which involved a cognitively loaded inspection task requiring extensive manipulation of the box. We compared Specimen Box to Grab-and-Twirl, a naturalistic bimanual manipulation technique that closely mimics the mechanics of our proposed technique. Results show that performance was significantly faster with Specimen Box. Further, performance of the control technique was positively affected by experience with Specimen Box.},
  keywords={Haptic interfaces;Inspection;Face;Three-dimensional displays;Virtual environments;Biomedical engineering;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892256},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892257,
  author={Kobori, Norimasa and Deguchi, Daisuke and Ide, Ichiro and Murase, Hiroshi},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Proposal of a spectral random dots marker using local feature for posture estimation}, 
  year={2017},
  volume={},
  number={},
  pages={223-224},
  abstract={We propose a novel marker for robot's grasping task which has the following three aspects: (i) it is easy-to-find in a cluttered background, (ii) it is calculable for its posture (iii) its size is compact. The proposed marker is composed of a random dots pattern, and uses keypoint detection and a scale estimation by Spectral SIFT for dots detection and data decoding. The data is encoded by the scale size of dots, and the same dots in the marker work for both marker detection and data decoding. As a result, the proposed marker size can be compact. We confirmed the effectiveness of the proposed marker through experiments.},
  keywords={Decoding;Image edge detection;Estimation;Cameras;Noise reduction;Robots;Electronic mail;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892257},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892258,
  author={Shriram, Ketaki and Schwartz, Raz},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={All are welcome: Using VR ethnography to explore harassment behavior in immersive social virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={225-226},
  abstract={The growing ubiquity of VR headsets has given rise to questions around harassment in social virtual reality. This paper presents two studies. In the first, a pilot ethnographic study, users were interviewed in immersive social virtual reality about their experiences and behaviors in these spaces. Harassment was occasional, and those in female avatars reported more harassment than those in male avatars. In Study Two, a quantitative survey was conducted to validate ethnographic results. A large percentage of users witness harassment in virtual reality. These studies provide mixed methods insight of user demographics and behaviors in the relatively new social VR space.},
  keywords={Avatars;Interviews;Games;Business;Headphones;Buildings;Experimental methodology;Ethnography},
  doi={10.1109/VR.2017.7892258},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892259,
  author={Ricca, Aylen and Chellali, Amine and Otmane, Samir},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Study of interaction fidelity for two viewpoint changing techniques in a virtual biopsy trainer}, 
  year={2017},
  volume={},
  number={},
  pages={227-228},
  abstract={Virtual Reality simulators are increasingly used for training novice surgeons. However, there is currently a lack of guidelines for achieving interaction fidelity for these systems. In this paper, we present the design of two navigation techniques for a needle insertion trainer. The two techniques were analyzed using a state-of-the-art fidelity framework to determine their levels of interaction fidelity. A user study comparing both techniques suggests that the higher fidelity technique is more suited as a navigation technique for the needle insertion virtual trainer.},
  keywords={Navigation;Training;Surgery;Needles;Virtual reality;Biopsy;Visualization;Interaction fidelity;Surgical training;User evaluation},
  doi={10.1109/VR.2017.7892259},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892260,
  author={Nakano, Takuya and Yanagida, Yasuyuki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Conditions influencing perception of wind direction by the head}, 
  year={2017},
  volume={},
  number={},
  pages={229-230},
  abstract={Recently, several virtual reality (VR) systems using wind have been built to enhance the user's sense of presence. If wind is used, users need not use additional devices, and some studies have concluded that using wind and presenting a video simultaneously improves this sense of presence. In these studies, however, wind sources were sparsely arranged; with such an arrangement, it is unclear whether an accurate environment was reproduced. A number of variables including gender, age, and the facial hit rate may affect the perception of wind direction. In the proposed study, we have examined the effect of these variables on perception of wind direction by the head.},
  keywords={Fans;Virtual reality;Standards;Motion pictures;Haptic interfaces;Face;Wind forecasting;perception;haptics;presence;psychology},
  doi={10.1109/VR.2017.7892260},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892261,
  author={Steed, Anthony and Adipradana, Yonathan Widya and Friston, Sebastian},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={The AR-Rift 2 prototype}, 
  year={2017},
  volume={},
  number={},
  pages={231-232},
  abstract={Video see-through augmented reality (VSAR) is an effective way of combing real and virtual scenes for head-mounted human computer interfaces. In this paper we present the AR-Rift 2 system, a cost-effective prototype VSAR system based around the Oculus Rift CV1 head-mounted display (HMD). Current consumer camera systems however typically have latencies far higher than the rendering pipeline of current consumer HMDs. They also have lower update rate than the display. We thus measure the latency of the video and implement a simple image-warping method to ensure smooth movement of the video.},
  keywords={Augmented reality;Prototypes;Streaming media;Resists;Lenses;Webcams;augmented reality;latency;image-based rendering},
  doi={10.1109/VR.2017.7892261},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892262,
  author={Krum, David M. and Phan, Thai and Kang, Sin-Hwa},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Motor adaptation in response to scaling and diminished feedback in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={233-234},
  abstract={As interaction techniques involving scaling of motor space in virtual reality are becoming more prevalent, it is important to understand how individuals adapt to such scalings and how they re-adapt back to non-scaled norms. This preliminary work examines how individuals, performing a targeted ball throwing task, adapted to addition and removal of a translational scaling of the ball's forward flight. This was examined under various conditions: flight of the ball shown with no delay, hidden flight of the ball with no delay, and hidden flight with a 2 second delay. Hiding the ball's flight, as well as the delay, created disruptions in the ability of the participants to perform the task and adapt to new scaling conditions.},
  keywords={Delays;Physics;Virtual environments;Target tracking;Visualization;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.1.2 [Information Systems]: User/Machine Systems — Human Factors},
  doi={10.1109/VR.2017.7892262},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892263,
  author={Shimana, Isao and Amano, Toshiyuki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Separation of reflective and fluorescent components using the color mixing matrix}, 
  year={2017},
  volume={},
  number={},
  pages={235-236},
  abstract={In the field of SAR, the projector-camera system has been well studied; its radiometric model can be easily described by a color mixing matrix. Many SAR applications have proposed and created by using this model. However, this model can be used for reflectance component, but not for fluorescence component. In this paper, we propose RKS Projector-Camera response model for separating of the color mixing matrix's reflectance components and fluorescence components and describe how to decompose them.},
  keywords={Matrix decomposition;Image color analysis;Solid modeling;Color;Lighting;Cameras;Computational modeling;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Color;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892263},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892264,
  author={Yu, Ka Chun and Saham, Kamran and Sahami, Victoria and Sessions, Larry and Denn, Grant},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Group immersive education with digital fulldome planetariums}, 
  year={2017},
  volume={},
  number={},
  pages={237-238},
  abstract={Although fulldome video digital theaters evolved from traditional planétariums, they are more akin to virtual reality (VR) theaters that create large-scale, group immersive experiences. In order to help understand how immersion and wide fields-of-view (FOV) impact learning, we studied the use of visualizations on topics that do and do not require spatial understanding in astronomy classes. We find a significant difference between students who viewed visualizations in the dome versus those that saw non-immersive content in their classrooms, with the former showing the greatest retention. Our results suggest that immersive visuals help free up cognitive resources that can be used to build mental models requiring spatial understanding, and the physical display size combined with the wide FOV may result in greater attention. Although fulldome is a complementary medium to traditional VR, our results have implications for future head-mounted displays.},
  keywords={Visualization;Education;Astronomy;Virtual environments;Three-dimensional displays;Cognitive science;1.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;1.4.0 [Image Processing and Computer Vision]: General — Image displays},
  doi={10.1109/VR.2017.7892264},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892265,
  author={Zhou, Zhong and Bian, Zhiyi and Zhuo, Zheng},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={MR sand table: Mixing real-time video streaming in physical models}, 
  year={2017},
  volume={},
  number={},
  pages={239-240},
  abstract={A novel prototype of MR (Mixed Reality) Sand Table is presented in this paper, that fuses multiple real-time video streaming into a physically united view. The main processes include geometric calibration and alignment, image blending and the final projection. Firstly we proposed a two-step MR alignment scheme which estimates the transform matrix between input video streaming and the sand table for coarse alignment, and deforms the input frame using moving least squares for accurate alignment. To overcome the video border distinction problem, we make a border-adaptive image stitching with brightness diffusion to merge the overlapping area. With the projection, the video area can be mixed into the sand table in real-time to provide a live physical mixed reality model. We build a prototype to demonstrate the effectiveness of the proposed method. This design could also be easily extended to large size with help of multiple projectors. The system proposed in this paper supports multiple user interaction in a broad area of applications such as surveillance, demonstration, action preview and discussion assistances.},
  keywords={Streaming media;Cameras;Virtual reality;Prototypes;Real-time systems;Surveillance;Solid modeling;Mixed Reality;Spatial Augmented Reality;Video Streaming;Sand Table},
  doi={10.1109/VR.2017.7892265},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892266,
  author={You, Suya and Thompson, Charles. K.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Mobile collaborative mixed reality for supporting scientific inquiry and visualization of earth science data}, 
  year={2017},
  volume={},
  number={},
  pages={241-242},
  abstract={This work seeks to apply the emerging virtual and mixed reality techniques to visual exploration and visualization of earth science data. A novel system is developed to facilitate a collaborative mixed reality visualization, enabling both in-situ and off-site users to simultaneously interact with and visualize science data within mixed reality realm. We implement the prototype system in the context of visualizing earth terrain data. We report our current prototype effort and preliminary results.},
  keywords={Data visualization;Target tracking;Visualization;Collaboration;Cameras;Augmented reality;Mixed reality;augment reality;data visualization},
  doi={10.1109/VR.2017.7892266},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892267,
  author={Georgiev, Georgi V. and Yamada, Kaori and Taura, Toshiharu and Kostakos, Vassilis and Pouke, Matti and Yung, Sylvia Tzvetanova and Ojala, Timo},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Augmenting creative design thinking using networks of concepts}, 
  year={2017},
  volume={},
  number={},
  pages={243-244},
  abstract={Here we propose an interactive system to augment creative design thinking using networks of concepts in a virtual reality environment. We discuss how to augment the human capacity to be creative through dynamic suggestions providing new and original ideas, based on specific semantic network characteristics. We outline directions to explore the structures of the concept network and their connection to creative concept generation. It is expected that augmented creative thinking will allow the user to have more original ideas and thus be more innovative.},
  keywords={Semantics;Visualization;Virtual environments;Ubiquitous computing;Creativity;Three-dimensional displays;Creativity;concept generation;creative thinking;augmented thinking;design thinking;concept network},
  doi={10.1109/VR.2017.7892267},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892268,
  author={Zhang, Zhenliang and Weng, Dongdong and Liu, Yue and Wang, Yongtian and Zhao, Xinjun},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={RIDE: Region-induced data enhancement method for dynamic calibration of optical see-through head-mounted displays}, 
  year={2017},
  volume={},
  number={},
  pages={245-246},
  abstract={The most commonly used single point active alignment method (SPAAM) is based on a static pinhole camera model, in which it is assumed that both the eye and the HMD are fixed. This leads to a limitation for calibration precision. In this work, we propose a dynamic pinhole camera model according to the fact that the human eye would experience an obvious displacement over the whole calibration process. Based on such a camera model, we propose a new calibration data acquisition method called the region-induced data enhancement (RIDE) to revise the calibration data. The experimental results prove that the proposed dynamic model performs better than the traditional static model in actual calibration.},
  keywords={Calibration;Cameras;Solid modeling;Standards;Adaptive optics;Three-dimensional displays;Virtual reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892268},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892269,
  author={Matsumoto, Keigo and Narumi, Takuji and Ban, Yuki and Tanikawa, Tomohiro and Hirose, Michitaka},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Turn physically curved paths into virtual curved paths}, 
  year={2017},
  volume={},
  number={},
  pages={247-248},
  abstract={Redirected walking allows users to explore a large virtual environment while there is a limitation of the room size. Previous works tried to present users straight path in a virtual environment while they walked on a curved path in reality. We expand a previous technique to present users a various curved path in a virtual environment while they walked on a particular curved path or a straight path with/without haptics. Furthermore, we propose a novel estimation methodology to quantify walking paths which user has thought he walked in reality. The data from our experiment shows that users feel walking a various curved path in VR as same as one-to-one mapping condition.},
  keywords={Haptic interfaces;Legged locomotion;Estimation;Virtual environments;Cameras;Visualization;Tracking;VR;redirected walking;visuo-haptic interaction},
  doi={10.1109/VR.2017.7892269},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892270,
  author={Guo, Jie and Weng, Dongdong and Been-Lirn Duh, Henry and Liu, Yue and Wang, Yongtian},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Effects of using HMDs on visual fatigue in virtual environments}, 
  year={2017},
  volume={},
  number={},
  pages={249-250},
  abstract={There are few negative effects to make people discomfort using virtual reality systems. In this paper, we investigated the effects of visual fatigue when wearing head-mounted displays (HMD) and compared the results with those from the smartphones. Forty subjects were recruited and divided into two different groups. The visual fatigue scale was measured to assess the subjects' performance. The results indicated that visual fatigue caused by the conflict of focal distance and vergence distance was less severe than visual fatigue caused by long-term focus without accommodation.},
  keywords={Visualization;Fatigue;Resists;Smart phones;Games;Strain;TV;H.1.2 [Information System]: Users/Machine System — Human factors},
  doi={10.1109/VR.2017.7892270},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892271,
  author={Jung, Jinwoong and Lee, Joon-Young and Kim, Byungmoon and Lee, Seungyong},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Upright adjustment of 360 spherical panoramas}, 
  year={2017},
  volume={},
  number={},
  pages={251-252},
  abstract={With the recent advent of 360 cameras, spherical panorama images are becoming more popular and widely available. In a spherical panorama, alignment of the scene orientation to the image axes is important for providing comfortable and pleasant viewing experiences using VR headsets and traditional displays. This paper presents an automatic framework for upright adjustment of 360 spherical panorama images without any prior information, such as depths and Gyro sensor data. We take the Atlanta world assumption and use the horizontal and vertical lines in the scene to formulate a cost function for upright adjustment. Our method produces visually pleasing results for a variety of real-world spherical panoramas in less than a second.},
  keywords={Cameras;Image segmentation;Cost function;Three-dimensional displays;Image resolution;Image edge detection;Distortion;spherical panorama;upright adjustment},
  doi={10.1109/VR.2017.7892271},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892272,
  author={Collingwoode-Williams, Tara and Gillies, Marco and McCall, Cade and Pan, Xueni},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={The effect of lip and arm synchronization on embodiment: A pilot study}, 
  year={2017},
  volume={},
  number={},
  pages={253-254},
  abstract={We are interested the effect of lip and arm synchronization on body ownership in VR (the illusion that the users own a virtual body). Participants were invited to give a presentation in an HMD, while seeing in a virtual mirror a gender-matched avatar who copied their arm and lip movements in sync and a-sync conditions. We measure participants' reaction with questionnaires administrated verbally after their presentation while immersed in VR. The result suggested an interaction effect of arm and lip, showing reports of higher level of embodiment with the congruent as compared to the incongruent conditions. Further study is needed to confirm if the same interaction effect can be captured with objective measurements.},
  keywords={Synchronization;Lips;Avatars;Mirrors;Tracking;Atmospheric measurements;body ownership;embodiment;virtual characters;user studies},
  doi={10.1109/VR.2017.7892272},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892273,
  author={Zahedi, Ehsan and Rahmat-Khah, Hadi and Dargahi, Javad and Zadeh, Mehrdad},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Virtual reality based training: Evaluation of user performance by capturing upper limb motion}, 
  year={2017},
  volume={},
  number={},
  pages={255-256},
  abstract={This paper presents the results of a two-fold study on the incorporation of upper limb's movement into measuring of user performance in a virtual reality (VR) based training simulation. VR simulators have been developed to assess and improve minimally invasive surgery (MIS) skills. While these simulators are currently being used, most skill evaluation methods are limited to measuring and computing performance metrics regarding the MIS tool tip movement. In this study, a VR simulator is developed to measure and analyze the movements of upper limb joints. The movement analysis from the first experiment suggests that the kinematic data of upper limb can be used to discriminate an expert surgeon from a novice trainee. The results from the second experiment show that the motion of non-dominant hand has a significant effect on the performance of dominant hand.},
  keywords={Surgery;Wrist;Elbow;Training;Virtual reality;Solid modeling;Motion measurement;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;H.1.2 [Models and Principles]: User/Machine Systems — Human factors},
  doi={10.1109/VR.2017.7892273},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892274,
  author={Erfanian, Aida and Tarng, Stanley and Hu, Yaoping and Plouzeau, Jérémy and Merienne, Frédéric},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Mechanism of integrating force and vibrotactile cues for 3D user interaction within virtual environments}, 
  year={2017},
  volume={},
  number={},
  pages={257-258},
  abstract={Proper integration of sensory cues facilitates 3D user interaction within virtual environments (VEs). Studies showed that the integration of visual and haptic cues follows maximum likelihood estimation (MLE). Little effort focuses however on the mechanism of integrating force and vibrotactile cues. We thus investigated MLE's suitability for integrating these cues. Within a VE, human users undertook 3D interaction of navigating a flying drone along a high-voltage transmission line for inspection. The users received individual force or vibrotactile cues, and their combinations in collocated and dislocated settings. The users' task performance including completion time and accuracy was assessed under each individual cue and setting. The presence of the vibrotactile cue promoted a better performance than the force cue alone. This agreed with the applicability of tactile cues for sensing 3D surfaces, herein setting a baseline for using MLE. The task performance under the collocated setting indicated a degree of combining the individual cues. In contrast, the performance under the dislocated setting was alike under the individual vibrotactile cue. These observations imply a possible role of MLE in integrating force and vibrotactile cues for 3D user interaction within VEs.},
  keywords={Force;Haptic interfaces;Maximum likelihood estimation;Three-dimensional displays;Testing;Visualization;Drones;H.5.1 [Multimedia Information Systems]: Artificial;augmented and virtual realities;H.5.2 [User Interfaces]: Haptic I/O},
  doi={10.1109/VR.2017.7892274},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892275,
  author={Roth, Daniel and Waldow, Kristoffer and Latoschik, Marc Erich and Fuhrmann, Arnulph and Bente, Gary},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Socially immersive avatar-based communication}, 
  year={2017},
  volume={},
  number={},
  pages={259-260},
  abstract={In this paper, we present SIAM-C, an avatar-mediated communication platform to study socially immersive interaction in virtual environments. The proposed system is capable of tracking, transmitting, representing body motion, facial expressions, and voice via virtual avatars and inherits the transmission of human behaviors that are available in real-life social interactions. Users are immersed using active stereoscopic rendering projected onto a life-size projection plane, utilizing the concept of “fish tank” virtual reality (VR). Our prototype connects two separate rooms and allows for socially immersive avatar-mediated communication in VR.},
  keywords={Tracking;Avatars;Virtual environments;Stereo image processing;Three-dimensional displays;Sensors;H5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities —;H.4.3 [Communications Applications]: —},
  doi={10.1109/VR.2017.7892275},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892276,
  author={Paris, Richard A. and McNamara, Timothy P. and Rieser, John J. and Bodenheimer, Bobby},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A comparison of methods for navigation and wayfinding in large virtual environments using walking}, 
  year={2017},
  volume={},
  number={},
  pages={261-262},
  abstract={Interesting virtual environments that permit free exploration are rarely small. A number of techniques have been developed to allow people to walk in larger virtual spaces than permitted by physical extent of the virtual reality hardware, and in this paper we compare three such methods in terms of how they affect presence and spatial awareness. In our first psychophysical study, we compared two methods of reorientation and one method of redirected walking on subjects' presence and spatial memory while navigating a pre-specified path. We further compared the two reorientation methods in a second psychophysical study involving free exploration and navigation in a large virtual environment. Our results provide criteria by which the choice of a locomotion method for navigating large virtual environments may be selected.},
  keywords={Legged locomotion;Navigation;Virtual environments;Tracking;Turning;Extraterrestrial measurements;Layout;Presence;Spatial Awareness;Reorientation;Redirected Walking},
  doi={10.1109/VR.2017.7892276},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892277,
  author={Ardulov, Victor and Pariser, Oleg},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Immersive data interaction for planetary and earth sciences}, 
  year={2017},
  volume={},
  number={},
  pages={263-264},
  abstract={The Multimission Instrument Processing Laboratory (MIPL) at Jet Propulsion Laboratory (JPL) processes and analyzes, orbital and in-situ instrument data for both planetary and Earth science missions. Presenting 3D data in a meaningful and effective manner is of the utmost importance to furthering scientific research and conducting engineering operations. Visualizing data in an intuitive way by utilizing Virtual Reality (VR), allows users to immersively interact with their data in their respective environments. This paper examines several use-cases, across various missions, instruments, and environments, demonstrating the strengths and insights that VR has to offer scientists.},
  keywords={Data visualization;Clouds;Three-dimensional displays;Virtual reality;Planning;Solid modeling;Atmospheric measurements;H.5.1 [Artificial, augmented, and virtual realities]},
  doi={10.1109/VR.2017.7892277},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892278,
  author={Murphy, Dooley},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Bodiless embodiment: A descriptive survey of avatar bodily coherence in first-wave consumer VR applications}, 
  year={2017},
  volume={},
  number={},
  pages={265-266},
  abstract={This preliminary study surveys whether/which avatar body parts are visible in first-wave consumer virtual reality (VR) applications for the HTC Vive (n = 200). A simple coding schema for assessing avatar bodily coherence (ABC) is piloted and evaluated. Results provide a snapshot of ABC in popular high-end VR applications in Q3 2016. It is reported (Table 1) that 86.5% of sampled items feature fully invisible avatars, 9% depict hands only, and 4.5% feature a head, torso, or legs, but with some degree of bodily incoherence. Findings suggest that users may experience a sense of ownership and/or agency over their virtual actions even in the absence of visible avatar body parts. This informs research questions and hypotheses for future experimental enquiry into how bodily representation interplays with user cognition, perceived virtual embodiment (body ownership illusion and sense of agency), and spatial telepresence (hereafter spatial presence). For instance: To what extent/under what conditions do the users of consumer VR systems demonstrate a sense of bodily vulnerability (a drive for bodily preservation) when no virtual body is present/visible?},
  keywords={Avatars;Coherence;Virtual environments;Media;Biological system modeling;Encoding;Agency;avatar;embodiment;spatial presence;VR},
  doi={10.1109/VR.2017.7892278},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892279,
  author={Nogalski, Malte and Fohl, Wolfgang},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Curvature gains in redirected walking: A closer look}, 
  year={2017},
  volume={},
  number={},
  pages={267-268},
  abstract={This paper summarizes the detailed paths of participants in redirected walking (RDW) curvature gain experiments. The experiments were carried out in a wave field synthesis (WFS) system of 5×6 meters. Some users were blindfolded and had to control their walking by acoustical cues only, others wore an Oculus Rift DK2 which presented them a virtual scenery in addition. A marker at the participant's head allowed us to record the paths with our high-precision tracking system. The naive assumption of RDW with curvature gains would be that the test persons walk on the circumference of a circle, but the observed walking patterns were much more complex. Test persons showed very individual walking patterns while exploring the virtual environment. Many of these patterns may be explained as a sequence: 1. walk a few steps toward the assumed target position, 2. check for deviations, 3. adjust path to new assumed target position, which results in different patterns of various path curvature. The consequences for the application of RDW techniques are: Curvature gain tries to guide the users on a circular arc: the “ideal path”, whereas the real paths are mostly outside of the circle of the ideal path. The deviations in the audio-only case are much larger than in the audio-visual case. The measured curvature gain thresholds systematically under-estimate the required walking space, as they do not account for the required extra space for walking outside the circular path.},
  keywords={Legged locomotion;Market research;Tracking;Meters;Acoustics;Virtual environments;H.1.2 [Models and Principles]: User/Machine Systems — Software psychology, Human factors;J.7 [Computers in other systems]: Real time —;H.5.1 [Information and Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892279},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892280,
  author={Pan, Matthew K. X. J. and Niemeyer, Günter},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Catching a real ball in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={269-270},
  abstract={We present a system enabling users to accurately catch a real ball while immersed in a virtual reality environment. We examine three visualizations: rendering a matching virtual ball, the predicted trajectory of the ball, and a target catching point lying on the predicted trajectory. In our demonstration system, we track the projectile motion of a ball as it is being tossed between users. Using Unscented Kalman Filtering, we generate predictive estimates of the ball's motion as it approaches the catcher. The predictive assistance visualizations effectively increases the user's senses but can also alter the user's strategy in catching.},
  keywords={Trajectory;Visualization;Virtual reality;Rendering (computer graphics);Target tracking;Floors;Tracking;virtual reality;motion prediction;physical interaction;mixed reality;catching},
  doi={10.1109/VR.2017.7892280},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892281,
  author={Costa, Raphael and Guo, Rongkai and Quarles, John},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Towards usable underwater virtual reality systems}, 
  year={2017},
  volume={},
  number={},
  pages={271-272},
  abstract={The objective of this research is to compare the effectiveness of different tracking devices underwater. There have been few works in aquatic virtual reality (VR) - i.e., VR systems that can be used in a real underwater environment. Moreover, the works that have been done have noted limitations on tracking accuracy. Our initial test results suggest that inertial measurement units work well underwater for orientation tracking but a different approach is needed for position tracking. Towards this goal, we have waterproofed and evaluated several consumer tracking systems intended for gaming to determine the most effective approaches. First, we informally tested infrared systems and fiducial marker based systems, which demonstrated significant limitations of optical approaches. Next, we quantitatively compared inertial measurement units (IMU) and a magnetic tracking system both above water (as a baseline) and underwater. By comparing the devices rotation data, we have discovered that the magnetic tracking system implemented by the Razer Hydra is more accurate underwater as compared to a phone-based IMU. This suggests that magnetic tracking systems should be further explored for underwater VR applications.},
  keywords={Games;Cameras;Magnetomechanical effects;Augmented reality;Measurement units;Tracking;Virtual Reality;Underwater;Tracking;Rehabilitation;Games},
  doi={10.1109/VR.2017.7892281},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892282,
  author={Freiberg, Jacob and Kitson, Alexandra and Riecke, Bernhard E.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Development and evaluation of a hands-free motion cueing interface for ground-based navigation}, 
  year={2017},
  volume={},
  number={},
  pages={273-274},
  abstract={With affordable high performance VR displays becoming commonplace, users are becoming increasingly aware of the need for well-designed locomotion interfaces that support these displays. After considering the needs of users, we quantitatively evaluated an embodied locomotion interface called the NaviChair according to usability needs and fulfillment of system requirements. Specifically, we investigated influences of locomotion interfaces (joystick vs. an embodied motion cueing chair) and display type (HMD vs. projection screen) on a spatial updating pointing task. Our findings indicate that our embodied VR locomotion interface provided users with an immersive experience of a space without requiring a significant investment of set up time.},
  keywords={Resists;Three-dimensional displays;Navigation;Controllability;Tracking;Usability;Virtual reality;Spatial orientation;locomotion interfaces;immersion;usability},
  doi={10.1109/VR.2017.7892282},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892283,
  author={Peer, Alex and Ponto, Kevin},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Preliminary exploration: Perceived egocentric distance measures in room-scale spaces using consumer-grade head mounted displays}, 
  year={2017},
  volume={},
  number={},
  pages={275-276},
  abstract={Distance misperception (sometimes, distance compression) in immersive virtual environments is an active area of study, and the recent availability of consumer-grade display and tracking technologies raises new questions: Can misperceptions be measured within the small tracking volumes of consumer-grade technology? Are measures practical within this space directly comparable, or are some preferable to others? Do contemporary displays even induce distance misperceptions? This work explores these questions.},
  keywords={Atmospheric measurements;Particle measurements;Measurement uncertainty;Virtual environments;Legged locomotion;Extraterrestrial measurements;I.3.7 [Computing Methodologies]: Graphics Utilities — Virtual reality},
  doi={10.1109/VR.2017.7892283},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892284,
  author={Sawabe, Taishi and Kanbara, Masayuki and Hagita, Norihiro},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Diminished reality for acceleration stimulus: Motion sickness reduction with vection for autonomous driving}, 
  year={2017},
  volume={},
  number={},
  pages={277-278},
  abstract={This paper presents an approach for motion sickness reduction while riding an autonomous vehicle. It proposes the Diminished Reality (DR) method for an acceleration stimulus to reduce motion sickness for the autonomous vehicle. One of the main causes of motion sickness is a repeated acceleration. In order to diminish the acceleration stimulus in the autonomous vehicle, vection illusion is used to induce the user to make a preliminary movement against the real acceleration. The Balance Wii Board is used to measure participant's movement of the center of gravity to verify the effectiveness of the method with vection. The experimental result of 9 participants shows that the proposed method of using vection could reduce acceleration stimulus compared with the conventional method.},
  keywords={Acceleration;Gravity;Autonomous vehicles;Motion measurement;Virtual reality;Visualization;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities —},
  doi={10.1109/VR.2017.7892284},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892285,
  author={Pakkanen, Toni and Hakulinen, Jaakko and Jokela, Tero and Rakkolainen, Ismo and Kangas, Jari and Piippo, Petri and Raisamo, Roope and Salmimaa, Marja},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Interaction with WebVR 360° video player: Comparing three interaction paradigms}, 
  year={2017},
  volume={},
  number={},
  pages={279-280},
  abstract={Immersive 360° video needs new ways of interaction. We compared three different interaction methods to find out which one of them is the most applicable for controlling 360° video playback. The compared methods were: remote control, pointing with head orientation, and hand gestures. A WebVR-based 360° video player was built for the experiment.},
  keywords={Streaming media;Head;User interfaces;TV;Headphones;Usability;Indexes;Immersive 360° video;interaction methods;gestures},
  doi={10.1109/VR.2017.7892285},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892286,
  author={Weidner, Florian and Hoesch, Anne and Poeschl, Sandra and Broll, Wolfgang},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Comparing VR and non-VR driving simulations: An experimental user study}, 
  year={2017},
  volume={},
  number={},
  pages={281-282},
  abstract={Up to now, most driving simulators use either small monitors or large immersive projection setups like 2D/3D screens or a CAVE. The recent improvements of VR-HMDs led to an increased application in driving simulation. However, the influence and comparability of various VR and non-VR displays has been hardly investigated. We present results of a user study investigating the different influence of non-VR (2D, stereoscopic 3D) and VR (HMD) on physiological responses, simulation sickness, and driving performance within a single driving simulator. In the study, 94 participants performed the Lane Change Task. Results indicate that a VR-HMD leads to similar data as stereoscopic 3D or 2D screens. We observed no significant difference regarding physiological responses or lane change performance. However, we measured significantly increased simulator sickness in the VR-HMD condition compared to stereoscopic 3D.},
  keywords={Two dimensional displays;Solid modeling;Three-dimensional displays;Physiology;Biological system modeling;Atmospheric measurements;Particle measurements;Virtual Reality;Driving Simulation;Human Factors;Simulator Sickness},
  doi={10.1109/VR.2017.7892286},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892287,
  author={Kurosawa, Masato and Ito, Ken and Ikei, Yasushi and Hirota, Koichi and Kitazaki, Michiteru},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Evaluation of airflow effect on a VR walk}, 
  year={2017},
  volume={},
  number={},
  pages={283-284},
  abstract={The present study investigates the augmentation effect of airflow on the sensation of a virtual reality walk. The intensity of cutaneous sensation evoked by airflow during the real and virtual walk was measured. The airflow stimulus was added to the participant with passive vestibular motion and visual presentation. The result suggests that the sensation of walking was strongly increased by adding the airflow stimulus to the vestibular and optic presentations. The cutaneous sensation of airflow was perceived higher for the sitting participant than during a real walk in both a single and the combined stimuli. The equivalent speed of airflow for the sitting participant was lowered from the airflow speed in the real walk.},
  keywords={Legged locomotion;Optical sensors;Adaptive optics;Standards;Virtual environments;Electronic mail;Stimulated emission;Airflow;cutaneous sensation;virtual/real walking},
  doi={10.1109/VR.2017.7892287},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892288,
  author={Men, Liang and Bryan-Kinns, Nick and Hassard, Amelia Shivani and Ma, Zixiang},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={The impact of transitions on user experience in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={285-286},
  abstract={In recent years, Virtual Reality (VR) applications have become widely available. An increase in popular interest raises questions about the use of the new medium for communication. While there is a wide variety of literature regarding scene transitions in films, novels and computer games, transitions in VR are not yet widely understood. As a medium that requires a high level of immersion [2], transitions are a desirable tool. This poster delineates an experiment studying the impact of transitions on user experience of presence in VR.},
  keywords={Virtual environments;Portals;Computers;Atmospheric measurements;Particle measurements;Haptic interfaces;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892288},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892289,
  author={Skarbez, Richard and Welch, Gregory F. and Brooks, Frederick P. and Whitton, Mary C.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Coherence changes gaze behavior in virtual human interactions}, 
  year={2017},
  volume={},
  number={},
  pages={287-288},
  abstract={We discuss the design and results of an experiment investigating Plausibility Illusion in virtual human (VH) interactions, in particular, the coherence of conversation with a VH. This experiment was performed in combination with another experiment evaluating two display technologies. As that aspect of the study is not relevant to this poster, it will be mentioned only in the Materials section. Participants who interacted with a low-coherence VH looked around the room markedly more than participants interacting with a high-coherence VH, demonstrating that the level of coherence of VHs can have a detectable effect on user behavior and that head and gaze behavior can be used to evaluate the quality of a VH interaction.},
  keywords={Coherence;Interviews;Head;Virtual reality;Speech recognition;Biomedical imaging;Software;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial;augmented and virtual realities;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Evaluation/methodology;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892289},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892290,
  author={Porssut, Thibault and Chardonnet, Jean-Rémy},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Asymetric telecollaboration in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={289-290},
  abstract={We present a first study where we combine two asymetric virtual reality systems for telecollaboration purposes: a CAVE system and a head-mounted display (HMD), using a server-client type architecture. Experiments on a puzzle game in limited time, alone and in collaboration, show that combining asymetric systems reduces cognitive load. Moreover, the participants reported preferring working in collaboration and showed to be more efficient in collaboration. These results provide insights in combining several low cost HMDs with a unique expensive CAVE.},
  keywords={Collaboration;Resists;Atmospheric measurements;Particle measurements;Virtual reality;Games;Indexes;Telecollaboration;CAVE;HMD},
  doi={10.1109/VR.2017.7892290},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892291,
  author={Rodriguez, Grace M. and Cruz, Marvis and Solis, Andrew and Ordóñez, Patricia and McCann, Brian C.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={An immersive approach to visualizing perceptual disturbances}, 
  year={2017},
  volume={},
  number={},
  pages={291-292},
  abstract={Through their experience with the ICERT REU program at the Texas Advanced Computing Center (TACC), two undergraduate students from the University of Puerto Rico and the University of Florida have initiated a collaboration between their home institutions and TACC exploring the possibility of using immersion to simulate perceptual disturbances. Perceptual disturbances are subjective in nature, and difficult to communicate verbally. Often caretakers or those closest to sufferers have difficulty understanding the nature of their suffering. Immersion provides an exciting opportunity to directly communicate percepts with clinicians and loved ones. Here, we present a prototype environment meant to simulate some of the perceptual disturbances associated with seizures and epilepsy. Following further validation of our approach, we hope to promote awareness and empathy for these often jarring phenomena.},
  keywords={Epilepsy;Visualization;Prototypes;Virtual reality;Solid modeling;Collaboration;Electronic mail;Software Engineering;I.3.7 [Computer Graphics];Three-Dimensional Graphics and RealismVirtual Reality},
  doi={10.1109/VR.2017.7892291},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892292,
  author={Ng, Adrian K. T. and Chan, Leith K. Y. and Lau, Henry Y. K.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Corrective feedback for depth perception in CAVE-like systems}, 
  year={2017},
  volume={},
  number={},
  pages={293-294},
  abstract={The perceived distance estimation in an immersive virtual reality system is generally underestimated to the actual distance. Approaches had been found to provide users with better dimensional perception. One method used in head-mounted displays is to interact by walking with visual feedback, but it is not suitable for a CAVE-like system, like imseCAVE, with confined spaces for walking. A verbal corrective feedback mechanism is proposed. The result shows that estimation accuracy generally improves after eight feedback trials although some estimations become overestimated. One possible explanation is the need of more verbal feedback trials. Further research on top-down approach for improvement in depth perception is suggested.},
  keywords={Estimation;Training;Legged locomotion;Virtual environments;Three-dimensional displays;Visualization;Distance estimation;error correction;training from feedback;CAVE-like systems;imseCAVE},
  doi={10.1109/VR.2017.7892292},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892293,
  author={Kumazawa, Itsuo and Kai, Toshihiro and Onuki, Yoshikazu and Ono, Shunsuke},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Measurement of 3D-velocity by high-frame-rate optical mouse sensors to extrapolate 3D position captured by a low-frame-rate stereo camera}, 
  year={2017},
  volume={},
  number={},
  pages={295-296},
  abstract={The frame rate of existing stereo cameras is not enough to track quick hand or finger actions. It also requires lots of computational cost to find correspondence between stereo images to compute distance. The recently commercialized 3D position sensors such as TOF cameras or Leap Motion needs strong illumination to ensure sufficient optical energy for the high frame rate sensing. To overcome these problems, this paper proposes to use a pair of optical-mouse-sensors as a stereo image sensor to measure 3D-velocity and use it to extrapolate 3D position measured by a low-frame-rate stereo camera. It is shown that quick hand actions are tracked under ordinary in-door lighting condition. As 2D velocities are computed inside the optical-mouse-sensors, computation and communication costs are drastically reduced.},
  keywords={Optical sensors;Three-dimensional displays;Adaptive optics;Mice;High-speed optical techniques;Optical imaging;high speed camera;stereo camera;optical mouse sensor;extrapolation;3D position},
  doi={10.1109/VR.2017.7892293},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892294,
  author={Brandão, William Losina and Pinho, Márcio Sarroglia},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Using augmented reality to improve dismounted operators' situation awareness}, 
  year={2017},
  volume={},
  number={},
  pages={297-298},
  abstract={Whether it in the military, law enforcement or private security, dismounted operators tend to deal with a large amount of volatile information that may or may not be relevant according to a variety of factors. In this paper we draft some ideas on the building blocks of an augmented reality system aimed to improve the situational awareness of dismounted operators by filtering, organizing, and displaying this information in a way that reduces the strain over the operator.},
  keywords={Augmented reality;Navigation;Machine vision;Time measurement;Position measurement;Hardware;Situation Awareness;Augmented Reality},
  doi={10.1109/VR.2017.7892294},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892295,
  author={Tomberlin, Mathew and Tahai, Liudmila and Pietroszek, Krzysztof},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Gauntlet: Travel technique for immersive environments using non-dominant hand}, 
  year={2017},
  volume={},
  number={},
  pages={299-300},
  abstract={We present Gauntlet, a travel technique for immersive environments that uses non-dominant hand tracking and a fist gesture to translate and rotate the viewport. The technique allows for simultaneous use of the dominant hand for other spatial input tasks. Applications of Gauntlet include FPS games, and other application domains where navigation should be performed together with other tasks. We release the technique along with an example application, a VR horror game, as an open source project.},
  keywords={Games;Fatigue;Navigation;Performance evaluation;Tracking;Virtual reality;Optical devices;navigation;travel;virtual reality;head mounted display},
  doi={10.1109/VR.2017.7892295},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892296,
  author={Bönsch, Andrea and Wendt, Jonathan and Overath, Heiko and Gürerk, Özgür and Harbring, Christine and Grund, Christian and Kittsteiner, Thomas and Kuhlen, Torsten W.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Peers at work: Economic real-effort experiments in the presence of virtual co-workers}, 
  year={2017},
  volume={},
  number={},
  pages={301-302},
  abstract={Traditionally, experimental economics uses controlled and incentivized field and lab experiments to analyze economic behavior. However, investigating peer effects in the classic settings is challenging due to the reflection problem: Who is influencing whom? To overcome this, we enlarge the methodological toolbox of these experiments by means of Virtual Reality. After introducing and validating a real-effort sorting task, we embed a virtual agent as peer of a human subject, who independently performs an identical sorting task. We conducted two experiments investigating (a) the subject's productivity adjustment due to peer effects and (b) the incentive effects on competition. Our results indicate a great potential for Virtual-Reality-based economic experiments.},
  keywords={Economics;Sorting;Productivity;Belts;Face;Virtual reality;Incentive schemes;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality;J.4 [Computer Applications]: Social And Behavioral Sciences — Economics},
  doi={10.1109/VR.2017.7892296},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892297,
  author={Wang, Kai and Cheng, Haonan and Liu, Shiguang},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Efficient sound synthesis for natural scenes}, 
  year={2017},
  volume={},
  number={},
  pages={303-304},
  abstract={This paper presents a novel framework to generate the sound of outdoor natural scenes, such as waterfall, ocean, etc. Our method firstly simulates liquid with a grid-based method. Then combined with the movement of liquid, we generate seed-particles which represent bubbles, foams or splashes. Next, we assign each seed-particles a radius with a new radius distribution model. By calculating the bubbles' pressure wave we generate the sound. Experiments demonstrated that our novel framework can efficiently synthesize the sounds for natural scenes.},
  keywords={Liquids;Solid modeling;Computational modeling;Oceans;Graphics;Complexity theory;sound synthesis;seed-particles;bubble;radius distribution model},
  doi={10.1109/VR.2017.7892297},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892298,
  author={Lindemann, Patrick and Rigoll, Gerhard},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A diminished reality simulation for driver-car interaction with transparent cockpits}, 
  year={2017},
  volume={},
  number={},
  pages={305-306},
  abstract={We anticipate advancements in mixed reality device technology which might benefit driver-car interaction scenarios and present a simulated diminished reality interface for car drivers. It runs in a custom driving simulation and allows drivers to perceive otherwise occluded objects of the environment through the car body. We expect to obtain insights that will be relevant to future real-world applications. We conducted a pre-study with participants performing a driving task with the prototype in a CAVE-like virtual environment. Users preferred large-sized see-through areas over small ones but had differing opinions on the level of transparency to use. In future work, we plan additional evaluations of the driving performance and will further extend the simulation.},
  keywords={Automobiles;Visualization;Prototypes;Virtual reality;Cameras;Solid modeling;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.1 [Information Interfaces and Presentation]: User Interfaces — User-centered design},
  doi={10.1109/VR.2017.7892298},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892299,
  author={He, Tianyu and Chen, Xiaoming and Chen, Zhibo and Li, Ye and Liu, Sen and Hou, Junhui and He, Ying},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Immersive and collaborative Taichi motion learning in various VR environments}, 
  year={2017},
  volume={},
  number={},
  pages={307-308},
  abstract={Learning “motion” online or from video tutorials is usually inefficient since it is difficult to deliver “motion” information in traditional ways and in the ordinary PC platform. This paper presents ImmerTai, a system that can efficiently teach motion, in particular Chinese Taichi motion, in various immersive environments. ImmerTai captures the Taichi expert's motion and delivers to students the captured motion in multi-modal forms in immersive CAVE, HMD as well as ordinary PC environments. The students' motions are captured too for quality assessment and utilized to form a virtual collaborative learning atmosphere. We built up a Taichi motion dataset with 150 fundamental Taichi motions captured from 30 students, on which we evaluated the learning effectiveness and user experience of ImmerTai. The results show that ImmerTai can enhance the learning efficiency by up to 17.4% and the learning quality by up to 32.3%.},
  keywords={Resists;Avatars;Training;Virtual environments;Layout;Immersive education;motion training;VR},
  doi={10.1109/VR.2017.7892299},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892300,
  author={Porssut, Thibault and Debarba, Henrique G and Canzoneri, Elisa and Herbelin, Bruno and Boulic, Ronan},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Virtual zero gravity impact on internal gravity model}, 
  year={2017},
  volume={},
  number={},
  pages={309-310},
  abstract={This project investigates the impact of a virtual zero gravity experience on the human gravity model. In the planned experiment, subjects are immersed with HMD and full body motion capture in a virtual world exhibiting either normal gravity or the apparent absence of gravity (i.e. body and objects floating in space). The study evaluates changes in the subjects' gravity model by observing changes on motor planning of actions dependent on gravity. Our goal is to demonstrate that a virtual reality exposure can induce some modifications to the humans internal gravity model, analogous to those resulting from real exposure (e.g. parabolic flights), even if users remain under normal gravity condition in reality.},
  keywords={Gravity;Solid modeling;Visualization;Biological system modeling;Planning;Adaptation models;Kinematics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892300},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892301,
  author={Freitag, Sebastian and Löbbert, Clemens and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Approximating optimal sets of views in virtual scenes}, 
  year={2017},
  volume={},
  number={},
  pages={311-312},
  abstract={Viewpoint quality estimation methods allow the determination of the most informative position in a scene. However, a single position usually cannot represent an entire scene, requiring instead a set of several viewpoints. Measuring the quality of such a set of views, however, is not trivial, and the computation of an optimal set of views is an NP-hard problem. Therefore, in this work, we propose three methods to estimate the quality of a set of views. Furthermore, we evaluate three approaches for computing an approximation to the optimal set (two of them new) regarding effectiveness and efficiency.},
  keywords={Genetic algorithms;Greedy algorithms;Visualization;Observers;Histograms;Computational modeling;Approximation algorithms;I.3.3 [Computer Graphics]: Picture/Image Generation — Viewing algorithms},
  doi={10.1109/VR.2017.7892301},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892302,
  author={Zhao, Jingbo and Allison, Robert S. and Vinnikov, Margarita and Jennings, Sion},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Estimating the motion-to-photon latency in head mounted displays}, 
  year={2017},
  volume={},
  number={},
  pages={313-314},
  abstract={We present a method for estimating the Motion-to-Photon (End-to-End) latency of head mounted displays (HMDs). The specific HMD evaluated in our study was the Oculus Rift DK2, but the procedure is general. We mounted the HMD on a pendulum to introduce damped sinusoidal motion to the HMD during the pendulum swing. The latency was estimated by calculating the phase shift between the captured signals of the physical motion of the HMD and a motion-dependent gradient stimulus rendered on the display. We used the proposed method to estimate both rotational and translational Motion-to-Photon latencies of the Oculus Rift DK2.},
  keywords={Resists;Potentiometers;Photodiodes;Virtual reality;Cameras;Estimation;Frequency-domain analysis;Motion-to-Photon Latency;End-to-End Latency;Head-Mounted Displays},
  doi={10.1109/VR.2017.7892302},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892303,
  author={Xu, Mengxin and Murcia-López, María and Steed, Anthony},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Object location memory error in virtual and real environments}, 
  year={2017},
  volume={},
  number={},
  pages={315-316},
  abstract={We aim to further explore the transfer of spatial knowledge from virtual to real spaces. Based on previous research on spatial memory in immersive virtual reality (VR) we ran a study that looked at the effect of three locomotion techniques (joystick, pointing-and-teleporting and walking-in-place) on object location learning and recall. Participants were asked to learn the location of a virtual object in a virtual environment (VE). After a short period of time they were asked to recall the location by placing a real version of the object in the real-world equivalent environment. Results indicate that the average placement error, or distance between original and recalled object location, is approximately 20cm for all locomotion technique conditions. This result is similar to the outcome of a previous study on spatial memory in VEs that used real walking. We report this unexpected finding and suggest further work on spatial memory in VR by recommending the replication of this study in different environments and using objects with a wider diversity of properties, including varying sizes and shapes.},
  keywords={Legged locomotion;Virtual environments;Navigation;Visualization;Standards;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892303},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892304,
  author={Kumazawa, Itsuo and Suzuki, Souma and Onuki, Yoshikazu and Ono, Shunsuke},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Tactile feedback enhanced with discharged elastic energy and its effectiveness for in-air key-press and swipe operations}, 
  year={2017},
  volume={},
  number={},
  pages={317-318},
  abstract={This paper presents a simple but effective way of enhancing tactile stimulus by a mechanism with springs to preserve elastic energies charged in a prior energy-charging phase and discharge them to enhance the force to hit a finger in the stimulating phase. With this mechanism, a small and light stimulator attached to the fingertip is developed and demonstrated to generate the tactile feedback strong enough to make people feel as if their fingers collide with a virtual object. It is also shown that the durations of the two phases can be as short as a few milliseconds so that the latency in tactile feedback can be negligible. The performance of the mechanism and the effectiveness of its tactile feedback are evaluated for in-air keypress and swipe operations.},
  keywords={Force;Prototypes;Tactile sensors;Vibrations;Springs;Piezoelectric transducers;Electromagnetic forces;tactile display;electromagnetic force;spring force;elastic energy;feedback latency;in-air operation},
  doi={10.1109/VR.2017.7892304},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892305,
  author={Zielasko, Daniel and Neha, Neha and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={BlowClick 2.0: A trigger based on non-verbal vocal input}, 
  year={2017},
  volume={},
  number={},
  pages={319-320},
  abstract={The use of non-verbal vocal input (NVVI) as a hand-free trigger approach has proven to be valuable in previous work [7]. Nevertheless, BlowClick's original detection method is vulnerable to false positives and, thus, is limited in its potential use, e.g., together with acoustic feedback for the trigger. Therefore, we extend the existing approach by adding common machine learning methods. We found that a support vector machine (SVM) with Gaussian kernel performs best for detecting blowing with at least the same latency and more precision as before. Furthermore, we added acoustic feedback to the NVVI trigger, which increases the user's confidence. To evaluate the advanced trigger technique, we conducted a user study (n = 33). The results confirm that it is a reliable trigger; alone and as part of a hands-free point-and-click interface.},
  keywords={Acoustics;Support vector machines;Speech recognition;Speech;Performance evaluation;ISO Standards;Kernel;H.5.2 [Information Interfaces and Presentation]: User Interfaces — [Voice I/O];I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — [Virtual reality];I.2.6 [Artificial Intelligence]: Learning — Parameter Learning},
  doi={10.1109/VR.2017.7892305},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892306,
  author={Saito, Shiho and Hirota, Koichi and Nojima, Takuya},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={KKse: Safety education system of the child in the kitchen knife cooking}, 
  year={2017},
  volume={},
  number={},
  pages={321-322},
  abstract={The Kitchen Knife Safety Educator (KKse) is a safety education system designed to teach children how to correctly use cooking knives. Cooking is important for children to learn about what they eat. In addition, that is also important for daily communication between children and their parents. However, it is dangerous for young children to handle cooking knives. Because of this danger, parents often try to keep their young children away from the kitchen. Our proposed system will contribute to not only improving children's cooking skills, but also improving communication between parents and children. The system composed of a virtual knife with haptic feedback function, a touch/force sensitive virtual food and a two-dimensional force sensitive cutting board. This system was developed to teach a fundamental cutting method, the “thrusting cut”. This paper describes the detail of the system.},
  keywords={Safety;Education;Haptic interfaces;Gravity;Pediatrics;Pressure sensors;Education;haptic interface;kitchen knife;skill;safety},
  doi={10.1109/VR.2017.7892306},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892307,
  author={Onuki, Yoshikazu and Ono, Shunsuke and Kumazawa, Itsuo},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Air cushion: A pilot study of the passive technique to mitigate simulator sickness by responding to vection}, 
  year={2017},
  volume={},
  number={},
  pages={323-324},
  abstract={Simulator sickness is an issue in virtual reality environments. In a virtual world, sensory conflict between visual sensation and self-motion perception occurs readily. Contradiction between visual and vestibular sensation is a dominant cause of motion sickness. Vection is a visually evoked illusion of self-motion. Vection occurs when a stationary human experiences locomotor stimulation over a wider area of the field of view, and senses motion when in fact there is none. Strong vection has been associated with simulator sickness. In this poster, the authors present results of a pilot study based on a hypothesis that simulator sickness can be mitigated by passively responding to the body sway. Commercially available air cushions were applied for VR environments. Measurable mitigation of simulator sickness was achieved by physically responding to vection. Allowing body sway encourages moderating the sensory conflict between visual sensation and self-motion perception. Also, the shapes of air cushions on seat backs were found to be an important variable.},
  keywords={Games;Visualization;Fatigue;Stomach;Virtual reality;Shape;Tactile sensors;Simulator sickness;vection;tactile feedback;HMD},
  doi={10.1109/VR.2017.7892307},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892308,
  author={Handa, Takuya and Murase, Kenji and Azuma, Makiko and Shimizu, Toshihiro and Kondo, Satoru and Shinoda, Hiroyuki},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A haptic three-dimensional shape display with three fingers grasping}, 
  year={2017},
  volume={},
  number={},
  pages={325-326},
  abstract={The main goal of our research is to develop a haptic display that makes it possible to convey shapes, hardness, and textures of objects displayed on 3D TV. Our evolved device has three 5 mm diameter actuating spheres arranged in triangular geometry on each of three fingertips (thumb, index finger, middle finger). In this paper, we describe an overview of a novel haptic device and the first experimental results that twelve subjects had succeeded to recognize the size of cylinders and side geometry of a cuboid and a hexagonal prism.},
  keywords={Shape;Three-dimensional displays;Haptic interfaces;Thumb;Grippers;Virtual reality;Haptic Display;3D Shape;Universal Design},
  doi={10.1109/VR.2017.7892308},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892309,
  author={Teo, Theophilus and Norman, Mitchell and Adcock, Matt and Thomas, Bruce H.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Data fragment: Virtual reality for viewing and querying large image sets}, 
  year={2017},
  volume={},
  number={},
  pages={327-328},
  abstract={This paper presents our new Virtual Reality (VR) interactive visualization techniques to assist users querying large image sets. The VR system allows users to query a set of images on four different filters such as locations and keywords. The goal is to investigate if a VR platform is preferred over a non-VR platform for viewing and querying large image sets. We employed an HTC Vive and a traditional desktop screen to represent VR and non-VR platforms. We found users preferred the VR platform over the traditional desktop screen.},
  keywords={Three-dimensional displays;Metadata;Virtual reality;Australia;Data visualization;Visualization;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities —},
  doi={10.1109/VR.2017.7892309},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892310,
  author={Vierjahn, Tom and Zielasko, Daniel and van Kooten, Kees and Messmer, Peter and Hentschel, Bernd and Kuhlen, Torsten W. and Weyers, Benjamin},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Towards a design space characterizing workflows that take advantage of immersive visualization}, 
  year={2017},
  volume={},
  number={},
  pages={329-330},
  abstract={Immersive visualization (IV) fosters the creation of mental images of a data set, a scene, a procedure, etc. We devise an initial version of a design space for categorizing workflows that take advantage of IV. From this categorization, specific requirements for an actual, seamless IV-integration can be derived. We validate the design space with three workflows investigated in our research projects.},
  keywords={Training;Data visualization;Ultrasonic imaging;Production facilities;Hardware;Planning;Switches;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892310},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892311,
  author={Peng, Chao and Hansberger, Jeffrey T. and Cao, Lizhou and Shanthakumar, Vaidyanath Areyur},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Hand gesture controls for image categorization in immersive virtual environments}, 
  year={2017},
  volume={},
  number={},
  pages={331-332},
  abstract={In a situation where a large and chaotic collection of digital images must be manually sorted or categorized, there are two challenges: (1) unnatural actions during a prolonged human-computer interaction and (2) limited display space for image browsing. An immersive 3D interface is prototyped, where a person sorts a large collection of digital images with his or her bare hands in a virtual environment, and performs hand motions matching characteristics of sorting gestures in the real world. The virtual reality environment provides extra levels of immersion for displaying images.},
  keywords={Digital images;Tracking;Virtual environments;Resists;Computer science;Human computer interaction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Ergonomics;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Graphical user interfaces (GUI)},
  doi={10.1109/VR.2017.7892311},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892312,
  author={Khooshabeh, Peter and Choromanski, Igor and Neubauer, Catherine and Krum, David M. and Spicer, Ryan and Campbell, Julia},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Mixed reality training for tank platoon leader communication skills}, 
  year={2017},
  volume={},
  number={},
  pages={333-334},
  abstract={Here we describe the design and usability evaluation of a mixed reality prototype to simulate the role of a tank platoon leader, who is an individual who not only is a tank commander, but also directs a platoon of three other tanks with their own respective tank commanders. The domain of tank commander training has relied on physical simulators of the actual Abrams tank and encapsulates the whole crew. The TALK-ON system we describe here focuses on training communication skills of the leader in a simulated tank crew. We report results from a usability evaluation and discuss how they will inform our future work for collective tank training.},
  keywords={Training;Virtual reality;Usability;Solid modeling;Resists;Prototypes;Companies;H.5.1 [Information Interfaces and Presentation (I.7)]: Multimedia Information Systems — Artificial, augmented, and virtual realities;J.7 [Computer Applications]: Computers in Other Systems — Military},
  doi={10.1109/VR.2017.7892312},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892313,
  author={Wang, Jianren and Qian, Long and Azimi, Ehsan and Kazanzides, Peter},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Prioritization and static error compensation for multi-camera collaborative tracking in augmented reality}, 
  year={2017},
  volume={},
  number={},
  pages={335-336},
  abstract={An effective and simple method is proposed for multi-camera collaborative tracking, based on the prioritization of all tracking units, and then modeling the discrepancy between different tracking units as a locally static transformation error. Static error compensation is applied to the lower-priority tracking systems when high-priority trackers are not available. The method does not require high-end or carefully calibrated tracking units, and is able to effectively provide a comfortable augmented reality experience for users. A pilot study demonstrates the validity of the proposed method.},
  keywords={Cameras;Augmented reality;Trajectory;Error compensation;Target tracking;Calibration;Augmented reality;multi-camera tracking},
  doi={10.1109/VR.2017.7892313},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892314,
  author={Dunn, Charles and Knott, Brian},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Resolution-defined projections for virtual reality video compression}, 
  year={2017},
  volume={},
  number={},
  pages={337-338},
  abstract={Spherical data compression methods for Virtual Reality (VR) currently leverage popular rectangular data encoding algorithms. Traditional compression algorithms have massive adoption and hardware support on computers and mobile devices. Efficiently utilizing these two-dimensional compression methods for spherical data necessitates a projection from the three-dimensional surface of a sphere to a two-dimensional rectangle. Any such projection affects the final resolution distribution of the data after decoding. Popular projections used for VR video benefit from mathematical or geometric simplicity, but result in suboptimal resolution distributions. We introduce a method for generating a projection to match a desired resolution function. This method allows for customized projections with smooth, continuous and optimal resolution functions. Compared to commonly used projections, our resolution-defined projections drastically improve compression ratios for any given quality.},
  keywords={Encoding;Streaming media;Measurement;Spatial resolution;Virtual reality;Jacobian matrices;Distortion;H.5.1 [Multimedia Information Systems]: Virtual Reality;E.4 [Coding and Information Theory]: Data compaction and compression},
  doi={10.1109/VR.2017.7892314},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892315,
  author={Hvass, Jonatan S. and Larsen, Oliver and Vendelbo, Kasper B. and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={The effect of geometric realism on presence in a virtual reality game}, 
  year={2017},
  volume={},
  number={},
  pages={339-340},
  abstract={Previous research on visual realism and presence has not involved scenarios, graphics, and hardware representative of commercially available VR games. This poster details a between-subjects study (n=50) exploring if polygon count and texture resolution influence presence during exposure to a VR game. The results suggest that a higher polygon count and texture resolution increased presence as assessed by means of self-reports and physiological measures.},
  keywords={Visualization;Games;Physiology;Virtual environments;Atmospheric measurements;Particle measurements;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892315},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892316,
  author={Høeg, Emil R. and Ruder, Kevin V. and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={An exploration of input conditions for virtual teleportation}, 
  year={2017},
  volume={},
  number={},
  pages={341-342},
  abstract={This poster describes a within-groups study (n=17) comparing participants' experience of three different input conditions for instigating virtual teleportation (button clicking, physical jumping, and fist clenching). The results indicated that teleportation by clicking a button generally required less explicit attention and was perceived as more enjoyable, less disorienting, and less physically demanding.},
  keywords={Teleportation;Virtual reality;Three-dimensional displays;Legged locomotion;Atmospheric measurements;Particle measurements;Space exploration;H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892316},
  ISSN={2375-5334},
  month={March},}
@INPROCEEDINGS{7892317,
  author={Andersen, Thea and Anisimovaite, Gintare and Christiansen, Anders and Hussein, Mohamed and Lund, Carol and Nielsen, Thomas and Rafferty, Eoin and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A preliminary study of users' experiences of meditation in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={343-344},
  abstract={This poster describes a between-groups study (n=24) exploring the use of virtual reality (VR) for facilitating focused meditation. Half of the participants were exposed to a meditation session combing the sound of a guiding voice and a visual environment including virtual objects for the participants to focus on. The other half of the participants were only exposed to the auditory guide. The participants' experience of the sessions was assessed using self-reported measures of perceived concentration, temporal duration, stress reduction, and comfort. Interestingly, no statistically significant differences were found between the two conditions. This finding may be revealing in regards to the usefulness of VR-based meditation.},
  keywords={Stress;Psychology;Virtual reality;Atmospheric measurements;Particle measurements;Boats;Visualization;H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892317},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892318,
  author={Inoue, Yasuyuki and Kato, Fumihiro and Saraiji, Mhd Yamen and Fernando, Charith Lasantha and Tachi, Susumu},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Observation of mirror reflection and voluntary self-touch enhance self-recognition for a telexistence robot}, 
  year={2017},
  volume={},
  number={},
  pages={345-346},
  abstract={In this paper, we analyze the subjective feelings about the body of the operator of a telexistence system. We investigate whether a mirror reflection and self-touch affect body ownership and agency for a surrogate robot avatar in a virtual reality experiment. Results showed that the presence of tactile sensations synchronized with the view of self-touch events enhanced mirror self-recognition.},
  keywords={Mirrors;Robot sensing systems;Delays;Face;Virtual reality;Correlation;telexistence;body ownership;agency;self-touch;mirror recognition},
  doi={10.1109/VR.2017.7892318},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892319,
  author={Nasrabadi, Afshin Taghavi and Mahzari, Anahita and Beshay, Joseph D. and Prakash, Ravi},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Adaptive 360-degree video streaming using layered video coding}, 
  year={2017},
  volume={},
  number={},
  pages={347-348},
  abstract={Virtual reality and 360-degree video streaming are growing rapidly; however, streaming 360-degree video is very challenging due to high bandwidth requirements. To address this problem, the video quality is adjusted according to the user viewport prediction. High quality video is only streamed for the user viewport, reducing the overall bandwidth consumption. Existing solutions use shallow buffers limited by the accuracy of viewport prediction. Therefore, playback is prone to video freezes which are very destructive for the Quality of Experience (QoE). We propose using layered encoding for 360-degree video to improve QoE by reducing the probability of video freezes and the latency of response to the user head movements. Moreover, this scheme reduces the storage requirements significantly and improves in-network cache performance.},
  keywords={Streaming media;Bandwidth;Encoding;Static VAr compensators;Virtual reality;Video coding;Adaptive 360 video streaming;SVC;Video freeze},
  doi={10.1109/VR.2017.7892319},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892320,
  author={Lee, Hojun and Ha, Gyutae and Lee, Sangho and Kim, Shiho},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A mixed reality tele-presence platform to exchange emotion and sensory information based on MPEG-V standard}, 
  year={2017},
  volume={},
  number={},
  pages={349-350},
  abstract={We have implemented a mixed reality telepresence platform providing a user experience (UX) of exchanging emotional expressions as well as information among a group of participants. The implemented system provides a platform to experience an immersive live scene through a Head-Mounted Display (HMD) and sensory information to a VR HMD user at a remote place. Moreover, the user at a remote place can share and exchange emotional expressions with other users at another remote location by using 360° cameras, environmental sensors compliant with MPEG-V, and a game cloud server combined with a technique of holographic display. We demonstrated that emotional expressions of an HMD worn participant were shared with a group of other participants in the remote place while watching a sports game on a big screen TV.},
  keywords={Resists;Transform coding;Virtual reality;Standards;Sensors;Games;Three-dimensional displays;Tele-Presence;Mixed Reality;Emotional Exchange;hologram display},
  doi={10.1109/VR.2017.7892320},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892321,
  author={Li, Gang and Liu, Yue and Wang, Yongtian},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Evaluation of labelling layout methods in augmented reality}, 
  year={2017},
  volume={},
  number={},
  pages={351-352},
  abstract={View management techniques are commonly used for labelling of objects in augmented reality environments. Combining with image analysis, search space and adaptive representations, they can be utilized to achieve desired labelling tasks. However, the evaluation of different search space methods on labelling are still an open problem. In this paper, we propose an image analysis based view management method, which first adopts the image processing to superimpose 2D labels to the specific object. We then conduct three search space methods to an augmented reality scenario. Without the requirements of setting rules and constraints for occlusion among the labels, the results of three search space methods are evaluated by using objective analysis of related parameters. The evaluation results indicate that different search space methods could generate different time costs and occlusion, thereby affecting the final labelling effects.},
  keywords={Labeling;Augmented reality;Search problems;Layout;Cameras;Two dimensional displays;Image edge detection;Augmented reality;labelling;search space},
  doi={10.1109/VR.2017.7892321},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892322,
  author={Cho, Hyunwoo and Jung, Sung-Uk and Jee, Hyung-Keun},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Real-time interactive AR system for broadcasting}, 
  year={2017},
  volume={},
  number={},
  pages={353-354},
  abstract={For live television broadcast such as the educational program for children conducted through viewer participation, the smooth integration of virtual contents and the interaction between the casts and them are quite important issues. Recently there have been many attempts to make aggressive use of interactive virtual contents in live broadcast due to the advancement of AR/VR technology and virtual studio technology. These previous works have many limitations that do not support real-time 3D space recognition or immersive interaction. In this sense, we propose an augmented reality based real-time broadcasting system which perceives the indoor space using a broadcasting camera and a RGB-D camera. Also, the system can support the real-time interaction between the augmented virtual contents and the casts. The contribution of this work is the development of a new augmented reality based broadcasting system that not only enables filming using compatible interactive 3D contents in live broadcast but also drastically reduces the production costs. For the practical use, the proposed system was demonstrated in the actual broadcast program called “Ding Dong Dang Kindergarten” which is a representative children educational program on the national broadcasting channel of Korea.},
  keywords={Three-dimensional displays;Broadcasting;Cameras;Real-time systems;Augmented reality;Image recognition;Educational programs;AR broadcasting;Mesh reconstruction;Indoor structure},
  doi={10.1109/VR.2017.7892322},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892323,
  author={Zhao, Hengheng and Huang, Ping and Yao, Junfeng},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Texturing of augmented reality character based on colored drawing}, 
  year={2017},
  volume={},
  number={},
  pages={355-356},
  abstract={Coloring book can inspire imaginary and creativity of children. However, with the rapid development of digital devices and internet, traditional coloring book tends to be not attractive for children any more. Thus, we propose an idea of applying augmented reality technology to traditional coloring book. After children finish coloring characters in the printed coloring book, they can inspect their work using a mobile device. The drawing is detected and tracked so that the video stream is augmented with a 3D character textured according to their coloring. This is possible thanks to several novel technical contributions. We present a texture process that generates texture map for 3D augmented reality character from 2D colored drawing using a lookup map. Considering the movement of the mobile device and drawing, we give an efficient method to track the drawing surface.},
  keywords={Three-dimensional displays;Augmented reality;Mobile handsets;Streaming media;Real-time systems;Solid modeling;Two dimensional displays;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;K.3.1 [Computers and education]: Computer Uses in Education — Computer-assisted instruction},
  doi={10.1109/VR.2017.7892323},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892324,
  author={Mossel, Annette and Froeschl, Mario and Schoenauer, Christian and Peer, Andreas and Goellner, Johannes and Kaufmann, Hannes},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={VROnSite: Towards immersive training of first responder squad leaders in untethered virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={357-358},
  abstract={We present the VROnSite platform that enables immersive training of first responder on-site squad leaders. Our training platform is fully immersive, entirely untethered to ease use and provides two means of navigation - abstract and natural walking - to simulate stress and exhaustion, two important factors for decision making. With the platform's capabilities, we close a gap in prior art for first responder training. Our research is closely interlocked with stakeholders from fire brigades and paramedics to gather early feedback in an iterative design process. In this paper, we present our first research results, which are the system's design rationale, the single user training prototype and results from a preliminary user study.},
  keywords={Training;Virtual reality;Stakeholders;Stress;Prototypes;Electronic mail;Mobile communication;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892324},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892325,
  author={Garcia Estrada, Jose and Simeone, Adalberto L.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Recommender system for physical object substitution in VR}, 
  year={2017},
  volume={},
  number={},
  pages={359-360},
  abstract={This poster introduces the development of a recommender system to guide users in adapting a virtual environment into matching objects in the physical world. Emphasis is placed on avoiding cognitive overload resulting from providing options for substitution without considering the number of physical objects present. This is the first step towards a comprehensive recommender system for user-driven adaptation of Virtual Environments through immersive Virtual Reality systems.},
  keywords={Recommender systems;Virtual environments;Hardware;Training;Feeds;Standards;I.3.7 [Computer Graphics]: Virtual reality —;H.5.2 [Information Interfaces and Presentation]: Interaction interfaces —},
  doi={10.1109/VR.2017.7892325},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892326,
  author={Borba, Eduardo Zilles and Montes, Andre and de Deus Lopes, Roseli and Zuffo, Marcelo Knorich and Kopper, Regis},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Itapeva 3D: Being Indiana Jones in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={361-362},
  abstract={This poster presents the conceptual process of developing Itapeva 3D, a Virtual Reality (VR) archeology experience. It describes the technical spectrum of cyber-archeology process applied to the creation of a fully immersive and interactive virtual environment (VE), which represents Itapeva Rocky Shelter, a prehistoric archeological site in Brazil. The workflow starts with a real world data capture - laser scanners, drones and photogrammetry, continues with the transposition of the captured information into a 3D surface model capable of real-time rendering to head-mounted displays (HMDs), and ends with the design of interactive features allowing users to experience the virtual archeological site. The main objective of this VR model is to make plausible to general public to feel what it means to explore an otherwise restricted and ephemeral place. As final thoughts it is reported on preliminary results from an initial user observation.},
  keywords={Three-dimensional displays;Solid modeling;Visualization;Rocks;Virtual environments;Laser modes;Virtual reality;3D simulation;cyber-archeology;presence;gamification},
  doi={10.1109/VR.2017.7892326},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892327,
  author={Lind, Rasmus B. and Milesen, Victor and Smed, Dina M. and Vinkel, Simone P. and Grani, Francesco and Nilsson, Niels C. and Reng, Lars and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Sound design in virtual reality concert experiences using a wave field synthesis approach}, 
  year={2017},
  volume={},
  number={},
  pages={363-364},
  abstract={In this paper we propose an experiment that evaluates the influence of audience noise on the feeling of presence and the perceived quality in a virtual reality concert experience delivered using Wave Field Synthesis. A 360 degree video of a live rock concert from a local band was recorded. Single sound sources from the stage and the PA system were recorded, as well as the audience noise, and impulse responses of the concert venue. The audience noise was implemented in the production phase. A comparative study compared an experience with and without audience noise. In a between subject experiment with 30 participants we found that audience noise does not have a significant impact on presence. However, qualitative evaluations show that the naturalness of the sonic experience delivered through wavefield synthesis had a positive impact on the participants.},
  keywords={Visualization;Virtual reality;Cameras;Auditory displays;Rocks;Production;Microphones;H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892327},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892328,
  author={Ryge, Andreas and Thomsen, Lui and Berthelsen, Theis and Hvass, Jonatan S. and Koreska, Lars and Vollmers, Casper and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Effect on high versus low fidelity haptic feedback in a virtual reality baseball simulation}, 
  year={2017},
  volume={},
  number={},
  pages={365-366},
  abstract={In this paper we present a within-subjects study (n=26) comparing participants' experience of three kinds of haptic feedback (no haptic feedback, low fidelity haptic feedback and high fidelity haptic feedback) simulating the impact between a virtual baseball bat and ball. We noticed some minor effect on high fidelity versus low fidelity haptic feedback, but haptic feedback generally enhanced realism and quality of experience.},
  keywords={Haptic interfaces;Virtual reality;Vibrations;Solid modeling;Actuators;Legged locomotion;Sports equipment;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Input devices and strategies;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892328},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892329,
  author={Bhadsavle, Sarang S. and Yap, Xie Hunt Shannon and Segler, Justin and Jaisimha, Rahul and Raman, Nishant and Feng, Yengzhou and Biggs, Sierra J. and Peoples, Micah and Brenner, Robert B. and McCann, Brian C.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Immerj: A novel system for democratizing immersive storytelling}, 
  year={2017},
  volume={},
  number={},
  pages={367-368},
  abstract={Immersive technologies such as 360° cameras and head-mounted displays (HMDs) have become affordable to the average consumer, opening up new audiences for storytellers. However, existing immersive post-production software often requires too great a technical or financial investment for smaller creative shops, including most of the country's newspapers and local broadcast journalism organizations. Game engines, for example, are unnecessarily complicated for simple 360° video projects. Introducing Immerj - an open-source abstraction layer simplifying the Unity3D game engine's interface for immersive content creators. Our primary collaborator, Professor R.B. Brenner, director of the School of Journalism at the University of Texas at Austin, organized hands-on demos with our team and journalists and designers from some of the top news organizations all over the country in order to follow a human-centered design process. In just over one year, a small team of undergraduate researchers at the Texas Advanced Computing Center (TACC) has created a potentially disruptive democratization of technology.},
  keywords={Games;Software;Engines;Three-dimensional displays;Standards;Cameras;Virtual reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces — User-centered design},
  doi={10.1109/VR.2017.7892329},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892330,
  author={Freitag, Sebastian and Weyers, Benjamin and Kuhlen, Torsten W.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Assisted travel based on common visibility and navigation meshes}, 
  year={2017},
  volume={},
  number={},
  pages={369-370},
  abstract={The manual adjustment of travel speed to cover medium or large distances in virtual environments may increase cognitive load, and manual travel at high speeds can lead to cybersickness due to inaccurate steering. In this work, we present an approach to quickly pass regions where the environment does not change much, using automated suggestions based on the computation of common visibility. In a user study, we show that our method can reduce cybersickness when compared with manual speed control.},
  keywords={Visualization;Histograms;Manuals;Virtual environments;Three-dimensional displays;Interpolation;Geometry;I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction techniques},
  doi={10.1109/VR.2017.7892330},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892331,
  author={Borba, Eduardo Zilles and Zuffo, Marcelo Knorich},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Advertising perception with immersive virtual reality devices}, 
  year={2017},
  volume={},
  number={},
  pages={371-372},
  abstract={This poster presents an initial study about people experience with advertising messages in Virtual Reality (VR) that simulates the urban space. Besides looking to the plastic and textual factors perceived by the users in the Virtual Environment (VE), this work also reflects about effects of immersion provided by different technological devices and its possible influences in the advertising message reception process - a head-mounted display (Oculus Rift DK2), a cavern automatic virtual environment (CAVE) and a desktop monitor (PC). To carry this empirical experiment, a 3D scenario that simulates a real city urban space was created and several advertising image formats were inserted on its landscape. User navigation through the urban space was designed in a firstperson perspective. In short, we intend to accomplish two objectives: (a) to identify which factors lead people to pay attention to adverting in immersive VE; (b) to verify the immersion effects produced by different VR interfaces in the perception of advertising.},
  keywords={Advertising;Resists;Monitoring;Solid modeling;Virtual environments;Urban areas;Advertising;Virtual reality;Perception;Marketing},
  doi={10.1109/VR.2017.7892331},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892332,
  author={Bork, Felix and Barmaki, Roghayeh and Eck, Ulrich and Fallavolita, Pascal and Fuerst, Bernhard and Navab, Nassir},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Exploring non-reversing magic mirrors for screen-based augmented reality systems}, 
  year={2017},
  volume={},
  number={},
  pages={373-374},
  abstract={Screen-based Augmented Reality (AR) systems can be built as a window into the real world as often done in mobile AR applications or using the Magic Mirror metaphor, where users can see themselves with augmented graphics on a large display. The term Magic Mirror implies that the display shows the users enantiomorph, i.e. the mirror image, such that the system mimics a real-world physical mirror. However, the question arises whether one should design a traditional mirror, or instead display the true mirror image by means of a non-reversing mirror? We discuss the perceptual differences between these two mirror visualization concepts and present a first comparative study in the context of Magic Mirror anatomy teaching.},
  keywords={Mirrors;Augmented reality;Education;Biomedical imaging;Psychology;Context;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Ergonomics},
  doi={10.1109/VR.2017.7892332},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892333,
  author={Moghadam, Kasra Rahimi and Ragan, Eric D.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Towards understanding scene transition techniques in immersive 360 movies and cinematic experiences}, 
  year={2017},
  volume={},
  number={},
  pages={375-376},
  abstract={Many researchers have studied methods of effective travel in virtual environments, but little work has considered scene transitions, which may be important for virtual reality experiences like immersive 360 degree movies. In this research, we designed and evaluated three different scene transition techniques in two environments, conducted a pilot study, and collected metrics related to sickness, spatial orientation, and preference. Our preliminary results indicate that faster techniques are generally preferred by gamers and more gradual transitions are preferred by participants with less experience with 3D gaming and virtual reality.},
  keywords={Interpolation;Teleportation;Motion pictures;Three-dimensional displays;Virtual environments;Google;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892333},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892334,
  author={Brown, Cullen and Bhutra, Ghanshyam and Suhail, Mohamed and Xu, Qinghong and Ragan, Eric D.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Coordinating attention and cooperation in multi-user virtual reality narratives}, 
  year={2017},
  volume={},
  number={},
  pages={377-378},
  abstract={Limited research has been performed attempting to handle multiuser storytelling environments in virtual reality. As such, a number of questions about handling story progression and maintaining user presence in a multi-user virtual environment have yet to be answered. We created a multi-user virtual reality story experience in which we intend to study a set of guided camera techniques and a set of gaze distractor techniques to determine how best to attract disparate users to the same story. Additionally, we describe our preliminary work and plans to study the effectiveness of these techniques, their effect on user presence, and generally how multiple users feel their actions affect the outcome of a story.},
  keywords={Virtual environments;Cameras;Collaboration;Multimedia communication;Electronic mail;Face;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892334},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892335,
  author={Fisher, Joshua A. and Garg, Amit and Singh, Karan Pratap and Wang, Wesley},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Designing intentional impossible spaces in virtual reality narratives: A case study}, 
  year={2017},
  volume={},
  number={},
  pages={379-380},
  abstract={Natural movement and locomotion in Virtual Environments (VE) is constrained by the user's immediate physical space. To overcome this obstacle, researchers have established the use of impossible spaces. This work illustrates how impossible spaces can be utilized to enhance the aesthetics of, and presence within, an interactive narrative. This is done by creating impossible spaces with a narrative intent. First, locomotion and impossible spaces in VR are surveyed; second, the benefits of using intentional impossible spaces from a narrative design perspective is presented; third, a VR narrative called Ares is put forth as a prototype; and fourth, a user study is explored. Impossible spaces with a narrative intent intertwines narratology with the world's aesthetics to enhance dramatic agency.},
  keywords={Virtual environments;Navigation;Teleportation;Geometry;Buildings;Computers;Human-centered computing;Virtual Reality;Interaction design theory;design},
  doi={10.1109/VR.2017.7892335},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892336,
  author={Pick, Sebastian and Puika, Andrew S. and Kuhlen, Torsten W.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Comparison of a speech-based and a pie-menu-based interaction metaphor for application control}, 
  year={2017},
  volume={},
  number={},
  pages={381-382},
  abstract={Choosing an adequate system control technique is crucial to support complex interaction scenarios in virtual reality applications. In this work, we compare an existing hierarchical pie-menu-based approach with a speech-recognition-based one in terms of task performance and user experience in a formal user study. As testbed, we use a factory planning application featuring a large set of system control options.},
  keywords={Speech;Control systems;Production facilities;Visualization;Speech recognition;Virtual reality;Planning;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Evaluation/methodology},
  doi={10.1109/VR.2017.7892336},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892337,
  author={Mann, Jessie and Polys, Nicholas and Diana, Rachel and Ananth, Manasa and Herald, Brad and Platel, Sweetuben},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Virginia tech's study hall: A virtual method of loci mnemotechnic study using a neurologically-based, mechanism-driven, approach to immersive learning research}, 
  year={2017},
  volume={},
  number={},
  pages={383-384},
  abstract={The design of Virginia Tech's (VT) Study Hall emerges from the current cognitive neuroscience understanding of memory as a spatially mediated encoding process. The driving questions are: Does the sense of spatial navigation generated by an immersive virtual experience aid in memory formation? Does virtual spatial navigation, when paired with learning cues, enhance information encoding relative to nonspatial and nonvirtual processes? A pilot study was executed comparing recall on non-navigational memorization processes to processes involving mental and virtual navigation and we are currently running a full study to see if we can replicate these effects with a more demanding memory task and refined study design.},
  keywords={Navigation;Encoding;Legged locomotion;Psychology;Neuroscience;Buildings;Virtual reality;Applied computing-Psychology;Virtual reality;Spatial navigation;Cognitive Neuroscience;Memory & learning;Mnemotechnics;Immersive learning},
  doi={10.1109/VR.2017.7892337},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892338,
  author={Spicer, Ryan and Anglin, Julia and Krum, David M. and Liew, Sook-Lei},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={REINVENT: A low-cost, virtual reality brain-computer interface for severe stroke upper limb motor recovery}, 
  year={2017},
  volume={},
  number={},
  pages={385-386},
  abstract={There are few effective treatments for rehabilitation of severe motor impairment after stroke. We developed a novel closed-loop neurofeedback system called REINVENT to promote motor recovery in this population. REINVENT (Rehabilitation Environment using the Integration of Neuromuscular-based Virtual Enhancements for Neural Training) harnesses recent advances in neuroscience, wearable sensors, and virtual technology and integrates low-cost electroencephalography (EEG) and electromyography (EMG) sensors with feedback in a head-mounted virtual reality display (VR) to provide neurofeedback when an individual's neuromuscular signals indicate movement attempt, even in the absence of actual movement. Here we describe the REINVENT prototype and provide evidence of the feasibility and safety of using REINVENT with older adults.},
  keywords={Electroencephalography;Electromyography;Virtual reality;Sensors;Neurofeedback;Brain-computer interfaces;Safety;Brain computer interface;neurofeedback;stroke;virtual reality;EEG;EMG},
  doi={10.1109/VR.2017.7892338},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892339,
  author={Han, Dustin T. and Sargunam, Shyam Prathish and Ragan, Eric D.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Simulating anthropomorphic upper body actions in virtual reality using head and hand motion data}, 
  year={2017},
  volume={},
  number={},
  pages={387-388},
  abstract={The use of self avatars in virtual reality (VR) can bring users a stronger sense of presence and produce a more compelling experience by providing additional visual feedback during interactions. Avatars also become increasingly more relevant in VR as they provide a user with an identity for social interactions in multi-user settings. However, with current consumer VR setups that include only a head mounted display and hand controllers, implementation of self avatars are generally limited in the ability to mimic actions performed in the real world. Our work explores the idea of simulating a wide range of upper body motions using motion and positional data from only the head and hand motion data. We present a method to differentiate head and hip motions using information from captured motion data and applying corresponding changes to a virtual avatar. We discuss our approach and initial results.},
  keywords={Avatars;Head;Turning;Tracking;Torso;Hip;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892339},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892340,
  author={Peña, Alyssa M. and Ragan, Eric D.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Contextualizing construction accident reports in virtual environments for safety education}, 
  year={2017},
  volume={},
  number={},
  pages={389-390},
  abstract={Safety education is important in the construction industry. While research has been done on virtual environments for construction safety education, there is no set method for effectively contextualizing safety information and engaging students. In this research, we study the design of virtual environments to represent construction accident reports provided by the Occupational Health and Safety Administration (OSHA). We looked at different designs to contextualize the report data through space, visuals, and text. Users can explore the environment and interact through immersive virtual reality to learn more about a particular accident.},
  keywords={Accidents;Virtual environments;Education;Visualization;Occupational safety;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892340},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892341,
  author={Kato, Fumihiro and Fernando, Charith Lasantha and Inoue, Yasuyuki and Tachi, Susumu},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Classification method of tactile feeling using stacked autoencoder based on haptic primary colors}, 
  year={2017},
  volume={},
  number={},
  pages={391-392},
  abstract={We have developed a classification method of tactile feeling using a stacked autoencoder-based neural network on haptic primary colors. The haptic primary colors principle is a concept of decomposing the human sensation of tactile feeling into force, vibration, and temperature. Images were obtained from variation in the frequency of the time series of the tactile feeling obtained when tracing a surface of an object, features were extracted by employing a stacked autoencoder using a neural network with two hidden layers, and supervised learning was conducted. We confirmed that the tactile feeling for three different surface materials can be classified with an accuracy of 82.0 [%].},
  keywords={Vibrations;Haptic interfaces;Image color analysis;Feature extraction;Force;Neural networks;Skin;I.5.1 [Pattern Recognition]: Models — Neural nets;H.5.2 [Information Interfaces and Presentation (e.g. HCI)]: User Interfaces (D.2.2;H.1.2;I.3.6) — Haptic I/O;h.5.1 [Human-centered computing]: Virtual reality},
  doi={10.1109/VR.2017.7892341},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892342,
  author={Tredinnick, Ross and Boettcher, Brady and Smith, Simon and Solovy, Sam and Ponto, Kevin},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Uni-CAVE: A Unity3D plugin for non-head mounted VR display systems}, 
  year={2017},
  volume={},
  number={},
  pages={393-394},
  abstract={Unity3D has become a popular, freely available 3D game engine for design and construction of virtual environments. Unfortunately, the few options that currently exist for adapting Unity3D to distributed immersive tiled or projection-based VR display systems rely on closed commercial products. Uni-CAVE aims to solve this problem by creating a freely-available and easy to use Unity3D extension package for cluster-based VR display systems. This extension provides support for head and device tracking, stereo rendering and display synchronization. Furthermore, Uni-CAVE enables configuration within the Unity environment enabling researchers to get quickly up and running.},
  keywords={Cameras;Games;Engines;Synchronization;Three-dimensional displays;Virtual reality;Rendering (computer graphics);I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality — Unity3D;I.3.2 [Graphics Systems]: Distributed/Network Graphics — Unity3D},
  doi={10.1109/VR.2017.7892342},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892343,
  author={Tanaka, Kazumoto},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={3D action reconstruction using virtual player to assist karate training}, 
  year={2017},
  volume={},
  number={},
  pages={395-396},
  abstract={It is well known that sport skill learning is facilitated by video observation of players' actions in the target sport. A viewpoint change function is desirable when a learner observes the actions using video images. However, in general, viewpoint changes for observation are not possible because most videos are filmed from a fixed point using a single video camera. The objective of this research is to develop a method that generates a 3D human model of a player (i.e., a virtual player) from a single image and enable observation of the virtual player's action from any point of view. As a first step, this study focused on karate training and developed a semiautomatic method for 3D reconstruction from video images of sparring in karate.},
  keywords={Three-dimensional displays;Cameras;Two dimensional displays;Mathematical model;Solid modeling;Image reconstruction;Image segmentation;Action reconstruction;karate training;virtual player},
  doi={10.1109/VR.2017.7892343},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892344,
  author={Skarbez, Richard and Brooks, Frederick P. and Whitton, Mary C.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Immersion and coherence in a visual cliff environment}, 
  year={2017},
  volume={},
  number={},
  pages={397-398},
  abstract={We report on the design and results of an experiment investigating Slater's Place Illusion (PI) and Plausibility Illusion (Psi) in a virtual visual cliff environment. Existing presence questionnaires could not reliably distinguish the effects of PI from those of Psi. They did, however, indicate that high levels of PI-eliciting characteristics and Psi-eliciting characteristics together result in higher presence, compared to any of the other three conditions. Also, participants' heart rates responded markedly differently in the two Psi conditions; no such difference was observed across the PI conditions.},
  keywords={Coherence;Heart rate;Virtual environments;Visualization;Atmospheric measurements;Particle measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial;augmented and virtual realities;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Evaluation/methodology;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
  doi={10.1109/VR.2017.7892344},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892345,
  author={Seo, Jinsil Hwaryoung and Smith, Brian and Cook, Margaret and Pine, Michelle and Malone, Erica and Leal, Steven and Suh, Jinkyo},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Anatomy builder VR: Applying a constructive learning method in the virtual reality canine skeletal system}, 
  year={2017},
  volume={},
  number={},
  pages={399-400},
  abstract={We present Anatomy Builder VR that examines how a virtual reality (VR) system can support embodied learning in anatomy education. The backbone of the project is to pursue an alternative constructivist pedagogical model for learning canine anatomy. The main focus of the study was to identify and assemble bones in the live-animal orientation, using real thoracic limb bones in a bone box and digital pelvic limb bones in the Anatomy Builder VR. Eleven college students participated in the study. The pilot study showed that participants most enjoyed interacting with anatomical contents within the VR program. Participants spent less time assembling bones in the VR, and instead spent a longer time tuning the orientation of each VR bone in the 3D space. This study showed how a constructivist method could support anatomy education while using virtual reality technology in an active and experiential way.},
  keywords={Bones;Virtual reality;Education;Solid modeling;Three-dimensional displays;Learning systems;Visualization;anatomy education;virtual reality;constructive method;canine anatomy},
  doi={10.1109/VR.2017.7892345},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892346,
  author={Anglin, Julia and Saldana, David and Schmiesing, Allie and Liew, Sook-Lei},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Transfer of a skilled motor learning task between virtual and conventional environments}, 
  year={2017},
  volume={},
  number={},
  pages={401-402},
  abstract={Immersive, head-mounted virtual reality (HMD-VR) can be a potentially useful tool for motor rehabilitation. However, it is unclear whether the motor skills learned in HMD-VR transfer to the non-virtual world and vice-versa. Here we used a well-established test of skilled motor learning, the Sequential Visual Isometric Pinch Task (SVIPT), to train individuals in either an HMD-VR or conventional training (CT) environment. Participants were then tested in both environments. Our results show that participants who train in the CT environment have an improvement in motor performance when they transfer to the HMD-VR environment. In contrast, participants who train in the HMD-VR environment show a decrease in skill level when transferring to the CT environment. This has implications for how training in HMD-VR and CT may affect performance in different environments.},
  keywords={Training;Logic gates;Virtual reality;Computed tomography;Electroencephalography;Visualization;Indexes;Virtual reality;skilled motor learning;transfer},
  doi={10.1109/VR.2017.7892346},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892347,
  author={Moraes, Ígor Andrade and Cardoso, Alexandre and Lamounier, Edgard and Neto, Milton Miranda and dos Santos Peres, Isabela Cristina},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={A methodology for optimized generation of virtual environments based on hydroelectric power plants}, 
  year={2017},
  volume={},
  number={},
  pages={403-404},
  abstract={The profits and benefits offered by Virtual Reality technology had drawn attention of professionals from several scientific fields, including the power systems', either for training or maintenance. For this purpose, 3D modeling is evidently pointed out as an imperative process for the conception of a Virtual Environment. Before the complexity of Hydroelectric Power Plants and Virtual Reality's contribution for the Industrial area, planning the tridimensional construction of the virtual environment becomes necessary. Thus, this paper presents modeling techniques applicable to several hydroelectric structures, aiming to optimize the 3D construction of the target complexes.},
  keywords={Power generation;Training;Three-dimensional displays;Virtual environments;Solid modeling;Maintenance engineering;Virtual Reality;Hydroelectric;Power Plant;3D Modeling;Power Systems},
  doi={10.1109/VR.2017.7892347},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892348,
  author={Sra, Misha},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Steering locomotion by vestibular perturbation in room-scale VR}, 
  year={2017},
  volume={},
  number={},
  pages={405-406},
  abstract={Advances in consumer virtual reality (VR) technology have made using natural locomotion for navigation in VR a possibility. While walking in VR can enhance immersion and reduce motion sickness, it introduces a few challenges. Walking is only possible within virtual environments (VEs) that fit inside the boundaries of the tracked physical space, which for most users is quite small and carries a high potential for collisions with physical objects around the tracked area. In my thesis, I explore visual and physiological steering techniques that complement the traditional redirected walking technique of scene rotation to alter a user's walking trajectory in the physical space. In this paper, I present the physiological technique.},
  keywords={Legged locomotion;Physiology;Navigation;Visualization;Virtual reality;Trajectory;Electrodes;Virtual Reality;Redirected Walking;Galvanic Vestibular Stimulation;Orientation Response},
  doi={10.1109/VR.2017.7892348},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892349,
  author={Ng, Adrian K. T.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Cognitive psychology and human factors engineering of virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={407-408},
  abstract={This position paper summarizes the author's research interest in Cognitive Psychology and Human-Computer Interaction in the imseCAVE, a CAVE-like system in the University of Hong Kong. Several areas of interest were explored while finding the thesis topic for the Ph.D. research. They include a perception research on distance estimation with proposed error correction mechanism, neurofeedback meditation with EEG in VR and the effect with audio and video, the study of training transfer in VR training, the comparison and research of cybersickness between HMD and the imseCAVE, and comparing VR gaming in TV, HMD, and the imseCAVE by performance, activity level and time perception. With a broad interest, the exact direction is still in the search and requires future exploration.},
  keywords={Training;Psychology;Resists;Virtual reality;Human factors;Ergonomics;Human computer interaction;Virtual environment;cognitive psychology;HCI},
  doi={10.1109/VR.2017.7892349},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892350,
  author={de Jesus Oliveira, Victor Adriel},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Design and assessment of haptic interfaces: An essay on proactive haptic articulation}, 
  year={2017},
  volume={},
  number={},
  pages={409-410},
  abstract={We looked up to elements present in speech articulation to introduce the proactive haptic articulation as a novel approach for intercommunication. The ability to use a haptic interface as a tool for implicit communication can supplement communication and support near and remote collaborative tasks in virtual and physical environments. In addition, the proactive articulation can be applied during the design process, including the user in the construction of more dynamic and optimized vibrotactile vocabularies. In this proposal, we discuss the thesis of the haptic proactive communication and our method to assess and implement it. Our goal is to understand the phenomena related to the proactive articulation of haptic signals and its use for communication and for the design of optimized tactile vocabularies.},
  keywords={Haptic interfaces;Vocabulary;Context;Collaboration;Three-dimensional displays;Speech;Proposals;H.5.1 [Human computer interaction (HCI)]: Interaction devices — Haptic devices},
  doi={10.1109/VR.2017.7892350},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892351,
  author={Kang, Hyo Jeong},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Designing next generation marketplace: The effect of 3D VR store interface design on shopping behavior}, 
  year={2017},
  volume={},
  number={},
  pages={411-412},
  abstract={The medium of virtual reality enables new opportunities for the experience products and shopping environment that may combine best features of both physical and digital market place. As little is known on how best to create virtual reality marketplace, the current research aims to explore required features for VR market user interface and its impact on shopping behavior. As a first step toward endeavor, we will empirically test three different user interfaces; 2D interface style, 3D skeuomorphic interface style and interface that combines features of both 2D and 3D interaction techniques.},
  keywords={Three-dimensional displays;Two dimensional displays;Virtual reality;Navigation;Human computer interaction;IEEE merchandise;Virtual store;User interface design;User studies},
  doi={10.1109/VR.2017.7892351},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892352,
  author={Soccini, Agata Marta},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Gaze estimation based on head movements in virtual reality applications using deep learning}, 
  year={2017},
  volume={},
  number={},
  pages={413-414},
  abstract={Gaze detection in Virtual Reality systems is mostly performed using eye-tracking devices. The coordinates of the sight, as well as other data regarding the eyes, are used as input values for the applications. While this trend is becoming more and more popular in the interaction design of immersive systems, most visors do not come with an embedded eye-tracker, especially those that are low cost and maybe based on mobile phones. We suggest implementing an innovative gaze estimation system into virtual environments as a source of information regarding users intentions. We propose a solution based on a combination of the features of the images and the movement of the head as an input of a Deep Convolutional Neural Network capable of inferring the 2D gaze coordinates in the imaging plane.},
  keywords={Head;Estimation;Neural networks;Resists;Virtual environments;Two dimensional displays;Gaze;Virtual Reality;Deep Learning;Neural Networks;CNN;ConvNet},
  doi={10.1109/VR.2017.7892352},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892353,
  author={Peer, Alex},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={On exploring the mitigation of distance misperception in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={415-416},
  abstract={Misperception of egocentric distances in virtual reality is a well established effect, with many known influencing factors but no clear cause or correction. Herein is proposed a course of research that explores this effect on three fronts: exploring perceptual calibrations, corrections based on known influences and observed misperception rather than a perfect understanding of the causes of misperception; exploring when adaptations due to feedback might exhibit undesirable effects; establishing contexts within practical tasks when distance misperceptions should be expected to have an effect.},
  keywords={Calibration;Virtual environments;Context;Legged locomotion;Visualization;I.3.7 [Computing Methodologies]: Graphics Utilities — Virtual reality},
  doi={10.1109/VR.2017.7892353},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892354,
  author={Daher, Salam},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Optical see-through vs. spatial augmented reality simulators for medical applications}, 
  year={2017},
  volume={},
  number={},
  pages={417-418},
  abstract={Currently healthcare practitioners use standardized patients, physical mannequins, and virtual patients as surrogates for real patients to provide a safe learning environment for students. Each of these simulators has different limitation that could be mitigated with various degrees of fidelity to represent medical cues. As we are exploring different ways to simulate a human patient and their effects on learning, we would like to compare the dynamic visuals between spatial augmented reality and a optical see-through augmented reality where a patient is rendered using the HoloLens and how that affects depth perception, task completion, and social presence.},
  keywords={Augmented reality;Medical services;Head;Visualization;Biomedical imaging;Solid modeling;Magnetic heads;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;I.3.8 [Computer Graphics]: Applications —;I.68 [Simulation and Modeling]: Types of Simulation — Combined;Visual;J.3 [Computer Applications]: Life and Medical Sciences — Medical information systems},
  doi={10.1109/VR.2017.7892354},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892355,
  author={Grandi, Jeronimo G},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Design of collaborative 3D user interfaces for virtual and augmented reality}, 
  year={2017},
  volume={},
  number={},
  pages={419-420},
  abstract={We explore design approaches for cooperative work in virtual manipulation tasks. We seek to understand the fundamental aspects of the human cooperation and design interfaces and manipulation actions to enhance the group's ability to solve complex manipulation tasks in various immersion scenarios.},
  keywords={Three-dimensional displays;Collaboration;User interfaces;Virtual environments;Visualization;Augmented reality;H.5.2. [Information Interfaces and Presentation]: User Interfaces — Input devices and strategies;H.5.3. [Information Interfaces and Presentation]: Group and Organization Interfaces — Computer-supported cooperative work},
  doi={10.1109/VR.2017.7892355},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892356,
  author={Ferdous, Sharif Mohammad Shahnewaz},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Improve accessibility of virtual and augmented reality for people with balance impairments}, 
  year={2017},
  volume={},
  number={},
  pages={421-422},
  abstract={Most people experience some imbalance in a fully immersive Virtual Environment (VE) (i.e., wearing a Head Mounted Display (HMD) that blocks the users view of the real world). However, this imbalance is significantly worse in People with Balance Impairments (PwBIs) and minimal research has been done to improve this. In addition to imbalance problem, lack of proper visual cues can lead to different accessibility problems for PwBIs (e.g., small reach from the fear of imbalance, decreased gait performance, etc.) We plan to explore the effects of different visual cues on peoples' balance, reach, gait, etc. Based on our primary study, we propose to incorporate additional visual cues in VEs that proved to significantly improve balance of PwBIs while they are standing and playing in a VE. We plan to further investigate if additional visual cues have similar effects in augmented reality. We are also developing studies to research reach and gait in VR as our future work.},
  keywords={Visualization;Games;Augmented reality;Multiple sclerosis;Parkinson's disease;Elevators;Virtual Reality;Balance;Accessibility;Visual Feedback;Gait;Head-mounted Display},
  doi={10.1109/VR.2017.7892356},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892357,
  author={Hosseini, Mohammad},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={View-aware tile-based adaptations in 360 virtual reality video streaming}, 
  year={2017},
  volume={},
  number={},
  pages={423-424},
  abstract={We have proposed an adaptive view-aware bandwidth-efficient 360 VR video streaming framework based on the tiling features of MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR videos, and showcase a dynamic view-aware adaptation technique to tackle the high bandwidth demands of streaming 360 VR videos to wireless VR headsets. As a part of our contributions, we spatially partition the underlying 3D mesh into multiple 3D sub-meshes, and construct an efficient 3D geometry mesh called hexaface sphere to optimally represent tiled 360 VR videos in the 3D space. We then spatially divide the 360 videos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe the spatial relationship of tiles in the 3D space, and prioritize the tiles in the Field of View (FoV) for view-aware adaptation. The initial evaluations that we conducted show that we can save up to 72% of the required bandwidth on 360 VR video streaming with minor negative quality impacts compared to the baseline scenario when no adaptations is applied.},
  keywords={Streaming media;Three-dimensional displays;Bandwidth;Transform coding;Resists;Geometry;Multimedia systems;360 VR videos;VR streaming;MPEG-DASH SRD;Hexaface Sphere;Field-of-View;Adaptive Streaming},
  doi={10.1109/VR.2017.7892357},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892358,
  author={Schmalstieg, Dieter and Höllerer, Tobias},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Augmented reality: Principles and practice}, 
  year={2017},
  volume={},
  number={},
  pages={425-426},
  abstract={This tutorial will provide a detailed introduction to Augmented Reality (AR). AR is a key user-interface technology for personalized, situated information delivery, navigation, on-demand instruction and games. The widespread availability and rapid evolution of smartphones and new devices such as Hololens enables software-only solutions for AR, where it was previously necessary to assemble custom hardware solutions. However, ergonomic and technical limitations of existing devices make this a challenging endeavor. In particular, it is necessary to design novel efficient real-time computer vision and computer graphics algorithms, and create new lightweight forms of interaction with the environment through small form-factor devices. This tutorial will present selected technical achievements in this field and highlight some examples of successful application prototypes.},
  keywords={Tutorials;Augmented reality;Solid modeling;Optical sensors;Smart phones;Computer vision;Augmented Reality;mixed reality},
  doi={10.1109/VR.2017.7892358},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892359,
  author={González-Zúñiga, Diego and O'Shaughnessy, Peter and Blix, Michael},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Hypertextual reality: VR on the web}, 
  year={2017},
  volume={},
  number={},
  pages={427-428},
  abstract={The tutorial focuses on Virtual Reality on the web and how researchers and developers can leverage its power to create content. The WebVR specification is presented, along with examples of how it works in a browser. Content creation is addressed by mentioning the available frameworks accompanied by a hands-on session in A-Frame. Additionally, the concept of Progressive Web App is explained and how it enables web experiences to work offline.},
  keywords={Tutorials;Virtual reality;Browsers;Three-dimensional displays;Internet;Stereo image processing;W3C;WebVR;Virtual Reality;Web Development},
  doi={10.1109/VR.2017.7892359},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892360,
  author={Rouse, Rebecca and Chang, Benjamin and Ruzanka, Silvia},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Diving into the multiplicity: Liberating your design process from a convention-centered approach}, 
  year={2017},
  volume={},
  number={},
  pages={429-430},
  abstract={Stop feeling bad about not having a language of VR, and embrace the multiplicity! This full day tutorial explores ways of applying the vibrant creativity of early media to VR, AR, and MR work today, using a new cross-historical concept called media of attraction. Participants will be guided through a prototyping process focused not on best practices, but on restriction mining, bespoke solutions, and associative creative strategies inspired by fascinating historical examples and artistic methods. The session concludes with prototype creation, and the development of speculative design work envisioning next technologies for media of attraction of the future.},
  keywords={Media;Motion pictures;Tutorials;Virtual reality;Art;Games;History;virtual reality;mixed reality;design;media history;future technology},
  doi={10.1109/VR.2017.7892360},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892361,
  author={Jerald, Jason},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Human-centered design for immersive interactions}, 
  year={2017},
  volume={},
  number={},
  pages={431-432},
  abstract={VR has the potential to provide experiences and deliver results that cannot be otherwise achieved. However, interacting with immersive applications is not always straightforward and it is not just about an interface for the user to reach their goals. It is also about users working in an intuitive manner that is a pleasurable experience and devoid of frustration. Although VR systems and applications are incredibly complex, it is up to designers to take on the challenge of having the VR application intuitively communicate to users how the virtual world and its tools work so that those users can achieve their goals in an elegant and comfortable manner.},
  keywords={Virtual reality;Focusing;Three-dimensional displays;Tutorials;Computer science;Indexes;Virtual reality;immersion;interaction;human-centered design},
  doi={10.1109/VR.2017.7892361},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892362,
  author={Kruijff, Ernst and Riecke, Bernhard E.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Navigation interfaces for virtual reality and gaming: Theory and practice}, 
  year={2017},
  volume={},
  number={},
  pages={433-434},
  abstract={In this course, we will take a detailed look at various breeds of spatial navigation interfaces that allow for locomotion in digital 3D environments such as games, virtual environments or even the exploration of abstract data sets. We will closely look into the basics of navigation, unraveling the psychophysics (including wayfinding) and actual navigation (travel) aspects. The theoretical foundations form the basis for the practical skillset we will develop, by providing an in-depth discussion of navigation devices and techniques, and a step-by-step discussion of multiple real-world case studies. Doing so, we will cover the full range of navigation techniques from handheld to full-body, highly engaging and partly unconventional methods and tackle spatial navigation with hands-on-experience and tips for design and validation of novel interfaces. In particular, we will be looking at affordable setups and ways to “trick” out users to enable a realistic feeling of self-motion in the explored environments. As such, the course unites the theory and practice of spatial navigation, serving as entry point to understand and improve upon currently existing methods for the application domain at hand.},
  keywords={Navigation;Virtual reality;Three-dimensional displays;Guidelines;User interfaces;Games;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892362},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892363,
  author={Michalski, Quba and Hogan, Brendan J. and Hunsdale, Jamie},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={The pull}, 
  year={2017},
  volume={},
  number={},
  pages={435-435},
  abstract={In a secret science facility, gravity has been conquered. “Down” is no longer a direction, but a choice. Step into the center of modified chambers and witness the laws of nature be broken in this five-experiment series. VR has all but torn down the barriers between the imagination of the creator and the experience of the viewer. A concept like The Pull simply does not translate into traditional 2D. We can suggest concepts through TVs and monitors, but we can't truly experience them — and breaking the very laws of nature is something that can only be experienced. While flat media limits us to hinting and coaxing at an experience, by creating in VR, I can more faithfully share my vision with you, the viewer. For a few minutes — for five chambers — I can truly invite you into my world.},
  keywords={Solid modeling;Motion pictures;Art;Virtual reality;Gravity;Acoustics;Engines;Gravity;simulation;art},
  doi={10.1109/VR.2017.7892363},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892364,
  author={Kim, June and Bednarz, Tomasz},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Virtual reality to save endangered animals: Many eyes on the wild}, 
  year={2017},
  volume={},
  number={},
  pages={436-436},
  abstract={Immersive technologies and particularly the Virtual Reality (VR) provide new exciting ways to see the world. Today, we introduce our research that successfully employed VR to the biodiversity conservation sciences. Jaguars are one of the endangered animals. It is certainly critical and compelling to preserve ecosystem for endangered animals. With the awareness of this, we endeavoured to establish a multidisciplinary VR project that implemented data from indigenous villagers (jaguar experts group A), the conventional knowledge of the field of jaguar ecosystem (from jaguar experts group B), and mathematical and statistical models. Our fascination lies in these questions: can we effectively bring together VR and analytical capabilities? Can VR be used to make this world a better place for living beings? Please enjoy our 360-degree images of jaguar habitats taken in the Peruvian Amazon.},
  keywords={Cameras;Ecosystems;Animals;Virtual reality;Vegetation;Rivers;Streaming media;H.5.1 [Multimedia Interfaces and Presentation (e.g. HCI)]: Artificial, augmented, and Virtual Realities — Video;I.2.3 [Inference Engines]: Knowledge Acquisition — Uncertainty},
  doi={10.1109/VR.2017.7892364},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892365,
  author={Southard, Dylan and Allan-Blitz, Elijah and Halsey, Jordan and Heller, Christina and Joukowsky, Artemis},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Defying the Nazis VR}, 
  year={2017},
  volume={},
  number={},
  pages={437-437},
  abstract={“Defying the Nazis VR” uses CGI, motion graphics, and archival documentary footage to re-create a heroic episode from World War II in VR, experimenting with the emotional power of virtual reality and with the medium as an educational tool.},
  keywords={Production;Virtual reality;Marine vehicles;History;Europe;Visualization;Urban areas;History;documentary;virtual reality;World War II;refugees},
  doi={10.1109/VR.2017.7892365},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892366,
  author={Chok, Lionel},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Singapore inside out London 2015 in VR}, 
  year={2017},
  volume={},
  number={},
  pages={438-438},
  abstract={Singapore: Inside Out 2015 is an international creative showcase featuring a collection of multisensorial experiences designed by the country's creative talents. After making its successful debut in Beijing, the travelling showcase stops at Brick Lane Yard here in London from 24 to 28 June for its stint in the capital, before heading to New York in September and a homecoming finale in Singapore in November 2015 (Singapore Inside Out 2015). This Virtual Reality Experience enables users to relive this unique festival.},
  keywords={Videos;Virtual reality;Visualization;Smart phones;Mobile applications;Testing;Navigation;creative;festival;virtual reality},
  doi={10.1109/VR.2017.7892366},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892367,
  author={Patterson, Kate},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Genome gazing: A 360° stereoscopic animation for Google cardboard}, 
  year={2017},
  volume={},
  number={},
  pages={439-439},
  abstract={Scientific concepts related to genomics and epigenetics can be abstract and are mostly invisible to the human eye. Communication of these concepts to a lay audience is particularly challenging. Visualizing DNA and associated molecules via an embodied experience offers the user an opportunity to interact with this scientific data in a more meaningful way. This project explores new ways to visualize and represent the major molecular players in genomics and epigenetics using virtual reality via portable head mounted displays.},
  keywords={Virtual reality;Genomics;Bioinformatics;DNA;Animation;Epigenetics;Education;Virtual reality;genomics;epigenetics;education;embodied cognition;science communication},
  doi={10.1109/VR.2017.7892367},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892368,
  author={Stock, Mark J.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Smoke Water Fire (2016)}, 
  year={2017},
  volume={},
  number={},
  pages={440-440},
  abstract={Smoke Water Fire, (2016), is a digital video for stereoscopic virtual reality systems. It is a radiosity rendered computer simulation of the collision of many fluid volumes, and required 8 months to generate. Instead of the forms being rendered as a natural material, though, they are presented in their pure, computational form. By stripping the fluid of its physical context, it can exist in a purely ephemeral, numerical, virtual state.},
  keywords={Visualization;Solids;Art;Workstations;Virtual reality;Surface treatment;Rendering (computer graphics);algorithmic art;contemporary art;generative art;computational physics;fluid dynamics;virtual reality},
  doi={10.1109/VR.2017.7892368},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892369,
  author={Mavridou, Ifigeneia and McGhee, James T. and Hamedi, Mahyar and Fatoorechi, Mohsen and Cleal, Andrew and Ballaguer-Balester, Emili and Seiss, Ellen and Cox, Graeme and Nduka, Charles},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={FACETEQ interface demo for emotion expression in VR}, 
  year={2017},
  volume={},
  number={},
  pages={441-442},
  abstract={Faceteq prototype v.05 is a wearable technology for measuring facial expressions and biometric responses for experimental studies in Virtual Reality. Developed by Emteq Ltd laboratory, Faceteq can enable new avenues for virtual reality research through combination of high performance patented dry sensor technologies, proprietary algorithms and real-time data acquisition and streaming. Emteq founded the Faceteq project with the aim to provide a human-centered additional tool for emotion expression, affective human-computer interaction and social virtual environments. The proposed demonstration will exhibit the hardware and its functionality by allowing attendees to experience three of the showcasing applications we developed this year.},
  keywords={Electromyography;Virtual reality;Games;Prototypes;Real-time systems;Data acquisition;Psychology;virtual reality;facial expression;emotion;EMG;affective computing},
  doi={10.1109/VR.2017.7892369},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892370,
  author={Mori, Shohei and Maezawa, Momoko and Ienaga, Naoto and Saito, Hideo},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Diminished hand: A diminished reality-based work area visualization}, 
  year={2017},
  volume={},
  number={},
  pages={443-444},
  abstract={Live instructors perspective videos are useful to present intuitive visual instructions for trainees in medical and industrial settings. In such videos, the instructors hands often hide the work area. In this demo, we present a diminished hand for visualizing the work area hidden by hands by capturing the work area with multiple cameras. To achieve the diminished reality, we use a light field rendering technique, in which light rays avoid passing through penalty points set in the unstructured light fields reconstructed from the multiple viewpoint images.},
  keywords={Cameras;Videos;Rendering (computer graphics);Three-dimensional displays;Visualization;Real-time systems;Image reconstruction;K.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.4.9 [Image Processing and Computer Vision]: Applications},
  doi={10.1109/VR.2017.7892370},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892371,
  author={Hamada, Takeo and Okada, Michio and Kitazaki, Michiteru},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Jogging with a virtual runner using a see-through HMD}, 
  year={2017},
  volume={},
  number={},
  pages={445-446},
  abstract={We present a novel assistive method for leading casual joggers by showing a virtual runner on see-through head-mounted display they worn. It moves at a constant pace specified in advance by them, and its motion synchronizes the user's one. People can always visually check the pace by looking at it as a personal pacemaker. They are also motivated to keep running by regarding it as a jogging companion. Moreover, proposed method overcomes safety problem of AR apps. Its most body parts are transparent so that it doesn't obstruct their view. This study, thus, may contribute to augment daily jogging experience.},
  keywords={Resists;Smart phones;Synchronization;Pacemakers;Accelerometers;Head;Virtual reality;Augmented reality;jogging companion;pace keeping},
  doi={10.1109/VR.2017.7892371},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892372,
  author={Malleson, Charles and Kosek, Maggie and Klaudiny, Martin and Huerta, Ivan and Bazin, Jean-Charles and Sorkine-Hornung, Alexander and Mine, Mark and Mitchell, Kenny},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Demonstration: Rapid one-shot acquisition of dynamic VR avatars}, 
  year={2017},
  volume={},
  number={},
  pages={447-448},
  abstract={In this demonstration, we showcase a system for rapid acquisition of bespoke avatars for each participant (subject) in a social VR environment is presented. For each subject, the system automatically customizes a parametric avatar model to match the captured subject by adjusting its overall height, body and face shape parameters and generating a custom face texture.},
  keywords={Face;Avatars;Shape;Three-dimensional displays;Cameras;Solid modeling;Animation;Avatars;Capture;Virtual Reality},
  doi={10.1109/VR.2017.7892372},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892373,
  author={Langbehn, Eike and Lubos, Paul and Bruder, Gerd and Steinicke, Frank},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Application of redirected walking in room-scale VR}, 
  year={2017},
  volume={},
  number={},
  pages={449-450},
  abstract={Redirected walking (RDW) promises to allow near-natural walking in an infinitely large virtual environment (VE) by subtle manipulations of the virtual camera. Previous experiments showed that a physical radius of at least 22 meters is required for undetectable RDW. However, we found that it is possible to decrease this radius and to apply RDW to room-scale VR, i. e., up to approximately 5m × 5m. This is done by using curved paths in the Ve instead of straight paths, and by coupling them together in a way that enables continuous walking. Furthermore, the corresponding paths in the real world are laid out in a way that fits perfectly into room-scale VR. In this research demo, users can experience RDW in a room-scale head-mounted display VR setup and explore a VE of approximately 25m × 25m.},
  keywords={Legged locomotion;Visualization;Human computer interaction;Virtual environments;Sensitivity;Virtual Reality;Redirected Walking;Room-scale},
  doi={10.1109/VR.2017.7892373},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892374,
  author={Tanaka, Eduardo H. and Paludo, Juliana A. and Bacchetti, Rafael and Gadbem, Edgar V. and Domingues, Leonardo R. and Cordeiro, Carlúcio S. and Giraldi, Olavo and Alcarde Gallo, Guilherme and Mendes da Silva, Adam and Cascone, Marcos H.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Immersive virtual training for substation electricians}, 
  year={2017},
  volume={},
  number={},
  pages={451-452},
  abstract={This research demonstration presents an Immersive Virtual Substation for Electricians Training. A substation is one of the most critical facilities of the electrical distribution system, so it is mandatory to keep its normal operation to deliver high standards of service, power quality and avoid blackouts to consumers. Therefore, it is necessary to give an effective training to the electricians who will operate the equipment in the substation and to prepare them to face emergencies on daily basis. The main purpose of the proposed Immersive Virtual Environment is to provide a realistic experience to trainees in a safe environment where they can interact with equipment, explore the facility and, mainly, practice basic and complex maneuvers to recover substation operations. Users can interact with this Immersive Virtual Environment using HMDs, joysticks or even an ordinary keyboard, mouse and monitor. Feedback from trainees and instructors who used the Immersive Virtual Environment was very positive, indicating that the objectives were fully achieved.},
  keywords={Substations;Training;Resists;Three-dimensional displays;Solid modeling;Virtual environments;Immersive Virtual Reality;Training Staff;Distribution Substation},
  doi={10.1109/VR.2017.7892374},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892375,
  author={de Jesus Oliveira, Victor Adriel and Brayda, Luca and Nedel, Luciana and Maciel, Anderson},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Experiencing guidance in 3D spaces with a vibrotactile head-mounted display}, 
  year={2017},
  volume={},
  number={},
  pages={453-454},
  abstract={Vibrotactile feedback is broadly used to support different tasks in virtual and augmented reality applications, such as navigation, communication, attentional redirection, or to enhance the sense of presence in virtual environments. Thus, we aim to include the haptic component to the most popular wearable used in VR applications: the VR headset. After studying the acuity around the head for vibrating stimuli, and trying different parameters, actuators, and configurations, we developed a haptic guidance technique to be used in a vibrotactile Head-mounted Display (HMD). Our vi-brotactile HMD was made to render the position of objects in a 3D space around the subject by varying both stimulus loci and vibration frequency. In this demonstration, the participants will interact with different scenarios where the mission is to select a number of predefined objects. However, instead of displaying occlusive graphical information to point to these objects, vibrotactile cues will provide guidance in the VR setup. Participants will see that our haptic guidance technique can be both easy to use and entertaining. (See Video: https://youtu.be/_H0MQy6QD7M).},
  keywords={Resists;Haptic interfaces;Three-dimensional displays;Headphones;Vibrations;Navigation;Human computer interaction;H.5.1 [Human computer interaction (HCI)]: Interaction devices — Haptic devices;H.5.2 [Human computer interaction (HCI)]: Interaction paradigms — Virtual reality},
  doi={10.1109/VR.2017.7892375},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892376,
  author={Zhou, Qian and Wu, Kai and Miller, Gregor and Stavness, Ian and Fels, Sidney},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={3DPS: An auto-calibrated three-dimensional perspective-corrected spherical display}, 
  year={2017},
  volume={},
  number={},
  pages={455-456},
  abstract={We describe an auto-calibrated 3D perspective-corrected spherical display that uses multiple rear projected pico-projectors. The display system is auto-calibrated via 3D reconstruction of each projected pixel on the display using a single inexpensive camera. With the automatic calibration, the multiple-projector system supports a seamless blended imagery on the spherical screen. Furthermore, we incorporate head tracking with the display to present 3D content with motion parallax by rendering perspective-corrected images based on the viewpoint. To show the effectiveness of this design, we implemented a view-dependent application that allows walk-around visualization from all angles for a single head-tracked user. We also implemented a view-independent application that supports a wall-papered rendering for multi-user viewing. Thus, both view-dependent 3D VR content and spherical 2D content, such as a globe, can be easily experienced with this display.},
  keywords={Three-dimensional displays;Calibration;Visualization;Cameras;Rendering (computer graphics);Fish;Virtual reality;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892376},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892377,
  author={Gunkel, Simon and Prins, Martin and Stokking, Hans and Niamut, Omar},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={WebVR meets WebRTC: Towards 360-degree social VR experiences}, 
  year={2017},
  volume={},
  number={},
  pages={457-458},
  abstract={Virtual Reality (VR) and 360-degree video are reshaping the media landscape, creating a fertile business environment. During 2016 new 360-degree cameras and VR headsets entered the consumer market, distribution platforms are being established and new production studios are emerging. VR is evermore becoming a hot topic in research and industry and many new and exciting interactive VR content and experiences are emerging. The biggest gap we see in these experiences are social and shared aspects of VR. In this demo we present our ongoing efforts towards social and shared VR by developing a modular web based VR framework, that extends current video conferencing capabilities with new functionalities of Virtual and Mixed Reality. It allows us to connect two people together for mediated audio-visual interaction, while being able to engage in interactive content. Our framework allows to run extensive technological and user based trials in order to evaluate VR experiences and to build immersive multi-user interaction spaces. Our first results indicate that a high level of engagement and interaction between users is possible in our 360-degree VR set-up utilizing current web technologies.},
  keywords={Games;WebRTC;Virtual environments;Green products;Cameras;Three-dimensional displays;Virtual Reality;VR;Social VR;WebRTC;WebVR;Interactive Content;Immersive Virtual Environments},
  doi={10.1109/VR.2017.7892377},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892378,
  author={Grubert, Jens and Kranz, Matthias},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={mpCubee: Towards a mobile perspective cubic display using mobile phones}, 
  year={2017},
  volume={},
  number={},
  pages={459-460},
  abstract={While we witness significant changes in display technologies, to date, the majority of display form factors remain flat. The research community has investigated other geometric display configuration given the rise to cubic displays that create the illusion of a 3D virtual scene within the cube. We present a self-contained mobile perspective cubic display (mpCubee) assembled from multiple smartphones. We achieve perspective correct projection of 3D content through head-tracking using built-in cameras in smartphones. Furthermore, our prototype allows to spatially manipulate 3D objects on individual axes due to the orthogonal configuration of touch displays.},
  keywords={Three-dimensional displays;Mobile communication;Smart phones;Head;Rendering (computer graphics);Cameras;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892378},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892379,
  author={Grubert, Jens and Kränz, Matthias},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Towards ad hoc mobile multi-display environments on commodity mobile devices}, 
  year={2017},
  volume={},
  number={},
  pages={461-462},
  abstract={We present a demonstration of HeadPhones (Headtracking + smart-Phones), a novel approach for the spatial registration of multiple mobile devices into an ad hoc multi-display environment. We propose to employ the user's head as external reference frame for the registration of multiple mobile devices into a common coordinate system. Our approach allows for dynamic repositioning of devices during runtime without the need for external infrastructure such as separate cameras or fiducials. Specifically, our only requirements are local network connections and mobile devices with built-in front facing cameras. This way, HeadPhones enables spatially-aware multi-display applications in mobile contexts.},
  keywords={Mobile handsets;Mobile communication;Headphones;Cameras;Head;Fires;Two dimensional displays;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892379},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892380,
  author={Borba, Eduardo Zilles and Montes, Andre and Almeida, Marcio and Nagamura, Mario and Lopes, Roseli and Zuffo, Marcelo Knorich and Araujo, Astolfo and Kopper, Regis},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={ArcheoVR: Exploring Itapeva's archeological site}, 
  year={2017},
  volume={},
  number={},
  pages={463-464},
  abstract={This demo presents a fully immersive and interactive virtual environment (VE) - the ArcheoVR, which represents Itapeva Rocky Shelter, a prehistoric archeological site in Brazil. W workflow started with a real world data capture - laser scanners, drones and photogrammetry. Captured information was transformed into a carefully designed realistic 3D scene and interactive features that allows users to experience the virtual archeological site in real-time. The main objective of this VR model is to allow the general public to feel and explore an otherwise restricted and ephemeral site and to assess prototype tools intended for future digital archaeological exploration.},
  keywords={Three-dimensional displays;Solid modeling;Visualization;Rocks;Virtual environments;Laser modes;Virtual reality;3D simulation;cyber-archeology;presence;gamification},
  doi={10.1109/VR.2017.7892380},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892381,
  author={Ard, Tyler and Krum, David M. and Phan, Thai and Duncan, Dominique and Essex, Ryan and Bolas, Mark and Toga, Arthur},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={NIVR: Neuro imaging in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={465-466},
  abstract={Visualization is a critical component of neuroimaging, and how to best view data that is naturally three dimensional is a long standing question in neuroscience. Many approaches, programs, and techniques have been developed specifically for neuroimaging. However, exploration of 3D information through a 2D screen is inherently limited. Many neuroscientific researchers hope that with the recent commercialization and popularization of VR, it can offer the next-step in data visualization and exploration. Neuro Imaging in Virtual Reality (NIVR), is a visualization suite that employs various immersive visualizations to represent neuroimaging information in VR. Some established techniques, such as raymarching volume visualization, are paired with newer techniques, such as near-field rendering, to provide a broad basis of how we can leverage VR to improve visualization and navigation of neuroimaging data. Several of the neuroscientific visualization approaches presented are, to our knowledge, the first of their kind. NIVR offers not only an exploration of neuroscientific data visualization, but also a tool to expose and educate the public regarding recent advancements in the field of neuroimaging. By providing an engaging experience to explore new techniques and discoveries in neuroimaging, we hope to spark scientific interest through a broad audience. Furthermore, neuroimaging offers deep and expansive datasets; a single scan can involve several gigabytes of information. Visualization and exploration of this type of information can be challenging, and real-time exploration of this information in VR even more so. NIVR explores pathways which make this possible, and offers preliminary stereo visualizations of these types of massive data.},
  keywords={Data visualization;Neuroimaging;Rendering (computer graphics);Magnetic resonance imaging;Virtual reality;Navigation;Sockets;MRI;Virtual Reality;Medical Imaging},
  doi={10.1109/VR.2017.7892381},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892382,
  author={Duncan, Dominique and Newman, Bradley and Saslow, Adam and Wanserski, Emily and Ard, Tyler and Essex, Ryan and Toga, Arthur},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={VRAIN: Virtual reality assisted intervention for neuroimaging}, 
  year={2017},
  volume={},
  number={},
  pages={467-468},
  abstract={The USC Stevens Neuroimaging and Informatics Institute in the Laboratory of Neuro Imaging (http://loni.usc.edu) has the largest collection/repository of neuroanatomical MRI scans in the world and is at the forefront of both brain imaging and data storage/processing technology. One of our workflow processes involves algorithmic segmentation of MRI scans into labeled anatomical regions (using FreeSurfer, currently the best software for this purpose). This algorithm is imprecise, and users must tediously correct errors manually by using a mouse and keyboard to edit individual MRI slices at a time. We demonstrate preliminary work to improve efficiency of this task by translating it into 3 dimensions and utilizing virtual reality user interfaces to edit multiple slices of data simultaneously.},
  keywords={Magnetic resonance imaging;Table lookup;Virtual reality;Neuroimaging;Engines;Laser applications;Visualization;MRI;Virtual Reality;Medical Imaging},
  doi={10.1109/VR.2017.7892382},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892383,
  author={Chang, Yun Suk and Nuernberger, Benjamin and Luan, Bo and Höllerer, Tobias and O'Donovan, John},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Gesture-based augmented reality annotation}, 
  year={2017},
  volume={},
  number={},
  pages={469-470},
  abstract={Drawing annotations with 3D hand gestures in augmented reality is useful for creating visual and spatial references in the real world, especially when these gestures can be issued from a distance. Different techniques exist for highlighting physical objects with hand-drawn annotations from a distance, assuming an approximate 3D scene model (e.g., as provided by the Microsoft HoloLens). However, little is known about user preference and performance of such methods for annotating real-world 3D environments. To explore and evaluate different 3D hand-gesture-based annotation drawing methods, we have developed an annotation drawing application using the HoloLens augmented reality development platform. The application can be used for highlighting objects at a distance and multi-user collaboration by annotating in the real world.},
  keywords={Three-dimensional displays;Augmented reality;Transforms;Solid modeling;Surface treatment;User interfaces;Augmented Reality;Annotations;Spatial Referencing;HoloLens},
  doi={10.1109/VR.2017.7892383},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892384,
  author={Woodworth, Jason W. and Ekong, Sam and Borst, Christoph W.},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Virtual field trips with networked depth-camera-based teacher, heterogeneous displays, and example energy center application}, 
  year={2017},
  volume={},
  number={},
  pages={471-472},
  abstract={This demo presents an approach to networked educational virtual reality for virtual field trips and guided exploration. It shows an asymmetric collaborative interface in which a remote teacher stands in front of a large display and depth camera (Kinect) while students are immersed with HMDs. The teacher's front-facing mesh is streamed into the environment to assist students and deliver instruction. Our project uses commodity virtual reality hardware and high-performance networks to allow students who are unable to visit a real facility with an alternative that provides similar educational benefits. Virtual facilities can further be augmented with educational content through interactables or small games. We discuss motivation, features, interface challenges, and ongoing testing.},
  keywords={Mirrors;Visualization;Virtual reality;Three-dimensional displays;Resists;Testing;Monitoring;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892384},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892385,
  author={Chen, Chih-Fan and Bolas, Mark and Rosenberg, Evan Suma},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Rapid creation of photorealistic virtual reality content with consumer depth cameras}, 
  year={2017},
  volume={},
  number={},
  pages={473-474},
  abstract={Virtual objects are essential for building environments in virtual reality (VR) applications. However, creating photorealistic 3D models is not easy, and handcrafting the detailed 3D model from a real object can be time and labor intensive. An alternative way is to build a structured camera array such as a light-stage to reconstruct the model from a real object. However, these technologies are very expensive and not practical for most users. In this work, we demonstrate a complete end-to-end pipeline for the capture, processing, and rendering of view-dependent 3D models in virtual reality from a single consumer-grade RGB-D camera. The geometry model and the camera trajectories are automatically reconstructed from a RGB-D image sequence captured offline. Based on the HMD position, selected images are used for real-time model rendering. The result of this pipeline is a 3D mesh with view-dependent textures suitable for real-time rendering in virtual reality. Specular reflections and light-burst effects are especially noticeable when users view the objects from different perspectives in a head-tracked environment.},
  keywords={Solid modeling;Cameras;Three-dimensional displays;Image color analysis;Virtual reality;Computational modeling;Image reconstruction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.3.7 [Computing Methodologies]: Three-Dimensional Graphics and Realism — Color, shading, shadowing, and texture;I.2.10 [Computing Methodologies]: Vision and Scene Understanding — 3D/stereo scene analysis},
  doi={10.1109/VR.2017.7892385},
  ISSN={2375-5334},
  month={March},}@INPROCEEDINGS{7892386,
  author={Elvezio, Carmine and Sukan, Mengu and Feiner, Steven and Tversky, Barbara},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Travel in large-scale head-worn VR: Pre-oriented teleportation with WIMs and previews}, 
  year={2017},
  volume={},
  number={},
  pages={475-476},
  abstract={We demonstrate an interaction technique that allows a user to point at a world-in-miniature representation of a city-scale virtual environment and perform efficient and precise teleportation by pre-orienting an avatar. A preview of the post-teleport view of the full-scale virtual environment updates interactively as the user adjusts the position, yaw, and pitch of the avatar's head with a pair of 6DoF-tracked controllers. We describe design decisions and contrast with alternative approaches to virtual travel.},
  keywords={Avatars;Teleportation;Legged locomotion;Virtual environments;Head;Buildings;Portals;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces — Input devices and strategies, Interaction styles;I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction techniques},
  doi={10.1109/VR.2017.7892386},
  ISSN={2375-5334},
  month={March},}
