Search Topic {Explore how to associate spatial text with objects or people? Possible methods include Spatial Proximity, Symbolic Coding, Visual Connector.}

@inproceedings{10.1145/3313831.3376614,
author = {Lin, Chuan-en and Cheng, Ta Ying and Ma, Xiaojuan},
title = {ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376614},
doi = {10.1145/3313831.3376614},
abstract = {Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {affordance, architect, asymmetric, passive haptics, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3025453.3025717,
author = {M\"{u}ller, Jens and R\"{a}dle, Roman and Reiterer, Harald},
title = {Remote Collaboration With Mixed Reality Displays: How Shared Virtual Landmarks Facilitate Spatial Referencing},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025717},
doi = {10.1145/3025453.3025717},
abstract = {HCI research has demonstrated Mixed Reality (MR) as being beneficial for co-located collaborative work. For remote collaboration, however, the collaborators' visual contexts do not coincide due to their individual physical environments. The problem becomes apparent when collaborators refer to physical landmarks in their individual environments to guide each other's attention. In an experimental study with 16 dyads, we investigated how the provisioning of shared virtual landmarks (SVLs) influences communication behavior and user experience. A quantitative analysis revealed that participants used significantly less ambiguous spatial expressions and reported an improved user experience when SVLs were provided. Based on these findings and a qualitative video analysis we provide implications for the design of MRs to facilitate remote collaboration.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {6481–6486},
numpages = {6},
keywords = {virtual landmarks, remote collaboration, mixed reality},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3313831.3376436,
author = {Zhu-Tian, Chen and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},
title = {Augmenting Static Visualizations with PapARVis Designer},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376436},
doi = {10.1145/3313831.3376436},
abstract = {This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality.Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workflow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wall-sized visualizations. A user study shows high user satisfaction of our environment and confirms that participants can create augmented visualizations in an average of 4.63 minutes.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {augmented static visualization, data visualization authoring, visualization in augmented reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3491102.3501946,
author = {Luo, Weizhou and Lehmann, Anke and Widengren, Hjalmar and Dachselt, Raimund},
title = {Where Should We Put It? Layout and Placement Strategies of Documents in Augmented Reality for Collaborative Sensemaking},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501946},
doi = {10.1145/3491102.3501946},
abstract = {Future offices are likely reshaped by Augmented Reality (AR) extending the display space while maintaining awareness of surroundings, and thus promise to support collaborative tasks such as brainstorming or sensemaking. However, it is unclear how physical surroundings and co-located collaboration influence the spatial organization of virtual content for sensemaking. Therefore, we conducted a study (N=28) to investigate the effect of office environments and work styles during a document classification task using AR with regard to content placement, layout strategies, and sensemaking workflows. Results show that participants require furniture, especially tables and whiteboards, to assist sensemaking and collaboration regardless of room settings, while generous free spaces (e.g., walls) are likely used when available. Moreover, collaborating participants tend to use furniture despite personal layout preferences. We identified different placement and layout strategies, as well as the transitions in-between. Finally, we propose design implications for future immersive sensemaking applications and beyond.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {627},
numpages = {16},
keywords = {Augmented Reality, Mixed Reality, affordance, collaborative sensemaking, content organization, qualitative user study, sensemaking, spatial layout, spatiality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3411764.3445593,
author = {Langner, Ricardo and Satkowski, Marc and B\"{u}schel, Wolfgang and Dachselt, Raimund},
title = {MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445593},
doi = {10.1145/3411764.3445593},
abstract = {We present Marvis, a conceptual framework that combines mobile devices and head-mounted Augmented Reality (AR) for visual data analysis. We propose novel concepts and techniques addressing visualization-specific challenges. By showing additional 2D and 3D information around and above displays, we extend their limited screen space. AR views between displays as well as linking and brushing are also supported, making relationships between separated visualizations plausible. We introduce the design process and rationale for our techniques. To validate Marvis’ concepts and show their versatility and widespread applicability, we describe six implemented example use cases. Finally, we discuss insights from expert hands-on reviews. As a result, we contribute to a better understanding of how the combination of one or more mobile devices with AR can benefit visual data analysis. By exploring this new type of visualization environment, we hope to provide a foundation and inspiration for future mobile data visualizations.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {468},
numpages = {17},
keywords = {mobile devices, mobile data visualization, immersive analytics, head-mounted augmented reality, data visualization, data analysis, cross-device interaction, augmented displays},
location = {Yokohama, Japan},
series = {CHI '21}
}

@INPROCEEDINGS{7892383,
  author={Chang, Yun Suk and Nuernberger, Benjamin and Luan, Bo and Höllerer, Tobias and O'Donovan, John},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Gesture-based augmented reality annotation}, 
  year={2017},
  volume={},
  number={},
  pages={469-470},
  abstract={Drawing annotations with 3D hand gestures in augmented reality is useful for creating visual and spatial references in the real world, especially when these gestures can be issued from a distance. Different techniques exist for highlighting physical objects with hand-drawn annotations from a distance, assuming an approximate 3D scene model (e.g., as provided by the Microsoft HoloLens). However, little is known about user preference and performance of such methods for annotating real-world 3D environments. To explore and evaluate different 3D hand-gesture-based annotation drawing methods, we have developed an annotation drawing application using the HoloLens augmented reality development platform. The application can be used for highlighting objects at a distance and multi-user collaboration by annotating in the real world.},
  keywords={Three-dimensional displays;Augmented reality;Transforms;Solid modeling;Surface treatment;User interfaces;Augmented Reality;Annotations;Spatial Referencing;HoloLens},
  doi={10.1109/VR.2017.7892383},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{9417699,
  author={Singh, Abbey and Kaur, Ramanpreet and Haltner, Peter and Peachey, Matthew and Gonzalez-Franco, Mar and Malloch, Joseph and Reilly, Derek},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Story CreatAR: a Toolkit for Spatially-Adaptive Augmented Reality Storytelling}, 
  year={2021},
  volume={},
  number={},
  pages={713-722},
  abstract={Headworn Augmented Reality (AR) and Virtual Reality (VR) displays are an exciting new medium for locative storytelling. Authors face challenges planning and testing the placement of story elements when the story is experienced in multiple locations or the environment is large or complex. We present Story CreatAR, the first locative AR/VR authoring tool that integrates spatial analysis techniques. Story CreatAR is designed to help authors think about, experiment with, and reflect upon spatial relationships between story elements, and between their story and the environment. We motivate and validate our design through developing different locative AR/VR stories with several authors.},
  keywords={Three-dimensional displays;Design methodology;Games;Tools;User interfaces;Media;Planning;augmented reality;space syntax;storytelling;prox-emics;f-formations;authoring toolkit;head-mounted display: Human-centered computing-Human computer interaction (HCI)-Interactive systems and tools-User interface toolkits;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Interaction design-Interaction design process and methods-User centered design},
  doi={10.1109/VR50410.2021.00098},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3411764.3445298,
author = {Hubenschmid, Sebastian and Zagermann, Johannes and Butscher, Simon and Reiterer, Harald},
title = {STREAM: Exploring the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445298},
doi = {10.1145/3411764.3445298},
abstract = {Recent research in the area of immersive analytics demonstrated the utility of head-mounted augmented reality devices for visual data analysis. However, it can be challenging to use the by default supported mid-air gestures to interact with visualizations in augmented reality (e.g. due to limited precision). Touch-based interaction (e.g. via mobile devices) can compensate for these drawbacks, but is limited to two-dimensional input. In this work we present STREAM: Spatially-aware Tablets combined with Augmented Reality Head-Mounted Displays for the multimodal interaction with 3D visualizations. We developed a novel eyes-free interaction concept for the seamless transition between the tablet and the augmented reality environment. A user study reveals that participants appreciated the novel interaction concept, indicating the potential for spatially-aware tablets in augmented reality. Based on our findings, we provide design insights to foster the application of spatially-aware touch devices in augmented reality and research implications indicating areas that need further investigation.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {469},
numpages = {14},
keywords = {visualizations, multimodal interaction, mobile devices, immersive analytics, augmented reality},
location = {Yokohama, Japan},
series = {CHI '21}
}

@INPROCEEDINGS{8446287,
  author={Yamada, Shohei and Chandrasiri, Naiwala P.},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Hand Gesture Annotation in Remote Collaboration Using Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={727-728},
  abstract={In this research we have devised a system which can tell works to the worker by using the hand gestures of the helper. In this system, by using augmented reality, it is possible to display as if the helper's hand model actually exist in front of the worker. In order to evaluate the usefulness of the proposed system, we conducted comparative experiments on remote work support by instruction annotations using conventional method of drawn lines, and the proposed method of by using hand gesture instructions. As a result, no significant difference was found between two methods in terms of ease of understanding in the instructions. However, regarding working time, the hand gesture instructions were shorter by 20 seconds (shortened by 19%) on average than the other.},
  keywords={Conferences;Virtual reality;Three-dimensional displays;User interfaces;Hand gesture;augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
  doi={10.1109/VR.2018.8446287},
  ISSN={},
  month={March},}

@inproceedings{10.1145/3173574.3173714,
author = {Dillman, Kody R. and Mok, Terrance Tin Hoi and Tang, Anthony and Oehlberg, Lora and Mitchell, Alex},
title = {A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173714},
doi = {10.1145/3173574.3173714},
abstract = {Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {interaction cues, guidance, game design, augmented reality},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3706598.3714289,
author = {Kim, You-Jin and Kumaran, Radha and Luo, Jingjing and Bullock, Tom and Giesbrecht, Barry and H\"{o}llerer, Tobias},
title = {On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714289},
doi = {10.1145/3706598.3714289},
abstract = {Augmented reality is projected to be a primary mode of information consumption on the go, seamlessly integrating virtual content into the physical world. However, the potential perceptual demands of viewing virtual annotations while navigating a physical environment could impact user efficacy and safety, and the implications of these demands are not well understood. Here, we investigate the impact of virtual path guidance and augmentation density (visual clutter) on search performance and memory. Participants walked along a predefined path, searching for physical or virtual items. They experienced two levels of augmentation density, and either walked freely or with enforced speed and path guidance. Augmentation density impacted behavior and reduced awareness of uncommon objects in the environment. Analysis of search task performance and post-experiment item recall revealed differing attention to physical and virtual objects. On the basis of these findings we outline considerations for AR apps designed for use on the go.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1158},
numpages = {16},
keywords = {Mobile Augmented Reality, Extended Reality, Mixed Reality, Perception, Behavior},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3411764.3445651,
author = {B\"{u}schel, Wolfgang and Lehmann, Anke and Dachselt, Raimund},
title = {MIRIA: A Mixed Reality Toolkit for the In-Situ Visualization and Analysis of Spatio-Temporal Interaction Data},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445651},
doi = {10.1145/3411764.3445651},
abstract = {In this paper, we present MIRIA, a Mixed Reality Interaction Analysis toolkit designed to support the in-situ visual analysis of user interaction in mixed reality and multi-display environments. So far, there are few options to effectively explore and analyze interaction patterns in such novel computing systems. With MIRIA, we address this gap by supporting the analysis of user movement, spatial interaction, and event data by multiple, co-located users directly in the original environment. Based on our own experiences and an analysis of the typical data, tasks, and visualizations used in existing approaches, we identify requirements for our system. We report on the design and prototypical implementation of MIRIA, which is informed by these requirements and offers various visualizations such as 3D movement trajectories, position heatmaps, and scatterplots. To demonstrate the value of MIRIA for real-world analysis tasks, we conducted expert feedback sessions using several use cases with authentic study data.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {470},
numpages = {15},
keywords = {visualization, interaction analysis, in-situ visualization, in-situ analysis, immersive analytics, human-computer interaction, augmented reality},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3706598.3713747,
author = {Han, Seung Hyeon and Han, Yeeun and Park, Kyeongho and Lee, Sangjun and Lee, Woohun},
title = {SpatIO: Spatial Physical Computing Toolkit Based on Extended Reality},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713747},
doi = {10.1145/3706598.3713747},
abstract = {Proper placement of sensors and actuators is one of the key factors when designing spatial and proxemic interactions. However, current physical computing tools do not effectively support placing components in three-dimensional space, often forcing designers to build and test prototypes without precise spatial configuration. To address this, we propose the concept of spatial physical computing and present SpatIO, an XR-based physical computing toolkit that supports a continuous end-to-end workflow. SpatIO consists of three interconnected subsystems: SpatIO Environment for composing and testing prototypes with virtual sensors and actuators, SpatIO Module for converting virtually placed components into physical ones, and SpatIO Code for authoring interactions with spatial visualization of data flow. Through a comparative user study with 20 designers, we found that SpatIO significantly altered workflow order, encouraged broader exploration of component placement, enhanced spatial correlation between code and components, and promoted in-situ bodily testing.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {978},
numpages = {22},
keywords = {Spatial Physical Computing, Prototyping, Extended Reality, Digital Fabrication, Visual Computing},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{10494114,
  author={Wen, Jiqing and Gold, Lauren and Ma, Qianyu and LiKamWa, Robert},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented Coach: Volumetric Motion Annotation and Visualization for Immersive Sports Coaching}, 
  year={2024},
  volume={},
  number={},
  pages={137-146},
  abstract={Remote sports coaching connects athletes to interactive training sessions, despite busy schedules and/or lack of access to local trainers. In current formats, athletes record videos of themselves, which they send to coaches for feedback or use in self-coaching. Additionally, some coaches turn to video conferencing platforms such as FaceTime or Zoom for live coaching. A significant challenge with these methods of remote sports coaching is the absence of spatial analysis capabilities, which hinders in-depth assessment of athletic performance.This paper introduces Augmented Coach, an immersive and interactive sports coaching system. Augmented Coach utilizes volumetric data to reconstruct the 3D representations of the athletes. As a result, coaches can not only view the resulting point cloud videos of the athletes performing athletic movements, but also employ the system’s spatial annotation and visualization tools to gain insights into movement patterns and communicate with remote athletes. Unlike existing tools tailored to specific sports, Augmented Coach explores spatial kinesthetic values shared across various sports through a pilot study and designs adaptable features applicable to diverse sports coaching scenarios. To assess the system’s usability, we conducted a user study involving ten users, spanning certified coaches and experienced athletes from various sports, illustrating how they can utilize the system’s features to enhance coaching in their respective disciplines.},
  keywords={Three-dimensional displays;Annotations;Design methodology;Data visualization;Virtual reality;Trajectory;Usability;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Visualization;Visualization systems and tools;Interaction design;Interaction design process and methods;User centered design},
  doi={10.1109/VR58804.2024.00037},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10494086,
  author={Bozgeyikli, Lal “Lila”},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Real-Virtual Objects: Exploring Bidirectional Embodied Tangible Interaction with a Virtual Human in World-Fixed Virtual Reality}, 
  year={2024},
  volume={},
  number={},
  pages={147-156},
  abstract={In everyday interactions with others, we often affect each other’s space through shared physical objects. Despite the commonality of such bidirectional interactions in real life, few have explored this form of interaction in virtual reality. This paper explores bidirectional embodied tangible interaction between a human and a virtual human through shared objects that span the real-virtual boundary in world-fixed virtual reality. The shared objects extend from the real world into the virtual world (and vice versa). We discuss the novel interaction concept and implementation details and present the results of a between-subjects user study with 40 participants where we compared the developed novel real-virtual shared object interaction with a control version where the shared objects were separated as completely physical and virtual. In summary, the results showed that presence and co-presence were increased with the real-virtual object interaction, along with affective attraction to the virtual human and enjoyment of interaction.},
  keywords={Three-dimensional displays;Virtual reality;User interfaces;World-fixed virtual reality;bidirectional interaction;tangible interaction;embodied interaction;real-virtual objects;real and virtual boundary;presence;co-presence;• Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00038},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9089591,
  author={Wallgrün, Jan Oliver and Bagher, Mahda M. and Sajjadi, Pejman and Klippel, Alexander},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Comparison of Visual Attention Guiding Approaches for 360° Image-Based VR Tours}, 
  year={2020},
  volume={},
  number={},
  pages={83-91},
  abstract={Mechanisms for guiding a user’s visual attention to a particular point of interest play a crucial role in areas such as collaborative VR and AR, cinematic VR, and automated or live guided tour experiences in xR-based education. The attention guiding mechanism serves as a communication tool that helps users find entities currently not visible in their view, referenced for instance by another user or in some accompanying audio commentary. We report on a user study in which we compared three different visual guiding mechanisms (arrow, butterfly guide, and radar) in the context of 360° image-based educational VR tour applications of real-world sites. A fourth condition with no guidance tool available was added as a baseline. We investigate the question: How do the different approaches compare in terms of target finding performance and participants’ assessments of the experiences. While all three mechanisms were perceived as improvements over the no-guidance condition and resulted in significantly improved target finding times, the arrow mechanism stands out as the most generally accepted and favored approach, whereas the other two (butterfly guide and radar) received a more polarized assessment due to their specific strengths and drawbacks.},
  keywords={Visualization;Three-dimensional displays;Virtual reality;Radar;Human computer interaction;Two dimensional displays;Head;Human-centered computing—User studies;Human-centered computing—Virtual reality;Human-centered computing— Empirical studies in HCI;Human-centered computing—Empirical studies in visualization},
  doi={10.1109/VR46266.2020.00026},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9417649,
  author={Lu, Feiyu and Bowman, Doug A.},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Potential of Glanceable AR Interfaces for Authentic Everyday Uses}, 
  year={2021},
  volume={},
  number={},
  pages={768-777},
  abstract={In the near future, augmented reality (AR) glasses are envisioned to become the next-generation personal computing platform. They could be always on and worn all day, delivering continuous and pervasive AR experiences for general-purpose everyday use cases. However, it remains unclear how we could enable unobtrusive and easy information access without distracting users, while being acceptable to use at the same time. To address this question, we implemented two prototypes based on the Glanceable AR paradigm, a promising way of managing and acquiring information through glancing at the periphery of AR head-worn displays (HWDs). We conducted two separate studies to evaluate our designs. In the first study, we obtained feedback from a large sample of participants of varied age and background about a video prototype that showcased some envisioned scenarios of using Glanceable AR for everyday tasks. In the second study, we asked participants to use a working prototype during authentic real-world activities for three days. We found that users appreciated the Glanceable AR approach. They found it less distracting or intrusive than existing devices in authentic everyday use cases, and would like to use the interface on a daily basis if the form factor of the AR headset was more like eyeglasses.},
  keywords={Headphones;Three-dimensional displays;Head-mounted displays;Prototypes;Glass;User interfaces;Task analysis;Human-centered computing-Mixed / augmented reality;Human-centered computing-User interface design},
  doi={10.1109/VR50410.2021.00104},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8446381,
  author={Whitlock, Matt and Harnner, Ethan and Brubaker, Jed R. and Kane, Shaun and Szafir, Danielle Albers},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interacting with Distant Objects in Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={41-48},
  abstract={Augmented reality (AR) applications can leverage the full space of an environment to create immersive experiences. However, most empirical studies of interaction in AR focus on interactions with objects close to the user, generally within arms reach. As objects move farther away, the efficacy and usability of different interaction modalities may change. This work explores AR interactions at a distance, measuring how applications may support fluid, efficient, and intuitive interactive experiences in room-scale augmented reality. We conducted an empirical study (N = 20) to measure trade-offs between three interaction modalities-multimodal voice, embodied freehand gesture, and handhelds devices-for selecting, rotating, and translating objects at distances ranging from 8 to 16 feet (2.4m-4.9m). Though participants performed comparably with embodied freehand gestures and handheld remotes, they perceived embodied gestures as significantly more efficient and usable than device-mediated interactions. Our findings offer considerations for designing efficient and intuitive interactions in room-scale AR applications.},
  keywords={Task analysis;Usability;Visualization;Pervasive computing;Augmented reality;Electronic mail;Human-centered computing-Interaction design-Interaction design process and methods-User interface design},
  doi={10.1109/VR.2018.8446381},
  ISSN={},
  month={March},}

@inproceedings{10.1145/3313831.3376637,
author = {Nebeling, Michael and Lewis, Katy and Chang, Yu-Cheng and Zhu, Lihan and Chung, Michelle and Wang, Piaoyang and Nebeling, Janet},
title = {XRDirector: A Role-Based Collaborative Immersive Authoring System},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376637},
doi = {10.1145/3313831.3376637},
abstract = {Immersive authoring is an increasingly popular technique to design AR/VR scenes because design and testing can be done concurrently. Most existing systems, however, are single-user and limited to either AR or VR, thus constrained in the interaction techniques. We present XRDirector, a role-based collaborative immersive authoring system that enables designers to freely express interactions using AR and VR devices as puppets to manipulate virtual objects in 3D physical space. In XRDirector, we adapt roles known from filmmaking to structure the authoring process and help coordinate multiple designers in immersive authoring tasks. We study how novice AR/VR creators can take advantage of the roles and modes in XRDirector to prototype complex scenes with animated 3D characters, light effects, and camera movements, and also simulate interactive system behavior in a Wizard of Oz style. XRDirector's design was informed by case studies around complex 3D movie scenes and AR/VR games, as well as workshops with novice AR/VR creators. We show that XRDirector makes it easier and faster to create AR/VR scenes without the need for coding, characterize the issues in coordinating designers between AR and VR, and identify the strengths and weaknesses of each role and mode to mitigate the issues.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {ar/vr, immersive authoring, mixed-reality collaboration},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@INPROCEEDINGS{8446295,
  author={Grandi, Jerônimo G and Debarba, Henrique G and Bemdt, Iago and Nedel, Luciana and Maciel, Anderson},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Design and Assessment of a Collaborative 3D Interaction Technique for Handheld Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={49-56},
  abstract={We present the design of a handheld-based interface for collaborative manipulations of 3D objects in mobile augmented reality. Our approach combines touch gestures and device movements for fast and precise control of 7-DOF transformations. Moreover, the interface creates a shared medium where several users can interact through their point-of-view and simultaneously manipulate 3D virtual augmentations. We evaluated our collaborative solution in two parts. First, we assessed our interface in single user mode, comparing the user task performance in three conditions: touch gestures, device movements and hybrid. Then, we conducted a study with 30 participants to understand and classify the strategies that arise while working in pairs, when partners are free to make their task organization. Furthermore, we investigated the effectiveness of simultaneous manipulations compared with the individual approach.},
  keywords={Three-dimensional displays;Collaboration;Task analysis;Augmented reality;Performance evaluation;Cameras;Handheld computers;Human-centered computing-Human computer interaction (HCI)-Interaction techniques;Human-centered computing-Human computer interaction CHCI)-Interaction paradigms-Mixed/augmented reality Human-centered computing-Collaborative and social computing},
  doi={10.1109/VR.2018.8446295},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10494071,
  author={Wang, Xuanyu and Zhang, Weizhan and Fu, Hongbo},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A3RT: Attention-Aware AR Teleconferencing with Life-Size 2.5D Video Avatars}, 
  year={2024},
  volume={},
  number={},
  pages={211-221},
  abstract={Augmented Reality (AR) teleconferencing aims to enable remotely separated users to meet with each other in their own physical spaces as if they are face-to-face. Among all solutions, the video-avatar-based approach has the advantage of balancing fidelity and the sense of co-presence using easy-to-setup devices, including only a camera and an AR Head-Mounted Display (HMD). However, non-verbal cues indicating “who is looking at whom” are always lost or misdelivered in multiparty teleconferencing experiences. To make users aware of such non-verbal cues, existing solutions explore screen-based visualizations, incorporate additional hardware, or alter to use a virtual avatar representation. However, they lack immersion, are less feasible for everyday usage due to complex installations, or lose the fidelity of remote users’ authentic appearances. In this paper, we decompose such attention awareness into the awareness of being looked at and the awareness of attention between other users and address them in a decoupled process. Specifically, through a user study, we first find an unobtrusive and reasonable layout “Attention Circle” to retarget a looker’s head gaze to the one being looked at. We then conduct the second user study to find an effective and intuitive “rotatable 2.5D video avatar with attention thumbnail” visualization to aid users in being aware of other users’ attention. With the design choice distilled from the studies, we implement A3RT, a proof-of-concept prototype system that empowers attention-aware 2.5D-video-avatar-based multiparty AR teleconferencing in an easy, everyday setup. Ablation and usability studies on the prototype verify the effectiveness of our proposed components and the full system.},
  keywords={Teleconferencing;Visualization;Three-dimensional displays;Head-mounted displays;Avatars;Layout;Prototypes;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI)},
  doi={10.1109/VR58804.2024.00044},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3613904.3642925,
author = {Li, Jingyi and Kristensson, Per Ola},
title = {On the Benefits of Image-Schematic Metaphors when Designing Mixed Reality Systems},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642925},
doi = {10.1145/3613904.3642925},
abstract = {A Mixed Reality (MR) system encompasses various aspects, such as visualization and spatial registration of user interface elements, user interactions and interaction feedback. Image-schematic metaphors (ISMs) are universal knowledge structures shared by a wide range of users. They hold a theoretical promise of facilitating greater ease of learning and use for interactive systems without costly adaptations. This paper investigates whether image-schematic metaphors (ISMs) can improve user learning, by comparing an existing MR instruction authoring system with or without ISM enhancements. In a user study with 32 participants, we found that the ISM-enhanced system significantly improved task performance, learnability and mental efficiency compared to the baseline. Participants also rated the ISM-enhanced system significantly higher in terms of perspicuity, efficiency, and novelty. These results empirically demonstrate multiple benefits of ISMs when integrated into the design of this MR system and encourage further studies to explore the wider applicability of ISMs in user interface design.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {953},
numpages = {20},
keywords = {Image Schema, Instruction Authoring, Mixed Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642360,
author = {He, Ziyao and Li, Shiyuan and Song, Yunpeng and Cai, Zhongmin},
title = {Towards Building Condition-Based Cross-Modality Intention-Aware Human-AI Cooperation under VR Environment},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642360},
doi = {10.1145/3613904.3642360},
abstract = {To address critical challenges in effectively identifying user intent and forming relevant information presentations and recommendations in VR environments, we propose an innovative condition-based multi-modal human-AI cooperation framework. It highlights the intent tuples (intent, condition, intent prompt, action prompt) and 2-Large-Language-Models (2-LLMs) architecture. This design, utilizes “condition” as the core to describe tasks, dynamically match user interactions with intentions, and empower generations of various tailored multi-modal AI responses. The architecture of 2-LLMs separates the roles of intent detection and action generation, decreasing the prompt length and helping with generating appropriate responses. We implemented a VR-based intelligent furniture purchasing system based on the proposed framework and conducted a three-phase comparative user study. The results conclusively demonstrate the system’s superiority in time efficiency and accuracy, intention conveyance improvements, effective product acquisitions, and user satisfaction and cooperation preference. Our framework provides a promising approach towards personalized and efficient user experiences in VR.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {955},
numpages = {13},
keywords = {Action Generation, Human-AI Cooperation, Intention Detection, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@INPROCEEDINGS{7504787,
  author={Cabral, Marcio and Roque, Gabriel and Nagamura, Mario and Montes, Andre and Borba, Eduardo Zilles and Kurashima, Celso and Zuffo, Marcelo Knorich},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Batmen — Hybrid collaborative object manipulation using mobile devices}, 
  year={2016},
  volume={},
  number={},
  pages={328-328},
  abstract={In this work we present an interactive and collaborative 3D object manipulation system using the shelf mobile devices coupled with Augmented Reality (AR) technology that allows multiple users to collaborate concurrently on a scene. Each user interested in participating in this collaboration uses both a mobile device running android and a desktop (or laptop) working in tandem. The 3D scene was visualized by the user in the desktop system. The changes in the scene viewpoint changes and the object manipulations were performed using a mobile device through the AR. The system leverages user's knowledge of common tasks performed on current mobile devices such as pinching for zooming in and out; swiping with one or two fingers for object rotation and press-and-hold for 2 seconds for object translation. As you will see in this video, we built a prototype system (in a maze style) and applied an informal user study with three experienced VR researchers. Users had to carry a 3D cube through three square rings along the maze. In resume, we diagnosed that working in a collaborative way (users A and B) was better and easier than individual one (user C). We registered more than 2 minutes late for the individual experience comparing to the teamwork. It may happen because the two player team shared information, functions and had a multi-perspective view during the task.},
  keywords={Mobile handsets;Three-dimensional displays;Task analysis;Performance evaluation;Visualization;Urban areas;Teamwork;Interaction techniques;collaborative environment;hybrid reality;object manipulation;computer graphics;user experience},
  doi={10.1109/VR.2016.7504787},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{10494192,
  author={Quere, Clément and Menin, Aline and Julien, Raphaël and Wu, Hui-Yin and Winckler, Marco},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={HandyNotes: using the hands to create semantic representations of contextually aware real-world objects}, 
  year={2024},
  volume={},
  number={},
  pages={265-275},
  abstract={This paper uses Mixed Reality (MR) technologies to provide a seamless integration of digital information in physical environments through human-made annotations. Creating digital annotations of physical objects evokes many challenges for performing (simple) tasks such as adding digital notes and connecting them to real-world objects. For that, we have developed an MR system using the Microsoft HoloLens2 to create semantic representations of contextually-aware real-world objects while interacting with holographic virtual objects. User interaction is enhanced with use of fingers as placeholders for menu items. We demonstrate our approach through two real-world scenarios. We also discuss the challenges for using MR technologies.},
  keywords={Three-dimensional displays;Annotations;Semantics;Mixed reality;Virtual reality;User interfaces;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interaction techniques;Gestural input;Graphical user interfaces},
  doi={10.1109/VR58804.2024.00049},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3544548.3580715,
author = {Luo, Weizhou and Yu, Zhongyuan and Rzayev, Rufat and Satkowski, Marc and Gumhold, Stefan and McGinity, Matthew and Dachselt, Raimund},
title = {Pearl: Physical Environment based Augmented Reality Lenses for In-Situ Human Movement Analysis},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580715},
doi = {10.1145/3544548.3580715},
abstract = {This paper presents Pearl, a mixed-reality approach for the analysis of human movement data in situ. As the physical environment shapes human motion and behavior, the analysis of such motion can benefit from the direct inclusion of the environment in the analytical process. We present methods for exploring movement data in relation to surrounding regions of interest, such as objects, furniture, and architectural elements. We introduce concepts for selecting and filtering data through direct interaction with the environment, and a suite of visualizations for revealing aggregated and emergent spatial and temporal relations. More sophisticated analysis is supported through complex queries comprising multiple regions of interest. To illustrate the potential of Pearl, we developed an Augmented Reality-based prototype and conducted expert review sessions and scenario walkthroughs in a simulated exhibition. Our contribution lays the foundation for leveraging the physical environment in the in-situ analysis of movement data.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {381},
numpages = {15},
keywords = {Immersive Analytics, In-situ visualization, affordance, augmented/mixed reality, movement data analysis, physical referents},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3706598.3713954,
author = {Tsimbalistaia, Uliana and Berger, Caroline and Gellersen, Hans and Manakhov, Pavel},
title = {On-body Icons: Designing a 3D Interface for Launching Apps in Augmented Reality},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713954},
doi = {10.1145/3706598.3713954},
abstract = {On-body tapping provides a quick way to launch augmented reality (AR) apps using virtual shortcuts placed on the user’s skin, clothes, and jewelry. While prior work has focused on tapping performance, social acceptance, and sensing techniques, users’ behaviour in placing shortcuts on their body has been underexplored. In this work, we propose On-body Icons — a novel interface for launching apps via touching virtual icons placed across the user’s entire body, and use it to investigate locations, reasons for chosen icon placement, and users’ attitudes towards the feature. Results of the qualitative study conducted with 24 participants demonstrated that people employ a wide variety of placement strategies that balance memorability of the locations with accuracy and comfort of reaching the icons. We discuss these findings in regard to current understanding of memorability of icon placement, placement appropriateness, and privacy, and offer design implications for similar features in spatial applications.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {629},
numpages = {15},
keywords = {embodied interaction, app launcher, spatial UIs, body-based UIs, extended reality, augmented reality},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{9089476,
  author={Jakl, Andreas and Lienhart, Anna-Maria and Baumann, Clemens and Jalaeefar, Arian and Schlager, Alexander and Schöffer, Lucas and Bruckner, Franziska},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Enlightening Patients with Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={195-203},
  abstract={Enlightening Patients with Augmented Reality (EPAR) enhances patient education with new possibilities offered by Augmented Reality. Medical procedures are becoming increasingly complex and printed information sheets are often hard to understand for patients. EPAR developed an augmented reality prototype that helps patients with strabismus to better understand the processes of examinations and eye surgeries. By means of interactive storytelling, three identified target groups based on user personas were able to adjust the level of information transfer based on their interests. We performed a 2-phase evaluation with a total of 24 test subjects, resulting in a final system usability score of 80.0. For interaction prompts concerning virtual 3D content, visual highlights were considered to be sufficient. Overall, participants thought that an AR system as a complementary tool could lead to a better understanding of medical procedures.},
  keywords={Education;Three-dimensional displays;Augmented reality;Surgery;Human computer interaction;Usability;Prototypes;Human-centered computing;Mixed / augmented reality Human-centered computing;Interface design prototyping Human-centered computing;Interaction design theory;concepts and paradigms Human-centered computing;Usability testing},
  doi={10.1109/VR46266.2020.00038},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10494151,
  author={Lee, Geonsun and Healey, Jennifer and Manocha, Dinesh},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={DocuBits: VR Document Decomposition for Procedural Task Completion}, 
  year={2024},
  volume={},
  number={},
  pages={309-319},
  abstract={Reading monolithic instructional documents in VR is often challenging, especially when tasks are collaborative. Here we present DocuBits, a novel method for transforming monolithic documents into small, interactive instructional elements. Our approach allows users to:(i) create instructional elements (ii) position them within VR and (iii) use them to monitor and share progress in a multi-user VR learning environment. We describe our design methodology as well as two user studies evaluating how both individual users and pairs of users interact with DocuBits compared to monolithic documents while performing a chemistry lab task. Our analysis shows that, for both studies, DocuBits had substantially higher usability, while decreasing perceived workload $(p \lt 0.001)$. Our collaborative study showed that participants perceived higher social presence, collaborator awareness as well as immersion and presence $(p \lt 0.001)$. We discuss our insights for using text-based instructions to support enhanced collaboration in VR environments.},
  keywords={Training;Performance evaluation;Three-dimensional displays;Design methodology;Collaboration;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques;Collaborative interaction},
  doi={10.1109/VR58804.2024.00053},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8446396,
  author={Renner, Patrick and Pfeiffer, Thies},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Attention Guiding Using Augmented Reality in Complex Environments}, 
  year={2018},
  volume={},
  number={},
  pages={771-772},
  abstract={The localization of objects or locations in an environment is an essential task relevant for many work processes. An augmented reality (AR)-based assistance system may support users by guiding their attention towards the relevant targets. This will reduce the time needed for visual search and reduce errors, such as wrongly picked items or false placements. The design of proper attention guiding techniques is thus one area of research in augmented assistance. We developed a number of new attention guiding techniques and evaluated them in several experiments together with recent or classic existing techniques in varying picking scenarios. In our current work, we adapted a standardized assembly scenario to a more complex environment including occlusions to provide a scenario supporting reproducibility of our results. In the research demo, a number of visual and acoustic guiding techniques can be tested and combined. The demonstration is implemented on the Microsoft HoloLens.},
  keywords={Three-dimensional displays;Augmented reality;Task analysis;Visualization;Glass;Maintenance engineering;User interfaces;Attention guiding;augmented reality assistance;evaluation: H.5.2 [Information Interfaces and Presentation (e.g. HCI)]: User Interfaces-Miscellaneous},
  doi={10.1109/VR.2018.8446396},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9089453,
  author={Englmeier, David and Dörner, Julia and Butz, Andreas and Höllerer, Tobias},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Tangible Spherical Proxy for Object Manipulation in Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={221-229},
  abstract={In this paper, we explore how a familiarly shaped object can serve as a physical proxy to manipulate virtual objects in Augmented Reality (AR) environments. Using the example of a tangible, handheld sphere, we demonstrate how irregularly shaped virtual objects can be selected, transformed, and released. After a brief description of the implementation of the tangible proxy, we present a buttonless interaction technique suited to the characteristics of the sphere. In a user study (N = 30), we compare our approach with three different controller-based methods that increasingly rely on physical buttons. As a use case, we focused on an alignment task that had to be completed in mid-air as well as on a flat surface. Results show that our concept has advantages over two of the controller-based methods regarding task completion time and user ratings. Our findings inform research on integrating tangible interaction into AR experiences.},
  keywords={Three-dimensional displays;Human computer interaction;Shape;Manipulators;Task analysis;Augmented reality;Haptic interfaces;Human-centered computing—Human computer interaction (HCI)—Interaction devices—Haptic devices;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms— Mixed / augmented reality},
  doi={10.1109/VR46266.2020.00041},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3313831.3376652,
author = {Jetter, Hans-Christian and R\"{a}dle, Roman and Feuchtner, Tiare and Anthes, Christoph and Friedl, Judith and Klokmose, Clemens Nylandsted},
title = {"In VR, everything is possible!": Sketching and Simulating Spatially-Aware Interactive Spaces in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376652},
doi = {10.1145/3313831.3376652},
abstract = {We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with finger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identified the designers' experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based "World Editor".},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {design tools, interaction design, interactive spaces, prototyping, simulation, sketching, spatial awareness, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3491102.3517682,
author = {Stemasov, Evgeny and Wagner, Tobias and Gugenheimer, Jan and Rukzio, Enrico},
title = {ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact Retrieval using Mixed Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517682},
doi = {10.1145/3491102.3517682},
abstract = {Personal fabrication is made more accessible through repositories like Thingiverse, as they replace modeling with retrieval. However, they require users to translate spatial requirements to keywords, which paints an incomplete picture of physical artifacts: proportions or morphology are non-trivially encoded through text only. We explore a vision of in-situ spatial search for (future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to search for 3D models using in-situ sketches blended with textual queries. With ShapeFindAR, users search for geometry, and not necessarily precise labels, while coupling the search process to the physical environment (e.g., by sketching in-situ, extracting search terms from objects present, or tracing them). We developed ShapeFindAR for HoloLens 2, connected to a database of 3D-printable artifacts. We specify in-situ spatial search, describe its advantages, and present walkthroughs using ShapeFindAR, which highlight novel ways for users to articulate their wishes, without requiring complex modeling tools or profound domain knowledge.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {292},
numpages = {12},
keywords = {3D-Printing, In-Situ Search, Mixed Reality, Model Repositories, Personal Fabrication, Physical Artifact Retrieval, Spatial Search},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3313831.3376657,
author = {Wang, Xiyao and Besan\c{c}on, Lonni and Rousseau, David and Sereno, Mickael and Ammi, Mehdi and Isenberg, Tobias},
title = {Towards an Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376657},
doi = {10.1145/3313831.3376657},
abstract = {We present an observational study with domain experts to understand how augmented reality (AR) extensions to traditional PC-based data analysis tools can help particle physicists to explore and understand 3D data. Our goal is to allow researchers to integrate stereoscopic AR-based visual representations and interaction techniques into their tools, and thus ultimately to increase the adoption of modern immersive analytics techniques in existing data analysis workflows. We use Microsoft's HoloLens as a lightweight and easily maintainable AR headset and replicate existing visualization and interaction capabilities on both the PC and the AR view. We treat the AR headset as a second yet stereoscopic screen, allowing researchers to study their data in a connected multi-view manner. Our results indicate that our collaborating physicists appreciate a hybrid data exploration setup with an interactive AR extension to improve their understanding of particle collision events.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {3D visualization, hybrid visualization system, immersive analytics, user interface},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3290605.3300915,
author = {Du, Ruofei and Li, David and Varshney, Amitabh},
title = {Geollery: A Mixed Reality Social Media Platform},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300915},
doi = {10.1145/3290605.3300915},
abstract = {We present Geollery, an interactive mixed reality social media platform for creating, sharing, and exploring geotagged information. Geollery introduces a real-time pipeline to progressively render an interactive mirrored world with three-dimensional (3D) buildings, internal user-generated content, and external geotagged social media. This mirrored world allows users to see, chat, and collaborate with remote participants with the same spatial context in an immersive virtual environment. We describe the system architecture of Geollery, its key interactive capabilities, and our design decisions. Finally, we conduct a user study with 20 participants to qualitatively compare Geollery with another social media system, Social Street View. Based on the participants' responses, we discuss the benefits and drawbacks of each system and derive key insights for designing an interactive mirrored world with geotagged social media. User feedback from our study reveals several use cases for Geollery including travel planning, virtual meetings, and family gathering.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {3d reconstruction, 3d user interface, augmented reality, geographic information system, gis, mixed reality, social media, street view, virtual reality, visualization},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3544548.3581442,
author = {Liu, Ziyi and Zhu, Zhengzhe and Jiang, Enze and Huang, Feichi and Villanueva, Ana M and Qian, Xun and Wang, Tianyi and Ramani, Karthik},
title = {InstruMentAR: Auto-Generation of Augmented Reality Tutorials for Operating Digital Instruments Through Recording Embodied Demonstration},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581442},
doi = {10.1145/3544548.3581442},
abstract = {Augmented Reality tutorials, which provide necessary context by directly superimposing visual guidance on the physical referent, represent an effective way of scaffolding complex instrument operations. However, current AR tutorial authoring processes are not seamless as they require users to continuously alternate between operating instruments and interacting with virtual elements. We present InstruMentAR, a system that automatically generates AR tutorials through recording user demonstrations. We design a multimodal approach that fuses gestural information and hand-worn pressure sensor data to detect and register the user’s step-by-step manipulations on the control panel. With this information, the system autonomously generates virtual cues with designated scales to respective locations for each step. Voice recognition and background capture are employed to automate the creation of text and images as AR content. For novice users receiving the authored AR tutorials, we facilitate immediate feedback through haptic modules. We compared InstruMentAR with traditional systems in the user study.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {32},
numpages = {17},
keywords = {Augmented Reality, Embodied Demonstration, Haptic Feedback, Immersive Authoring, Tangible Interaction, wearable Devices},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{8798262,
  author={Gattullo, Michele and Dalena, Vito and Evangelista, Alessandro and Uva, Antonio E. and Fiorentino, Michele and Boccaccio, Antonio and Ruta, Michele and Gabbard, Joseph L.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Context-Aware Technical Information Manager for Presentation in Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={939-940},
  abstract={Technical information presentation is evolving from static contents presented on paper or via digital publishing to real-time context-aware contents displayed via virtual and augmented reality devices. We present a Context-Aware Technical Information Management system (CATIM), that dynamically manages (1) what information as well as (2) how information is presented in an augmented reality interface. CATIM acquires context data about activity, operator, and environment, and then based on these data, proposes a dynamic augmented reality output tailored to the current context. The system was successfully implemented and preliminarily evaluated in a case study regarding the maintenance of a hydraulic valve.},
  keywords={Augmented reality;User interfaces;Task analysis;Mathematics;Maintenance engineering;Ontologies;Layout;Industrial Augmented Reality;Technical Information Manager;Context-aware information;Augmented Reality;Context Aware;Visualization},
  doi={10.1109/VR.2019.8798262},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9089660,
  author={Buck, Lauren E. and Park, Sohee and Bodenheimer, Bobby},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Determining Peripersonal Space Boundaries and Their Plasticity in Relation to Object and Agent Characteristics in an Immersive Virtual Environment}, 
  year={2020},
  volume={},
  number={},
  pages={332-342},
  abstract={In this paper we examine the extent of functional reaching space, or peripersonal space, in immersive three-dimensional virtual reality. In the real world a person’s peripersonal space boundaries can be altered by factors in the environment and by social context. We completed two studies with visual and tactile stimuli to determine peripersonal space boundaries. These studies investigated whether peripersonal space boundaries in an immersive virtual environment are consistent with those in the real world, and could be altered by object and virtual agent interactions. We found that while peripersonal space boundaries were consistent with those in the real world, they were responsive to object and agent interactions. Moreover, while people’s reactions to the objects and agents varied, the peripersonal space boundaries remained consistent. These findings have potential implications for the design of virtual environments.},
  keywords={Virtual environments;Visualization;Task analysis;Tools;Three-dimensional displays;Avatars;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction design;Empirical studies in interaction design},
  doi={10.1109/VR46266.2020.00053},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8446138,
  author={Gerstweiler, Georg},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Guiding People in Complex Indoor Environments Using Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={801-802},
  abstract={Complex public buildings like airports use various systems to guide people to a certain destination. Such approaches are usually implemented by showing a floor plan, having guiding signs or color coded lines on the floor. With a technology that supports 6DOF tracking in indoor environments it is possible to guide people individually by using augmented reality guiding visualizations. The proposed research concentrates on three topics which are the main reason, why such a guiding system is still not available in real world situations. At first a tracking solution HyMoTrack is presented, based on a visual hybrid tracking approach for smart phones and tested in a real world airport scenario. The tracking and the guiding part of a reliable indoor navigation requests a 3D model of the environment. For that reason a 3D model generation algorithm was implemented, which automatically creates a 3D mesh out of a vectorized 2D floor plan. Finally the human aspect of an AR guiding system is researched and a novel AR path concept is presented for guiding people with AR devices. This FOVPath is designed to react not only to the position of the user and the target, but is also dependent on the view direction and the field of view (FOV) capabilities of the used device. This ensures that the user always gets reasonable information within the current FOV. To evaluate the concept technical evaluations and user studies were and will be performed.},
  keywords={Solid modeling;Three-dimensional displays;Floors;Augmented reality;Indoor environments;Airports;indoor tracking;navigation;augmented reality;Path Planning;Path Visualization;3D Model Generation;CAD;Human-centered computing-Mixed / augmented reality;Computing methodologies~ Tracking},
  doi={10.1109/VR.2018.8446138},
  ISSN={},
  month={March},}

@inproceedings{10.1145/3544548.3580978,
author = {Li, Wanwan and Li, Changyang and Kim, Minyoung and Huang, Haikun and Yu, Lap-Fai},
title = {Location-Aware Adaptation of Augmented Reality Narratives},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580978},
doi = {10.1145/3544548.3580978},
abstract = {The recent popularity of augmented reality (AR) devices has enabled players to participate in interactive narratives through virtual events and characters populated in a real-world environment, where different actions may lead to different story branches. In this paper, we propose a novel approach to adapt narratives to real spaces for AR experiences. Our optimization-based approach automatically assigns contextually compatible locations to story events, synthesizing a navigation graph to guide players through different story branches while considering their walking experiences. We validated the effectiveness of our approach for adapting AR narratives to different scenes through experiments and user studies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {33},
numpages = {15},
keywords = {augmented reality, interactive narratives, path generation, storytelling},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3290605.3300921,
author = {Poretski, Lev and Arazy, Ofer and Lanir, Joel and Shahar, Shalev and Nov, Oded},
title = {Virtual Objects in the Physical World: Relatedness and Psychological Ownership in Augmented Reality},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300921},
doi = {10.1145/3290605.3300921},
abstract = {As technology advances, people increasingly interact with virtual objects in settings such as augmented reality (AR) where the virtual layer is superimposed on top of the physical world. Similarly to interactions with physical objects, users may assign virtual objects with value, experience a sense of relatedness, and develop psychological ownership over these objects. The objective of this study is to understand how AR's unique characteristics influences the emergence of meaning and ownership perceptions amongst users. We conducted a study of users' interactions with a virtual dog over a three-week period, comparing AR and fully virtual settings. Our findings show that engagement with the application is a key determinant of the relation users develop with virtual objects. However, the effect of the background layer-whether physical or virtual-dominates the development of relatedness and ownership feelings, highlighting the importance of the "real" physical layer in shaping users' perceptions.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {augmented reality, material culture, ownership, qualitative analysis, relatedness, virtual possessions},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@INPROCEEDINGS{9089500,
  author={Lee, Geonsun and Kang, HyeongYeop and Lee, JongMin and Han, JungHyun},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A User Study on View-sharing Techniques for One-to-Many Mixed Reality Collaborations}, 
  year={2020},
  volume={},
  number={},
  pages={343-352},
  abstract={In a one-to-many mixed reality collaboration environment, where multiple local users wearing AR headsets are supervised by a remote expert wearing a VR HMD, we evaluated three view-sharing techniques: 2D video, 360 video, and 3D model augmented with 2D video. Through a pilot test, the weaknesses of the techniques were identified, and additional features were integrated into them. Then, their performances were compared in two different collaboration scenarios based on search and assembling. In the first scenario, a local user performed both search and assembling. In the second scenario, two local users had dedicated roles, one for search and the other for assembling. The experiment results showed that the 3D model augmented with 2D video was time-efficient, usable, less demanding and most preferred in one-to-many mixed reality collaborations.},
  keywords={Collaboration;Three-dimensional displays;Cameras;Virtual reality;Two dimensional displays;Resists;Solid modeling;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Collaborative and social computing;Collaborative and social computing theory;concepts and paradigms;Computer supported cooperative work},
  doi={10.1109/VR46266.2020.00054},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3544548.3580873,
author = {Niyazov, Aziz and Ens, Barrett and Satriadi, Kadek Ananta and Mellado, Nicolas and Barthe, Loic and Dwyer, Tim and Serrano, Marcos},
title = {User-Driven Constraints for Layout Optimisation in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580873},
doi = {10.1145/3544548.3580873},
abstract = {Automatic layout optimisation allows users to arrange augmented reality content in the real-world environment without the need for tedious manual interactions. This optimisation is often based on modelling the intended content placement as constraints, defined as cost functions. Then, applying a cost minimization algorithm leads to a desirable placement. However, such an approach is limited by the lack of user control over the optimisation results. In this paper we explore the concept of user-driven constraints for augmented reality layout optimisation. With our approach users can define and set up their own constraints directly within the real-world environment. We first present a design space composed of three dimensions: the constraints, the regions of interest and the constraint parameters. Then we explore which input gestures can be employed to define the user-driven constraints of our design space through a user elicitation study. Using the results of the study, we propose a holistic system design and implementation demonstrating our user-driven constraints, which we evaluate in a final user study where participants had to create several constraints at the same time to arrange a set of virtual contents.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {35},
numpages = {16},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3613904.3642527,
author = {Ye, Hui and Leng, Jiaye and Xu, Pengfei and Singh, Karan and Fu, Hongbo},
title = {ProInterAR: A Visual Programming Platform for Creating Immersive AR Interactions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642527},
doi = {10.1145/3613904.3642527},
abstract = {AR applications commonly contain diverse interactions among different AR contents. Creating such applications requires creators to have advanced programming skills for scripting interactive behaviors of AR contents, repeated transferring and adjustment of virtual contents from virtual to physical scenes, testing by traversing between desktop interfaces and target AR scenes, and digitalizing AR contents. Existing immersive tools for prototyping/authoring such interactions are tailored for domain-specific applications. To support programming general interactive behaviors of real object(s)/environment(s) and virtual object(s)/environment(s) for novice AR creators, we propose ProInterAR, an integrated visual programming platform to create immersive AR applications with a tablet and an AR-HMD. Users can construct interaction scenes by creating virtual contents and augmenting real contents from the view of an AR-HMD, script interactive behaviors by stacking blocks from a tablet UI, and then execute and control the interactions in the AR scene. We showcase a wide range of AR application scenarios enabled by ProInterAR, including AR game, AR teaching, sequential animation, AR information visualization, etc. Two usability studies validate that novice AR creators can easily program various desired AR applications using ProInterAR.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {610},
numpages = {15},
keywords = {AR contents, AR interactions, Visual programming},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642220,
author = {Pei, Siyou and Kim, David and Olwal, Alex and Zhang, Yang and Du, Ruofei},
title = {UI Mobility Control in XR: Switching UI Positionings between Static, Dynamic, and Self Entities},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642220},
doi = {10.1145/3613904.3642220},
abstract = {Extended reality (XR) has the potential for seamless user interface (UI) transitions across people, objects, and environments. However, the design space, applications, and common practices of 3D UI transitions remain underexplored. To address this gap, we conducted a need-finding study with 11 participants, identifying and distilling a taxonomy based on three types of UI placements — affixed to static, dynamic, or self entities. We further surveyed 113 commercial applications to understand the common practices of 3D UI mobility control, where only 6.2\% of these applications allowed users to transition UI between entities. In response, we built interaction prototypes to facilitate UI transitions between entities. We report on results from a qualitative user study (N=14) on 3D UI mobility control using our FingerSwitches technique, which suggests that perceived usefulness is affected by types of entities and environments. We aspire to tackle a vital need in UI mobility within XR.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {611},
numpages = {12},
keywords = {Embodied Interactions, Extended Reality, Hand Gestures, Mode Switching, UI Mobility, User Interface Behaviors, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@INPROCEEDINGS{9756788,
  author={Barbotin, Nicolas and Baumeister, James and Cunningham, Andrew and Duval, Thierry and Grisvard, Olivier and Thomas, Bruce H.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating Visual Cues for Future Airborne Surveillance Using Simulated Augmented Reality Displays}, 
  year={2022},
  volume={},
  number={},
  pages={213-221},
  abstract={This work explores the interaction between Augmented Reality (AR) and eye accommodation for airborne surveillance by simulating AR environments in Virtual Reality (VR). We simulate the AR display as displays with the capabilities needed for airborne surveillance are limited and because it would be hazardous to experiment directly on surveillance aircraft. While there is precedent for simulating AR in a VR environment, our study account for two of the physical and physiological aspects of AR: we factor in the focal plane of the AR technology and simulate the eye accommodation reflex of the user to provide focus. We ran a study with 24 participants examining AR cues to support visual search. We also compare the effects of having secondary tasks (that surveillance operators are normally responsible for) directly on the observation window using AR. Our results show that the effectiveness of the AR cues is dependent on the modality of the secondary task. We also found that, under certain situations, operators’ performances for the search task are improved if the focal plane of the AR display is at the same distance as subsequent search targets.},
  keywords={Visualization;Three-dimensional displays;Surveillance;User interfaces;Physiology;Time factors;Sea level;Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR51125.2022.00040},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3491102.3517689,
author = {Ye, Hui and Fu, Hongbo},
title = {ProGesAR: Mobile AR Prototyping for Proxemic and Gestural Interactions with Real-world IoT Enhanced Spaces},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517689},
doi = {10.1145/3491102.3517689},
abstract = {Real-world IoT enhanced spaces involve diverse proximity- and gesture-based interactions between users and IoT devices/objects. Prototyping such interactions benefits various applications like the conceptual design of ubicomp space. AR (Augmented Reality) prototyping provides a flexible way to achieve early-stage designs by overlaying digital contents on real objects or environments. However, existing AR prototyping approaches have focused on prototyping AR experiences or context-aware interactions from the first-person view instead of full-body proxemic and gestural (pro-ges&nbsp;for short) interactions of real users in the real world. In this work, we conducted interviews to figure out the challenges of prototyping pro-ges interactions in real-world IoT enhanced spaces. Based on the findings, we present ProGesAR, a mobile AR tool for prototyping pro-ges interactions of a subject in a real environment from a third-person view, and examining the prototyped interactions from both the first- and third- person views. Our interface supports the effects of virtual assets dynamically triggered by a single subject, with the triggering events based on four features: location, orientation, gesture, and distance. We conduct a preliminary study by inviting participants to prototype in a freeform manner using ProGesAR. The early-stage findings show that with ProGesAR, users can easily and quickly prototype their design ideas about pro-ges interactions.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {130},
numpages = {14},
keywords = {AR prototyping, Gestural interaction, Mobile augmented reality, Proxemic interaction},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@INPROCEEDINGS{9756787,
  author={Jing, Allison and Lee, Gun and Billinghurst, Mark},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Speech to Visualise Shared Gaze Cues in MR Remote Collaboration}, 
  year={2022},
  volume={},
  number={},
  pages={250-259},
  abstract={In this paper, we present a 360° panoramic Mixed Reality (MR) sys-tem that visualises shared gaze cues using contextual speech input to improve task coordination. We conducted two studies to evaluate the design of the MR gaze-speech interface exploring the combinations of visualisation style and context control level. Findings from the first study suggest that an explicit visual form that directly connects the collaborators’ shared gaze to the contextual conversation is preferred. The second study indicates that the gaze-speech modality shortens the coordination time to attend to the shared interest, making the communication more natural and the collaboration more effective. Qualitative feedback also suggest that having a constant joint gaze indicator provides a consistent bi-directional view while establishing a sense of co-presence during task collaboration. We discuss the implications for the design of collaborative MR systems and directions for future research.},
  keywords={Manufacturing industries;Visualization;Three-dimensional displays;Collaboration;Mixed reality;Virtual reality;Bidirectional control;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/VR51125.2022.00044},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8446292,
  author={Renner, Patrick},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Prompting Techniques for Guidance and Action Assistance Using Augmented-Reality Smart-Glasses}, 
  year={2018},
  volume={},
  number={},
  pages={820-822},
  abstract={In the context of picking and assembly tasks, assistance systems based on Augmented Reality (AR) can help finding target objects and conducting correct actions. The aim is to develop guiding and action assistance techniques for smart glasses, which are easily understandable not only for workers, but also for impaired and elderly people.},
  keywords={Task analysis;Three-dimensional displays;Visualization;Augmented reality;Manuals;Gaze tracking},
  doi={10.1109/VR.2018.8446292},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9756753,
  author={Heinrich, Florian and Schwenderling, Lovis and Joeres, Fabian and Hansen, Christian},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={2D versus 3D: A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices}, 
  year={2022},
  volume={},
  number={},
  pages={260-269},
  abstract={Surgical procedures requiring needle navigation assistance suffer from complicated hand-eye coordination and are mentally demanding. Augmented reality (AR) can help overcome these issues. How-ever, only an insufficient amount of fundamental research has focused on the design and hardware selection of such AR needle navigation systems. This work contributes to this research area by presenting a user study (n=24) comparing three state-of-the-art navigation concepts displayed by an optical see-through head-mounted display and a stereoscopic projection system. A two-dimensional glyph visualization resulted in higher targeting accuracy but required more needle insertion time. In contrast, punctures guided by a three-dimensional see-through vision concept were less accurate but faster and were favored in a qualitative interview. The third concept, a static representation of the correctly positioned needle, showed too high target errors for clinical accuracy needs. This concept per-formed worse when displayed by the projection system. Besides that, no meaningful differences between the evaluated AR display devices were detected. User preferences and use case restrictions, e.g., sterility requirements, seem to be more crucial selection criteria. Future work should focus on improving the accuracy of the see-through vision concept. Until then, the glyph visualization is recommended.},
  keywords={Visualization;Three-dimensional displays;Target tracking;Navigation;Stereo image processing;Surgery;User interfaces;Human-centered computing—Visualization—Empirical studies in visualization;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Hardware—Communication hardware, interfaces and storage—Displays and imagers;Applied computing—Life and medical sciences—Health informatics},
  doi={10.1109/VR51125.2022.00045},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3411764.3445246,
author = {Johnson, Janet G and Gasques, Danilo and Sharkey, Tommy and Schmitz, Evan and Weibel, Nadir},
title = {Do You Really Need to Know Where “That” Is? Enhancing Support for Referencing in Collaborative Mixed Reality Environments},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445246},
doi = {10.1145/3411764.3445246},
abstract = {Mixed Reality has been shown to enhance remote guidance and is especially well-suited for physical tasks. Conversations during these tasks are heavily anchored around task objects and their spatial relationships in the real world, making referencing - the ability to refer to an object in a way that is understood by others - a crucial process that warrants explicit support in collaborative Mixed Reality systems. This paper presents a 2x2 mixed factorial experiment that explores the effects of providing spatial information and system-generated guidance to task objects. It also investigates the effects of such guidance on the remote collaborator’s need for spatial information. Our results show that guidance increases performance and communication efficiency while reducing the need for spatial information, especially in unfamiliar environments. Our results also demonstrate a reduced need for remote experts to be in immersive environments, making guidance more scalable, and expertise more accessible.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {514},
numpages = {14},
keywords = {Remote Guidance, Referencing, Mixed Reality, Collaboration},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3613904.3642814,
author = {Irlitti, Andrew and Latifoglu, Mesut and Hoang, Thuong and Syiem, Brandon Victor and Vetere, Frank},
title = {Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642814},
doi = {10.1145/3613904.3642814},
abstract = {Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {802},
numpages = {16},
keywords = {augmented reality, collaboration, mixed reality, partially distributed teams, telepresence, volumetric capture, workspace awareness},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3491102.3517715,
author = {Satriadi, Kadek Ananta and Smiley, Jim and Ens, Barrett and Cordeil, Maxime and Czauderna, Tobias and Lee, Benjamin and Yang, Ying and Dwyer, Tim and Jenny, Bernhard},
title = {Tangible Globes for Data Visualisation in Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517715},
doi = {10.1145/3491102.3517715},
abstract = {Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the “tangible-virtual interplay” of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {505},
numpages = {16},
keywords = {augmented reality, geographic visualisation, immersive analytics, tangible user interface},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3290605.3300577,
author = {Hartmann, Jeremy and Holz, Christian and Ofek, Eyal and Wilson, Andrew D.},
title = {RealityCheck: Blending Virtual Environments with Situated Physical Reality},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300577},
doi = {10.1145/3290605.3300577},
abstract = {Today's virtual reality (VR) systems offer chaperone rendering techniques that prevent the user from colliding with physical objects. Without a detailed geometric model of the physical world, these techniques offer limited possibility for more advanced compositing between the real world and the virtual. We explore this using a realtime 3D reconstruction of the real world that can be combined with a virtual environment. RealityCheck allows users to freely move, manipulate, observe, and communicate with people and objects situated in their physical space without losing the sense of immersion or presence inside their virtual world. We demonstrate RealityCheck with seven existing VR titles, and describe compositing approaches that address the potential conflicts when rendering the real world and a virtual environment together. A study with frequent VR users demonstrate the affordances provided by our system and how it can be used to enhance current VR experiences.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {3d compositing, depth cameras, virtual reality},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3313831.3376688,
author = {Cao, Yuanzhi and Qian, Xun and Wang, Tianyi and Lee, Rachel and Huo, Ke and Ramani, Karthik},
title = {An Exploratory Study of Augmented Reality Presence for Tutoring Machine Tasks},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376688},
doi = {10.1145/3313831.3376688},
abstract = {Machine tasks in workshops or factories are often a compound sequence of local, spatial, and body-coordinated human-machine interactions. Prior works have shown the merits of video-based and augmented reality (AR) tutoring systems for local tasks. However, due to the lack of a bodily representation of the tutor, they are not as effective for spatial and body-coordinated interactions. We propose avatars as an additional tutor representation to the existing AR instructions. In order to understand the design space of tutoring presence for machine tasks, we conduct a comparative study with 32 users. We aim to explore the strengths/limitations of the following four tutor options: video, non-avatar-AR, half-body+AR, and full-body+AR. The results show that users prefer the half-body+AR overall, especially for the spatial interactions. They have a preference for the full-body+AR for the body-coordinated interactions and the non-avatar-AR for the local interactions. We further discuss and summarize design recommendations and insights for future machine task tutoring systems.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {augmented reality, avatar tutor, exploratory study, machine task, tutoring system design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3613904.3642819,
author = {Shin, Jae-Eun and Kim, Hayun and Park, Hyerim and Woo, Woontack},
title = {Investigating the Design of Augmented Narrative Spaces Through Virtual-Real Connections: A Systematic Literature Review},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642819},
doi = {10.1145/3613904.3642819},
abstract = {Augmented Reality (AR) is regarded as an innovative storytelling medium that presents novel experiences by layering a virtual narrative space over a real 3D space. However, understanding of how the virtual narrative space and the real space are connected with one another in the design of augmented narrative spaces has been limited. For this, we conducted a systematic literature review of 64 articles featuring AR storytelling applications and systems in HCI, AR, and MR research. We investigated how virtual narrative spaces have been paired, functionalized, placed, and registered in relation to the real spaces they target. Based on these connections, we identified eight dominant types of augmented narrative spaces that are primarily categorized by whether they virtually narrativize reality or realize the virtual narrative. We discuss our findings to propose design recommendations on how virtual-real connections can be incorporated into a more structured approach to AR storytelling.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {86},
numpages = {18},
keywords = {Augmented Reality, Mixed Reality, augmented narrative space, storytelling},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642811,
author = {Lystb\ae{}k, Mathias N. and Pfeuffer, Ken and Langlotz, Tobias and Gr\o{}nb\ae{}k, Jens Emil Sloth and Gellersen, Hans},
title = {Spatial Gaze Markers: Supporting Effective Task Switching in Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642811},
doi = {10.1145/3613904.3642811},
abstract = {Task switching can occur frequently in daily routines with physical activity. In this paper, we introduce Spatial Gaze Markers, an augmented reality tool to support users in immediately returning to the last point of interest after an attention shift. The tool is task-agnostic, using only eye-tracking information to infer distinct points of visual attention and to mark the corresponding area in the physical environment. We present a user study that evaluates the effectiveness of Spatial Gaze Markers in simulated physical repair and inspection tasks against a no-marker baseline. The results give insights into how Spatial Gaze Markers affect user performance, task load, and experience of users with varying levels of task type and distractions. Our work is relevant to assist physical workers with simple AR techniques and render task switching faster with less effort.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {633},
numpages = {11},
keywords = {attention switching, augmented reality, eye-tracking, gaze interaction, task switching},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@INPROCEEDINGS{8446602,
  author={Wang, Bin and Wang, Guofeng and Sharf, Andrei and Li, Yangyan and Zhong, Fan and Qin, Xueying and Cohenor, Daniel and Chen, Baoquan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Active Assembly Guidance with Online Video Parsing}, 
  year={2018},
  volume={},
  number={},
  pages={459-466},
  abstract={In this paper, we introduce an online video-based system that actively assists users in assembly tasks. The system guides and monitors the assembly process by providing instructions and feedback on possibly erroneous operations, enabling easy and effective guidance in AR/MR applications. The core of our system is an online video-based assembly parsing method that can understand the assembly process, which is known to be extremely hard previously. Our method exploits the availability of the participating parts to significantly alleviate the problem, reducing the recognition task to an identification problem, within a constrained search space. To further constrain the search space, and understand the observed assembly activity, we introduce a tree-based global-inference technique. Our key idea is to incorporate part-interaction rules as powerful constraints which significantly regularize the search space and correctly parse the assembly video at interactive rates. Complex examples demonstrate the effectiveness of our method.},
  keywords={Three-dimensional displays;Solid modeling;Task analysis;Two dimensional displays;Monitoring;Visualization;Shape;Computing methodologies-Computer graphics-Mixed / augmented reality},
  doi={10.1109/VR.2018.8446602},
  ISSN={},
  month={March},}

@inproceedings{10.1145/3706598.3714258,
author = {Rasch, Julian and Wilhalm, Matthias and M\"{u}ller, Florian and Chiossi, Francesco},
title = {AR You on Track? Investigating Effects of Augmented Reality Anchoring on Dual-Task Performance While Walking},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714258},
doi = {10.1145/3706598.3714258},
abstract = {With the increasing spread of AR head-mounted displays suitable for everyday use, interaction with information becomes ubiquitous, even while walking. However, this requires constant shifts of our attention between walking and interacting with virtual information to fulfill both tasks adequately. Accordingly, we as a community need a thorough understanding of the mutual influences of walking and interacting with digital information to design safe yet effective interactions. Thus, we systematically investigate the effects of different AR anchors (hand, head, torso) and task difficulties on user experience and performance. We engage participants (n = 26) in a dual-task paradigm involving a visual working memory task while walking. We assess the impact of dual-tasking on both virtual and walking performance, and subjective evaluations of mental and physical load. Our results show that head-anchored AR content least affected walking while allowing for fast and accurate virtual task interaction, while hand-anchored content increased reaction times and workload.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1217},
numpages = {21},
keywords = {Augmented Reality, Dual-Tasking, Cognitive-Motor Interference},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{9756789,
  author={Davari, Shakiba and Lu, Feiyu and Bowman, Doug A.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Validating the Benefits of Glanceable and Context-Aware Augmented Reality for Everyday Information Access Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={436-444},
  abstract={Glanceable Augmented Reality interfaces have the potential to provide fast and efficient information access for the user. However, where to place the virtual content and how to access them depend on the user context. We designed a Context-Aware AR interface that can intelligently adapt for two different contexts: solo and social. We evaluated information access using Context-Aware AR compared to current mobile phones and non-adaptive Glanceable AR interfaces. We found that in a solo scenario, compared to a mobile phone, the Context-Aware AR interface was preferred, easier, and significantly faster; it improved the user experience; and it allowed the user to better focus on their primary task. In the social scenario, we discovered that the mobile phone was slower, more intrusive, and perceived as the most difficult. Meanwhile, Context-Aware AR was faster for responding to information needs triggered by the conversation; it was preferred and perceived as the easiest for resuming conversation after information access; and it improved the user’s awareness of the other person’s facial expressions.},
  keywords={Human computer interaction;Three-dimensional displays;Conferences;User experience;Task analysis;Augmented reality;Smart phones;Human-centered computing;Mixed/augmented reality;Interaction techniques;Empirical Studies in HCI},
  doi={10.1109/VR51125.2022.00063},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3706598.3713518,
author = {Luo, Weizhou and Ellenberg, Mats Ole and Satkowski, Marc and Dachselt, Raimund},
title = {Documents in Your Hands: Exploring Interaction Techniques for Spatial Arrangement of Augmented Reality Documents},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713518},
doi = {10.1145/3706598.3713518},
abstract = {Augmented Reality (AR) promises to enhance daily office activities involving numerous textual documents, slides, and spreadsheets by expanding workspaces and enabling more direct interaction. However, there is a lack of systematic understanding of how knowledge workers can manage multiple documents and organize, explore, and compare them in AR environments. Therefore, we conducted a user-centered design study (N&nbsp;=&nbsp;21) using predefined spatial document layouts in AR to elicit interaction techniques, resulting in 790 observation notes. Thematic analysis identified various interaction methods for aggregating, distributing, transforming, inspecting, and navigating document collections. Based on these findings, we propose a design space and distill design implications for AR document arrangement systems, such as enabling body-anchored storage, facilitating layout spreading and compressing, and designing interactions for layout transformation. To demonstrate their usage, we developed a rapid prototyping system and exemplify three envisioned scenarios. With this, we aim to inspire the design of future immersive offices.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1218},
numpages = {22},
keywords = {spatial layout, content organization, interaction design, user-centered design, Mixed Reality},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{9089581,
  author={Dominic, James and Robb, Andrew},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Effects of Screen-Fixed and World-Fixed Annotation on Navigation in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={607-615},
  abstract={In this paper, we consider the effect of different types of virtual annotations on performance during a navigation task in virtual reality. Two major types of annotations were shown to users: screen-fixed annotations that remained fixed in the user’s field of view, and world- fixed annotations that are linked to specific locations in the world. We also considered three different levels of navigation information, including destination markers, maps visualizing the layout of the space being navigated, and path markers showing the optimal route to the destination. We ran a within-subjects study where participants completed three trials with each of the six combinations of annotation type and information level, for a total of 18 trials in a virtual environment. Average speed, distance traveled, and the time taken to reach the destination were recorded during each trial. Participants were also asked to point back to where they started the trial upon reaching the destination, as a measure of spatial memory. Finally, participants were tasked with completing a secondary activity while navigating, so as to assess what effect annotation types had on multitasking performance. Participants navigated significantly more quickly when using world-fixed annotations; however an interaction effect was observed between the type of annotation and the level of information, which suggests that world-fixed annotations are not inherently better than screen-fixed annotations; instead, it is important to consider both the type of annotation and what information it displays.},
  keywords={Task analysis;Three-dimensional displays;Space exploration;Aircraft navigation;Virtual reality;Layout;Human factors and ergonomics;locomotion and navigation},
  doi={10.1109/VR46266.2020.00083},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10494091,
  author={Torres, Ángel and Molina, José P. and García, Arturo S. and González, Pascual},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Prototyping of Augmented Reality interfaces for air traffic alert and their evaluation using a Virtual Reality aircraft-proximity simulator}, 
  year={2024},
  volume={},
  number={},
  pages={817-826},
  abstract={Despite pilots’ training and technology aids, mid-air collisions can occur and do occur, especially near airfields and in non-controlled airspaces where different kinds of aircraft fly. Technology typically helps pilots in the form of a flat display in the cockpit that shows nearby air traffic. New airliners are now fitted with see-through displays (HUD) to present information right in front of the pilot, and modern military fighters mount that kind of displays in the pilot’s helmet to help them no matter what direction they are looking in. This technology, however, could reach light and sport aviation in the next years thanks to new light and affordable Augmented Reality (AR) glasses, such as the ones targeting applications in urban mobility. Looking ahead to that moment, in this work we rely on Virtual Reality (VR) -in particular, Cardboard VR- to prototype and test different AR interfaces for air traffic alert. Firstly, we proposed and tested four different head-mounted display (HMD) AR interfaces with four pilots in our own VR aircraft-proximity simulator. Then, the two best-scored interfaces were selected for a second evaluation, and they were compared against another proposal (Circular HUD by Alce et al.) and a fixed-mounted (FM) conventional HUD radar (FMHUD Radar), tested by four additional pilots. Overall, pilots showed preference for our AR proposals. Interestingly, pilots with more experience preferred the more conventional, radar-like designs, while those with less flight hours were more open to a different, novel design (HMD 3D Arrow).},
  keywords={Training;Three-dimensional displays;Resists;Radar;User interfaces;Military aircraft;Safety;Air traffic alert;guidance techniques;augmented reality;virtual prototyping;H.5.2 [Information Interfaces and Presentation];User Interfaces},
  doi={10.1109/VR58804.2024.00101},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10494148,
  author={Wagner, Jorge and Silva, Claudio T. and Stuerzlinger, Wolfgang and Nedel, Luciana},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and reflecting on potential benefits of Immersive Analytics for urban data exploration}, 
  year={2024},
  volume={},
  number={},
  pages={827-838},
  abstract={Current visualization research has identified the potential of more immersive settings for data exploration, leveraging VR and AR technologies. To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City. Here, we reimagine how TaxiVis’ functionalities could be implemented and extended in a 3D immersive environment. Among the unique features we identify as being enabled by the Immersive TaxiVis prototype are alternative uses of the additional visual dimension, a fully visual 3D spatio-temporal query framework, and the opportunity to explore the data at different scales and frames of reference. By revisiting the case studies from the original paper, we demonstrate workflows that can benefit from this immersive perspective. Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and on how the exploration of urban datasets can be facilitated in the coming years.},
  keywords={Visualization;Three-dimensional displays;Urban areas;Data visualization;Prototypes;Virtual reality;Reflection;Human-centered computing;Visualization;Visualization systems and tools;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR58804.2024.00102},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3706598.3713293,
author = {Zhang, Nandi and Yan, Yukang and Suzuki, Ryo},
title = {From Following to Understanding: Investigating the Role of Reflective Prompts in AR-Guided Tasks to Promote User Understanding},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713293},
doi = {10.1145/3706598.3713293},
abstract = {Augmented Reality (AR) is a promising medium for guiding users through tasks, yet its impact on fostering deeper task understanding remains underexplored. This paper investigates the impact of reflective prompts—strategic questions that encourage users to challenge assumptions, connect actions to outcomes, and consider hypothetical scenarios—on task comprehension and performance. We conducted a two-phase study: a formative survey and co-design sessions (N=9) to develop reflective prompts, followed by a within-subject evaluation (N=16) comparing AR instructions with and without these prompts in coffee-making and circuit assembly tasks. Our results show that reflective prompts significantly improved objective task understanding and resulted in more proactive information acquisition behaviors during task completion. These findings highlight the potential of incorporating reflective elements into AR instructions to foster deeper engagement and learning. Based on data from both studies, we synthesized design guidelines for integrating reflective elements into AR systems to enhance user understanding without compromising task performance.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1227},
numpages = {18},
keywords = {Augmented Reality; Task Guidance; Instruction Following; Reflective Prompts},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{8446533,
  author={Aschenbrenner, Doris and Li, Meng and Dukalski, Radoslaw and Verlinden, Jouke and Lukosch, Stephan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative Production Line Planning with Augmented Fabrication}, 
  year={2018},
  volume={},
  number={},
  pages={509-510},
  abstract={The project “Factory-in-a-day” aims at reducing the installation time of a new hybrid robot-human production line, from weeks or months that current industrial systems now take, down to one day. The ability to rapidly install (and reconfigure) production lines where robots work alongside humans will strongly reduce operating cost and open a range of new opportunities for industry. In this paper, we explore a method of collaborative fabrication planning with the help of Augmented Reality as part of the concept Augmented Fabrication. In order to plan a new production line, two co-located workers at the factory wear a Microsoft Hololens head-mounted display and thus share a common visual context on the planed position of the robots and the production machines. They are assisted by an external remote expert connected via the Internet who is virtually co-located. We developed three different visualizations of the state of the local collaboration and plan to compare them in a user study.},
  keywords={Planning;Fabrication;Robots;Augmented reality;Three-dimensional displays;Task analysis;Human-centered computing [Mixed / augmented reality];[Social and professional topics]: Computer supported cooperative work;Applied computing [Industry and manufacturing];[Computer systems organization]: Robotics},
  doi={10.1109/VR.2018.8446533},
  ISSN={},
  month={March},}

@inproceedings{10.1145/3411764.3445675,
author = {Shin, Jae-eun and Yoon, Boram and Kim, Dooyoung and Woo, Woontack},
title = {A User-Oriented Approach to Space-Adaptive Augmentation: The Effects of Spatial Affordance on Narrative Experience in an Augmented Reality Detective Game},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445675},
doi = {10.1145/3411764.3445675},
abstract = {Space-adaptive algorithms aim to effectively align the virtual with the real to provide immersive user experiences for Augmented Reality(AR) content across various physical spaces. While such measures are reliant on real spatial features, efforts to understand those features from the user’s perspective and reflect them in designing adaptive augmented spaces have been lacking. For this, we compared factors of narrative experience in six spatial conditions during the gameplay of Fragments, a space-adaptive AR detective game. Configured by size and furniture layout, each condition afforded disparate degrees of traversability and visibility. Results show that whereas centered furniture clusters are suitable for higher presence in sufficiently large rooms, the same layout leads to lower narrative engagement. Based on our findings, we suggest guidelines that can enhance the effects of space adaptivity by considering how users perceive and navigate augmented space generated from different physical environments.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {722},
numpages = {13},
keywords = {storytelling, spatial mapping, spatial affordance, space adaptivity, narrative experience, Head Mounted Displays, Augmented Reality},
location = {Yokohama, Japan},
series = {CHI '21}
}

@INPROCEEDINGS{9756811,
  author={Riegler, Andreas and Riener, Andreas and Holzmann, Clemens},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Content Presentation on 3D Augmented Reality Windshield Displays in the Context of Automated Driving}, 
  year={2022},
  volume={},
  number={},
  pages={543-552},
  abstract={Increasing vehicle automation presents challenges as drivers of automated vehicles become more disengaged from the primary driving task, as there will still be activities that require interfaces for vehicle-passenger interactions. Windshield displays provide large-content areas supporting drivers in non-driving related tasks. This work addresses user preferences as well as task and safety aspects for 3D augmented reality (AR) windshield displays in automated driving. Participants of a user study (N = 24) were presented with two modes of content presentation (multiple content-specific windows vs. one main window), and could freely choose their preferred positions, content types, as well as size, and transparency levels for these content windows using a simulated "ideal" windshield display in a virtual reality driving simulator. We found that using one main content window resulted in better task performance and lower take-over times, however, subjective user experience was higher for the multi-window user interface. These insights help designers of in-vehicle applications to provide a rich user experience in automated vehicles.},
  keywords={Productivity;Three-dimensional displays;Automation;User interfaces;User experience;Safety;Task analysis;Human-centered computing;Visualization;Visualization techniques;Visualization design and evaluation methods;User Interfaces;Graphical user interfaces (GUI);Information Interfaces and Presentation;Miscellaneous},
  doi={10.1109/VR51125.2022.00074},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9089450,
  author={Timmerman, Matthew and Sadagic, Amela and Irvine, Cynthia},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Peering Under the Hull: Enhanced Decision Making via an Augmented Environment}, 
  year={2020},
  volume={},
  number={},
  pages={704-712},
  abstract={Daily operation and management of complex systems typically include multiple working sessions during which a team presents a set of information and discusses issues relevant to their decision making. A complex set of operational technology (OT) networks installed onboard a Navy ship is an example of such a system. A crew’s ability to effectively communicate OT networks status to the ship commander, visualize, and discuss the options available in a given situation, has a significant impact on mission success. While the complexity of contemporary OT networks has dramatically increased, visualization tools have witnessed little improvement over several decades—they include sets of two-dimensional blueprints that are inherently hard to understand and conceptualize as three-dimensional (3D) information. To address this problem, we designed and implemented an augmented reality (AR) system that allowed a small team to visualize a 3D model of the ship with details of its computer networks. We recruited 30 individuals familiar with network management tasks central to our study and examined the usability of the tool on a set of real-world scenarios focused on network management. Analysis of objective and subjective data suggested that there was a general agreement among the participants that AR portrayal of the network was very supportive of their understanding of the physical-to-logical relationship within the network and that it fostered constructive collaboration among the team members. The reported levels of discomfort associated with oculomotor symptoms made the highest contribution to the total Simulator Sickness Questionnaire score; we believe that those symptoms should be given more attention in future studies with AR setups. The results provided in this empirical study offer early insights into the benefits and challenges of AR approaches applied to the decision making of small teams in high stakes scenarios and real-world situations.},
  keywords={Task analysis;Three-dimensional displays;Collaboration;Marine vehicles;Two dimensional displays;Tools;Computer networks;augmented reality;collaborative environment;usability;network visualization;small team collaboration;decision making;complex domains},
  doi={10.1109/VR46266.2020.00093},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3706598.3714274,
author = {Numan, Nels and Brostow, Gabriel and Park, Suhyun and Julier, Simon and Steed, Anthony and Van Brummelen, Jessica},
title = {CoCreatAR: Enhancing Authoring of Outdoor Augmented Reality Experiences Through Asymmetric Collaboration},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714274},
doi = {10.1145/3706598.3714274},
abstract = {Authoring site-specific outdoor augmented reality (AR) experiences requires a nuanced understanding of real-world context to create immersive and relevant content. Existing ex-situ authoring tools typically rely on static 3D models to represent spatial information. However, in our formative study (n=25), we identified key limitations of this approach: models are often outdated, incomplete, or insufficient for capturing critical factors such as safety considerations, user flow, and dynamic environmental changes. These issues necessitate frequent on-site visits and additional iterations, making the authoring process more time-consuming and resource-intensive. To mitigate these challenges, we introduce CoCreatAR, an asymmetric collaborative mixed reality authoring system that integrates the flexibility of ex-situ workflows with the immediate contextual awareness of in-situ authoring. We conducted an exploratory study (n=32) comparing CoCreatAR to an asynchronous workflow baseline, finding that it enhances engagement, creativity, and confidence in the authored output while also providing preliminary insights into its impact on task load. We conclude by discussing the implications of our findings for integrating real-world context into site-specific AR authoring systems.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1232},
numpages = {22},
keywords = {collaborative mixed reality, augmented reality, co-creation, site-specific, authoring tools, reconstruction, context-aware systems},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3173574.3173792,
author = {Lee, Joon Hyub and An, Sang-Gyun and Kim, Yongkwan and Bae, Seok-Hyung},
title = {Projective Windows: Bringing Windows in Space to the Fingertip},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173792},
doi = {10.1145/3173574.3173792},
abstract = {In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {virtual reality, augmented reality, 3d window management},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@INPROCEEDINGS{10494177,
  author={Yang, Jackie Junrui and Qiu, Leping and Corona-Moreno, Emmanuel Angel and Shi, Louisa and Bui, Hung and Lam, Monica S. and Landay, James A.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={AMMA: Adaptive Multimodal Assistants Through Automated State Tracking and User Model-Directed Guidance Planning}, 
  year={2024},
  volume={},
  number={},
  pages={892-902},
  abstract={Novel technologies such as augmented reality and computer perception lay the foundation for smart assistants that can guide us through real-world tasks, such as cooking or home repair. However, the nature of real-world interaction requires assistants that adapt to users’ mistakes, environments, and communication preferences. We propose Adaptive Multimodal Assistants (AMMA), a software architecture for task guidance with generated adaptive interfaces from step-by-step instructions. This is achieved through 1) an automatically generated user action state tracker and 2) a guidance planner that leverages a continuously trained user model. The assistant also adjusts its guidance and communication delivery methods based on observed user performance as well as implicit and explicit user feedback. We demonstrated the viability of AMMA by building an adaptive cooking assistant running in a high-fidelity virtual reality-based simulator. A user study of the cooking assistant showed that AMMA can reduce the task completion time and the number of manual communication methods changes.},
  keywords={Solid modeling;Adaptation models;Adaptive systems;Three-dimensional displays;Navigation;Software architecture;Virtual assistants;Augmented reality;interface generation;smart assistant;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Interactive systems and tools;User interface toolkits},
  doi={10.1109/VR58804.2024.00108},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3173574.3173793,
author = {Huo, Ke and Cao, Yuanzhi and Yoon, Sang Ho and Xu, Zhuangying and Chen, Guiming and Ramani, Karthik},
title = {Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173793},
doi = {10.1145/3173574.3173793},
abstract = {The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {uwb, spatial interactions, smart environment, slam, localization, iot, context awareness, augmented reality},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@INPROCEEDINGS{9089461,
  author={Wei, Chunxue and Yu, Difeng and Dingler, Tilman},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reading on 3D Surfaces in Virtual Environments}, 
  year={2020},
  volume={},
  number={},
  pages={721-728},
  abstract={While text tends to lead a rather static life on paper and screens, virtual reality (VR) allows readers to interact with it in novel ways: the reading surface is no longer confined to a 2D plane. We conducted two user studies, in which we assessed text rendered on different surface shapes in VR and their effects on legibility and the reading experience. Comparing differently curved surfaces, these studies disclose the impact of warp angles and view box widths on reading comfort, speed, and distraction. Our results suggest that text should be warped around the horizontal rather than the vertical axis, and we provide recommendations for the extent of warp and view box width. In a proof-of-concept application, we used everyday 3D objects as text canvases and studied them through an information-seeking task. The studies’ implications inform VR interfaces and, more generally, the rendering of text on 3D objects.},
  keywords={Three-dimensional displays;Rendering (computer graphics);Virtual environments;Shape;Two dimensional displays;Task analysis;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality;Interaction design;Interaction design process and methods;User interface design},
  doi={10.1109/VR46266.2020.00095},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3706598.3713949,
author = {Rau, Tobias and Isenberg, Tobias and Koehn, Andreas and Sedlmair, Michael and Lee, Benjamin},
title = {Traversing Dual Realities: Investigating Techniques for Transitioning 3D Objects between Desktop and Augmented Reality Environments},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713949},
doi = {10.1145/3706598.3713949},
abstract = {Desktop environments can integrate augmented reality (AR) head-worn devices to support 3D representations, visualizations, and interactions in a novel yet familiar setting. As users navigate across the dual realities—desktop and AR—a way to move 3D objects between them is needed. We devise three baseline transition techniques based on common approaches in the literature and evaluate their usability and practicality in an initial user study (N=18). After refining both our transition techniques and the surrounding technical setup, we validate the applicability of the overall concept for real-world activities in an expert user study (N=6). In it, computational chemists followed their usual desktop workflows to build, manipulate, and analyze 3D molecular structures, but now aided with the addition of AR and our transition techniques. Based on our findings from both user studies, we provide lessons learned and takeaways for the design of 3D object transition techniques in desktop + AR environments.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1236},
numpages = {16},
keywords = {Augmented reality, Cross-reality, Hybrid user interfaces, Usability study, Expert study, Computational Chemistry, Gestural input},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{8798080,
  author={Grandi, Jerônimo Gustavo and Debarba, Henrique Galvan and Maciel, Anderson},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities}, 
  year={2019},
  volume={},
  number={},
  pages={127-135},
  abstract={We present an assessment of asymmetric interactions in Collaborative Virtual Environments (CVEs). In our asymmetric setup, two co-located users interact with virtual 3D objects, one in immersive Virtual Reality (VR) and the other in mobile Augmented Reality (AR). We conducted a study with 36 participants to evaluate performance and collaboration aspects of pair work, and compare it with two symmetric scenarios, either with both users in immersive VR or mobile AR. To perform this experiment, we adopt a collaborative AR manipulation technique from literature and develop and evaluate a VR manipulation technique of our own. Our results indicate that pairs in asymmetric VR-AR achieved significantly better performance than the AR symmetric condition, and similar performance to VR symmetric. Regardless of the condition, pairs had similar work participation indicating a high cooperation level even when there is a visualization and interaction asymmetry between the participants.},
  keywords={Collaboration;Three-dimensional displays;Visualization;Task analysis;Augmented reality;User interfaces;Human-centered computing—Human computer interaction (HCI)—Interaction techniques;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/augmented reality Human-centered computing—Collaborative and social computing},
  doi={10.1109/VR.2019.8798080},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3544548.3581069,
author = {Thanyadit, Santawat and Heintz, Matthias and Law, Effie L-C},
title = {Tutor In-sight: Guiding and Visualizing Students’ Attention with Mixed Reality Avatar Presentation Tools},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581069},
doi = {10.1145/3544548.3581069},
abstract = {Remote conferencing systems are increasingly used to supplement or even replace in-person teaching. However, prevailing conferencing systems restrict the teacher’s representation to a webcam live-stream, hamper the teacher’s use of body-language, and result in students’ decreased sense of co-presence and participation. While Virtual Reality (VR) systems may increase student engagement, the teacher may not have the time or expertise to conduct the lecture in VR. To address this issue and bridge the requirements between students and teachers, we have developed Tutor In-sight, a Mixed Reality (MR) avatar augmented into the student’s workspace based on four design requirements derived from the existing literature, namely: integrated virtual with physical space, improved teacher’s co-presence through avatar, direct attention with auto-generated body language, and usable workflow for teachers. Two user studies were conducted from the perspectives of students and teachers to determine the advantages of Tutor In-sight in comparison to two existing conferencing systems, Zoom (video-based) and Mozilla Hubs (VR-based). The participants of both studies favoured Tutor In-sight. Among others, this main finding indicates that Tutor In-sight satisfied the needs of both teachers and students. In addition, the participants’ feedback was used to empirically determine the four main teacher requirements and the four main student requirements in order to improve the future design of MR educational tools.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {449},
numpages = {20},
keywords = {Augmented Reality, Remote Presentation, Virtual Avatar},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3491102.3501873,
author = {Schjerlund, Jonas and Hornb\ae{}k, Kasper and Bergstr\"{o}m, Joanna},
title = {OVRlap: Perceiving Multiple Locations Simultaneously to Improve Interaction in VR},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501873},
doi = {10.1145/3491102.3501873},
abstract = {We introduce OVRlap, a VR interaction technique that lets the user perceive multiple places at the same time from a first-person perspective. OVRlap achieves this by overlapping viewpoints. At any time, only one viewpoint is active, meaning that the user may interact with objects therein. Objects seen from the active viewpoint are opaque, whereas objects seen from passive viewpoints are transparent. This allows users to perceive multiple locations at once and easily switch to the one in which they want to interact. We compare OVRlap and a single-viewpoint technique in a study where 20 participants complete object-collection and monitoring tasks. We find that in both tasks, participants are significantly faster and move their head significantly less with OVRlap. We propose how the technique might be improved through automated switching of the active viewpoint and intelligent viewpoint rendering.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {355},
numpages = {13},
keywords = {interaction techniques, large environments, user studies, virtual reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3706598.3714284,
author = {Abe, Yuki and Matsushima, Keisuke and Hara, Kotaro and Sakamoto, Daisuke and Ono, Tetsuo},
title = {“I can run at night!": Using Augmented Reality to Support Nighttime Guided Running for Low-vision Runners},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714284},
doi = {10.1145/3706598.3714284},
abstract = {Dark environment challenges low-vision (LV) individuals to engage in running by following sighted guide—a Caller-style guided running—due to insufficient illumination, because it prevents them from using their residual vision to follow the guide and be aware about their environment. We design, develop, and evaluate RunSight, an augmented reality (AR)-based assistive tool to support LV individuals to run at night. RunSight combines see-through HMD and image processing to enhance one’s visual awareness of the surrounding environment (e.g., potential hazard) and visualize the guide’s position with AR-based visualization. To demonstrate RunSight’s efficacy, we conducted a user study with 8 LV runners. The results showed that all participants could run at least 1km (mean = 3.44 km) using RunSight, while none could engage in Caller-style guided running without it. Our participants could run safely because they effectively synthesized RunSight-provided cues and information gained from runner-guide communication.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1244},
numpages = {20},
keywords = {accessibility, augmented reality, low-vision individuals, guided running, nighttime outdoor exercise},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713350,
author = {Acherki, Chaymae and Nigay, Laurence and Roy, Quentin and Salque, Thibault},
title = {An Evaluation of Spatial Anchoring to position AR Guidance in Arthroscopic Surgery},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713350},
doi = {10.1145/3706598.3713350},
abstract = {This work examines spatial anchoring strategies to position augmented reality guidance during surgery. We consider three strategies: anchoring to the Patient, the surgical Tool, and the Surgeon’s head. These strategies were evaluated in a first experiment involving 24 non-professional participants, using two guidance techniques: 3D Trajectory and 2D Crosshair. For 3D Trajectory, Patient and Tool anchoring were more precise than Surgeon anchoring, and Patient anchoring was the most preferred. For 2D Crosshair, no significant effect of anchoring strategies on precision was observed. However, participants preferred Patient and Surgeon anchoring. A second experiment with 6 surgeons confirmed the first experiment’s results. For 3D trajectory, Tool anchoring proved more precise than Patient anchoring, despite surgeons’ preference for Patient anchoring. These findings contribute to empirical evidence for the design of surgical AR guidance, with potential applications for similar, less critical tasks.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {699},
numpages = {17},
keywords = {Augmented reality, Human-computer interaction, Arthroscopic surgery, Drilling, Spatial anchoring, Guidance technique},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3173574.3173620,
author = {Piumsomboon, Thammathip and Lee, Gun A. and Hart, Jonathon D. and Ens, Barrett and Lindeman, Robert W. and Thomas, Bruce H. and Billinghurst, Mark},
title = {Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173620},
doi = {10.1145/3173574.3173620},
abstract = {We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {virtual reality, remote embodiment, remote collaboration, redirected, mixed reality, gesture, gaze, awareness, avatar, augmented reality},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3613904.3642862,
author = {Schenkluhn, Marius and Knierim, Michael Thomas and Kiss, Francisco and Weinhardt, Christof},
title = {Connecting Home: Human-Centric Setup Automation in the Augmented Smart Home},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642862},
doi = {10.1145/3613904.3642862},
abstract = {Controlling smart homes via vendor-specific apps on smartphones is cumbersome. Augmented Reality (AR) offers a promising alternative by enabling direct interactions with Internet of Things (IoT) devices. However, using AR for smart home control requires knowledge of each device’s 3D position. In this paper, we introduce and evaluate three concepts for identifying IoT device positions with varying degrees of automation. Our mixed-methods laboratory study with 28 participants revealed that, despite being recognized as the most efficient option, the majority of participants opted against a fast, fully automated detection, favoring a balance between efficiency and perceived autonomy and control. We link this decision to psychological needs grounded in self-determination theory and discuss the strengths and weaknesses of each alternative, motivating a user-adaptive solution. Additionally, we observed a “wow-effect” in response to AR interaction for smart homes, suggesting potential benefits of a human-centric approach to the smart home of the future.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {847},
numpages = {16},
keywords = {Augmented Reality, Laboratory Experiment, Self-Determination Theory, Smart Home},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3290605.3300431,
author = {Teo, Theophilus and Lawrence, Louise and Lee, Gun A. and Billinghurst, Mark and Adcock, Matt},
title = {Mixed Reality Remote Collaboration Combining 360 Video and 3D Reconstruction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300431},
doi = {10.1145/3290605.3300431},
abstract = {Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {360 panorama, 3d scene reconstruction, interaction methods, mixed reality, remote collaboration, virtual reality},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@INPROCEEDINGS{7892294,
  author={Brandão, William Losina and Pinho, Márcio Sarroglia},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Using augmented reality to improve dismounted operators' situation awareness}, 
  year={2017},
  volume={},
  number={},
  pages={297-298},
  abstract={Whether it in the military, law enforcement or private security, dismounted operators tend to deal with a large amount of volatile information that may or may not be relevant according to a variety of factors. In this paper we draft some ideas on the building blocks of an augmented reality system aimed to improve the situational awareness of dismounted operators by filtering, organizing, and displaying this information in a way that reduces the strain over the operator.},
  keywords={Augmented reality;Navigation;Machine vision;Time measurement;Position measurement;Hardware;Situation Awareness;Augmented Reality},
  doi={10.1109/VR.2017.7892294},
  ISSN={2375-5334},
  month={March},}

@inproceedings{10.1145/3613904.3642043,
author = {Chen, John and Zhao, Lexie and Li, Yinmiao and Xie, Zhennian and Wilensky, Uri and Horn, Mike},
title = {“Oh My God! It’s Recreating Our Room!” Understanding Children’s Experiences with A Room-Scale Augmented Reality Authoring Toolkit},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642043},
doi = {10.1145/3613904.3642043},
abstract = {Human-Computer Interaction (HCI) and education researchers have applied Augmented Reality (AR) to support spatial thinking in K-12 education. However, fewer studies support spatial thinking through spatial exploration. Room-scale AR, a recent technology development, brings new opportunities not yet researched. We developed NetLogo AR, an AR authoring toolkit, that allows children to play with, design, and create room-scale AR experiences that combine AR with computational models. To acquire a deeper and more nuanced understanding of children’s interactions with this new technology, we conducted eight-week participatory design sessions with seven children aged 11-13. We analyzed 48 hours of video data, interview transcripts, and design artifacts. Children were enthusiastic and engaged in spatial thinking activities. We affirmed room-scale AR’s role in spatial exploration by comparing it with other supported modalities. Building on existing studies, we propose a new AR design framework around spatial movement and exploration that could help inform design decisions.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {667},
numpages = {17},
keywords = {AR Authoring Toolkit, AR and Children, Augmented Reality, NetLogo AR, Participatory Design, Spatial AR},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3544548.3581515,
author = {Gr\o{}nb\ae{}k, Jens Emil Sloth and Pfeuffer, Ken and Velloso, Eduardo and Astrup, Morten and Pedersen, Melanie Isabel S\o{}nderk\ae{}r and Kj\ae{}r, Martin and Leiva, Germ\'{a}n and Gellersen, Hans},
title = {Partially Blended Realities: Aligning Dissimilar Spaces for Distributed Mixed Reality Meetings},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581515},
doi = {10.1145/3544548.3581515},
abstract = {Mixed Reality allows for distributed meetings where people’s local physical spaces are virtually aligned into blended interaction spaces. In many cases, people’s physical rooms are dissimilar, making it challenging to design a coherent blended space. We introduce the concept of Partially Blended Realities (PBR) — using Mixed Reality to support remote collaborators in partially aligning their physical spaces. As physical surfaces are central in collaborative work, PBR supports users in transitioning between different configurations of tables and whiteboard surfaces. In this paper, we 1) describe the design space of PBR, 2) present RealityBlender to explore interaction techniques for how users may configure and transition between blended spaces, and 3) provide insights from a study on how users experience transitions in a remote collaboration task. With this work, we demonstrate new potential for using partial solutions to tackle the alignment problem of dissimilar spaces in distributed Mixed Reality meetings.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {456},
numpages = {16},
keywords = {augmented and virtual reality, blended realities, mixed reality, proxemic transitions, remote collaboration},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{8797812,
  author={Ssin, Seung Youb and Walsh, James A. and Smith, Ross T. and Cunningham, Andrew and Thomas, Bruce H.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={GeoGate: Correlating Geo-Temporal Datasets Using an Augmented Reality Space-Time Cube and Tangible Interactions}, 
  year={2019},
  volume={},
  number={},
  pages={210-219},
  abstract={This paper introduces GeoGate, an Augmented Reality tabletop system that extends the Space-Time Cube and utilizes a ring-shaped tangible user interface to explore correlations between entities in multiple location datasets. We demonstrate GeoGate in the context of the maritime domain, where operators seek to find geo-temporal associations between trajectories recorded from a global positioning system, and light data extracted from night time satellite images. GeoGate utilizes a tabletop system displaying a traditional 2D map in conjunction with a Microsoft Hololens to present a single view of the data with a novel Augmented Reality extension of the Space-Time Cube. To validate GeoGate, we present the results of a user study comparing GeoGate with the existing 2D approach used in a normal desktop environment. The outcomes of the user study show that GeoGate's approach reduces mistakes in the interpretation of the correlations between various datasets, while the qualitative results show that such a system is preferable for the majority of geo-temporal maritime tasks compared.},
  keywords={Three-dimensional displays;Data visualization;Two dimensional displays;Trajectory;Complexity theory;Augmented reality;Uncertainty;Augmented Reality;Space Time Cube;Multivariate Network Visualization;Multiple Data-sets;Maritime Visualization;Geo-visualization;Tangible User Interface;Tabletop;H.1.2 [Information Systems]: User/Machine Systems—Human factors;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques},
  doi={10.1109/VR.2019.8797812},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10937341,
  author={Li, Ziming and Zhang, Huadong and Peng, Chao and Peiris, Roshan},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring Large Language Model-Driven Agents for Environment-Aware Spatial Interactions and Conversations in Virtual Reality Role-Play Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  abstract={Recent research has begun adopting Large Language Model (LLM) agents to enhance Virtual Reality (VR) interactions, creating immersive chatbot experiences. However, while current studies focus on generating dialogue from user speech inputs, their abilities to generate richer experiences based on the perception of LLM agents’ VR environments and interaction cues remain unexplored. Hence, in this work, we propose an approach that enables LLM agents to perceive virtual environments and generate environment-aware interactions and conversations for an embodied human-AI interaction experience in VR environments. Here, we define a schema for describing VR environments and their interactions through text prompts. We evaluate the performance of our method through five role-play scenarios created using our approach in a study with 14 participants. The findings discuss the opportunities and challenges of our proposed approach for developing environment-aware LLM agents that facilitate spatial interactions and conversations within VR role-play scenarios.},
  keywords={Solid modeling;Ethics;Translation;Three-dimensional displays;Generative AI;Large language models;Virtual environments;Oral communication;User interfaces;Reliability;Virtual reality;role-play simulations;generative AI;human-AI interaction;large language models;context-awareness},
  doi={10.1109/VR59515.2025.00025},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3544548.3580750,
author = {Ye, Hui and Leng, Jiaye and Xiao, Chufeng and Wang, Lili and Fu, Hongbo},
title = {ProObjAR: Prototyping Spatially-aware Interactions of Smart Objects with AR-HMD},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580750},
doi = {10.1145/3544548.3580750},
abstract = {The rapid advances in technologies have brought new interaction paradigms of smart objects (e.g., digital devices) beyond digital device screens. By utilizing spatial properties, configurations, and movements of smart objects, designing spatial interaction, which is one of the emerging interaction paradigms, efficiently promotes engagement with digital content and physical facility. However, as an important phase of design, prototyping such interactions still remains challenging, since there is no ad-hoc approach for this emerging paradigm. Designers usually rely on methods that require fixed hardware setup and advanced coding skills to script and validate early-stage concepts. These requirements restrict the design process to a limited group of users in indoor scenes. To facilitate the prototyping to general usages, we aim to figure out the design difficulties and underlying needs of current design processes for spatially-aware object interactions by empirical studies. Besides, we explore the design space of the spatial interaction for smart objects and discuss the design space in an input-output spatial interaction model. Based on these findings, we present ProObjAR, an all-in-one novel prototyping system with an Augmented Reality Head Mounted Display (AR-HMD). Our system allows designers to easily obtain the spatial data of smart objects being prototyped, specify spatially-aware interactive behaviors from an input-output event triggering workflow, and test the prototyping results in situ. From the user study, we find that ProObjAR&nbsp;simplifies the design procedure and increases design efficiency to a large extent and thus advancing the development of spatially-aware applications in smart ecosystems.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {457},
numpages = {15},
keywords = {AR prototyping, smart objects, spatial interaction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{7504683,
  author={Lee, Myungho and Kim, Kangsoo and Daher, Salam and Raij, Andrew and Schubert, Ryan and Bailenson, Jeremy and Welch, Greg},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={The wobbly table: Increased social presence via subtle incidental movement of a real-virtual table}, 
  year={2016},
  volume={},
  number={},
  pages={11-17},
  abstract={While performing everyday interactions, we often incidentally touch and move objects in subtle ways. These objects are not necessarily directly related to the task at hand, and the movement of an object might even be entirely unintentional. If another person is touching the object at the same time, the movement can transfer through the object and be experienced — however subtly — by the other person. For example, when one person hands a drink to another, at some point both individuals will be touching the glass, and consequently exerting small (often unnoticed) forces on the other person. Despite the frequency of such subtle incidental movements of shared objects in everyday interactions, few have examined how these movements affect human-virtual human (VH) interaction. We ran an experiment to assess how presence and social presence are affected when a person experiences subtle, incidental movement through a shared real-virtual object. We constructed a real-virtual room with a table that spanned the boundary between the real and virtual environments. The participant was seated on the real side of the table, which visually extended into the virtual world via a projection screen, and the VH was seated on the virtual side of the table. The two interacted by playing a game of “Twenty Questions,” where one player asked the other a series of 20 yes/no questions to deduce what object the other player was thinking about. During the game, the “wobbly” group of subjects experienced subtle incidental movements of the real-virtual table: the entire real-virtual table tilted slightly away/toward the subject when the virtual/real human leaned on it. The control group also played the same game, except the table did not wobble. Results indicate that the wobbly group had higher presence and social presence with the virtual human in general, with statistically significant increases in presence, co-presence, and attentional allocation. We present the experiment and results, and discuss some potential implications for virtual human systems and some potential future studies.},
  keywords={Electronic mail;Measurement by laser beam;Virtual environments;Games;Training;Haptic interfaces;Force;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology},
  doi={10.1109/VR.2016.7504683},
  ISSN={2375-5334},
  month={March},}

@inproceedings{10.1145/2858036.2858250,
author = {Nuernberger, Benjamin and Ofek, Eyal and Benko, Hrvoje and Wilson, Andrew D.},
title = {SnapToReality: Aligning Augmented Reality to the Real World},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858250},
doi = {10.1145/2858036.2858250},
abstract = {Augmented Reality (AR) applications may require the precise alignment of virtual objects to the real world. We propose automatic alignment of virtual objects to physical constraints calculated from the real world in real time ("snapping to reality"). We demonstrate SnapToReality alignment techniques that allow users to position, rotate, and scale virtual content to dynamic, real world scenes. Our proof-of-concept prototype extracts 3D edge and planar surface constraints. We furthermore discuss the unique design challenges of snapping in AR, including the user's limited field of view, noise in constraint extraction, issues with changing the view in AR, visualizing constraints, and more. We also report the results of a user study evaluating SnapToReality, confirming that aligning objects to the real world is significantly faster when assisted by snapping to dynamically extracted constraints. Perhaps more importantly, we also found that snapping in AR enables a fresh and expressive form of AR content creation.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {1233–1244},
numpages = {12},
keywords = {3D user interaction, augmented reality, interaction techniques, snapping, user studies},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3544548.3581449,
author = {Monteiro, Kyzyl and Vatsal, Ritik and Chulpongsatorn, Neil and Parnami, Aman and Suzuki, Ryo},
title = {Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581449},
doi = {10.1145/3544548.3581449},
abstract = {This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {459},
numpages = {15},
keywords = {Augmented Reality, Everyday Objects, Human-Centered Machine Learning;, Interactive Machine Teaching, Mixed Reality, Prototyping Tools, Tangible Interactions},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/2858036.2858043,
author = {M\"{u}ller, Jens and R\"{a}dle, Roman and Reiterer, Harald},
title = {Virtual Objects as Spatial Cues in Collaborative Mixed Reality Environments: How They Shape Communication Behavior and User Task Load},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858043},
doi = {10.1145/2858036.2858043},
abstract = {In collaborative activities, collaborators can use physical objects in their shared environment as spatial cues to guide each other's attention. Collaborative mixed reality environments (MREs) include both, physical and digital objects. To study how virtual objects influence collaboration and whether they are used as spatial cues, we conducted a controlled lab experiment with 16 dyads. Results of our study show that collaborators favored the digital objects as spatial cues over the physical environment and the physical objects: Collaborators used significantly less deictic gestures in favor of more disambiguous verbal references and a decreased subjective workload when virtual objects were present. This suggests adding additional virtual objects as spatial cues to MREs to improve user experience during collaborative mixed reality tasks.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {1245–1249},
numpages = {5},
keywords = {collaboration, mixed reality, virtual spatial cues},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3544548.3581243,
author = {Maddali, Hanuma Teja and Lazar, Amanda},
title = {Understanding Context to Capture when Reconstructing Meaningful Spaces for Remote Instruction and Connecting in XR},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581243},
doi = {10.1145/3544548.3581243},
abstract = {Recent technological advances are enabling HCI researchers to explore interaction possibilities for remote XR collaboration using high-fidelity reconstructions of physical activity spaces. However, creating these reconstructions often lacks user involvement with an overt focus on capturing sensory context that does not necessarily augment an informal social experience. This work seeks to understand social context that can be important for reconstruction to enable XR applications for informal instructional scenarios. Our study involved the evaluation of an XR remote guidance prototype by 8 intergenerational groups of closely related gardeners using reconstructions of personally meaningful spaces in their gardens. Our findings contextualize physical objects and areas with various motivations related to gardening and detail perceptions of XR that might affect the use of reconstructions for remote interaction. We discuss implications for user involvement to create reconstructions that better translate real-world experience, encourage reflection, incorporate privacy considerations, and preserve shared experiences with XR as a medium for informal intergenerational activities.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {275},
numpages = {18},
keywords = {3D reconstruction, Extended Reality, contextual capture, gardening, hobby activities, intergenerational study, metaverse, remote instruction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580704,
author = {He, Fengming and Hu, Xiyun and Shi, Jingyu and Qian, Xun and Wang, Tianyi and Ramani, Karthik},
title = {Ubi Edge: Authoring Edge-Based Opportunistic Tangible User Interfaces in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580704},
doi = {10.1145/3544548.3580704},
abstract = {Edges are one of the most ubiquitous geometric features of physical objects. They provide accurate haptic feedback and easy-to-track features for camera systems, making them an ideal basis for Tangible User Interfaces (TUI) in Augmented Reality (AR). We introduce Ubi Edge, an AR authoring tool that allows end-users to customize edges on daily objects as TUI inputs to control varied digital functions. We develop an integrated AR-device and an integrated vision-based detection pipeline that can track 3D edges and detect the touch interaction between fingers and edges. Leveraging the spatial-awareness of AR, users can simply select an edge by sliding fingers along it and then make the edge interactive by connecting it to various digital functions. We demonstrate four use cases including multi-function controllers, smart homes, games, and TUI-based tutorials. We also evaluated and proved our system’s usability through a two-session user study, where qualitative and quantitative results are positive.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {461},
numpages = {14},
keywords = {Augmented Reality, Tangible User Interface, immersive authoring},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{10937413,
  author={Hu, Xuning and Xu, Wenxuan and Wei, Yushi and Zhang, Hao and Huang, Jin and Liang, Hai-Ning},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimizing Moving Target Selection in VR by Integrating Proximity-Based Feedback Types and Modalities}, 
  year={2025},
  volume={},
  number={},
  pages={52-62},
  abstract={Proximity-based feedback provides users with real-time guidance as they approach an interaction goal. This type of feedback is particularly useful for tasks that require guidance during the interaction process, such as selecting moving targets. This work explores proximity-based feedback types and modalities to improve the selection of moving targets in VR by leveraging three feedback types that combine visual, auditory, and haptic modalities. We evaluated the performance of these mechanisms through two user studies, analyzing both objective data (e.g., selection time, error rate) and subjective data (e.g., user experience, preferences) to explore the characteristics of feedback types across different modalities and to examine the roles of various modalities within multimodal combinations. Our findings suggest optimal selection mechanisms for developers and should be tailored to different goals: achieving user precision, enabling quick movement to a target, considering task duration, and enhancing entertainment value. We also discuss applications that correspond to these different perspectives.},
  keywords={Visualization;Three-dimensional displays;Error analysis;Entertainment industry;Virtual reality;User interfaces;User experience;Real-time systems;Haptic interfaces;Virtual Reality;Moving target selection;Multi-modal interaction and perception;Feedback Mechanism},
  doi={10.1109/VR59515.2025.00030},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3491102.3517723,
author = {Lu, Feiyu and Xu, Yan},
title = {Exploring Spatial UI Transition Mechanisms with Head-Worn Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517723},
doi = {10.1145/3491102.3517723},
abstract = {Imagine in the future people comfortably wear augmented reality (AR) displays all day, how do we design interfaces that adapt to the contextual changes as people move around? In current operating systems, the majority of AR content defaults to staying at a fixed location until being manually moved by the users. However, this approach puts the burden of user interface (UI) transition solely on users. In this paper, we first ran a bodystorming design workshop to capture the limitations of existing manual UI transition approaches in spatially diverse tasks. Then we addressed these limitations by designing and evaluating three UI transition mechanisms with different levels of automation and controllability (low-effort manual, semi-automated, fully-automated). Furthermore, we simulated imperfect contextual awareness by introducing prediction errors with different costs to correct them. Our results provide valuable lessons about the trade-offs between UI automation levels, controllability, user agency, and the impact of prediction errors.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {550},
numpages = {16},
keywords = {adaptive interfaces, agency, automation, controllability},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@INPROCEEDINGS{9089433,
  author={Lu, Feiyu and Davari, Shakiba and Lisle, Lee and Li, Yuan and Bowman, Doug A.},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={930-939},
  abstract={Augmented reality head-worn displays (AR HWDs) have the potential to assist personal computing and the acquisition of everyday information. In this research, we propose Glanceable AR, an interaction paradigm for accessing information in AR HWDs. In Glanceable AR, secondary information resides at the periphery of vision to stay unobtrusive and can be accessed by a quick glance whenever needed. We propose two novel hands-free interfaces: "head-glance", in which virtual contents are fixed to the user’s body and can be accessed by head rotation, and "gaze-summon" in which contents can be "summoned" into central vision by eye-tracked gazing at the periphery. We compared these techniques with a baseline heads-up display (HUD), which we call "eye-glance" interface in two dual-task scenarios. We found that the head-glance and eye-glance interfaces are more preferred and more efficient than the gaze-summon interface for discretionary information access. For a continuous monitoring task, the eye-glance interface was preferred. We discuss the implications of our findings for designing Glanceable AR interfaces in AR HWDs.},
  keywords={Augmented reality;User interfaces;Human computer interaction;Head-mounted displays;Human-centered computing;Mixed / augmented reality;Human-centered computing;User interface design},
  doi={10.1109/VR46266.2020.00113},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9756757,
  author={Lee, Jaewook and Jin, Fanjie and Kim, Younsoo and Lindlbauer, David},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={User Preference for Navigation Instructions in Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={802-811},
  abstract={Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users’ visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users’ contexts.},
  keywords={Visualization;Three-dimensional displays;Navigation;Avatars;Multimedia systems;Mixed reality;User interfaces;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
  doi={10.1109/VR51125.2022.00102},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3491102.3502026,
author = {Qian, Jing and Sun, Qi and Wigington, Curtis and Han, Han L. and Sun, Tong and Healey, Jennifer and Tompkin, James and Huang, Jeff},
title = {Dually Noted: Layout-Aware Annotations with Smartphone Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502026},
doi = {10.1145/3491102.3502026},
abstract = {Sharing annotations encourages feedback, discussion, and knowledge passing among readers and can be beneficial for personal and public use. Prior augmented reality (AR) systems have expanded these benefits to both digital and printed documents. However, despite smartphone AR now being widely available, there is a lack of research about how to use AR effectively for interactive document annotation. We propose Dually Noted, a smartphone-based AR annotation system that recognizes the layout of structural elements in a printed document for real-time authoring and viewing of annotations. We conducted experience prototyping with eight users to elicit potential benefits and challenges within smartphone AR, and this informed the resulting Dually Noted system and annotation interactions with the document elements. AR annotation is often unwieldy, but during a 12-user empirical study our novel structural understanding component allows Dually Noted to improve precise highlighting and annotation interaction accuracy by 13\%, increase interaction speed by 42\%, and significantly lower cognitive load over a baseline method without document layout understanding. Qualitatively, participants commented that Dually Noted was a swift and portable annotation experience. Overall, our research provides new methods and insights for how to improve AR annotations for physical documents.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {552},
numpages = {15},
keywords = {Annotation, augmented reality, document interaction, layout structure, paper, smartphone, text},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3544548.3580752,
author = {James, Rapha\"{e}l and Bezerianos, Anastasia and Chapuis, Olivier},
title = {Evaluating the Extension of Wall Displays with AR for Collaborative Work},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580752},
doi = {10.1145/3544548.3580752},
abstract = {Wall displays are well suited for collaborative work and are often placed in rooms with ample space in front of them that remains largely unused. Augmented Reality (AR) headsets can seamlessly extend the collaboration space around the Wall. Nevertheless, it is unclear if extending Walls with AR is effective and how it may affect collaboration. We first present a prototype combining a Wall and AR headsets to extend the Wall workspace. We then use this prototype to study how users utilize the virtual space created in AR. In an experiment with 24 participants, we compare how pairs solve collaborative tasks with the Wall alone and with Wall+AR. Our qualitative and quantitative results highlight that with Wall+AR, participants use the physical space in front and around the Wall extensively, and while this creates interaction overhead, it does not impact performance and improves the user experience.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {99},
numpages = {17},
keywords = {Augmented Reality, Collaboration, Empirical Study, Wall Display},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{7504691,
  author={Tatzgern, Markus and Orso, Valeria and Kalkofen, Denis and Jacucci, Giulio and Gamberini, Luciano and Schmalstieg, Dieter},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Adaptive information density for augmented reality displays}, 
  year={2016},
  volume={},
  number={},
  pages={83-92},
  abstract={Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.},
  keywords={Clutter;Data visualization;Visualization;Browsers;Semantics;Electronic mail;Clustering algorithms},
  doi={10.1109/VR.2016.7504691},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{8797910,
  author={McNamara, Ann and Boyd, Katherine and George, Joanne and Suther, Annie and Jones, Weston and Oh, Somyung},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Information Placement in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1078-1079},
  abstract={In this poster, we develop a technique for placing informational labels in complex Virtual Environments (VEs). The ability to effectively and efficiently present labels in VEs is valuable in Virtual Reality (VR) for many reasons, but the motivation is to bring us closer to a system that delivers information in VR in an optimal way without causing information overload. The novelty of this technique lies in the use of eye tracking as an accurate indicator of attention to identifying objects of interest. Labels associated with such objects of interest are revealed when the user attends to them. We conduct a series of experiments to evaluate label placement based on user attention. This attention is measured using eye tracking. Results show that one method in particular results in 100% accuracy in some scenarios in a search task with little difference in time taken to complete the task.},
  keywords={Task analysis;Gaze tracking;Virtual reality;Information retrieval;Visualization;Three-dimensional displays;Resists;Human-centered computing;Virtual Reality;Information Placement;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797910},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3313831.3376550,
author = {Bai, Huidong and Sasikumar, Prasanth and Yang, Jing and Billinghurst, Mark},
title = {A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376550},
doi = {10.1145/3313831.3376550},
abstract = {Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {3d panorama, augmented reality, eye gaze, hand gesture, mixed reality, remote collaboration, scene reconstruction, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@INPROCEEDINGS{10108465,
  author={Bao, Yiwei and Wang, Jiaxi and Wang, Zhimin and Lu, Feng},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring 3D Interaction with Gaze Guidance in Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={22-32},
  abstract={Recent research based on hand-eye coordination has shown that gaze could improve object selection and translation experience under certain scenarios in AR. However, several limitations still exist. Specifically, we investigate whether gaze could help object selection with heavy 3D occlusions and help 3D object translation in the depth dimension. In addition, we also investigate the possibility of reducing the gaze calibration burden before use. Therefore, we develop new methods with proper gaze guidance for 3D interaction in AR, and also an implicit online calibration method. We conduct two user studies to evaluate different interaction methods and the results show that our methods not only improve the effectiveness of occluded objects selection but also alleviate the arm fatigue problem significantly in the depth translation task. We also evaluate the proposed implicit online calibration method and find its accuracy comparable to standard 9 points explicit calibration, which makes a step towards practical use in the real world.},
  keywords={Three-dimensional displays;Design methodology;Estimation;User interfaces;Fatigue;Calibration;Task analysis;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality},
  doi={10.1109/VR55154.2023.00018},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108413,
  author={Belani, Manshul and Singh, Harsh Vardhan and Parnami, Aman and Singh, Pushpendra},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating Spatial Representation of Learning Content in Virtual Reality Learning Environments}, 
  year={2023},
  volume={},
  number={},
  pages={33-43},
  abstract={A recent surge in the application of Virtual Reality in education has made VR Learning Environments (VRLEs) prevalent in fields ranging from aviation, medicine, and skill training to teaching factual and conceptual content. In spite of multiple 3D affordances provided by VR, learning content placement in VRLEs has been mostly limited to a static placement in the environment. We conduct two studies to investigate the effect of different spatial representations of learning content in virtual environments on learning outcomes and user experience. In the first study, we studied the effects of placing content at four different places - world-anchored (TV screen placed in the environment), user-anchored (panel anchored to the wrist or head-mounted display of the user) and object-anchored (panel anchored to the object associated with current content) - in the VR environment with forty-two participants in the context of learning how to operate a laser cutting machine through an immersive tutorial. In the follow-up study, twenty-two participants from this study were given the option to choose from these four placements to understand their preferences. The effects of placements were examined on learning outcome measures - knowledge gain, knowledge transfer, cognitive load, user experience, and user preferences. We found that participants preferred user-anchored (controller condition) and object-anchored placement. While knowledge gain, knowledge transfer, and cognitive load were not found to be significantly different between the four conditions, the object-anchored placement scored significantly better than the TV screen and head-mounted display conditions on the user experience scales of attractiveness, stimulation, and novelty.},
  keywords={Wrist;Three-dimensional displays;TV;Head-mounted displays;Virtual environments;Tutorials;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Interaction design-Empirical studies in interaction design},
  doi={10.1109/VR55154.2023.00019},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3313831.3376742,
author = {Osmers, Niklas and Prilla, Michael},
title = {Getting out of Out of Sight: Evaluation of AR Mechanisms for Awareness and Orientation Support in Occluded Multi-Room Settings},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376742},
doi = {10.1145/3313831.3376742},
abstract = {Augmented Reality can provide orientation and awareness in situations in which objects or people are occluded by physical structures. This is relevant for many situations in the workplace, where objects are scattered across rooms and people are out of sight. While several AR mechanisms have been proposed to provide awareness and orientation in these situations, little is known about their effect on people's performance when searching objects and coordinating with each other. In this paper, we compare three AR based mechanisms (map, x-ray, compass) according to their utility, usability, social presence, task load and users' preferences. 48 participants had to work together in groups of four to find people and objects located around different rooms. Results show that map and x-ray performed best but provided least social presence among participants. We discuss these and other observations as well as potential impacts on designing AR awareness and orientation support.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {ar, awareness, cooperation, coordination, occlusion, orientation support, social presence},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@INPROCEEDINGS{8797966,
  author={Norman, Mitchell and Lee, Gun and Smith, Ross T. and Billinqhurs, Mark},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Mixed Presence Collaborative Mixed Reality System}, 
  year={2019},
  volume={},
  number={},
  pages={1106-1107},
  abstract={Research has shown that Mixed Presence Groupware (MPG) systems are a valuable collaboration tool. However research into MPG systems is limited to a handful of tabletop and Virtual Reality (VR) systems with no exploration of Head-Mounted Display (HMD) based Augmented Reality (AR) solutions. We present a new system with two local users and one remote user using HMD based AR interfaces. Our system provides tools allowing users to layout a room with the help of a remote user. The remote user has access to a marker and pointer tools to assist in directing the local users. Feedback collected from several groups of users showed that our system is easy to learn but could have increased accuracy and consistency.},
  keywords={Collaboration;Resists;Prototypes;Tools;Augmented reality;Webcams;Augmented Reality;remote collaboration;mixed presence;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces;Collaborative computing;H.l.2 [Models and Principles]: User/Machine Systems;Human Factors},
  doi={10.1109/VR.2019.8797966},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3290605.3300458,
author = {Piumsomboon, Thammathip and Lee, Gun A. and Irlitti, Andrew and Ens, Barrett and Thomas, Bruce H. and Billinghurst, Mark},
title = {On the Shoulder of the Giant: A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300458},
doi = {10.1145/3290605.3300458},
abstract = {We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {live panorama sharing, mixed reality, multi-scale, remote collaboration, tangible user interface, wearable interface},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@INPROCEEDINGS{7892321,
  author={Li, Gang and Liu, Yue and Wang, Yongtian},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={Evaluation of labelling layout methods in augmented reality}, 
  year={2017},
  volume={},
  number={},
  pages={351-352},
  abstract={View management techniques are commonly used for labelling of objects in augmented reality environments. Combining with image analysis, search space and adaptive representations, they can be utilized to achieve desired labelling tasks. However, the evaluation of different search space methods on labelling are still an open problem. In this paper, we propose an image analysis based view management method, which first adopts the image processing to superimpose 2D labels to the specific object. We then conduct three search space methods to an augmented reality scenario. Without the requirements of setting rules and constraints for occlusion among the labels, the results of three search space methods are evaluated by using objective analysis of related parameters. The evaluation results indicate that different search space methods could generate different time costs and occlusion, thereby affecting the final labelling effects.},
  keywords={Labeling;Augmented reality;Search problems;Layout;Cameras;Two dimensional displays;Image edge detection;Augmented reality;labelling;search space},
  doi={10.1109/VR.2017.7892321},
  ISSN={2375-5334},
  month={March},}

@inproceedings{10.1145/3706598.3714056,
author = {Gil, Hyunjae and Pratap, Ashish and Joseph, Iniyan and Kim, Jin Ryong},
title = {PropType: Everyday Props as Typing Surfaces in Augmented Reality},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714056},
doi = {10.1145/3706598.3714056},
abstract = {We introduce PropType, an interactive interface that transforms everyday objects into typing surfaces within an Augmented Reality (AR) environment. Users can interact with nearby props, such as cups, water bottles, boxes, and various other objects, utilizing them as on-the-go keyboards. To develop PropType, we conducted three studies. The first study involved observing users to understand how they naturally engage with prop surfaces for typing. The second study assessed the reachability and efficiency of touch input across four props with different sizes and shapes. Based on these insights, we designed customized keyboard layouts for each prop. In the third study, we evaluated typing performance using PropType, achieving an average typing speed of up to 26.1 words per minute (WPM) with 2.2\% corrected error rate (CER) and 1.1\% uncorrected error rate (UER). Finally, we present a PropType editing tool that allows users to customize keyboard layouts and visual effects for prop-based typing.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {730},
numpages = {18},
keywords = {Everyday objects, text entry, object typing, object interaction, augmented reality, mixed reality},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{8797988,
  author={Mardanbegi, Diako and Mayer, Benedikt and Pfeuffer, Ken and Jalaliniya, Shahram and Gellersen, Hans and Perzl, Alexander},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={EyeSeeThrough: Unifying Tool Selection and Application in Virtual Environments}, 
  year={2019},
  volume={},
  number={},
  pages={474-483},
  abstract={In 2D interfaces, actions are often represented by fixed tools arranged in menus, palettes, or dedicated parts of a screen, whereas 3D interfaces afford their arrangement at different depths relative to the user and the user can move them relative to each other. In this paper, we introduce EyeSeeThrough as a novel interaction technique that utilizes eye-tracking in VR. The user can apply an action to an intended object by visually aligning the object with the tool at the line-of-sight, and then issue a confirmation command. The underlying idea is to merge the two-step process of 1) selection of a mode in a menu and 2) applying it to a target, into one unified interaction. We present a user study where we compare the method to the baseline two-step selection. The results of our user study showed that our technique outperforms the two step selection in terms of speed and comfort. We further developed a prototype of a virtual living room to demonstrate the practicality of the proposed technique.},
  keywords={Tools;Visualization;Three-dimensional displays;Task analysis;User interfaces;Space exploration;Virtual environments;Human-centered computing—Human-centered-computing—Gestural input},
  doi={10.1109/VR.2019.8797988},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8798358,
  author={Huynh, Brandon and Orlosky, Jason and Höllerer, Tobias},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={In-Situ Labeling for Augmented Reality Language Learning}, 
  year={2019},
  volume={},
  number={},
  pages={1606-1611},
  abstract={Augmented Reality is a promising interaction paradigm for learning applications. It has the potential to improve learning outcomes by merging educational content with spatial cues and semantically relevant objects within a learner's everyday environment. The impact of such an interface could be comparable to the method of loci, a well known memory enhancement technique used by memory champions and polyglots. However, using Augmented Reality in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is a significant challenge, and interaction with arbitrary (unmodeled) physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a framework for in-situ object labeling and selection in Augmented Reality, with a particular focus on language learning applications. Our framework uses a generalized object recognition model to identify objects in the world in real time, integrates eye tracking to facilitate selection and interaction within the interface, and incorporates a personalized learning model that dynamically adapts to student's growth. We show our current progress in the development of this system, including preliminary tests and benchmarks. We explore challenges with using such a system in practice, and discuss our vision for the future of AR language learning applications.},
  keywords={Three-dimensional displays;Real-time systems;Two dimensional displays;Labeling;Augmented reality;Object recognition;Cameras;Human-centered computing;Mixed and augmented reality;Theory and algorithms for application domains;Semi-supervised learning},
  doi={10.1109/VR.2019.8798358},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108437,
  author={Wang, Junyi and Qi, Yue},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Simultaneous Scene-independent Camera Localization and Category-level Object Pose Estimation via Multi-level Feature Fusion}, 
  year={2023},
  volume={},
  number={},
  pages={254-264},
  abstract={In AR/MR applications, camera localization and object pose estimation both play crucial roles. The universality of learning techniques, often referred to as scene-independent localization and category-level pose estimation, presents challenges for both tasks. The two missions maintain close relationships due to the spatial geometry constraint, but differing task requirements result in distinct feature extraction. In this paper, we focus on simultaneous scene-independent camera localization and category-level object pose estimation with a unified learning framework. The system consists of a localization branch called SLO-LocNet, a pose estimation branch called SLO-ObjNet, a feature fusion module for feature sharing between two tasks, and two decoders for creating coordinate maps. In SLO-LocNet, localization features are produced for anticipating the relative pose between two adjusted frames using inputs of color and depth images. Furthermore, we establish an image fusion module in order to promote feature sharing in depth and color branches. With SLO-ObjNet, we take the detected depth image and its corresponding point cloud as inputs, and produce object pose features for pose estimation. A geometry fusion module is created to combine depth and point cloud information simultaneously. Between the two tasks, the image fusion module is also exploited to accomplish feature sharing. In terms of the loss function, we present a mixed optimization function that is composed of the relative camera pose, geometry constraint, absolute and relative object pose terms. To verify how well our algorithm could perform, we conduct experiments on both localization and pose estimation datasets, covering 7 Scenes, ScanNet, REAL275 and YCB-Video. All experiments demonstrate superior performance to other existing methods. We specifically train the network on ScanNet and test it on 7 Scenes to demonstrate the universality performance. Additionally, the positive effects of fusion modules and loss function are also demonstrated.},
  keywords={Location awareness;Point cloud compression;Geometry;Image color analysis;Pose estimation;Cameras;Feature extraction;Scene-independent camera localization;Cagetory-level object pose estimation;Feature fusion;Multi-task learning;Geometry constraint},
  doi={10.1109/VR55154.2023.00041},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9417801,
  author={Liu, Jingjing and Liane, Wei and Ning, Bing and Mao, Ting},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Work Surface Arrangement Optimization Driven by Human Activity}, 
  year={2021},
  volume={},
  number={},
  pages={270-278},
  abstract={In this paper, we aim at guiding people to accomplish a personalized task, work surface organizing, in mixed reality environment, which can also be applied to intelligent robots. Through the cameras mounted in a MR device, e.g., Hololens, we firstly capture a person's daily activities in real scene when he uses the work surface. From such activities, we model the individual behavior habits and apply them to optimize the arrangement of the work surface. A cost function is defined for the optimization, considering general arrangement rules and human habitual behavior. The optimized arrangement is suggested to the user by augmenting the virtual arrangement on the real scene. To evaluate the effectiveness of our approach, we conducted experiments on a variety of scenes.},
  keywords={Solid modeling;Three-dimensional displays;Robot vision systems;Mixed reality;Virtual reality;User interfaces;Cost function;Human-centered Design;Mixed Reality;Work Surface Design;Remodeling},
  doi={10.1109/VR50410.2021.00049},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{7504721,
  author={Kaneto, Yuki and Komuro, Takashi},
  booktitle={2016 IEEE Virtual Reality (VR)}, 
  title={Space-sharing AR interaction on multiple mobile devices with a depth camera}, 
  year={2016},
  volume={},
  number={},
  pages={197-198},
  abstract={In this paper, we propose a markerless augmented reality (AR) system that works on multiple mobile devices. The relative positions and orientations of the devices and their individual motions are estimated from 3D information in real space obtained by depth cameras attached to the devices. The system allows multiple users to share the AR space and to interact with the same virtual object. To estimate the relative positions and orientations of the devices, the system generates 2D images by looking down from above at the 3D scene obtained by the depth cameras, performs 2D registration using template matching, and obtains a transformation matrix that transforms the coordinate system of one camera to that of another camera. The motion of a camera is estimated using the ICP algorithm to realize markerless AR. Using the proposed system, we created an application that enables multiple users to interact with the same virtual object.},
  keywords={Cameras;Three-dimensional displays;Augmented reality;Mobile handsets;Performance evaluation;Iterative closest point algorithm;Feature extraction;Mobile AR;markerless AR;depth camera;registration;3D interaction},
  doi={10.1109/VR.2016.7504721},
  ISSN={2375-5334},
  month={March},}

@inproceedings{10.1145/3290605.3300838,
author = {Sousa, Maur\'{\i}cio and dos Anjos, Rafael Kufner and Mendes, Daniel and Billinghurst, Mark and Jorge, Joaquim},
title = {Warping Deixis: Distorting Gestures to Enhance Collaboration},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300838},
doi = {10.1145/3290605.3300838},
abstract = {When engaged in communication, people often rely on pointing gestures to refer to out-of-reach content. However, observers frequently misinterpret the target of a pointing gesture. Previous research suggests that to perform a pointing gesture, people place the index finger on or close to a line connecting the eye to the referent, while observers interpret pointing gestures by extrapolating the referent using a vector defined by the arm and index finger. In this paper we present Warping Deixis, a novel approach to improving the perception of pointing gestures and facilitate communication in collaborative Extended Reality environments. By warping the virtual representation of the pointing individual, we are able to match the pointing expression to the observer's perception. We evaluated our approach in a co-located side by side virtual reality scenario. Results suggest that our approach is effective in improving the interpretation of pointing gestures in shared virtual environments.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {body warping, collaboration, deixis, pointing gestures},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@INPROCEEDINGS{10937415,
  author={Ahmed Tamboli, Danish Nisar and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Raveendranath, Balagopal and Woodward, Julia and Wang, Isaac and Smith, Jesse and Ruiz, Jaime},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={May The Force be With You: Cloning Distant Objects to Improve Medium-Field Interactions in Augmented Reality}, 
  year={2025},
  volume={},
  number={},
  pages={472-482},
  abstract={Augmented Reality (AR) interactions feature users interacting with virtual objects registered in the physical world. With contemporary AR experiences increasingly featuring interactions at distances, we conceptualized The Force, a technique that allows users to clone distant objects and manipulate their replicas. An empirical evaluation was conducted, comparing it against two well-established techniques including controller-based ray-casting and a gaze-based pinching technique in a pick-and-place task. We employed a within-subjects design, collecting data on both objective performance and subjective user experience. Results suggest that The Force allows for higher levels of accuracy and efficiency in medium-field tasks that require precision and fine motor control. Furthermore, we discovered avenues towards iteratively refining this technique. We go on to discuss the implications of our findings in an effort to facilitate better interactions in augmented reality.},
  keywords={Motor drives;Three-dimensional displays;Accuracy;Force;Refining;Cloning;User interfaces;User experience;Augmented reality;Augmented Reality;Interaction Techniques;3D User Interaction;Distant Object Manipulation;Cloning},
  doi={10.1109/VR59515.2025.00071},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108496,
  author={Alghofaili, Rawan and Nguyen, Cuong and Krs, Vojtĕch and Carr, Nathan and Mĕch, Radomír and Yu, Lap-Fai},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={WARPY: Sketching Environment-Aware 3D Curves in Mobile Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={367-377},
  abstract={Three-dimensional curve drawing in Augmented Reality (AR) enables users to create 3D curves that fit within the real-world scene. It has applications in 3D design, sculpting, and animation. However, the task complexity increases when the desirable path for the curve is obstructed by the physical environment or by what the camera can see. For example, it is difficult to draw a curve that wraps around an object or scales to out-of-reach places. We propose WARPY, an environment-aware 3D curve drawing tool for mobile AR. Our system enables users to draw freeform curves from a distance in AR by combining 2D-to-3D sketch inference with geometric proxies. Geometric Proxies can be obtained via 3D scanning or from a list of pre-defined primitives. WARPY also provides a multi-view mode to enable users to sketch a curve from multiple viewpoints, which is useful if the target curve cannot fit within the camera's field of view. We conducted two user studies and found that WARPY can be a viable tool to help users create complex and large curves in AR.},
  keywords={Geometry;Three-dimensional displays;Spirals;Shape;User interfaces;Cameras;Animation;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality},
  doi={10.1109/VR55154.2023.00052},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8797992,
  author={Klose, Elisa Maria and Mack, Nils Adrian and Hegenberg, Jens and Schmidt, Ludger},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text Presentation for Augmented Reality Applications in Dual-Task Situations}, 
  year={2019},
  volume={},
  number={},
  pages={636-644},
  abstract={We investigate how reading text in augmented reality (AR) glasses and the simultaneous execution of three real-world tasks interfere with each other. The three tasks are a visual stimulus-response task (VSRT), a simple walking task and a walking obstacle course. Also, we investigate the effects of different AR text positions on primary task and reading performance as well as subjective preference. We propose a novel out of sight body-locked text placement for AR text presentation to be used in dual-task situations and compare it to head-locked text placement, each in two heights. AR reading affected performance in all tasks and reading speed was affected in all dual-task conditions. Participants subjectively preferred the body-locked text presentation, while objective measures do not reflect that preference. Differences between the tasks and several interaction effects between task and AR text placement demonstrate the necessity to carefully consider the context of use when designing AR reading UIs. The presented study with 12 participants provides insights into the effects of AR glasses usage in dual-task situations and several design recommendations are derived from the results.},
  keywords={Task analysis;Glass;Legged locomotion;Augmented reality;Visualization;Smart phones;body-locked;head-locked;dual task;user study;text presentation;Augmented Reality;Human-Computer Interaction;Human Factors},
  doi={10.1109/VR.2019.8797992},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9417638,
  author={Yu, Kevin and Winkler, Alexander and Pankratz, Frieder and Lazarovici, Marc and Wilhelm, Dirk and Eck, Ulrich and Roth, Daniel and Navab, Nassir},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Magnoramas: Magnifying Dioramas for Precise Annotations in Asymmetric 3D Teleconsultation}, 
  year={2021},
  volume={},
  number={},
  pages={392-401},
  abstract={When users create hand-drawn annotations in Virtual Reality they often reach their physical limits in terms of precision, especially if the region to be annotated is small. One intuitive solution employs magnification beyond natural scale. However, scaling the whole environment results in wrong assumptions about the coherence between physical and virtual space. In this paper, we introduce Mag-noramas, a novel interaction method for selecting and extracting a region of interest that the user can subsequently scale and transform inside the virtual space. Our technique enhances the user's capabilities to perform supernaturally precise virtual annotations on virtual objects. We explored our technique in a user study within asimplified clinical scenario of a teleconsultation-supported craniectomy procedure that requires accurate annotations on a human head. Teleconsultation was performed asymmetrically between a remote expert in Virtual Reality that collaborated with a local user through Augmented Reality. The remote expert operates inside a reconstructed environment, captured from RGB-D sensors at the local site, and is embodied by an avatar to establish co-presence. The results show that Magnoramas significantly improve the precision of annotations while preserving usability and perceived presence measures compared to the baseline method. By hiding the 3D reconstruction while keeping the Magnorama, users can intentionally choose to lower their perceived social presence and focus on their tasks.},
  keywords={Manifolds;Three-dimensional displays;Telepresence;Annotations;Transforms;User interfaces;Real-time systems;Interaction Techniques;Medical Information System;Virtual Reality},
  doi={10.1109/VR50410.2021.00062},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3613904.3642859,
author = {Bensch, Leonie and Nilsson, Tommy and Wulkop, Jan and Demedeiros, Paul and Herzberger, Nicolas Daniel and Preutenborbeck, Michael and Gerndt, Andreas and Flemisch, Frank and Dufresne, Florian and Albuquerque, Georgia and Cowley, Aidan},
title = {Designing for Human Operations on the Moon: Challenges and Opportunities of Navigational HUD Interfaces},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642859},
doi = {10.1145/3613904.3642859},
abstract = {Future crewed missions to the Moon will face significant environmental and operational challenges, posing risks to the safety and performance of astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs) have proven effective in providing intuitive navigational support on Earth, the design of novel human-spaceflight solutions typically relies on costly and time-consuming analogue deployments, leaving the potential use of lunar HUDs largely under-explored. This paper explores an alternative approach by simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR) representation of the lunar environment. In evaluating these concepts with astronauts and other aerospace experts (n=25), our mixed methods study demonstrates the efficacy of simulated analogues in facilitating rapid design assessments of early-stage HUD solutions. We illustrate this by elaborating key design challenges and guidelines for future lunar HUDs. In reflecting on the limitations of our approach, we propose directions for future design exploration of human-machine interfaces for the Moon.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {718},
numpages = {21},
keywords = {astronaut, augmented reality, head-up display, human factors, human space flight, human-system exploration, lunar exploration, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3706598.3713348,
author = {Shi, Jingyu and Jain, Rahul and Chi, Seunggeun and Doh, Hyungjun and Chi, Hyung-gun and Quinn, Alexander J. and Ramani, Karthik},
title = {CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction through Generative Artificial Intelligence},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713348},
doi = {10.1145/3706598.3713348},
abstract = {Context-aware AR instruction enables adaptive and in-situ learning experiences. However, hardware limitations and expertise requirements constrain the creation of such instructions. With recent developments in Generative Artificial Intelligence (Gen-AI), current research tries to tackle these constraints by deploying AI-generated content (AIGC) in AR applications. However, our preliminary study with six AR practitioners revealed that the current AIGC lacks contextual information to adapt to varying application scenarios and is therefore limited in authoring. To utilize the strong generative power of GenAI to ease the authoring of AR instruction while capturing the context, we developed CARING-AI, an AR system to author context-aware humanoid-avatar-based instructions with GenAI. By navigating in the environment, users naturally provide contextual information to generate humanoid-avatar animation as AR instructions that blend in the context spatially and temporally. We showcased three application scenarios of CARING-AI: Asynchronous Instructions, Remote Instructions, and Ad Hoc Instructions based on a design space of AIGC in AR Instructions. With two user studies (N=12), we assessed the system usability of CARING-AI and demonstrated the easiness and effectiveness of authoring with Gen-AI.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {29},
numpages = {23},
keywords = {Augmented Reality, Generative Artificial Intelligence},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713925,
author = {Li, Jingyu and Yang, Qingwen and Xu, Kenuo and Zhang, Yang and Xu, Chenren},
title = {EchoSight: Streamlining Bidirectional Virtual-physical Interaction with In-situ Optical Tethering},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713925},
doi = {10.1145/3706598.3713925},
abstract = {Emerging AR applications require seamless integration of the virtual and physical worlds, which calls for tools that support both passive perception and active manipulation of the environment, enabling bidirectional interaction. We introduce EchoSight, a system for AR glasses that enables efficient look-and-control bidirectional interaction. EchoSight exploits optical wireless communication to instantaneously connect virtual data with its physical counterpart. EchoSight’s unique dual-element optical design leverages beam directionality to automatically align the user’s focus with target objects, reducing the overhead in both target identification and subsequent communication. This approach streamlines user interaction, reducing cognitive load and enhancing engagement. Our evaluations demonstrate EchoSight’s effectiveness for room-scale communication, achieving distances up to 5 m and viewing angles up to 120 degrees. A study with 12 participants confirms EchoSight’s improved efficiency and user experience over traditional methods, such as QR Code scanning and voice control, in AR IoT applications.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {31},
numpages = {18},
keywords = {Augmented Reality, Bidirectional Interaction, Optical Wireless Communication, Intuitive Interface, Backscatters},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713528,
author = {Zhang, Hongbo and Chen, Pei and Yang, Jingwen and Wu, Yifei and Jiang, Zhaoqu and Xie, Xuelong and You, Weitao and Sun, Lingyun},
title = {IEDS: Exploring an Intelli-Embodied Design Space Combining Designer, AR, and GAI to Support Industrial Conceptual Design},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713528},
doi = {10.1145/3706598.3713528},
abstract = {Conceptual design is an important stage in industrial product development, influenced by the design space and materials available to designers. Advancements in human-computer interaction&nbsp;(HCI) and artificial intelligence&nbsp;(AI) technologies have broadened these aspects considerably. On the one hand, augmented reality&nbsp;(AR) technologies merge physical and virtual representations to enhance intuitive interaction and embodied cognition. On the other hand, generative artificial intelligence&nbsp;(GAI) serves as a novel design material, boosting creativity and productivity. Inspired by these technological strides, we proposed an Intelli-Embodied Design Space&nbsp;(IEDS), which integrates designers, AR, and GAI to support industrial conceptual design by combining embodied interaction with generative variability. Within IEDS, designers can interact with the physical prototypes intuitively, while GAI refines these into virtual forms that can be embedded in the physical world through AR technology. In this study, we established the theoretical framework and interaction modes of IEDS through literature reviews and expert interviews. Subsequently, we designed and implemented three GAI+AR tools, GAI + Head-mounted Display&nbsp;(HMD), GAI + Handheld Display&nbsp;(HHD), and GAI + Spatial Augmented Reality&nbsp;(SAR), based on three AR approaches in IEDS to practically examine the benefits and challenges of these interaction modes across industrial conceptual design tasks. We discussed IEDS’s influence on industrial conceptual design and released its application guidelines to the HCI community.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {33},
numpages = {25},
keywords = {conceptual design, augmented reality, generative AI},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3544548.3581444,
author = {Johnson, Janet G and Sharkey, Tommy and Butarbutar, Iramuali Cynthia and Xiong, Danica and Huang, Ruijie and Sy, Lauren and Weibel, Nadir},
title = {UnMapped: Leveraging Experts’ Situated Experiences to Ease Remote Guidance in Collaborative Mixed Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581444},
doi = {10.1145/3544548.3581444},
abstract = {Collaborative Mixed Reality (MR) systems that help extend expertise for physical tasks to remote environments often situate experts in an immersive view of the task environment to bring the collaboration closer to collocated settings. In this paper, we design UnMapped, an alternative interface for remote experts that combines a live 3D view of the active space within the novice’s environment with a static 3D recreation of the expert’s own workspace to leverage their existing spatial memories within it. We evaluate the impact of this approach on single and repeated use of collaborative MR systems for remote guidance through a comparative study. Our results indicate that despite having a limited understanding of the novice’s environment, using an UnMapped interface increased performance and communication efficiency while reducing experts’ task load. We also outline the various affordances of providing remote experts with a familiar and spatially-stable environment to assist novices.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {878},
numpages = {20},
keywords = {Augmented Reality, Mixed Reality, Physical Tasks, Remote Collaboration, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{8797891,
  author={McNamara, Ann and Boyd, Katherine and George, Joanne and Jones, Weston and Oh, Somyung and Suther, Annie},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Information Placement in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1765-1769},
  abstract={In this paper, we develop a technique for placing informational labels in complex Virtual Environments (VEs). The ability to effectively and efficiently present labels in VEs is valuable in Virtual Reality (VR) for many reasons, but the motivation is to bring us closer to a system that delivers information in VR in an optimal way without causing information overload. The novelty of this technique lies in the use of eye tracking as an accurate indicator of attention to identifying objects of interest. Labels associated with such objects of interest are revealed when the user attends to them. We conduct a series of experiments to evaluate label placement based on user attention. This investigation is a first step toward integrating attention into information placement strategies in VR.},
  keywords={Task analysis;Gaze tracking;Visualization;Clutter;Information retrieval;Virtual environments;Human-centered computing—Virtual Reality—Information Placement;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR.2019.8797891},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3491102.3517593,
author = {Huang, Ann and Knierim, Pascal and Chiossi, Francesco and Chuang, Lewis L and Welsch, Robin},
title = {Proxemics for Human-Agent Interaction in Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517593},
doi = {10.1145/3491102.3517593},
abstract = {Augmented Reality (AR) embeds virtual content in physical spaces, including virtual agents that are known to exert a social presence on users. Existing design guidelines for AR rarely consider the social implications of an agent’s personal space (PS) and that it can impact user behavior and arousal. We report an experiment (N=54) where participants interacted with agents in an AR art gallery scenario. When participants approached six virtual agents (i.e., two males, two females, a humanoid robot, and a pillar) to ask for directions, we found that participants respected the agents’ PS and modulated interpersonal distances according to the human-like agents’ perceived gender. When participants were instructed to walk through the agents, we observed heightened skin-conductance levels that indicate physiological arousal. These results are discussed in terms of proxemic theory that result in design recommendations for implementing pervasive AR experiences with virtual agents.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {421},
numpages = {13},
keywords = {Augmented Reality, Human-Agent Interaction, Perception, Personal Space, Proxemics, Virtual Agents},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@INPROCEEDINGS{8798061,
  author={Techasarntikul, Nattaon and Mashita, Tomohiro and Ratsamee, Photchara and Uranishi, Yuki and Takemura, Haruo and Orlosky, Jason and Kiyokawa, Kiyoshi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluation of Pointing Interfaces with an AR Agent for Multi-section Information Guidance}, 
  year={2019},
  volume={},
  number={},
  pages={1185-1186},
  abstract={In educational settings such as art galleries or museums, Augmented Reality (AR) has the potential to provide detailed information about exhibits. However, dealing with items that contain information in multiple sections or areas is still a significant challenge. For example, a large painting may contain many minute details, which requires a system that can explain its broader features rather than just a generic description. To address this challenge, we introduce an AR guidance system that uses an embodied agent to point out items and explain each piece and part of exhibit items in detail. We also designed and tested 3 different pointing interfaces for the embodied agent: gesture only, gesture with a dot laser, and gesture with line laser. To evaluate this interface, we conducted a user experiment simulating painting guidance to test interest and exhibit memory. During the experiment, the agent pointed to various areas of interest in the painting and provided a detailed description to participants. The result shows that the search times for target positions were the fastest with the line laser. However, no particular interface outperformed others in memory recall of exhibit content.},
  keywords={Painting;Tracking;Lasers;Augmented reality;Art;Conferences;Human-centered computing;User interface design;Computing methodologies;Mixed / augmented reality;Information systems;Retrieval effectiveness},
  doi={10.1109/VR.2019.8798061},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8798128,
  author={Teo, Theophilus and Lee, Gun A and Billinghurst, Mark and Adcock, Matt},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Supporting Visual Annotation Cues in a Live 360 Panorama-based Mixed Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={1187-1188},
  abstract={We propose enhancing live 360 panorama-based Mixed Reality (MR) remote collaboration through supporting visual annotation cues. Prior work on live 360 panorama-based collaboration used MR visualization to overlay visual cues, such as view frames and virtual hands, yet they were not registered onto the shared physical workspace, hence had limitations in accuracy for pointing or marking objects. Our prototype system uses spatial mapping and tracking feature of an Augmented Reality head-mounted display to show visual annotation cues accurately registered onto the physical environment. We describe the design and implementation details of our prototype system, and discuss on how such feature could help improve MR remote collaboration.},
  keywords={Visualization;Collaboration;Resists;Three-dimensional displays;Prototypes;Augmented reality;Mixed Reality;remote collaboration;360 panorama;annotation;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Collaborative computing;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798128},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108490,
  author={Lee, Hyunjin and Woo, Woontack},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Exploring the Effects of Augmented Reality Notification Type and Placement in AR HMD while Walking}, 
  year={2023},
  volume={},
  number={},
  pages={519-529},
  abstract={Augmented reality (AR) helps users easily accept information when they are walking by providing virtual information in front of their eyes. However, it remains unclear how to present AR notifications considering the expected user reaction to interruption. Therefore, we investigated to confirm appropriate placement methods for each type by dividing it into notification types that are handled immediately (high) or that are performed later (low). We compared two coordinate systems (display-fixed and body-fixed) and three positions (top, right, and bottom) for the notification placement. We found significant effects of notification type and placement on how notifications are perceived during the AR notification experience. Using a display-fixed coordinate system responded faster for high notification types, whereas using a body-fixed coordinate system resulted in quick walking speed for low ones. As for the position, the high types had a higher notification performance at the bottom position, but the low types had enhanced walking performance at the right position. Based on the finding of our experiment, we suggest some recommendations for the future design of AR notification while walking.},
  keywords={Legged locomotion;Three-dimensional displays;Design methodology;Resists;User interfaces;Time factors;Task analysis;Human-centered computing-Human computer in-teraction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Ubiquitous and mobile computing-Ubiquitous and mobile computing design and evaluation methods},
  doi={10.1109/VR55154.2023.00067},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108461,
  author={Liao, Shuqi and Zhou, Yuqi and Popescu, Voicu},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={AR Interfaces for Disocclusion—A Comparative Study}, 
  year={2023},
  volume={},
  number={},
  pages={530-540},
  abstract={An important application of augmented reality (AR) is the design of interfaces that reveal parts of the real world to which the user does not have line of sight. The design space for such interfaces is vast, with many options for integrating the visualization of the occluded parts of the scene into the user's main view. This paper compares four AR interfaces for disocclusion: X-ray, Cutaway, Picture-in-picture, and Multiperspective. The interfaces are compared in a within-subjects study (N = 33) over four tasks: counting dynamic spheres, pointing to the direction of an occluded person, finding the closest object to a given object, and finding pairs of matching numbers. The results show that Cutaway leads to poor performance in tasks where the user needs to see both the occluder and the occludee; that Picture-in-picture and Multiperspective have a visualization comprehensiveness advantage over Cutaway and X-ray, but a disadvantage in terms of directional guidance; that X-ray has a task completion time disadvantage due to the visualization complexity; and that participants gave Cutaway and Picture-in-picture high, and Multiperspective and X-ray low usability scores.},
  keywords={Visualization;Three-dimensional displays;User interfaces;Complexity theory;Task analysis;Usability;Augmented reality;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality},
  doi={10.1109/VR55154.2023.00068},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8798018,
  author={Lang, Yining and Liang, Wei and Yu, Lap-Fai},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Virtual Agent Positioning Driven by Scene Semantics in Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={767-775},
  abstract={When a user interacts with a virtual agent via a mixed reality device, such as a Hololens or a Magic Leap headset, it is important to consider the semantics of the real-world scene in positioning the virtual agent, so that it interacts with the user and the objects in the real world naturally. Mixed reality aims to blend the virtual world with the real world seamlessly. In line with this goal, in this paper, we propose a novel approach to use scene semantics to guide the positioning of a virtual agent. Such considerations can avoid unnatural interaction experiences, e.g., interacting with a virtual human floating in the air. To obtain the semantics of a scene, we first reconstruct the 3D model of the scene by using the RGB-D cameras mounted on the mixed reality device (e.g., a Hololens). Then, we employ the Mask R-CNN object detector to detect objects relevant to the interactions within the scene context. To evaluate the positions and orientations for placing a virtual agent in the scene, we define a cost function based on the scene semantics, which comprises a visibility term and a spatial term. We then apply a Markov chain Monte Carlo optimization technique to search for an optimized solution for placing the virtual agent. We carried out user study experiments to evaluate the results generated by our approach. The results show that our approach achieved a higher user evaluation score than that of the alternative approaches.},
  keywords={Virtual reality;Semantics;Three-dimensional displays;Solid modeling;Optimization;Geometry;Cameras;Mixed Reality—Scene Understanding—Virtual Agent Positioning},
  doi={10.1109/VR.2019.8798018},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3290605.3300676,
author = {Lilija, Klemen and Pohl, Henning and Boring, Sebastian and Hornb\ae{}k, Kasper},
title = {Augmented Reality Views for Occluded Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300676},
doi = {10.1145/3290605.3300676},
abstract = {We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {augmented reality, finger-camera, manipulation task},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@INPROCEEDINGS{10108457,
  author={Truong-Allié, Camille and Herbeth, Martin and Paljic, Alexis},
  booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={A study of the influence of AR on the perception, comprehension and projection levels of situation awareness}, 
  year={2023},
  volume={},
  number={},
  pages={541-551},
  abstract={In this work, we examine how Augmented Reality (AR) impacts user's situation awareness (SA) on elements secondary to an AR-assisted main task, i.e. not directly concerned by the main task. These secondary elements can still provide relevant information that we do not want the user to miss. A good understanding of user's awareness about them is therefore interesting, especially in a context of a daily use of AR, in which not all elements of user's environment are controlled. In this regard, we measured SA about secondary elements in an industrial workshop where the AR-assisted main task is a pedestrian navigation. We compared SA between three navigation guidance conditions: a paper map, a virtual path, and a virtual path with virtual cues about secondary elements. These secondary elements were either hazardous areas, for example, for mandatory helmets, or items which could be on user's path, for example, misplaced carts, boxes… We adapted an existing SA method evaluation to a real-world environment. With this method, participants were queried about their SA on three levels: perception, comprehension and projection about different items. We found that the use of AR decreased user's SA about secondary elements, and that this degradation mainly occurs at the perception level: with AR, participants are less likely to detect secondary elements. Participants still felt the most secure with AR and virtual cues about secondary elements.},
  keywords={Hazardous areas;Solid modeling;Three-dimensional displays;Limiting;Head;Navigation;Resists;Augmented Reality;Situation Awareness;Head Mounted Display;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VR55154.2023.00069},
  ISSN={2642-5254},
  month={March},}

@inproceedings{10.1145/3613904.3642646,
author = {McGee, Fintan and McCall, Roderick and Baixauli, Joan},
title = {Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642646},
doi = {10.1145/3613904.3642646},
abstract = {Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device’s GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide recommendations.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {188},
numpages = {15},
keywords = {Augmented Reality, CBRN Response Training, Spatial Awareness, Visualization},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3491102.3517665,
author = {Qian, Xun and He, Fengming and Hu, Xiyun and Wang, Tianyi and Ipsita, Ananya and Ramani, Karthik},
title = {ScalAR: Authoring Semantically Adaptive Augmented Reality Experiences in Virtual Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517665},
doi = {10.1145/3491102.3517665},
abstract = {Augmented Reality (AR) experiences tightly associate virtual contents with environmental entities. However, the dissimilarity of different environments limits the adaptive AR content behaviors under large-scale deployment. We propose ScalAR, an integrated workflow enabling designers to author semantically adaptive AR experiences in Virtual Reality (VR). First, potential AR consumers collect local scenes with a semantic understanding technique. ScalAR then synthesizes numerous similar scenes. In VR, a designer authors the AR contents’ semantic associations and validates the design while being immersed in the provided scenes. We adopt a decision-tree-based algorithm to fit the designer’s demonstrations as a semantic adaptation model to deploy the authored AR experience in a physical scene. We further showcase two application scenarios authored by ScalAR and conduct a two-session user study where the quantitative results prove the accuracy of the AR content rendering and the qualitative results show the usability of ScalAR.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {65},
numpages = {18},
keywords = {Adaptation, Augmented Reality, Immersive Authoring, Semantic Understanding, Virtual Reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3544548.3580714,
author = {Maria, Sophie and Mentis, Helena M. and Canlorbe, Geoffroy and Avellino, Ignacio},
title = {Supporting Collaborative Discussions In Surgical Teleconsulting Through Augmented Reality Head Mounted Displays},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580714},
doi = {10.1145/3544548.3580714},
abstract = {Although Augmented Reality (AR) has been touted as the future of surgery, its contribution to distributed collaboration such as in surgical teleconsulting has not been articulated. We propose AR-Head Mounted Displays (AR-HMD) to tackle two previously-identified challenges: operating surgeons needing to view and interact with imaging systems that reside away from the operative field, and, their lack of gesturing tools to point and annotate on the shared images and physical environment. We report on a controlled lab experiment where 12 expert gynecology surgeons perform a tumor localisation task guided by a remote radiologist (confederate) via an AR-HMD. We find that bringing the shared images to the place of work reduces the need for clarifications and provides opportunistic access to information when required, and, that pointing and annotating provides opportunities to further support verbal instruction in deictic communication. Our results inform the design of intraoperative AR-HMD systems for surgical telecollaboration.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {346},
numpages = {13},
keywords = {augmented reality, remote collaboration, teleconsulting},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3313831.3376613,
author = {Cordeil, Maxime and Bach, Benjamin and Cunningham, Andrew and Montoya, Bastian and Smith, Ross T. and Thomas, Bruce H. and Dwyer, Tim},
title = {Embodied Axes: Tangible, Actuated Interaction for 3D Augmented Reality Data Spaces},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376613},
doi = {10.1145/3313831.3376613},
abstract = {We present Embodied Axes, a controller which supports selection operations for 3D imagery and data visualisations in Augmented Reality. The device is an embodied representation of a 3D data space -- each of its three orthogonal arms corresponds to a data axis or domain specific frame of reference. Each axis is composed of a pair of tangible, actuated range sliders for precise data selection, and rotary encoding knobs for additional parameter tuning or menu navigation. The motor actuated sliders support alignment to positions of significant values within the data, or coordination with other input: e.g., mid-air gestures in the data space, touch gestures on the surface below the data, or another Embodied Axes device supporting multi-user scenarios. We conducted expert enquiries in medical imaging which provided formative feedback on domain tasks and refinements to the design. Additionally, a controlled user study was performed and found that the Embodied Axes was overall more accurate than conventional tracked controllers for selection tasks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {3d visualisation, actuation, augmented reality, device, tangible interaction},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3613904.3642049,
author = {Zhu, Qian and Wang, Zhuo and Zeng, Wei and Tong, Wai and Lin, Weiyue and Ma, Xiaojuan},
title = {Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642049},
doi = {10.1145/3613904.3642049},
abstract = {Situated visualization blends data into the real world to fulfill individuals’ contextual information needs. However, interacting with situated visualization in public environments faces challenges posed by users’ acceptance and contextual constraints. To explore appropriate interaction design, we first conduct a formative study to identify users’ needs for data and interaction. Informed by the findings, we summarize appropriate interaction modalities with eye-based, hand-based and spatially-aware object interaction for situated visualization in public environments. Then, through an iterative design process with six users, we explore and implement interactive techniques for activating and analyzing with situated visualization. To assess the effectiveness and acceptance of these interactions, we integrate them into an AR prototype and conduct a within-subjects study in public scenarios using conventional hand-only interactions as the baseline. The results show that participants preferred our prototype over the baseline, attributing their preference to the interactions being more acceptable, flexible, and practical in public.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {196},
numpages = {21},
keywords = {Interactive Techniques, Situated Visualization, Social Acceptability},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

