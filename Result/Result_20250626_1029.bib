Search Topic {Research exploring how to provide subtitle support for foreign language translation scenarios.}

@inproceedings{10.1145/3313831.3376261,
author = {Liebling, Daniel J. and Lahav, Michal and Evans, Abigail and Donsbach, Aaron and Holbrook, Jess and Smus, Boris and Boran, Lindsey},
title = {Unmet Needs and Opportunities for Mobile Translation AI},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376261},
doi = {10.1145/3313831.3376261},
abstract = {Translation apps and devices are often presented in the context of providing assistance while traveling abroad. However, the spectrum of needs for cross-language communication is much wider. To investigate these needs, we conducted three studies with populations spanning socioeconomic status and geographic regions: (1) United States-based travelers, (2) migrant workers in India, and (3) immigrant populations in the United States. We compare frequent travelers' perception and actual translation needs with those of the two migrant communities. The latter two, with low language proficiency, have the greatest translation needs to navigate their daily lives. However, current mobile translation apps do not meet these needs. Our findings provide new insights on the usage practices and limitations of mobile translation tools. Finally, we propose design implications to help apps better serve these unmet needs.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {emerging markets, immigrants, machine translation, migrants, mobile, speech},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376266,
author = {Kurzhals, Kuno and G\"{o}bel, Fabian and Angerbauer, Katrin and Sedlmair, Michael and Raubal, Martin},
title = {A View on the Viewer: Gaze-Adaptive Captions for Videos},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376266},
doi = {10.1145/3313831.3376266},
abstract = {Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {eye tracking, gaze input, gaze-responsive display, multimedia, subtitles, video captions},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/2858036.2858230,
author = {Brown, Deana and Grinter, Rebecca E.},
title = {Designing for Transient Use: A Human-in-the-loop Translation Platform for Refugees},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858230},
doi = {10.1145/2858036.2858230},
abstract = {Refugees undergoing resettlement in a new country post exile and migration face disruptive life changes. They rely on a network of individuals in the host country to help them rebuild their lives and livelihoods. We investigated whether technology could contribute to minimizing the vulnerabilities resettling refugees face. We designed Rivrtran, a messaging platform that provides 'human-in-the-loop' interpretation between individuals who don't share a common language. We report the findings from the deployment of Rivrtran to mediate communication between resettling refugee families in the United States and the American families they are paired with who serve as their mentors. Our findings suggest that scaffolding communication in such a way provides refugees one means of accessing diversified help outside their cultural group. Moreover human-in-the-loop interpretation may help to mitigate the effects of cultural barriers between those communicating. We establish the notion of designing for transient use in the development of systems to scaffold communication for short-term use by resettling refugees.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {321–330},
numpages = {10},
keywords = {Burmese, ICTD, IVR, computer-mediated communication, interactive voice response, migrants, refugees, translation},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3544548.3581129,
author = {Angelucci, Margherita and Marshall, Harrison and Tebourbi, Meriem and Seguin, Joshua Paolo and Varghese, Delvin and Olivier, Patrick and Bartindale, Tom},
title = {Action Translate: Supporting Students in Translation Volunteering},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581129},
doi = {10.1145/3544548.3581129},
abstract = {University students are well known for volunteering within non-governmental organisations (NGOs). A significant part of NGO practice is the production of documents that communicate their work to local communities and international stakeholders. However, organisations often struggle to resource translations of these documents, resulting in the exclusion of the very same communities that they want to reach. Although many students are multilingual and are willing to volunteer their time and language skills, there are few structured opportunities configured for such non-professional translation of content in the short-term mode that would fit into the student pattern of availability. We developed Action Translate to specifically support these motivated, non-professional translators within the volunteering constraints of university life. Action Translate leverages machine translation post-editing to support teams of volunteers working on NGO translation projects online. Through analysis of a real-world deployment, we discuss how digital systems can be developed to better support student volunteer translators, specifically in building collegiate interaction and identity as translators for a cause.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {224},
numpages = {14},
keywords = {machine translation, post-editing, translation, volunteering},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3173574.3173791,
author = {Lim, Hajin and Cosley, Dan and Fussell, Susan R.},
title = {Beyond Translation: Design and Evaluation of an Emotional and Contextual Knowledge Interface for Foreign Language Social Media Posts},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173791},
doi = {10.1145/3173574.3173791},
abstract = {Although many social media sites now provide machine translation (MT) for foreign language posts, translation of a post may not suffice to support understanding of, and engagement with, that post. We present SenseTrans, a tool that provides emotional and contextual annotations generated by natural language analysis in addition to machine translation. We evaluated SenseTrans in a laboratory experiment in which native English speakers browsed five Facebook profiles in foreign languages. One group used the SenseTrans interface while the other group used MT alone. Participants using SenseTrans reported significantly greater understanding of the posts, and greater willingness to engage with the posts. However, no additional cognitive load was associated with using an interface that provided more information. These results provide promising support for the idea of using computational tools to annotate communication to support multilingual sense making and interaction on social media.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {social media, sense making, multilingual communication, machine translation, cross-lingual communication, ai-augmented communication, ai-assisted communication},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3025453.3025779,
author = {Zhu, Yeshuang and Wang, Yuntao and Yu, Chun and Shi, Shaoyun and Zhang, Yankai and He, Shuang and Zhao, Peijun and Ma, Xiaojuan and Shi, Yuanchun},
title = {ViVo: Video-Augmented Dictionary for Vocabulary Learning},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025779},
doi = {10.1145/3025453.3025779},
abstract = {Research on Computer-Assisted Language Learning (CALL) has shown that the use of multimedia materials such as images and videos can facilitate interpretation and memorization of new words and phrases by providing richer cues than text alone. We present ViVo, a novel video-augmented dictionary that provides an inexpensive, convenient, and scalable way to exploit huge online video resources for vocabulary learning. ViVo automatically generates short video clips from existing movies with the target word highlighted in the subtitles. In particular, we apply a word sense disambiguation algorithm to identify the appropriate movie scenes with adequate contextual information for learning. We analyze the challenges and feasibility of this approach and describe our interaction design. A user study showed that learners were able to retain nearly 30\% more new words with ViVo than with a standard bilingual dictionary days after learning. They preferred our video-augmented dictionary for its benefits in memorization and enjoyable learning experience.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5568–5579},
numpages = {12},
keywords = {dictionary, movie clips, subtitles, vocabulary learning},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3706598.3713626,
author = {Xiao, Yimin and Hancock, Cartor and Agrawal, Sweta and Mehandru, Nikita and Salehi, Niloufar and Carpuat, Marine and Gao, Ge},
title = {Sustaining Human Agency, Attending to Its Cost: An Investigation into Generative AI Design for Non-Native Speakers' Language Use},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713626},
doi = {10.1145/3706598.3713626},
abstract = {AI systems and tools today can generate human-like expressions on behalf of people. It raises the crucial question about how to sustain human agency in AI-mediated communication. We investigated this question in the context of machine translation (MT) assisted conversations. Our participants included 45 dyads. Each dyad consisted of one new immigrant in the United States, who leveraged MT for English information seeking as a non-native speaker, and one local native speaker, who acted as the information provider. Non-native speakers could influence the English production of their message in one of three ways: labeling the quality of MT outputs, regular post-editing without additional hints, or augmented post-editing with LLM-generated hints. Our data revealed a greater exercise of non-native speakers’ agency under the two post-editing conditions. This benefit, however, came at a significant cost to the dyadic-level communication performance. We derived insights for MT and other generative AI design from our findings.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {345},
numpages = {16},
keywords = {Agency, Machine translation, AI-mediated communication, Non-native speakers},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713745,
author = {Chen, Tuochao and Wang, Qirui and He, Runlin and Gollakota, Shyamnath},
title = {Spatial Speech Translation: Translating Across Space With Binaural Hearables},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713745},
doi = {10.1145/3706598.3713745},
abstract = {Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer’s environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of upto 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system’s effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {352},
numpages = {19},
keywords = {Speech translation, spatial computing, augmented audio},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713622,
author = {Desai, Aashaka and Alharbi, Rahaf and Hsueh, Stacy and Ladner, Richard E. and Mankoff, Jennifer},
title = {Toward Language Justice: Exploring Multilingual Captioning for Accessibility},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713622},
doi = {10.1145/3706598.3713622},
abstract = {A growing body of research investigates how to make captioning experiences more accessible and enjoyable to disabled people. However, prior work has focused largely on English captioning, neglecting the majority of people who are multilingual (i.e., understand or express themselves in more than one language). To address this gap, we conducted semi-structured interviews and diary logs with 13 participants who used multilingual captions for accessibility. Our findings highlight the linguistic and cultural dimensions of captioning, detailing how language features (scripts and orthography) and the inclusion/negation of cultural context shape the accessibility of captions. Despite lack of quality and availability, participants emphasized the importance of multilingual captioning to learn a new language, build community, and preserve cultural heritage. Moving toward a future where all ways of communicating are celebrated, we present ways to orient captioning research to a language justice agenda that decenters English and engages with varied levels of fluency.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {218},
numpages = {18},
keywords = {Captioning, Multilingualism, Language Justice},
location = {
},
series = {CHI '25}
}

